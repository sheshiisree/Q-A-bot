




Network Security
‚Ä¢ Definition: The protection afforded to an automated information 
system in order to attain the application objectives to preserving the 
integrity, availability, and confidentiality of information system 
resources (includes hardware, software, firmware, information/data, 
and telecommunications).  
                                     - NIST Computer Security Handbook

Confidentiality
‚Ä¢ Data confidentiality: Assures that private or confidential information 
is not made available or disclosed to unauthorized individuals; 
‚Ä¢ Privacy: Assures that individual's control or influence what 
information related to them may be collected and stored and by 
whom and to whom that information may be disclosed
‚Ä¢ i.e., student grade information

Integrity
‚Ä¢ Data integrity: Assures that data (both stored and in transmitted 
packets) and programs are changed only in a specified and authorized 
manner;
‚Ä¢ System integrity: Assures that a system performs its intended 
function in an unimpaired manner, free from deliberate or 
inadvertent unauthorized manipulation of the system
‚Ä¢ i.e., a hospital patient‚Äôs allergy information

Availability
‚Ä¢ Availability: Assures that systems work promptly, and service is not 
denied to authorized users, ensuring timely and reliable access to and 
use of information
‚Ä¢ i.e., denial of service attack

Other security requirements
‚Ä¢ Authenticity
‚Ä¢ Accountability 
‚Ä¢ tracible data source, 
‚Ä¢ fault isolation
‚Ä¢ intrusion detection and prevention, 
‚Ä¢ recovery and legal action
‚Ä¢ system must keep records of their activities to permit later forensic analysis to 
trace security breaches or to aid in transaction disputes

Question
‚Ä¢ What security requirements does a blockchain system have achieved?

Project 
‚Ä¢ Task1: OnDemand Professor Q&A Bot 
‚Ä¢
Your task is to build a Q&A Bot over private data that answers questions about the network security course 
using the open-source alternatives to ChatGPT that can be run on your local machine. Data privacy can be 
compromised when sending data over the internet, so it is mandatory to keep it on your local system. 
‚Ä¢
Your Q&A Bot should be able to understand user questions and provide appropriate answers from the local 
database, then the citations should be added (must be accomplished) if the response is from the internet, 
then the web references should be added. 
‚Ä¢
Train your bot using network security lecture slides, network security textbook, and the Internet. 
‚Ä¢
By using Wireshark capture data for Step 4's of the LLM workflow shown in Figure 1. Provide detailed explanations of the trace data. Also, 
Maintain a record of Step 1's prompt and its mapping to the trace data in Step 4's. 
‚Ä¢ Task2: Quiz Bot 
‚Ä¢
Your task is to build a quiz bot based on a network security course using the open-source alternatives to 
ChatGPT that can be run on your local machine. Data privacy can be compromised when sending data over the 
internet, so it is mandatory to keep it on your local system. 
‚Ä¢
Two types of questions should be offered by the bot: randomly generated questions and specific topic questions 
and the answers should be pulled from the network security database. Train your bot using network security 
quizzes, lecture slides, network security textbook, and the Internet. 
‚Ä¢
The quiz must include multiple-choice questions, true/false questions, and open-ended questions. 
‚Ä¢
Finally, the bot should be able to provide feedback on the user's answers.



Form Project Groups
‚Ä¢ Form a group (no submission) 
‚Ä¢ The project will be assigned as a group project. Six students will be 
considered a group for a project. Here is the link to enter your project group 
member names. 
                    FALL 2023 CS5342 PROJECT GROUP NAMES.xlsx 
‚Ä¢ It is recommended that one person in the group fills in the form to avoid 
multiple entries and submits project files on Blackboard. If you cannot get a 
group, contact to TA. 
‚Ä¢ Deadline to submit your group members is 11:59PM on Sept 8th, 
2023

Email communication
‚Ä¢ Email subject: course# _course name_reason
‚Ä¢ such as ‚ÄúCS 5342_network security_late submission‚Äù

Lecture 2

Outline
‚Ä¢ Review
‚Ä¢ OSI Security Architecture
‚Ä¢ Attack model

Review
‚Ä¢ Security requirements 
‚Ä¢ Integrity
‚Ä¢ Availability
‚Ä¢ Confidentiality
‚Ä¢ Authenticity
‚Ä¢ Accountability

Challenges to achieve a secure 
system
‚Ä¢ The mechanisms used to meet those requirements can be quite complex, 
and understanding them may evolve rather subtle reasoning
‚Ä¢  When developing security mechanisms, must always consider potential 
attacks
‚Ä¢ Sometimes, security mechanisms are counterintuitive
‚Ä¢ Where to use them?
‚Ä¢ Involve more than a particular algorithm or protocol
‚Ä¢ No agreement on security for complex and heterogeneous systems i.e. 
trusts on data in different countries
‚Ä¢ etc.

OSI Security Architecture

OSI Security Architecture
‚Ä¢ International Telecommunication Union ‚Äì Telecommunication (ITU-T) 
recommends X.800
‚Ä¢ Security Architecture for Open Systems Interconnection (OSI)
‚Ä¢ Defines a systematic way of defining and providing security requirements
‚Ä¢ Used by IT managers and vendors in their products
Security attacks
Security mechanisms
Security services
a process (or a device incorporating such a process) 
to detect, prevent, or recover from an attack
enhances the security of the data processing 
systems and the information transfers, such as 
policies

Other Security Architectures
‚Ä¢ OWASP -  Open Web Application Security Project
‚Ä¢ web application security
‚Ä¢ OWASP foundation
‚Ä¢ NIST, Cybersecurity Framework
‚Ä¢  https://www.nist.gov/cyberframework
‚Ä¢ VIRTUAL WORKSHOP #2 | February 15, 2023 (9:00 AM ‚Äì 5:30 PM EST). Join us 
to discuss potential significant updates to the CSF as outlined in the soon-to-
be-released CSF Concept Paper. 
‚Ä¢ https://www.nist.gov/news-events/events/2023/02/journey-nist-cybersecurit
y-framework-csf-20-workshop-2

Security attack
‚Ä¢ Definition: any action that compromises the security of information 
owned by an organization
‚Ä¢ Two types of security attacks
‚Ä¢ Passive attack
‚Ä¢ Active attack

Passive attack
‚Ä¢ i.e. eavesdropping on or monitoring of transmissions
‚Ä¢ Goal: obtain information being transmitted
‚Ä¢ release of message contents
‚Ä¢ traffic analysis ‚Äì a promiscuous sniffer
‚Ä¢ Very difficult to detect ‚Äì no alteration of the data
‚Ä¢ But easy to prevent, why?

Active attack
‚Ä¢ active attack includes:
‚Ä¢ replay
‚Ä¢ Modification of messages
‚Ä¢ Denial of service
‚Ä¢ Masquerade

Example: two points communication 
‚Ä¢ Generic types of attacks

Example of modification attack in 
6LoWPAN

Example: a group of attackers

Know Your Threat Model
‚Ä¢ Threat model: A model of who your attacker is and what resources 
they have
‚Ä¢ One of the best ways to counter an attacker is to attack their reasons

Example: adversary model
‚Ä¢ ‚ÄúThe adversary is assumed to be intelligent and has 
limited number of resources. Before capturing the 
nodes, it exploits the various vulnerabilities of the 
networks. It knows the topology of the network, routing 
information. It aims to capture the sink node so as to 
disrupt the whole traffic. If it is not able to capture the 
sink node, it will capture the nearby nodes of the sink. It 
tries to disrupt the whole traffic of the network with 
minimum number of captured nodes. It is also assumed 
that the adversary tends to attack more on the nodes 
closer to the data sink than nodes that are far away‚Äù

Lecture 3

Story‚Ä¶
‚Ä¢ The bear race
‚Ä¢ Takeaway: Even if a defense is not 
perfect, it is important to always 
stay on top of best security 
measures
I don‚Äôt have to outrun the bear. I just have to outrun you

Design in security from the start
‚Ä¢ When building a new system, include security as part of the design 
considerations rather than patching it after the fact
‚Ä¢ A lot of systems today were not designed with security from the start, 
resulting in patches that don‚Äôt fully fix the problem!
‚Ä¢ Keep these security principles in mind whenever you write code!

Human Factors
‚Ä¢ The users
‚Ä¢ Users like convenience (ease of use)
‚Ä¢ If a security system is unusable, it will be unused
‚Ä¢ Users will find way to subvert security systems if it makes their lives easier
‚Ä¢ The programmers
‚Ä¢ Programmers make mistakes
‚Ä¢ Programmers use tools that allow them to make mistakes (e.g. C and C++)
‚Ä¢ Everyone else
‚Ä¢ Social engineering attacks exploit other people‚Äôs trust and access for personal 
gain

Summary for Chapter 1
‚Ä¢ Have learned:
‚Ä¢ Security requirements
‚Ä¢ Attack models
‚Ä¢ X.800 secure architecture, security services, mechanisms

Supplementary materials
‚Ä¢ Internet Security Glossary, v2 ‚Äì produced by Internet Society 
https://datatracker.ietf.org/doc/html/rfc4949
‚Ä¢ X.800 ‚Äì OSI network security
   
https://www.itu.int/rec/dologin_pub.asp?lang=f&id=T-REC-X.800-1991
03-I!!PDF-E&type=items

Review Questions
‚Ä¢ William Stallings (WS), ‚ÄúNetwork Security 
Essentials‚Äù, 6th Global Edition
‚Ä¢ RQ 1.1 - 1.3
‚Ä¢ Prob 1.5

Network Security
Chapter 2

Symmetric encryption
‚Ä¢ Sender and recipient share a common/same key
‚Ä¢ Was the only type of cryptography, prior to invention of public-key in 
1970‚Äôs

Symmetric Encryption Principles

Simplified model of symmetric encryption

Symmetric encryption
‚Ä¢ Has five ingredients
‚Ä¢ Plaintext:  the original message or data
‚Ä¢ Encryption algorithm: performs various substitutions and transformations on 
the plaintext
‚Ä¢ Secret key
‚Ä¢ Ciphertext: the coded message
‚Ä¢ Decryption algorithm: takes the ciphertext and the same secret key and 
produces the original plaintext

Other basic terminology
‚Ä¢ cipher - algorithm for transforming plaintext to ciphertext 
‚Ä¢ encipher (encrypt) - converting plaintext to ciphertext
‚Ä¢ decipher (decrypt) - recovering plaintext from ciphertext
‚Ä¢ cryptography - study of encryption principles/methods
‚Ä¢ cryptanalysis (codebreaking) - the study of principles/ methods of 
deciphering ciphertext without knowing key

Requirements
‚Ä¢ Two requirements for secure use of symmetric encryption:
‚Ä¢ a strong encryption algorithm
‚Ä¢ a secret key known only to sender / receiver
Y = EK(X)
X = DK(Y)
‚Ä¢ assume encryption algorithm is known
‚Ä¢ the security of symmetric encryption depends on the secrecy of the 
key
‚Ä¢ implies a secure channel to distribute key

Lecture 4

Requirements
‚Ä¢ Two requirements for secure use of symmetric encryption:
‚Ä¢ a strong encryption algorithm
‚Ä¢ a secret key known only to sender / receiver
Y = EK(X)
X = DK(Y)
‚Ä¢ assume encryption algorithm is known
‚Ä¢ the security of symmetric encryption depends on the secrecy of the 
key
‚Ä¢ implies a secure channel to distribute key

A strong encryption algorithm
attacker
encryption algorithm
plaintext / enquiry
cyphertext / response

Secure Encryption Scheme
‚Ä¢ Unconditional security
‚Ä¢ no matter how much computer power is available, the cipher cannot be 
broken since the ciphertext provides insufficient information to uniquely 
determine the corresponding plaintext
‚Ä¢ Computational security
‚Ä¢ the cost of breaking the cipher exceeds the value of the encrypted 
information;
‚Ä¢ or the time required to break the cipher exceeds the useful lifetime of the 
information

Desired characteristics
‚Ä¢ Cipher needs to completely obscure statistical properties of original 
message
‚Ä¢ more practically Shannon suggested combining elements to obtain:
‚Ä¢ Confusion ‚Äì how does changing a bit of the key affect the ciphertext?
‚Ä¢ Diffusion ‚Äì how does changing one bit of the plaintext affect the ciphertext?
confusion
ciphertext
plaintext
diffusion

Ways to achieve
‚Ä¢ Symmetric Encryption: 
‚Ä¢ substitution / transposition / hybrid
‚Ä¢ Asymmetric Encryption: 
‚Ä¢ Mathematical hardness - problems that are efficient to compute in one 
direction, but inefficient to reverse by the attacker
‚Ä¢ Examples: Modular arithmetic, factoring, discrete logarithm problem, Elliptic Logs over 
Elliptic Curves

Project
‚Ä¢ TA Name: Lin, Yu
‚Ä¢ Email:  Yu.Lin@ttu.edu
‚Ä¢ Form a group (no submission)
‚Ä¢The project will be assigned as a group project. Six students will 
be considered a group for a project. Here is the link to enter your project 
group member names.
                 FALL 2023 CS5342 PROJECT GROUP NAMES.xlsx
‚Ä¢It is recommended that one person in the group fills in the form 
to avoid multiple entries and submits project files on Blackboard. If you 
cannot get a group, contact the TA.
‚Ä¢ Deadline to submit your group members is 11:59 PM on Sept 8th, 2023

Lecture 5

Review
‚Ä¢ Strong Encryption Algorithm
‚Ä¢ Confusion
‚Ä¢ Diffusion

Symmetric Block Encryption

Block cipher
‚Ä¢ the most commonly used symmetric encryption algorithms
‚Ä¢ input: fixed-size blocks (Typically 64, 128 bit blocks), output: equal 
size blocks
‚Ä¢ provide secrecy and/or authentication services
‚Ä¢ Data Encryption Standard (DES), triple DES (3DES), and the Advanced 
Encryption Standard (AES)s
‚Ä¢ Usually employ Feistel structure

Feistel Cipher Structure

Feistel Cipher Structure
‚Ä¢ most symmetric block ciphers are based on a Feistel Cipher Structure
‚Ä¢ based on the two primitive cryptographic operations
‚Ä¢ substitution (S-box)
‚Ä¢ permutation (P-box)
‚Ä¢ provide confusion and diffusion of message

Feistel Cipher Structure
‚Ä¢ Horst Feistel devised the feistel cipher in the 1973
‚Ä¢ based on concept of invertible product cipher
‚Ä¢ partitions input block into two halves
‚Ä¢ process through multiple rounds which
‚Ä¢ perform a substitution on left data half
‚Ä¢ based on round function of right half & subkey
‚Ä¢ then have permutation swapping halves
‚Ä¢ implements Shannon‚Äôs substitution-permutation network concept

Feistel Encryption and Decryption

DES encryption
‚Ä¢
64 bits plaintext
‚Ä¢
56 bits effective key length

Lecture 6

Outline
‚Ä¢ DES
‚Ä¢ 3DES
‚Ä¢ AES

DES encryption
‚Ä¢
64 bits plaintext
‚Ä¢
56 bits effective key length

DES Weakness
‚Ä¢ short length key (56 bits) is not secure enough. Brutal force search 
takes short time.

Triple DES (3DES)
Decrypting with the wrong key will further convolute the output

3DES
‚Ä¢ Triple DES with three different keys ‚Äì brute-force complexity 2168
‚Ä¢ 3DES is the FIPS-approved symmetric encryption algorithm
‚Ä¢ Weakness: slow speed for encryption
FIPS ‚Äì Federal Information Processing Standards. The United States' Federal Information Processing Standards are publicly announced 
standards developed by the National Institute of Standards and Technology for use in computer systems by non-military American government 
agencies and government contractors

AES
‚Ä¢ clearly a replacement for DES was needed
‚Ä¢ have theoretical attacks that can break it
‚Ä¢ have demonstrated exhaustive key search attacks
‚Ä¢ can use Triple-DES ‚Äì but slow with small blocks
‚Ä¢ US NIST issued call for ciphers in 1997
‚Ä¢ 15 candidates accepted in Jun 98 
‚Ä¢ 5 were short-listed in Aug-99 
‚Ä¢ Rijndael was selected as the AES in Oct-2000
‚Ä¢ issued as FIPS PUB 197 standard in Nov-2001

Criteria to evaluate AES
‚Ä¢ General security
‚Ä¢ Software implementations
‚Ä¢ Restricted-space environments
‚Ä¢ Hardware implementations
‚Ä¢ Attacks on implementations
‚Ä¢ Encryption versus decryption
‚Ä¢ Key agility
‚Ä¢ Other versatility and flexibility
‚Ä¢ Potential for instruction-level parallelism
Cryptographic Standards and Guidelines | CSRC (nist.gov)

AES Specification
‚Ä¢ symmetric block cipher 
‚Ä¢ 128-bit data, 128/192/256-bit keys 
‚Ä¢ stronger & faster than Triple-DES 
‚Ä¢ provide full specification & design details 
‚Ä¢ both C & Java implementations
‚Ä¢ NIST have released all submissions & unclassified analyses
https://csrc.nist.gov/CSRC/media/Projects/Cryptographic-Standards-an
d-Guidelines/documents/aes-development/Rijndael-ammended.pdf

The AES Cipher - Rijndael 
‚Ä¢ designed by Rijmen-Daemen in Belgium 
‚Ä¢ has 128/192/256 bit keys
‚Ä¢ an iterative rather than feistel cipher
‚Ä¢ treats data in 4 groups of 4 bytes
‚Ä¢ operates an entire block in every round
‚Ä¢ designed to be:
‚Ä¢ resistant against known-plaintext attacks
‚Ä¢ speed and code compactness on many CPUs
‚Ä¢ design simplicity

Rijndael
‚Ä¢ processes data as 4 groups of 4 bytes (state) = 128 bits
‚Ä¢ has 10/12/14 rounds in which state undergoes: 
‚Ä¢ byte substitution (1 S-box used on every byte) 
‚Ä¢ shift rows (permute bytes row by row) 
‚Ä¢ mix columns (alter each byte in a column as a function of all of the bytes in 
the column) 
‚Ä¢ add round key (XOR state with key material) 
‚Ä¢ 128-bit keys ‚Äì 10 rounds, 192-bit keys ‚Äì 12 rounds, 256-bit keys ‚Äì 14 
rounds

Project
‚Ä¢ Task 1: G1 to G5
‚Ä¢ Task 2: G6 to G10
‚Ä¢ We will create submission portal on BB. 
‚Ä¢ The deadline for the first round of submissions is 10/13/2023 at 11:59 
PM.

Lecture 7

Outline
‚Ä¢ AES
‚Ä¢ Random number

Rijndael
‚Ä¢ processes data as 4 groups of 4 bytes (state) = 128 bits
‚Ä¢ has 10/12/14 rounds in which state undergoes: 
‚Ä¢ byte substitution (1 S-box used on every byte) 
‚Ä¢ shift rows (permute bytes row by row) 
‚Ä¢ mix columns (alter each byte in a column as a function of all of the bytes in 
the column) 
‚Ä¢ add round key (XOR state with key material) 
‚Ä¢ 128-bit keys ‚Äì 10 rounds, 192-bit keys ‚Äì 12 rounds, 256-bit keys ‚Äì 14 
rounds

AES Encryption and Decryption

AES encryption round

AES pros
‚Ä¢ Most operations can be combined into XOR and table lookups - hence 
very fast & efficient

Take-home Exercises
‚Ä¢ Find an AES code to encrypt a text (A), then decrypt it and check 
whether the original text (A) equals the decrypted text (B). Whether A 
= B?
‚Ä¢ Compare the decryption time with different key lengths, and with DES 
and 3DES. 
‚Ä¢ Suggestions: find a large A file. Run decryption a couple of times and take the 
average.

Reading materials
‚Ä¢ FIPS 197, Advanced Encryption Standard (AES) (nist.gov)

Lecture 8

Random and Pseudorandom Numbers

When to use random numbers?
‚Ä¢ Generation of a stream key for symmetric stream cipher 
‚Ä¢ Generation of keys for public-key algorithms
‚Ä¢  RSA public-key encryption algorithm (described in Chapter 3)
‚Ä¢ Generation of a symmetric key for use as a temporary session key
‚Ä¢ used in a number of networking applications, such as Transport Layer Security 
(Chapter 5), Wi-Fi (Chapter 6), e-mail security (Chapter 7), and IP security 
(Chapter 8) 
‚Ä¢ In a number of key distribution scenarios
‚Ä¢ Kerberos (Chapter 4)

Two types of random numbers
‚Ä¢ True random numbers:
‚Ä¢ generated in non-deterministic ways. They are not predictable and repeatable
‚Ä¢ Pseudorandom numbers:
‚Ä¢ appear random, but are obtained in a deterministic, repeatable, and 
predictable manner

Properties of Random Numbers
‚Ä¢ Randomness
‚Ä¢ Uniformity
‚Ä¢ distribution of bits in the sequence should be uniform 
‚Ä¢ Independence
‚Ä¢ no one subsequence in the sequence can be inferred from the others 
‚Ä¢ Unpredictable
‚Ä¢ satisfies the "next-bit test‚Äú

Entropy
‚Ä¢ A measure of uncertainty
‚Ä¢ In other words, a measure of how unpredictable the outcomes are
‚Ä¢ High entropy = unpredictable outcomes = desirable in cryptography
‚Ä¢ The uniform distribution has the highest entropy (every outcome equally 
likely, e.g. fair coin toss)
‚Ä¢ Usually measured in bits (so 3 bits of entropy = uniform, random distribution 
over 8 values)
Entropy of an information source

Lecture 9

True random numbers generators
‚Ä¢ Several sources of randomness ‚Äì natural sources of randomness
‚Ä¢ decay times of radioactive materials
‚Ä¢ electrical noise from a resistor or semiconductor
‚Ä¢ radio channel or audible noise
‚Ä¢ keyboard timings
‚Ä¢ disk electrical activity
‚Ä¢ mouse movements
‚Ä¢ Physical unclonable function (PUF)
‚Ä¢ Some are better than others

Combining sources of randomness
‚Ä¢ Suppose r1, r2, ‚Ä¶, rk are random numbers from different sources. 
E.g.,
   r1 = electrical noise from a resistor or semiconductor
   r2 = sample of hip-hop music on radio
   r3 = clock on computer
   b = r1r2‚Ä¶rk
   If any one of r1, r2, ‚Ä¶, rk is truly random, then so is b
   Many poor sources + 1 good source = good entropy

Pseudorandom Number Generators 
(PRNGs)
‚Ä¢ True randomness is expensive
‚Ä¢ Pseudorandom number generator (PRNGs): An algorithm that uses a 
little bit of true randomness to generate a lot of random-looking 
output 
‚Ä¢ Also called deterministic random bit generators (DRBGs)
‚Ä¢ PRNGs are deterministic: Output is generated according to a set 
algorithm
‚Ä¢ However, for an attacker who can‚Äôt see the internal state, the output is 
computationally indistinguishable from true randomness

PRNG: Definition
‚Ä¢ A PRNG has two functions:
‚Ä¢ PRNG.Seed(randomness): Initializes the internal state using the entropy
‚Ä¢ Input: Some truly random bits
‚Ä¢ PRNG.Generate(n): Generate n pseudorandom bits
‚Ä¢ Input: A number n
‚Ä¢ Output: n pseudorandom bits
‚Ä¢ Updates the internal state as needed
‚Ä¢ Properties
‚Ä¢ Correctness: Deterministic
‚Ä¢ Efficiency: Efficient to generate pseudorandom bits
‚Ä¢ Security: Indistinguishability from random
‚Ä¢ Rollback resistance: cannot deduce anything about any previously-generated bit

Example construction of PRNG
‚Ä¢ Using block cipher in Counter (CTR) mode:
‚Ä¢ If you want m random bits, and a block cipher with Ek has n bits, apply the 
block cipher m/n times and concatenate the result:
‚Ä¢ PRNG.Seed(K | IV) = Ek(IV, 1) | Ek(IV, 2) | Ek(IV, 3) ‚Ä¶ Ek(IV, ceil(m/n)),   
‚Ä¢ | is concatenation
‚Ä¢ Initialization vector (IV) / Nonce ‚Äì typically is random or pseudorandom
Randomness, 
PRNG output

PRNG: Security
‚Ä¢ Can we design a PRNG that is truly random?
‚Ä¢ A PRNG cannot be truly random
‚Ä¢ The output is deterministic given the initial seed
‚Ä¢ A secure PRNG is computationally indistinguishable from random to 
an attacker
‚Ä¢ Game: Present an attacker with a truly random sequence and a sequence 
outputted from a secure PRNG
‚Ä¢ An attacker should be able to determine which is which with probability  0
‚Ä¢ Equivalence: An attacker cannot predict future output of the PRNG

Create pseudorandom numbers
‚Ä¢ Truly random numbers are impossible with any program!
‚Ä¢ However, we can generate seemingly random numbers, called 
pseudorandom numbers
‚Ä¢ The function rand() returns a non-negative number between 0 and 
RAND_MAX
‚Ä¢ For C, it is defined in stdlib.h

PRNGs: Summary
‚Ä¢ True randomness requires sampling a physical process
‚Ä¢ PRNG: An algorithm that uses a little bit of true randomness to 
generate a lot of random-looking output
‚Ä¢ Seed(entropy): Initialize internal state
‚Ä¢ Generate(n): Generate n bits of pseudorandom output
‚Ä¢ Security: computationally indistinguishable from truly random bits

Lecture 10

Stream Ciphers

Stream Ciphers
‚Ä¢ process the message bit by bit (as a stream) 
‚Ä¢ typically have a (pseudo) random stream key 
‚Ä¢ combined (XOR) with plaintext bit by bit 
‚Ä¢ randomness of stream key completely destroys any statistically 
properties in the message 
‚Ä¢ Ci = Mi XOR StreamKeyi 
‚Ä¢ what could be simpler!!!! 
‚Ä¢ but must never reuse stream key
‚Ä¢ otherwise, can remove effect and recover messages, MKK = M

How to generate Stream Key?
‚Ä¢ How to generate Stream Key?

Stream Ciphers 
‚Ä¢ Idea: replace ‚Äúrand‚Äù by ‚Äúpseudo rand‚Äù
‚Ä¢ Use Pseudo Random Number Generator 
‚Ä¢ A secure PRNG produces output that looks indistinguishable from random
‚Ä¢ An attacker who can‚Äôt see the internal PRNG state can‚Äôt learn any output
‚Ä¢ PRNG: {0,1}s ÔÇÆ {0,1}n
‚Ä¢ expand a short (e.g., 128-bit) random seed into a long (typically unbounded) 
string that ‚Äúlooks random‚Äù
‚Ä¢ Secret key is the seed
‚Ä¢ Basic encryption method: Ekey[M] = M ÔÉÖ PRNG(key)

Stream Ciphers
‚Ä¢ Protocol: Alice and Bob both seed a secure PRNG with their 
symmetric secret key, and then use the output as the key for stream 
key
Alice
Bob
Seed(k)
Seed(k)
Generate(n)
Generate(n)
Plaintext
Plaintext
Ciphertext
‚äï
‚äï

Stream Ciphers: Encrypting Multiple 
Messages
‚Ä¢ How do we encrypt multiple messages without key reuses?
Alice
Bob
Seed(k)
Seed(k)
Generate(n)
Generate(n)
Plaintext
Plaintext
Ciphertext
‚äï
‚äï

Stream Ciphers: Encrypting Multiple 
Messages
‚Ä¢ Solution: For each message, seed the PRNG with the key and a 
random IV, concatenated(‚Äú|‚Äù). Send the IV with the ciphertext
Alice
Bob
Seed(k | IV)
Seed(k | IV)
Generate(n)
Generate(n)
Plaintext
Plaintext
Ciphertext
‚äï
‚äï
   IV
   IV

Real-world example: RC4
‚Ä¢ A proprietary cipher designed in 1987 
‚Ä¢ Extremely simple but effective!
‚Ä¢ Very fast - especially in software
‚Ä¢ Easily adapts to any key length, byte-oriented stream cipher 
‚Ä¢ Uses that permutation to scramble input info processed a byte at a 
time 
‚Ä¢ Widely used (web SSL/TLS, wireless WEP, WPA)

RC4 Stream Cipher
K
RC4 (K|IV)
011010010111
‚äï
M
C
key
(seed)
key stream
(pseudo random sequence)
message
ciphertext

RC4 Key Schedule
‚Ä¢ starts with an array S of 
numbers: 0‚Ä¶255
‚Ä¢ use key to well and truly 
shuffle
‚Ä¢ S forms internal state of 
the cipher
‚Ä¢ given a key k of length L 
bytes
Throw away T & K, retain S

RC4 Encryption
‚Ä¢ encryption continues 
shuffling array values
‚Ä¢ sum of shuffled pair 
selects "stream key" 
value
‚Ä¢ XOR with next byte of 
message to en/decrypt

RC4

Lecture 11

Summary ‚Äì Chapter 2
‚Ä¢ Symmetric block cipher
‚Ä¢ DES, 3DES
‚Ä¢ AES
‚Ä¢ Random number
‚Ä¢ true random number
‚Ä¢ pseudorandom number
‚Ä¢ Stream cipher 
‚Ä¢ The security of symmetric encryption depends on the secrecy of the 
key

Homework 1 - individual
‚Ä¢ For Chapter 1 & 2
‚Ä¢ Deadline: Oct. 2 (Monday), 11:59 pm
‚Ä¢ We will use the blackboard submission time as your final timestamp
‚Ä¢ 10% penalty per day for late submission

Network Security
Chapter 3
Public-Key Cryptography and Message Authentication

Public-Key Cryptography

Conventional cryptography
‚Ä¢ traditional private/secret/single-key cryptography uses one key 
‚Ä¢ shared by both sender and receiver 
‚Ä¢ if this key is disclosed communications are compromised 
‚Ä¢ also is symmetric, parties are equal

Pros and cons
‚Ä¢ Pros:
‚Ä¢ Encryption is fast for large amounts of data
‚Ä¢ Provide the same level of security with a shorter encryption key
‚Ä¢ By now, it‚Äôs unbreakable to quantum computing
‚Ä¢ Cons
‚Ä¢ Key distribution assumes a secure channel
‚Ä¢ Does not protect sender from receiver forging a message & 
claiming it‚Äôs sent by sender
‚Ä¢ It does not scale well for large networks. It requires a separate 
key for each pair of communicating parties, which can result 
in a large number of keys to manage and protect.

Public-Key Cryptography
‚Ä¢ In public-key schemes, each person has two keys
‚Ä¢ Public key: Known to everybody
‚Ä¢ Private key: Only known by that person
‚Ä¢ Keys come in pairs: every public key corresponds to one private key
‚Ä¢ Uses number theory
‚Ä¢ Examples: Modular arithmetic, factoring, discrete logarithm problem, 
Elliptic logs over Elliptic Curves
‚Ä¢ Contrast with symmetric-key cryptography (uses XORs and bit-shifts)
‚Ä¢ Messages are numbers
‚Ä¢ Contrast with symmetric-key cryptography (messages are bit strings)

Lecture 12

Public-key Cryptography
‚Ä¢ Benefit:
‚Ä¢ Drawback:
‚Ä¢ Benefit: No longer need to assume that Alice and Bob already share a 
secret
‚Ä¢ Drawback: Much slower than symmetric-key cryptography
‚Ä¢ Number theory calculations are much slower than XORs and bit-shifts

Reading materials
‚Ä¢ Encryption: Strengths and Weaknesses of Public-key Cryptography
‚Ä¢ Public-key cryptography is a public invention due to Whitfield Diffie & 
Martin Hellman at Stanford Uni in 1976

Public-key cryptography
‚Ä¢ public-key/two-key/asymmetric cryptography involves the use of 
two keys: 
‚Ä¢ a public-key, which may be known by anybody, and can be used to encrypt 
messages, and verify signatures 
‚Ä¢ a private-key, known only to the recipient, used to decrypt messages, and 
sign (create) signatures
‚Ä¢ is asymmetric because
‚Ä¢ Not the same key
‚Ä¢ those who encrypt messages or verify signatures cannot decrypt messages or 
create signatures

Public-Key Encryption
‚Ä¢ Everybody can encrypt with the public key
‚Ä¢ Only the recipient can decrypt with the private key

Public-Key Cryptography - 
Encryption

Encryption steps
‚Ä¢ step1: generate a pair of keys
‚Ä¢ step2: keep the private key / secret key (SK) and distribute the public 
key (PK) ‚Äì place PK in a public register or other accessible file
‚Ä¢ step3: Bob encrypts the message with Alice‚Äôs PK
‚Ä¢ step4: upon receiving the ciphertext (CT), Alice decrypt CT with SK

Public-Key Encryption: Definition
‚Ä¢ Three parts:
‚Ä¢ KeyGen() ‚Üí PK, SK: Generate a public/private keypair, where PK is the public 
key, and SK is the private (secret) key
‚Ä¢ Enc(PK, M) ‚Üí C: Encrypt a plaintext M using public key PK to produce 
ciphertext C
‚Ä¢ Dec(SK, C) ‚Üí M: Decrypt a ciphertext C using secret key SK
‚Ä¢ Properties
‚Ä¢ Correctness: Decrypting a ciphertext should result in the message that was 
originally encrypted
‚Ä¢ Dec(SK, Enc(PK, M)) = M for all PK, SK ‚Üê KeyGen() and M
‚Ä¢ Efficiency: Encryption/decryption should be fast
‚Ä¢ Security: 1. Alice (the challenger) just gives Eve (the adversary) the public key, 
and Eve doesn‚Äôt request encryptions. Eve cannot guess out anything; 2. 
computationally infeasible to recover M with PK and ciphertext

Public-Key Cryptography - Signature

Review
Private Key
Public Key
Signature
Encryption

Public-Key application
‚Ä¢ can classify uses into 3 categories:
‚Ä¢ encryption/decryption (provide secrecy)
‚Ä¢ digital signatures (provide authentication)
‚Ä¢ key exchange (of session keys)
‚Ä¢ some algorithms are suitable for all uses; others are specific to one
‚Ä¢ Either of the two related keys can be used for encryption, with the 
other used for decryption



Security of Public Key Schemes
‚Ä¢ Keys used are very large (>512bits) 
‚Ä¢ like private key schemes brute force exhaustive search attack is always 
theoretically possible 
‚Ä¢ Security relies on a large enough difference in difficulty between easy 
(en/decrypt) and hard (cryptanalyze) problems
‚Ä¢ more generally the hard problem is known, it‚Äôs just made too hard to do in 
practice 
‚Ä¢ Requires the use of very large numbers, hence is slow compared to 
private/symmetric key schemes

Public-Key Cryptography Algorithm
(RSA)

RSA Public-key encryption
‚Ä¢ by Rivest, Shamir & Adleman of MIT in 1977 
‚Ä¢ currently the ‚Äúwork horse‚Äù of Internet security
‚Ä¢ most public key infrastructure (PKI) products
‚Ä¢ SSL/TLS: certificates and key-exchange
‚Ä¢ secure e-mail: PGP, Outlook, ‚Ä¶.
‚Ä¢ based on exponentiation in a finite (Galois) field over integers modulo a prime 
‚Ä¢ exponentiation takes O((log n)3) operations (easy)
‚Ä¢ security due to cost of factoring large integer numbers 
‚Ä¢ factorization takes O(e log n log log n) operations (hard)
‚Ä¢ uses large integers (eg. 1024 bits)

RSA key setup
‚Ä¢ each user generates a public/private key pair by: 
‚Ä¢ selecting two large primes at random - p, q 
‚Ä¢ computing their system modulus n=pq
‚Ä¢ note √∏(n)=(p-1)(q-1) 
‚Ä¢ selecting at random the encryption key e
‚Ä¢ where 1<e<√∏(n), gcd(e,√∏(n))=1 
‚Ä¢ solve following equation to find decryption key d 
‚Ä¢ ed=1 mod √∏(n)
‚Ä¢ publish their public encryption key: pk={e,n} 
‚Ä¢ keep secret private decryption key: sk={d,p,q}

RSA example
1.
Select primes: p=17 & q=11
2.
Compute n = pq =17√ó11=187
3.
Compute √∏(n)=(p‚Äì1)(q-1)=16√ó10=160
4.
Select e : gcd(e,160)=1; choose e=7
5.
Determine d: de=1 mod 160 and d < 160 Value is d=23 since 
23√ó7=161= 10√ó160+1
6.
Publish public key pk={7,187}
7.
Keep secret private key sk={23,17,11}

RSA use
‚Ä¢ to encrypt a message M the sender:
‚Ä¢ obtains public key of recipient pk={e,n} 
‚Ä¢ computes: C=Me mod n, where 0‚â§M<n
‚Ä¢ to decrypt the ciphertext C the owner:
‚Ä¢ uses their private key sk={d,p,q}
‚Ä¢ computes: M=Cd mod n 
‚Ä¢ note that the message M must be smaller than the modulus n (block 
if needed)
Plaintext
Ciphertext
C = 
pk={e,n}
sk={d,p,q}

Lecture 14

RSA use
‚Ä¢ to encrypt a message M the sender:
‚Ä¢ obtains public key of recipient pk={e,n} 
‚Ä¢ computes: C=Me mod n, where 0‚â§M<n
‚Ä¢ to decrypt the ciphertext C the owner:
‚Ä¢ uses their private key sk={d,p,q}
‚Ä¢ computes: M=Cd mod n 
‚Ä¢ note that the message M must be smaller than the modulus n (block 
if needed)
Plaintext
Ciphertext
C = 
pk={e,n}
sk={d,p,q}

RSA example continue
‚Ä¢ sample RSA encryption/decryption is: 
‚Ä¢ given message M = 88 ( 88<187)
‚Ä¢ encryption:
C = 887 mod 187 = 11 
‚Ä¢ decryption:
M = 1123 mod 187 = 88

Example of RSA algorithm

RSA key generation
‚Ä¢ users of RSA must:
‚Ä¢ determine two primes at random - p, q 
‚Ä¢ select either e or d and compute the other
‚Ä¢ primes p,q must not be easily derived from modulus n=p.q
‚Ä¢ means must be sufficiently large
‚Ä¢ typically guess and use probabilistic test
‚Ä¢ exponents e, d  are inverses, so use Inverse algorithm to compute the 
other

Correctness of RSA
‚Ä¢ Euler‚Äôs theorem: if gcd (M, n) = 1, then mod n. Here œÜ(n) is Euler‚Äôs 
totient function: the number of integers in {1, 2, . . ., n-1} which are 
relatively prime to n. When n is a prime, this theorem is just Fermat‚Äôs 
little theorem
M‚Äô =  mod n
 mod n
mod n
¬ø[ùëÄ
ùúô (ùëõ)]
ùëò‚àô ùëÄ mod n
= M  mod n

Attack approaches
‚Ä¢ Mathematical attacks: several approaches, all equivalent in effort to 
factoring the product of two primes. The defense against 
mathematical attacks is to use a large key size. 
‚Ä¢ Timing attacks: These depend on the running time of the decryption 
algorithm
‚Ä¢ Chosen ciphertext attacks: this type of attacks exploits properties of 
the RSA algorithm by selecting blocks of data. These attacks can be 
thwarted by suitable padding of the plaintext, such as PKCS1 V1.5 in 
SSL

Homomorphic encryption 
‚Ä¢ Encryption scheme that allows computation on ciphertexts 
‚Ä¢ i.e. a public-key encryption scheme that allows anyone in possession of the 
public key to perform operations on encrypted data without access to the 
decryption key
‚Ä¢ Initial public-key systems that allow this for either addition or 
multiplication, but not both.
‚Ä¢ Fully homomorphic encryption (FHE)

Application of homomorphic 
encryption 
‚Ä¢ One Use case: cloud computing 
‚Ä¢ A weak computational device Alice (e.g., a mobile phone or a laptop) wishes 
to perform a computationally heavy task, beyond her computational means. 
She can delegate it to a much stronger (but still feasible) machine Bob (the 
cloud, or a supercomputer) who offers the service of doing so. The problem is 
that Alice does not trust Bob, who may give the wrong answer due to 
laziness, fault, or malice.
E (Pk, data)
E (Pk, f(data))

RSA reading materials
‚Ä¢ A Method for Obtaining Digital Signatures and Public-Key Cryptosyste
ms

Lecture 15

Message Authentication

Message authentication
‚Ä¢ message authentication is concerned with: 
‚Ä¢ protecting the integrity of a message 
‚Ä¢ validating identity of originator 
‚Ä¢ non-repudiation of origin (dispute resolution)
‚Ä¢ then three alternative functions used:
‚Ä¢ message encryption - symmetric
‚Ä¢ message authentication code (MAC)
‚Ä¢ digital signature

Message encryption
‚Ä¢ Symmetric message encryption by itself also provides a measure of 
authentication
‚Ä¢ if symmetric encryption is used then:
‚Ä¢ receiver knows sender must have created it
‚Ä¢ since only sender and receiver know key used
‚Ä¢ know content cannot be altered

Homework 1 questions
‚Ä¢ Q1: Symmetric Block Cypher provides authentication and 
confidentiality
‚Ä¢ Ans: True

Message encryption
‚Ä¢ if public-key encryption is used:
‚Ä¢ encryption provides no confidence of sender
‚Ä¢ since anyone potentially knows public-key
‚Ä¢ so, need to recognize corrupted messages
‚Ä¢ however, if 
‚Ä¢ sender signs message using their private-key
‚Ä¢ then encrypts with recipients‚Äô public key
‚Ä¢ have both secrecy and authentication
‚Ä¢ but at cost of two public-key uses on message

Reasons to avoid encryption 
authentication
‚Ä¢ Encryption software is quite slow
‚Ä¢ Encryption hardware costs are nonnegligible
‚Ä¢ Encryption hardware is optimized toward large data sizes
‚Ä¢ An encryption algorithm may be protected by a patent

Hash Function

Hash functions
‚Ä¢ Hash function: h = H(M)
‚Ä¢ M can be of any size 
‚Ä¢ h is always of fixed size 
‚Ä¢ Typically, h << size(M)

One use case - using hash function
‚Ä¢ Initialization: A and B share a 
common secret, SAB
‚Ä¢ Message, M
‚Ä¢ A calculates MDM = H (SAB || M)
‚Ä¢ B recalculates MD‚ÄôM, and check
‚Ä¢ MD‚ÄôM = MDM
This scheme cannot provide authentication.

Lecture 16

Hash Function

Hash functions
‚Ä¢ Hash function: h = H(M)
‚Ä¢ M can be of any size 
‚Ä¢ h is always of fixed size 
‚Ä¢ Typically, h << size(M)

One use case - using hash function
‚Ä¢ Initialization: A and B share a 
common secret, SAB
‚Ä¢ Message, M
‚Ä¢ A calculates MDM = H (SAB || M)
‚Ä¢ B recalculates MD‚ÄôM, and check
‚Ä¢ MD‚ÄôM = MDM
This scheme cannot provide authentication.

Requirements for secure hash 
functions
‚Ä¢ 1. can be applied to any sized message M
‚Ä¢ 2. produces fixed-length output h
‚Ä¢ 3. is easy to compute h=H(M) for any message M
‚Ä¢ 4. given h is infeasible to find x s.t. H(x)=h
‚Ä¢ one-way property or preimage resistance
‚Ä¢ 5. given x is infeasible to find x‚Äô s.t. H(x‚Äô)=H(x)
‚Ä¢ weak collision resistance or second pre-image resistant
‚Ä¢ 6. infeasible to find any pair of x,x‚Äô s.t. H(x‚Äô)=H(x)
‚Ä¢ strong collision resistance

Hash Function: Collision Resistance
‚Ä¢ Collision: Two different inputs with the same output
‚Ä¢ x ‚â† x' and H(x) = H(x')
‚Ä¢ Can we design a hash function with no collisions?
‚Ä¢ No, because there are more inputs than outputs (pigeonhole principle)
‚Ä¢ However, we want to make finding collisions infeasible for an attacker
‚Ä¢ Collision resistance: It is infeasible to (i.e. no polynomial time attacker 
can) find any pair of inputs x' ‚â† x such that H(x) = H(x')

Secure hash function
‚Ä¢ A hash function that satisfies the first five properties is referred to as a 
weak hash function
‚Ä¢ Security: random/unpredictability, no predictable patterns for how 
changing the input affects the output
‚Ä¢ Changing 1 bit in the input causes the output to be completely different
‚Ä¢ Also called ‚Äúrandom oracle‚Äù assumption
‚Ä¢ A message digest
‚Ä¢ a fixed size numeric representation of the contents of a message, computed 
by a hash function
‚Ä¢ Examples: SHA-1 (Secure Hash Algorithm 1), SHA-2, SHA-3, MD5

Hash Function: Examples
‚Ä¢ MD5
‚Ä¢ Output: 128 bits
‚Ä¢ Security: Completely broken
‚Ä¢ SHA-1
‚Ä¢ Output: 160 bits
‚Ä¢ Security: Completely broken in 2017
‚Ä¢ Was known to be weak before 2017, but still used sometimes
‚Ä¢ SHA-2
‚Ä¢ Output: 256, 384, or 512 bits (sometimes labeled SHA-256, SHA-384, SHA-512)
‚Ä¢ Not currently broken, but some variants are vulnerable to a length extension attack
‚Ä¢ Current standard
‚Ä¢ SHA-3 (Keccak)
‚Ä¢ Output: 256, 384, or 512 bits
‚Ä¢ Current standard (not meant to replace SHA-2, just a different construction)

Length Extension Attacks
‚Ä¢ Length extension attack: Given H(x) and the length of x, but not x, an 
attacker can create H(x || m) for any m of the attacker‚Äôs choosing
‚Ä¢ Length extension attack - Wikipedia
‚Ä¢ SHA-256 (256-bit version of SHA-2) is vulnerable
‚Ä¢ SHA-3 is not vulnerable

Lecture 17

Length Extension Attacks
‚Ä¢ Length extension attack: Given H(x) and the length of x, but not x, an 
attacker can create H(x || m) for any m of the attacker‚Äôs choosing
‚Ä¢ Length extension attack - Wikipedia
‚Ä¢ SHA-256 (256-bit version of SHA-2) is vulnerable
‚Ä¢ SHA-3 is not vulnerable

Does hashes provide integrity?
‚Ä¢ It depends on your threat model
‚Ä¢ Scenario
‚Ä¢ Mozilla publishes a new version of Firefox on some download servers
‚Ä¢ Alice downloads the program binary
‚Ä¢ How can she be sure that nobody tampered with the program?
‚Ä¢ Idea: use cryptographic hashes
‚Ä¢ Mozilla hashes the program binary and publishes the hash on its website
‚Ä¢ Alice hashes the binary she downloaded and checks that it matches the hash on the website
‚Ä¢ If Alice downloaded a malicious program, the hash would not match (tampering detected!)
‚Ä¢ An attacker can‚Äôt create a malicious program with the same hash (collision resistance)
‚Ä¢ Threat model: We assume the attacker cannot modify the hash on the website
‚Ä¢ We have integrity, as long as we can communicate the hash securely

Do hashes provide integrity?
‚Ä¢ It depends on your threat model
‚Ä¢ Scenario
‚Ä¢ Alice and Bob want to communicate over an insecure channel
‚Ä¢ David might tamper with messages
‚Ä¢ Idea: Use cryptographic hashes
‚Ä¢ Alice sends her message with a cryptographic hash over the channel
‚Ä¢ Bob receives the message and computes a hash on the message
‚Ä¢ Bob checks that the hash he computed matches the hash sent by Alice
‚Ä¢ Threat model: David can modify the message and the hash
‚Ä¢ No integrity!

Man-in-the-middle attack
Alice
 M‚Äô
 MD‚Äô
M
MD
M
MD
M‚Äô
 MD‚Äô
Bob
David

Do hashes provide integrity? 
‚Ä¢ It depends on your threat model
‚Ä¢ If the attacker can modify the hash, hashes don‚Äôt provide integrity
‚Ä¢ Main issue: Hashes are unkeyed functions
‚Ä¢ There is no secret key being used as input, so any attacker can compute a 
hash on any value

Solutions
‚Ä¢ A message digest created using a secret symmetric key is known as a 
Message Authentication Code (MAC), because it can provide 
assurance that the message has not been modified
‚Ä¢ The sender can also generate a message digest and then encrypt the 
digest using the private key of an asymmetric key pair, forming a 
digital signature. The signature must then be verified by the receiver 
through comparing it with a locally generated digest

Hashes: Summary
‚Ä¢ Map arbitrary-length input to fixed-length output
‚Ä¢ Output is deterministic
‚Ä¢ Security properties
‚Ä¢ One way: Given an output y, it is infeasible to find any input x such that H(x) = y.
‚Ä¢ Second preimage resistant: Given an input x, it is infeasible to find another input 
x' ‚â† x such that H(x) = H(x').
‚Ä¢ Collision resistant: It is infeasible to find another any pair of inputs x' ‚â† x such that 
H(x) = H(x').
‚Ä¢ Some hashes are vulnerable to length extension attacks
‚Ä¢ Hashes don‚Äôt provide integrity (unless you can publish the hash securely)

Lecture 18

Message Authentication Code

Message authentication code (MAC)
‚Ä¢ generated by an algorithm that creates a small fixed-sized block
‚Ä¢ depending on both message and some key
‚Ä¢ not be reversible
‚Ä¢ MACM = F(KAB, M)
‚Ä¢ appended to message as a signature
‚Ä¢ receiver performs same computation on message and checks it 
matches the MAC
‚Ä¢ provides assurance that message is unaltered and comes from sender

MACs: Usage
‚Ä¢ Alice wants to send M to Bob, but doesn‚Äôt want David to tamper with it
‚Ä¢ Alice sends M and T = MAC(K, M) to Bob
‚Ä¢ Bob receives M and T
‚Ä¢ Bob computes MAC(K, M) and checks that it matches T
‚Ä¢ If the MACs match, Bob is confident the message has not been 
tampered with (integrity)

MACs: Definition
‚Ä¢ Two parts:
‚Ä¢ KeyGen() ‚Üí K: Generate a key K
‚Ä¢ MAC(K, M) ‚Üí T: Generate a tag T for the message M using key K
‚Ä¢ Inputs: A secret key and an arbitrary-length message
‚Ä¢ Output: A fixed-length tag on the message
‚Ä¢ Properties
‚Ä¢ Correctness: Determinism
‚Ä¢ Note: Some more complicated MAC schemes have an additional Verify(K, M, T) function 
that don‚Äôt require determinism, but this is out of scope
‚Ä¢ Efficiency: Computing a MAC should be efficient
‚Ä¢ Security: existentially unforgeable under chosen plaintext attack

Existentially unforgeable
‚Ä¢ A secure MAC is existentially unforgeable: without the key, an 
attacker cannot create a valid tag on a message
‚Ä¢ David cannot generate MAC(K, M') without K
‚Ä¢ David cannot find any M' ‚â† M such that MAC(K, M') = MAC(K, M)

Example: HMAC
‚Ä¢ issued as RFC 2104 [1]
‚Ä¢ has been chosen as the mandatory-to-implement MAC for IP Security
‚Ä¢ Used in Transport Layer Security (TLS) and Secure Electronic 
Transaction (SET)
[1] ‚ÄúHMAC: Keyed-Hashing for Message Authentication‚Äù, RFC 2104, https://datatracker.ietf.org/doc/html/rfc2104

HMAC(K, M)
‚Ä¢ will produce two keys to increase security
‚Ä¢ If key is longer than the desired size, we can hash it first, but be 
careful with using keys that are too much smaller, they have to have 
enough randomness in them
‚Ä¢ Output H[(K+ 
 
‚äï opad) || H[(K+ 
 
‚äï ipad) || M]]

Example: HMAC
‚Ä¢ HMAC(K, M):
‚Ä¢ Output H[(K+ 
 
‚äï opad) || H[(K+ 
 
‚äï ipad) || M]] 
‚Ä¢ Use K to derive two different keys
‚Ä¢ opad (outer pad) is the hard-coded byte 0x5c repeated until it‚Äôs the same 
length as K+
‚Ä¢ ipad (inner pad) is the hard-coded byte 0x36 repeated until it‚Äôs the same 
length as K+
‚Ä¢ As long as opad and ipad are different, you‚Äôll get two different keys
‚Ä¢ For paranoia, the designers chose two very different bit patterns, even though 
they theoretically need only differ in one bit

HMAC 
            A
         B
    A  B
            0             0
          0
            0
            1
          1
            1
            0
          1
            1
            1
          0
K+ = 
ipad = 00110110 , repeat b/8 times
opad = 01011100, repeat b/8 times

HMAC procedure
‚Ä¢ Step 1: Append zeros to the left end of K to create a b-bit string K+ (e.g., if K 
is of length 160 bits and b = 512, then K will be appended with 44 zero 
bytes); 
‚Ä¢ Step 2: XOR (bitwise exclusive-OR) K+ with ipad to produce the b-bit block Si;
‚Ä¢ Step 3: Append M to Si; 
‚Ä¢ Step 4: Apply H to the stream generated in step 3; 
‚Ä¢ Step 5: XOR K+ with opad to produce the b-bit block So; 
‚Ä¢ Step 6: Append the hash result from step 4 to So; 
‚Ä¢ Step 7: Apply H to the stream generated in step 6 and output the result.

Mid-term Exam
‚Ä¢ Nov. 3, 2023 (Friday), 4:00 pm ‚Äì 4:50 pm, in class
‚Ä¢ Closed book
‚Ä¢ Chapter 1 ‚Äì 3
‚Ä¢ Will have a review class

Lecture 20

MAC-then-Encrypt or Encrypt-then-
MAC?
‚Ä¢ Method 1: Encrypt-then-MAC
‚Ä¢ First compute Enc(K1, M)
‚Ä¢ Then MAC the ciphertext: MAC(K2, Enc(K1, M))
‚Ä¢ Method 2: MAC-then-encrypt
‚Ä¢ First compute MAC(K2, M)
‚Ä¢ Then encrypt the message and the MAC together: Enc(k1, M || MAC(K2, M))
‚Ä¢ Which is better?
‚Ä¢ In theory, both are secure if applied properly
‚Ä¢ MAC-then-encrypt has a flaw: You don‚Äôt know if tampering has occurred until after decrypting
‚Ä¢ Attacker can supply arbitrary tampered input, and you always have to decrypt it
‚Ä¢ Passing attacker-chosen input through the decryption function can cause side-channel leaks
‚Ä¢ Always use encrypt-then-MAC because it‚Äôs more robust to mistakes

TLS 1.0 ‚ÄúLucky 13‚Äù Attack
‚Ä¢ TLS: A protocol for sending encrypted and authenticated messages over the 
Internet
‚Ä¢ TLS 1.0 uses MAC-then-encrypt: Enc(k1, M || MAC(k2, M))
‚Ä¢ The encryption algorithm is AES-CBC
‚Ä¢ The Lucky 13 attack abuses MAC-then-encrypt to read encrypted messages
‚Ä¢ Guess a byte of plaintext and change the ciphertext accordingly
‚Ä¢ The MAC will error, but the time it takes to error is different depending on if the guess is 
correct
‚Ä¢ Attacker measures how long it takes to error in order to learn information about plaintext
‚Ä¢ TLS will send the message again if the MAC errors, so the attacker can guess repeatedly
‚Ä¢ Takeaways
‚Ä¢ Side channel attack: The algorithm is proved secure, but poor implementation made it 
vulnerable
‚Ä¢ Always encrypt-then-MAC
‚Ä¢ https://medium.com/@c0D3M/lucky-13-attack-explained-dd9a9fd42fa6

Authenticated Encryption: Summary
‚Ä¢ Authenticated encryption: A scheme that simultaneously guarantees 
confidentiality and integrity (and authenticity) on a message
‚Ä¢ First approach: Combine schemes that provide confidentiality with 
schemes that provide integrity and authenticity
‚Ä¢ MAC-then-encrypt: Enc(K1, M || MAC(K2, M))
‚Ä¢ Encrypt-then-MAC: MAC(K2, Enc(K1, M))
‚Ä¢ Always use Encrypt-then-MAC because it's more robust to mistakes

Digital Signature

Digital Signatures
‚Ä¢ NIST FIPS PUB 186-4 - the result of a cryptographic transformation of 
data that, when properly implemented, provides a mechanism for 
verifying origin authentication, data integrity, and signatory non-
repudiation
‚Ä¢ Based on asymmetric keys

Digital Signatures
‚Ä¢ Asymmetric cryptography is good because we don‚Äôt need to share a 
secret key
‚Ä¢ Digital signatures are the asymmetric way of providing 
integrity/authenticity to data
‚Ä¢ Assume that Alice and Bob can communicate public keys without 
David interfering

Digital Signatures: Definition
‚Ä¢ Three parts:
‚Ä¢ KeyGen() ‚Üí PK, SK: Generate a public/private keypair, where PK is the verify (public) key, and SK 
is the signing (secret) key
‚Ä¢ Sign(SK, M) ‚Üí sig: Sign the message M using the signing key SK to produce the signature sig
‚Ä¢ Verify(PK, M, sig) ‚Üí {0, 1}: Verify the signature sig on message M using the verify key PK and 
output 1 if valid and 0 if invalid
‚Ä¢ Properties:
‚Ä¢ Correctness: Verification should be successful for a signature generated over any message
‚Ä¢ Verify(PK, M, Sign(SK, M)) = 1 for all PK, SK ‚Üê KeyGen() and M
‚Ä¢ Efficiency: Signing/verifying should be fast
‚Ä¢ Security: Same as for MACs except that the attacker also receives PK
‚Ä¢ Namely, no attacker can forge a signature for a message

Lecture 21

RSA Signature
‚Ä¢ KeyGen():
‚Ä¢ Randomly pick two large primes, p and q
‚Ä¢ Compute n = pq
‚Ä¢ n is usually between 2048 bits and 4096 bits long
‚Ä¢ Choose e
‚Ä¢ Requirement: e is relatively prime to (p - 1)(q - 1)
‚Ä¢ Requirement: 2 < e < (p - 1)(q - 1)
‚Ä¢ Compute d = e-1 mod (p - 1)(q - 1)
‚Ä¢ Public key: n and e
‚Ä¢ Private key: d

RSA Digital 
Signature Algo
Step1: Generate a hash value, or message digest, 
mHash from the message M to be signed 
Step2: Pad mHash with a constant value padding1 and 
pseudorandom value salt to form M‚Äô 
Step3: Generate hash value H from M‚Äô
Step4: Generate a block DB consisting of a constant 
value padding 2 and salt
Step5: Use the mask generating function MGF, which 
produces a randomized out-put from input H of the 
same length as DB
Step 6: Create the encoded message (EM) block by 
padding H with the hexadecimal constant bc and the 
XOR of DB and output of MGF
Step 7: Encrypt EM with RSA using the signer‚Äôs private 
key

RSA Signatures
‚Ä¢ Sign(d, M):
‚Ä¢ Compute H(M)d mod n
‚Ä¢ Verify(e, n, M, sig)
‚Ä¢ Verify that H(M) ‚â° sige mod n

RSA Signatures: Correctness
Theorem: sige ‚â° H(M) mod N   
Proof:
sige = 
 mod N
   mod N
¬ø[ùêª( ùëÄ)
ùúô (ùëõ)]
ùëò‚àô ùêª (ùëÄ)mod N
= H(M)    mod N

RSA Digital Signature: Security
‚Ä¢ Necessary hardness assumptions:
‚Ä¢ Factoring hardness assumption: Given n large, it is hard to find primes pq = n 
‚Ä¢ Discrete logarithm hardness assumption: Given n large, hash, and hashd mod 
n, it is hard to find d
‚Ä¢ Salt also adds security
‚Ä¢ Even the same message and private key will get different signatures

Hybrid Encryption
‚Ä¢ Issues with public-key encryption
‚Ä¢ Notice: We can only encrypt small messages because of the modulo operator
‚Ä¢ Notice: There is a lot of math, and computers are slow at math
‚Ä¢ Result: We don‚Äôt use asymmetric for large messages
‚Ä¢ Hybrid encryption: Encrypt data under a randomly generated key K 
using symmetric encryption, and encrypt K using asymmetric 
encryption
‚Ä¢ EncAsym(PK, K); EncSym(K, large message) 
‚Ä¢ Benefit: Now we can encrypt large amounts of data quickly using symmetric 
encryption, and we still have the security of asymmetric encryption

Homework ‚Äì no submission
‚Ä¢ RQ: 3.1, 3.2, 3.3, 3.4, 3.6, 3.7
‚Ä¢ Problems: 
‚Ä¢ prove correctness of RSA digital signature
‚Ä¢ 3.14

Homework 2 - individual
‚Ä¢ For Chapter 3
‚Ä¢ Deadline: Oct. 26 (Thursday), 11:59 pm
‚Ä¢ We will use the blackboard submission time as your final timestamp
‚Ä¢ 10% penalty per day for late submission

Thank you!

Lecture 22

Network Security
Chapter 4

Key Distribution

Symmetric Key Distribution and User Authentication
                                                             4.2

Ways to achieve symmetric key 
distribution
‚Ä¢ A key could be selected by A and physically delivered to B
‚Ä¢ A third party could select the key and physically deliver it to A and B 
‚Ä¢ If A and B have previously and recently used a key, one party could 
transmit the new key to the other, using the old key to encrypt the 
new key 
‚Ä¢ If A and B each have an encrypted connection to a third-party C, C 
could deliver a key on the encrypted links to A and B

Terminologies
‚Ä¢ Session key
‚Ä¢ Permanent key
‚Ä¢ key distribution center (KDC) 
‚Ä¢ third party authority, centralized infrastructure
‚Ä¢ give permissions for two parties to communicate

Kerberos
4.3

Many-to Many Authentication
How do users prove their identities when requesting services from 
machines on the network?

Threats
‚Ä¢ User impersonation
‚Ä¢ Malicious user with access to a workstation pretends to be another user from 
the same workstation
‚Ä¢ Network address impersonation
‚Ä¢ Malicious user changes network address of his workstation to impersonate 
another workstation 
‚Ä¢ Eavesdropping, tampering, replay 
‚Ä¢ Malicious user eavesdrops, tampers, or replays other users‚Äô conversations to 
gain unauthorized access

Requirements
‚Ä¢ Security
‚Ä¢ against attacks by eavesdroppers and malicious users
‚Ä¢ Transparency
‚Ä¢ users shouldn‚Äôt notice authentication taking place
‚Ä¢ entering password is ok, if done rarely
‚Ä¢ Scalability
‚Ä¢ Large number of users and servers

Kerberos
‚Ä¢ scenario: users at workstations wish to access services on servers 
distributed throughout the network ‚Äì many to many authentication

Kerberos
‚Ä¢ a centralized authentication server provides mutual authentication 
between users and servers
‚Ä¢ a key distribution and user authentication service developed at MIT
‚Ä¢ works in an open distributed environment
‚Ä¢ client-service model
‚Ä¢ Kerberos protocol messages are protected against eavesdropping and 
replay attacks 
‚Ä¢ Kerberos v4 and v5 [RFC 4120]

A Simple Authentication Dialogue
‚Ä¢ 1. C ‚Äî>AS: IDC ||PC ||IDV
‚Ä¢ 2. AS ‚Äî> C : Ticket = E(KV, [IDC ||ADC ||IDV])
‚Ä¢ 3. C ‚Äî> V: IDC || Ticket
‚Ä¢ AS ‚Äì authentication server 
‚Ä¢ ID* -  identifier
‚Ä¢ PC  - password of user
‚Ä¢ ADC - network address of C
‚Ä¢ KV  - secret encryption key shared by AS and V
AS
C
V
1
2
3

Cheat sheet
‚Ä¢ Allow for half of an A4 paper

Lecture 23

Kerberos
‚Ä¢ scenario: users at workstations wish to access services on servers 
distributed throughout the network ‚Äì many to many authentication

Kerberos
‚Ä¢ a centralized authentication server provides mutual authentication 
between users and servers
‚Ä¢ a key distribution and user authentication service developed at MIT
‚Ä¢ works in an open distributed environment
‚Ä¢ client-service model
‚Ä¢ Kerberos protocol messages are protected against eavesdropping and 
replay attacks 
‚Ä¢ Kerberos v4 and v5 [RFC 4120]

A Simple Authentication Dialogue
‚Ä¢ 1. C ‚Äî>AS: IDC ||PC ||IDV
‚Ä¢ 2. AS ‚Äî> C : Ticket = E(KV, [IDC ||ADC ||IDV])
‚Ä¢ 3. C ‚Äî> V: IDC || Ticket
‚Ä¢ AS ‚Äì authentication server 
‚Ä¢ ID* -  identifier
‚Ä¢ PC  - password of user
‚Ä¢ ADC - network address of C
‚Ä¢ KV  - secret encryption key shared by AS and V
AS
C
V
1
2
3

Advantage
‚Ä¢ Client and malicious attacker cannot alter IDC (impersonate), 
ADC(change of address) , IDV 
‚Ä¢ server V can verify the user is authenticated through IDC , and grants 
service to C
‚Ä¢ guarantee the ticket is valid only if it is transmitted from the same 
client that initially requested the ticket

Secure?
‚Ä¢ Insecure: password is transmitted openly and frequently
‚Ä¢ Solution: no password transmitted by involving ticket-granting server 
(TGS)

A More Secure Authentication 
Dialogue
‚Ä¢ Once per user logon session
‚Ä¢ (1) C ‚Äî>AS:    IDC ||IDtgs
‚Ä¢ (2) AS ‚Äî> C:   E(KC, Tickettgs)
‚Ä¢ Once per type of service:
‚Ä¢ (3) C ‚Äî>TGS:   IDC ||IDv|| Tickettgs
‚Ä¢ (4) TGS ‚Äî> C:   TicketV
‚Ä¢ Once per service session:
‚Ä¢ (5) C ‚Äî> V:  IDC || TicketV
AS
C
V
1
2
3, Tickettgs
TGS
4
5
TicketV

Lecture 24

Secure?
‚Ä¢ Insecure: password is transmitted openly and frequently
‚Ä¢ Solution: no password transmitted by involving ticket-granting server 
(TGS)

A More Secure Authentication 
Dialogue
‚Ä¢ Once per user logon session
‚Ä¢ (1) C ‚Äî>AS:    IDC ||IDtgs
‚Ä¢ (2) AS ‚Äî> C:   E(KC, Tickettgs)
‚Ä¢ Once per type of service:
‚Ä¢ (3) C ‚Äî>TGS:   IDC ||IDv|| Tickettgs
‚Ä¢ (4) TGS ‚Äî> C:   TicketV
‚Ä¢ Once per service session:
‚Ä¢ (5) C ‚Äî> V:  IDC || TicketV
AS
C
V
1
2
3, Tickettgs
TGS
4
5
TicketV

Advantage
‚Ä¢ No password transmitted in plaintext
‚Ä¢ Ticket is reusable. Timestamp is added to prevent reuse of ticket by an 
attacker

Secure?
‚Ä¢ Ticket hijacking 
‚Ä¢ Malicious user may steal the service ticket of another user on the same workstation 
and try to use it 
‚Ä¢ Network address verification does not help
‚Ä¢ Servers must verify that the user who is presenting the ticket is the same user to 
whom the ticket was issued 
‚Ä¢ No server authentication
‚Ä¢ Attacker may misconfigure the network so that he receives messages addressed to a 
legitimate server ‚Äì man in the middle attack
‚Ä¢ Capture private information from users and/or deny service
‚Ä¢ Servers must prove their identity to users
‚Ä¢ Solution: section key
no user authentication

Lecture 25

A More Secure Authentication Dialogue
‚Ä¢ Once per user logon session
‚Ä¢ (1) C ‚Äî>AS:    IDC ||IDtgs
‚Ä¢ (2) AS ‚Äî> C:   E(KC, Tickettgs)
‚Ä¢ Once per type of service:
‚Ä¢ (3) C ‚Äî>TGS:   IDC ||IDv|| Tickettgs
‚Ä¢ (4) TGS ‚Äî> C:   TicketV
‚Ä¢ Once per service session:
‚Ä¢ (5) C ‚Äî> V:  IDC || TicketV
AS
C
V
1
2
3, Tickettgs
TGS
4
5
TicketV

Advantage
‚Ä¢ No password transmitted in plaintext
‚Ä¢ Ticket is reusable. Timestamp is added to prevent reuse of ticket by 
an attacker

Secure?
‚Ä¢ Ticket hijacking 
‚Ä¢ Malicious user may steal the service ticket of another user on the same workstation 
and try to use it 
‚Ä¢ Network address verification does not help
‚Ä¢ Servers must verify that the user who is presenting the ticket is the same user to 
whom the ticket was issued 
‚Ä¢ No server authentication
‚Ä¢ Attacker may misconfigure the network so that he receives messages addressed to a 
legitimate server ‚Äì man in the middle attack
‚Ä¢ Capture private information from users and/or deny service
‚Ä¢ Servers must prove their identity to users
‚Ä¢ Solution: section key
no user authentication

Kerberos v4.  - once per user logon session
C
password
Program
AS
client will use this unforgeable ticket to get
other tickets without re-authenticating
Decrypts with 
Kc and obtains
Kc,tgs and Tickettgs
Implicit authentication:
only someone who knows Kc can decrypt
Client only needs to obtain Tickettgs once (say, every morning)
Ticket is encrypted; client cannot forge it or tamper with it 
All users must
pre-register their
passwords with AS
Kc,tgs

Kerberos v4.  - once per type of service
C
System command
e.g. ‚Äúlpr ‚Äì Pprint‚Äù
Program
TGS
client will use this unforgeable ticket to
get access to service V
Client uses Tickettgs to obtain a service ticket, Ticketv and a short-term session
key for each network service (printer, email, etc.)
Knows Kv
Proves that client knows key Kc,tgs
contained in encrypted TGS ticket
Kc,v

Kerberos v4.  - once per service session
C
System command
e.g. ‚Äúlpr ‚Äì Pprint‚Äù
Program
Authenticates server to client
Chain of Reasoning:
Server can produce this message only if he knows Kc,v
Server can learn key Kc,v only if he can decrypt service ticket
Server can decrypt service ticket only if he knows correct key KV
If server knows correct key KV, then he is the right server 
For each service request, client uses the short-term key, Kc,v , for that service and 
the ticket he received from TGS 
Proves that client knows key Kc,v
contained in encrypted ticket
V

Overview of Kerberos

Lecture 26

Overview of Kerberos

Important Ideas in Kerberos
‚Ä¢ Short-term session keys 
‚Ä¢ Long-term secrets used only to derive short-term keys 
‚Ä¢ Separate session key for each user-server pair
‚Ä¢ Re-used by multiple sessions between same user and server 
‚Ä¢ Proofs of identity based on authenticators 
‚Ä¢ Client encrypts his identity, addr, time with session key; knowledge of key 
proves client has authenticated to KDC/AS 
‚Ä¢ Also prevents replays (if clocks are globally synchronized)
‚Ä¢ Server learns this key separately (via encrypted ticket that client can‚Äôt 
decrypt), then verifies client‚Äôs authenticator 
‚Ä¢ Symmetric cryptography only

Kerberos in Large Networks
‚Ä¢ One KDC isn‚Äôt enough for large networks 
‚Ä¢ Network is divided into realms 
‚Ä¢ KDCs in different realms have different key databases 
‚Ä¢ To access a service in another realm, users must... 
‚Ä¢ Get ticket for home-realm TGS from home-realm KDC 
‚Ä¢ Get ticket for remote-realm TGS from home-realm TGS 
‚Ä¢ As if remote-realm TGS were just another network service 
‚Ä¢ Get ticket for remote service from that realm‚Äôs TGS
‚Ä¢ Use remote-realm ticket to access service
home-realm KDC
home-realm TGS
Ticket_hTGS
remote-realm TGS
Ticket_rTGS
remote service
Ticket_rS

Practical Uses of Kerberos
‚Ä¢ Microsoft Windows ‚Äì Active Directory
‚Ä¢ Email, FTP, network file systems, many other applications have been 
kerberized
‚Ä¢ Use of Kerberos is transparent for the end user 
‚Ä¢ Transparency is important for usability! 
‚Ä¢ Local authentication
‚Ä¢ login and su in OpenBSD 
‚Ä¢ Authentication for network protocols 
‚Ä¢ rsh
‚Ä¢ Secure windowing systems

Readings
‚Ä¢ Kerberos: The Network Authentication Protocol 
https://web.mit.edu/kerberos/

Practice 
‚Ä¢ William Stallings, ‚ÄúNetwork Security Essentials‚Äù, 6 Edition, 2017
‚Ä¢ Chapter 4‚Äôs problems: 4.8, 4.9, 4.10

Lecture 27

Diffie-Hellman Key Exchange
Section 3.5

Diffie-Hellman Key Exchange
‚Ä¢ No third party involved
‚Ä¢ After a common shared key, ùõºùëãùê¥ùëãùêµ is established, it can be used to 
encrypt message
‚Ä¢ A common shared key is symmetric

The Diffie-Hellman Key Exchange
‚Ä¢ From B‚Äôs view
‚Ä¢ ùêæ = ùëåùêµ
ùëãùê¥ ùëöùëúùëë ùëû
= (ùõºùëãùêµ ùëöùëúùëë ùëû)ùëãùê¥ ùëöùëúùëë ùëû
= ùõºùëãùêµùëãùê¥ ùëöùëúùëë ùëû

Example
‚Ä¢ ùëîùëñùë£ùëíùëõ ùëû = 353, ùõº = 3, ùëãùê¥ = 97, ùëãùêµ = 233
‚Ä¢ A computes ùëåùê¥ = 397 ùëöùëúùëë 353 = 40. B computes ùëåùêµ =
3233 ùëöùëúùëë 353 = 248
‚Ä¢ Then communication key exchange - ùëåùê¥, ùëåùêµ
‚Ä¢ A receives ùëåùêµ. B receives ùëåùê¥
‚Ä¢ A computes ùêæ = ùëåùêµ
ùëãùê¥ ùëöùëúùëë 353 = 24897 ùëöùëúùëë 353 = 160
B computes ùêæ = ùëåùê¥
ùëãùêµ ùëöùëúùëë 353 = 40233 ùëöùëúùëë 353 = 160

Attack
‚Ä¢ Adversary gets ùëû, ùõº, ùëåùê¥, ùëåùêµ.
‚Ä¢ She needs to compute either ùëãùê¥ or ùëãùêµ = ùëëùëôùëúùëîùõº,ùëùùëåùêµ
‚Ä¢ Secure?

Discrete Log Problem
Two cryptographic assumptions:
‚Ä¢ Discrete logarithm problem (discrete log problem): Given 
ùõº, ùëû, ùõºùëãùê¥ ùëöùëúùëë ùëû for random ùëãùê¥, it is computationally hard to find ùëãùê¥
‚Ä¢ Diffie-Hellman assumption: Given ùõº, ùëû, ùõºùëãùê¥ ùëöùëúùëë ùëû, and 
ùõºùëãùêµ ùëöùëúùëë ùëû for random ùëãùê¥, ùëãùêµ, no polynomial time attacker can 
distinguish between a random value R and ùõºùëãùê¥ùëãùêµ ùëöùëúùëë ùëû.
‚Ä¢ Intuition: The best known algorithm is to first calculate ùëãùê¥ and then compute 
(ùõºùëãùêµ)ùëãùê¥ ùëöùëúùëë ùëû , but this requires solving the discrete log problem, which is 
hard!
‚Ä¢ Note: Multiplying the values doesn‚Äôt work, since you get 
ùõºùëãùê¥+ùëãùêµ ùëöùëúùëë ùëù ‚â† ùõºùëãùê¥ùëãùêµ ùëöùëúùëë ùëù

Lecture 28

Ephemerality of Diffie-Hellman
‚Ä¢ Diffie-Hellman can be used ephemerally (called Diffie-Hellman 
ephemeral, or DHE)
‚Ä¢ Ephemeral: Short-term and temporary, not permanent
‚Ä¢ Alice and Bob discard ùëãùê¥, ùëãùêµ and ùêæ = ùõºùëãùê¥ùëãùêµ ùëöùëúùëë ùëû when they‚Äôre done
‚Ä¢ Because you need ùëãùê¥ and ùëãùêµ to derive ùêæ, you can never derive ùêæ again!
‚Ä¢ Sometimes ùêæ is called a session key, because it‚Äôs only used for an ephemeral 
session
‚Ä¢ Eve can‚Äôt decrypt any messages she recorded: Nobody saved ùëãùê¥, ùëãùêµ
or ùêæ, and her recording only has ùõºùëãùê¥ ùëöùëúùëë ùëû and ùõºùëãùêµ ùëöùëúùëë ùëû!

Diffie-Hellman is susceptible to man-in-the-
middle attacks
‚Ä¢ David can alter messages, block messages, and send her own 
messages
‚Ä¢ DH is not secure against a MITM attacker: David can just do a DH with 
both sides!

Diffie-Hellman: Security

Diffie-Hellman: issues
‚Ä¢ Diffie-Hellman is not secure against a MITM adversary
‚Ä¢ DHE is an active protocol: Alice and Bob need to be online at the 
same time to exchange keys
‚Ä¢ What if Bob wants to encrypt something and send it to Alice for her to read 
later?
‚Ä¢ Diffie-Hellman does not provide authentication
‚Ä¢ You exchanged keys with someone, but Diffie-Hellman makes no guarantees 
about who you exchanged keys with; it could be David!

Diffie-Hellman Key Exchange: Summary
‚Ä¢ Algorithm:
‚Ä¢ Alice chooses ùëãùê¥ and sends ùõºùëãùê¥ ùëöùëúùëë ùëû to Bob
‚Ä¢ Bob chooses ùëãùêµ and sends ùõºùëãùêµ ùëöùëúùëë ùëû to Alice
‚Ä¢ Their shared secret is (ùõºùëãùê¥)ùëãùêµ= (ùõºùëãùêµ)ùëãùê¥= ùõºùëãùê¥ùëãùêµ ùëöùëúùëë ùëû
‚Ä¢ Diffie-Hellman provides forwards secrecy: Nothing is saved or can be 
recorded that can ever recover the key
‚Ä¢ Diffie-Hellman can be performed over other mathematical groups, 
such as elliptic-curve Diffie-Hellman (ECDH)
‚Ä¢ Issues
‚Ä¢ Not secure against MITM
‚Ä¢ Both parties must be online
‚Ä¢ Does not provide authenticity

Homework ‚Äì no submission
‚Ä¢ SW, ‚ÄúNetwork Security Essentials‚Äù, 6th Edition, 2017 
‚Ä¢ Problems ‚Äì 3.21
Consider a Diffie-Hellman scheme with a common prime ùëû = 11 and a primitive 
root ùõº = 2.
a. if user A has public key ùëåùê¥ = 9, what is A‚Äôs private key ùëãùê¥?
b. If user B has public key ùëåùêµ = 3, what is the shared secret key ùêæ?

Next 
‚Ä¢ PKI and Certificates
‚Ä¢ Section 4.5

Lecture 29

Key Distribution Using Asymmetric Encryption

PKI and Certificates
(Section 4.5)

What is PKI? 
‚Ä¢ Use of public-key cryptography and X.509 certificates in a distributed 
server system to establish secure domains and trusted relationships

Why use public-key cryptography?
‚Ä¢ Review: Public-key cryptography is great! We can communicate 
securely without a shared secret
‚Ä¢ Public-key encryption: Everybody encrypts with the public key, but only the 
owner of the private key can decrypt
‚Ä¢ Digital signatures: Only the owner of the private key can sign, but everybody 
can verify with the public key

Problem: Distributing Public Keys
‚Ä¢ Public-key cryptography alone is not secure against man-in-the-
middle attacks
‚Ä¢ Scenario
‚Ä¢ Alice wants to send a message to Bob
‚Ä¢ Alice asks Bob for his public key
‚Ä¢ Bob sends his public key to Alice
‚Ä¢ Alice encrypts her message with Bob‚Äôs public key and sends it to Bob
‚Ä¢ What can David do?
‚Ä¢ Replace Bob's public key with David‚Äôs public key
‚Ä¢ Now Alice has encrypted the message with David‚Äôs public key, and David can 
read it!

Problem: Distributing Public Keys
Alice
David
Bob
Generate PKB, SKB
Man-in-the-Middle Attack

Solution: Distributing Public Keys
‚Ä¢ Idea: Sign Bob‚Äôs public key to prevent tampering
‚Ä¢ Problem
‚Ä¢ If Bob signs his public key, we need his public key to verify the signature
‚Ä¢ But Bob‚Äôs public key is what we were trying to verify in the first place!
‚Ä¢ Circular problem: Alice can never trust any public key she receives
‚Ä¢ You cannot gain trust if you trust nothing. You need a root of trust!
‚Ä¢ Trust anchor: Someone that we implicitly trust
‚Ä¢ From our trust anchor, we can begin to trust others

Trust-on-First-Use
‚Ä¢ Trust-on-first-use: The first time you communicate, trust the public 
key that is used and warn the user if it changes in the future
‚Ä¢ Used in SSH and a couple other protocols
‚Ä¢ Idea: Attacks aren‚Äôt frequent, so assume that you aren‚Äôt being attacked the 
first time communicate

Certificates

Certificates
‚Ä¢ Certificate: A signed endorsement of someone‚Äôs public key
‚Ä¢ A certificate contains at least two things: The identity of the person, and the 
key
‚Ä¢ Abbreviated notation
‚Ä¢ Signing with a private key SK: {‚ÄúMessage‚Äù}SK-1
‚Ä¢ Recall: A signed message must contain the message along with the signature; you can‚Äôt 
send the signature by itself!
‚Ä¢ Scenario: Alice wants Bob‚Äôs public key. Alice trusts Charlie (PKC, SKC)
‚Ä¢ Charlie is our trust anchor
‚Ä¢ If we trust PKC, a certificate we would trust is {‚ÄúBob‚Äôs public key is 
PKB‚Äù}SKC-1

How do we use public-key certificate?

X.509 Certificates
‚Ä¢ Certificate serial # - SN
‚Ä¢ Period validity - TA
‚Ä¢ Subject‚Äôs public key info - Ap
‚Ä¢ Signature signed by CA‚Äôs private key
‚Ä¢ Math notation:

readings
‚Ä¢ Barnes, R.; Hoffman-Andrews, J.; McCarney, D.; Kasten, J. (March 
2019). Automatic Certificate Management Environment (ACME) RFC 
8555. IETF
‚Ä¢ Internet Security Research Group, ‚ÄúAnnual Report‚Äù, 
https://www.abetterinternet.org/annual-reports/
‚Ä¢ Minkyu Kim, ‚ÄúA Survey of Kerberos V and Public-Key Kerberos 
Security‚Äù, https://www.cse.wustl.edu/~jain/cse571-
09/ftp/kerb5/index.html




The requirements of information security within an organization have undergone two 
major changes in the last several decades. Before the widespread use of data process-
ing equipment, the security of information felt to be valuable to an organization was 
provided primarily by physical and administrative means. An example of the former 
is the use of rugged filing cabinets with a combination lock for storing sensitive docu-
ments. An example of the latter is personnel screening procedures used during the 
hiring process.
With the introduction of the computer, the need for automated tools for pro-
tecting files and other information stored on the computer became evident. This 
is especially the case for a shared system, such as a time-sharing system, and the 
need is even more acute for systems that can be accessed over a public telephone 
network, data network, or the Internet. The generic name for the collection of tools 
designed to protect data and to thwart hackers is computer security.
The second major change that affected security is the introduction of distrib-
uted systems and the use of networks and communications facilities for carrying 
data between terminal user and computer and between computer and computer. 
Network security measures are needed to protect data during their transmission. In 
fact, the term network security is somewhat misleading, because virtually all busi-
ness, government, and academic organizations interconnect their data processing 
equipment with a collection of interconnected networks. Such a collection is often 
referred to as an internet,1 and the term internet security is used.
1We use the term internet with a lowercase ‚Äúi‚Äù to refer to any interconnected collection of network. 
A¬† corporate intranet is an example of an internet. The Internet with a capital ‚ÄúI‚Äù may be one of the facili-
ties used by an organization to construct its internet.
Learning Objectives
After studying this chapter, you should be able to:
‚óÜ‚óÜ
Describe the key security requirements of confidentiality, integrity, and 
availability.
‚óÜ‚óÜ
Describe the X.800 security architecture for OSI.
‚óÜ‚óÜ
Discuss the types of security threats and attacks that must be dealt with 
and give examples of the types of threats and attacks that apply to  different 
categories of computer and network assets.
‚óÜ‚óÜ
Explain the fundamental security design principles.
‚óÜ‚óÜ
Discuss the use of attack surfaces and attack trees.
‚óÜ‚óÜ
List and briefly describe key organizations involved in cryptography 
 standards.

There are no clear boundaries between these two forms of security. For ex-
ample, a computer virus may be introduced into a system physically when it arrives 
on a flash drive or an optical disk and is subsequently loaded onto a computer. 
 Viruses may also arrive over an internet. In either case, once the virus is resident 
on a computer system, internal computer security tools are needed to detect and 
recover from the virus.
This book focuses on internet security, which consists of measures to deter, 
prevent, detect, and correct security violations that involve the transmission of 
 information. That is a broad statement that covers a host of possibilities. To give 
you a feel for the areas covered in this book, consider the following examples of 
security violations:
1. User A transmits a file to user B. The file contains sensitive information (e.g., 
payroll records) that is to be protected from disclosure. User C, who is not 
 authorized to read the file, is able to monitor the transmission and capture a 
copy of the file during its transmission.
2. A network manager, D, transmits a message to a computer, E, under its man-
agement. The message instructs computer E to update an authorization file 
to include the identities of a number of new users who are to be given access 
to that computer. User F intercepts the message, alters its contents to add or 
delete entries, and then forwards the message to E, which accepts the message 
as coming from manager D and updates its authorization file accordingly.
3. Rather than intercept a message, user F constructs its own message with 
the  desired entries and transmits that message to E as if it had come from 
 manager¬†D. Computer E accepts the message as coming from manager D and 
 updates its authorization file accordingly.
4. An employee is fired without warning. The personnel manager sends a mes-
sage to a server system to invalidate the employee‚Äôs account. When the invali-
dation is accomplished, the server is to post a notice to the employee‚Äôs file as 
confirmation of the action. The employee is able to intercept the message and 
delay it long enough to make a final access to the server to retrieve sensitive 
information. The message is then forwarded, the action taken, and the confir-
mation posted. The employee‚Äôs action may go unnoticed for some consider-
able time.
5. A message is sent from a customer to a stockbroker with instructions for vari-
ous transactions. Subsequently, the investments lose value and the customer 
denies sending the message.
Although this list by no means exhausts the possible types of security violations, it 
 illustrates the range of concerns of network security.
This chapter provides a general overview of the subject matter that struc-
tures the material in the remainder of the book. We begin with a general discussion 
of network security services and mechanisms and of the types of attacks they are 
 designed for. Then we develop a general overall model within which the security 
services and mechanisms can be viewed.

1.1 cOmputer security cOncepts
A Definition of Computer Security
The NIST Computer Security Handbook [NIST95] defines the term computer 
 security as
Computer Security: The protection afforded to an automated information sys-
tem in order to attain the applicable objectives of preserving the integrity, avail-
ability, and confidentiality of information system resources (includes hardware, 
software, firmware, information/data, and telecommunications).
This definition introduces three key objectives that are at the heart of 
 computer security.
‚óÜ
‚ñ† Confidentiality: This term covers two related concepts:
Data2 confidentiality: Assures that private or confidential information is 
not made available or disclosed to unauthorized individuals.
Privacy: Assures that individuals control or influence what information 
 related to them may be collected and stored and by whom and to whom 
that information may be disclosed.
‚óÜ
‚ñ† Integrity: This term covers two related concepts:
Data integrity: Assures that data (both stored and in transmitted packets) 
and programs are changed only in a specified and authorized manner.
System integrity: Assures that a system performs its intended function in 
an unimpaired manner, free from deliberate or inadvertent unauthorized 
 manipulation of the system.
‚óÜ
‚ñ† Availability: Assures that systems work promptly and service is not denied to 
authorized users.
These three concepts form what is often referred to as the CIA triad. The 
three concepts embody the fundamental security objectives for both data and for 
information and computing services. For example, the NIST Standards for Security 
Categorization of Federal Information and Information Systems (FIPS 199) lists 
confidentiality, integrity, and availability as the three security objectives for infor-
mation and for information systems. FIPS 199 provides a useful characterization of 
these three objectives in terms of requirements and the definition of a loss of secu-
rity in each category.
‚óÜ
‚ñ† Confidentiality: Preserving authorized restrictions on information access and 
disclosure, including means for protecting personal privacy and proprietary in-
formation. A loss of confidentiality is the unauthorized disclosure of information.
2RFC 4949 defines information as ‚Äúfacts and ideas, which can be represented (encoded) as various forms 
of data,‚Äù and data as ‚Äúinformation in a specific physical representation, usually a sequence of symbols 
that have meaning; especially a representation of information that can be processed or produced by a 
computer.‚Äù Security literature typically does not make much of a distinction, nor does this book.

‚óÜ
‚ñ† Integrity: Guarding against improper information modification or destruction, 
including ensuring information nonrepudiation and authenticity. A loss of in-
tegrity is the unauthorized modification or destruction of information.
‚óÜ
‚ñ† Availability: Ensuring timely and reliable access to and use of information. 
A loss of availability is the disruption of access to or use of information or an 
information system.
Although the use of the CIA triad to define security objectives is well estab-
lished, some in the security field feel that additional concepts are needed to present 
a complete picture (Figure 1.1). Two of the most commonly mentioned are
‚óÜ
‚ñ† Authenticity: The property of being genuine and being able to be verified and 
trusted; confidence in the validity of a transmission, a message, or message 
originator. This means verifying that users are who they say they are and that 
each input arriving at the system came from a trusted source.
‚óÜ
‚ñ† Accountability: The security goal that generates the requirement for actions 
of an entity to be traced uniquely to that entity. This supports nonrepudiation, 
deterrence, fault isolation, intrusion detection and prevention, and after-action 
recovery and legal action. Because truly secure systems are not yet an achiev-
able goal, we must be able to trace a security breach to a responsible party. 
Systems must keep records of their activities to permit later forensic analysis 
to trace security breaches or to aid in transaction disputes.
Examples
We now provide some examples of applications that illustrate the requirements just 
enumerated.3 For these examples, we use three levels of impact on organizations or 
individuals should there be a breach of security (i.e., a loss of confidentiality, integ-
rity, or availability). These levels are defined in FIPS 199:
‚óÜ
‚ñ† Low: The loss could be expected to have a limited adverse effect on organiza-
tional operations, organizational assets, or individuals. A limited adverse effect 
means that, for example, the loss of confidentiality, integrity, or availability might 
3These examples are taken from a security policy document published by the Information Technology 
Security and Privacy Office at Purdue University.
Figure 1.1  Essential Network and Computer 
Security Requirements
Data
and
services
Availability
Integrity
Accountability
Authenticity
Confdentiality

(i) cause a degradation in mission capability to an extent and duration that the 
organization is able to perform its primary functions, but the effectiveness of the 
functions is noticeably reduced; (ii) result in minor damage to organizational 
assets; (iii) result in minor financial loss; or (iv) result in minor harm to individuals.
‚óÜ
‚ñ† Moderate: The loss could be expected to have a serious adverse effect on orga-
nizational operations, organizational assets, or individuals. A serious adverse 
effect means that, for example, the loss might (i) cause a significant degrada-
tion in mission capability to an extent and duration that the organization is 
able to perform its primary functions, but the effectiveness of the functions is 
significantly reduced; (ii) result in significant damage to organizational assets; 
(iii) result in significant financial loss; or (iv) result in  significant harm to indi-
viduals that does not involve loss of life or serious, life-threatening injuries.
‚óÜ
‚ñ† High: The loss could be expected to have a severe or catastrophic adverse effect 
on organizational operations, organizational assets, or individuals. A¬†severe or 
catastrophic adverse effect means that, for example, the loss might (i) cause 
a severe degradation in or loss of mission capability to an extent and dura-
tion that the organization is not able to perform one or more of its primary 
functions; (ii) result in major damage to organizational assets; (iii) result in 
major financial loss; or (iv) result in severe or catastrophic harm to individuals 
involving loss of life or serious, life-threatening injuries.
Confidentiality Student grade information is an asset whose confidentiality is 
considered to be highly important by students. In the United States, the release of 
such information is regulated by the Family Educational Rights and Privacy Act 
(FERPA). Grade information should only be available to students, their parents, 
and employees that require the information to do their job. Student enrollment 
information may have a moderate confidentiality rating. While still covered by 
FERPA, this information is seen by more people on a daily basis, is less likely to be 
targeted than grade information, and results in less damage if disclosed. Directory 
information (such as lists of students, faculty, or departmental lists) may be assigned 
a low confidentiality rating or indeed no rating. This information is typically freely 
available to the public and published on a school‚Äôs Web site.
integrity Several aspects of integrity are illustrated by the example of a hospital 
patient‚Äôs allergy information stored in a database. The doctor should be able to 
trust that the information is correct and current. Now suppose that an employee 
(e.g., a nurse) who is authorized to view and update this information deliberately 
falsifies the data to cause harm to the hospital. The database needs to be restored 
to a trusted basis quickly, and it should be possible to trace the error back to the 
person responsible. Patient allergy information is an example of an asset with a high 
requirement for integrity. Inaccurate information could result in serious harm or 
death to a patient and expose the hospital to massive liability.
An example of an asset that may be assigned a moderate level of integrity 
requirement is a Web site that offers a forum to registered users to discuss some 
specific topic. Either a registered user or a hacker could falsify some entries or 
deface the Web site. If the forum exists only for the enjoyment of the users, brings 
in little or no advertising revenue, and is not used for something important such

as research, then potential damage is not severe. The Web master may experience 
some data, financial, and time loss.
An example of a low-integrity requirement is an anonymous online poll. Many 
Web sites, such as news organizations, offer these polls to their users with very few 
safeguards. However, the inaccuracy and unscientific nature of such polls are well 
understood.
availability The more critical a component or service, the higher is the level of 
availability required. Consider a system that provides authentication services for 
critical systems, applications, and devices. An interruption of service results in the 
inability for customers to access computing resources and for the staff to access the 
resources they need to perform critical tasks. The loss of the service translates into 
a large financial loss due to lost employee productivity and potential  customer loss.
An example of an asset that typically would be rated as having a moderate 
availability requirement is a public Web site for a university; the Web site provides 
information for current and prospective students and donors. Such a site is not a 
critical component of the university‚Äôs information system, but its unavailability will 
cause some embarrassment.
An online telephone directory lookup application would be classified as a low-
availability requirement. Although the temporary loss of the application may be 
an annoyance, there are other ways to access the information, such as a hardcopy 
directory or the operator.
The Challenges of Computer Security
Computer and network security is both fascinating and complex. Some of the reasons 
include:
1. Security is not as simple as it might first appear to the novice. The require-
ments seem to be straightforward; indeed, most of the major requirements for 
security services can be given self-explanatory, one-word labels: confidential-
ity, authentication, nonrepudiation, and integrity. But the mechanisms used to 
meet those requirements can be quite complex, and understanding them may 
involve rather subtle reasoning.
2. In developing a particular security mechanism or algorithm, one must always 
consider potential attacks on those security features. In many cases, successful 
attacks are designed by looking at the problem in a completely different way, 
therefore exploiting an unexpected weakness in the mechanism.
3. Because of point 2, the procedures used to provide particular services are 
often counterintuitive. Typically, a security mechanism is complex, and it is not 
obvious from the statement of a particular requirement that such elaborate 
measures are needed. It is only when the various aspects of the threat are con-
sidered that elaborate security mechanisms make sense.
4. Having designed various security mechanisms, it is necessary to decide where 
to use them. This is true both in terms of physical placement (e.g., at what points 
in a network are certain security mechanisms needed) and in a logical sense 
[e.g., at what layer or layers of an architecture such as TCP/IP (Transmission 
Control Protocol/Internet Protocol) should mechanisms be placed].

5. Security mechanisms typically involve more than a particular algorithm or pro-
tocol. They also require that participants be in possession of some secret infor-
mation (e.g., an encryption key), which raises questions about the creation, 
distribution, and protection of that secret information. There also may be a reli-
ance on communications protocols whose behavior may complicate the task of 
developing the security mechanism. For example, if the proper functioning of 
the security mechanism requires setting time limits on the transit time of a 
message from sender to receiver, then any protocol or network that introduces 
variable, unpredictable delays may render such time limits meaningless.
6. Computer and network security is essentially a battle of wits between a per-
petrator who tries to find holes and the designer or administrator who tries to 
close them. The great advantage that the attacker has is that he or she need 
only find a single weakness, while the designer must find and eliminate all 
weaknesses to achieve perfect security.
7. There is a natural tendency on the part of users and system managers to 
 perceive little benefit from security investment until a security failure occurs.
8. Security requires regular, even constant, monitoring, and this is difficult in 
 today‚Äôs short-term, overloaded environment.
9. Security is still too often an afterthought to be incorporated into a system after 
the design is complete rather than being an integral part of the design process.
10. Many users (and even security administrators) view strong security as an 
 impediment to efficient and user-friendly operation of an information system 
or use of information.
The difficulties just enumerated will be encountered in numerous ways as we 
examine the various security threats and mechanisms throughout this book.
 1.2 the Osi security architecture
To assess effectively the security needs of an organization and to evaluate and 
choose various security products and policies, the manager responsible for com-
puter and network security needs some systematic way of defining the requirements 
for security and characterizing the approaches to satisfying those requirements. 
This is difficult enough in a centralized data processing environment; with the use of 
local and wide area networks, the problems are compounded.
ITU-T4 Recommendation X.800, Security Architecture for OSI, defines such a 
systematic approach.5 The OSI security architecture is useful to managers as a way 
of organizing the task of providing security. Furthermore, because this architecture 
4The International Telecommunication Union (ITU) Telecommunication Standardization Sector (ITU-T)  
is a United Nations-sponsored agency that develops standards, called Recommendations, relating to 
 telecommunications and to open systems interconnection (OSI).
5The OSI security architecture was developed in the context of the OSI protocol architecture, which is 
described in Appendix D. However, for our purposes in this chapter, an understanding of the OSI proto-
col architecture is not required.

was developed as an international standard, computer and communications vendors 
have developed security features for their products and services that relate to this 
structured definition of services and mechanisms.
For our purposes, the OSI security architecture provides a useful, if abstract, over-
view of many of the concepts that this book deals with. The OSI security architecture 
focuses on security attacks, mechanisms, and services. These can be defined briefly as
‚óÜ
‚ñ† Security attack: Any action that compromises the security of information 
owned by an organization.
‚óÜ
‚ñ† Security mechanism: A process (or a device incorporating such a process) that 
is designed to detect, prevent, or recover from a security attack.
‚óÜ
‚ñ† Security service: A processing or communication service that enhances the 
security of the data processing systems and the information transfers of an 
organization. The services are intended to counter security attacks, and they 
make use of one or more security mechanisms to provide the service.
In the literature, the terms threat and attack are commonly used to mean more 
or less the same thing. Table 1.1 provides definitions taken from RFC 4949, Internet 
Security Glossary.
 1.3 security attacks
A useful means of classifying security attacks, used both in X.800 and RFC 4949, is 
in terms of passive attacks and active attacks. A passive attack attempts to learn or 
make use of information from the system but does not affect system resources. An 
active attack attempts to alter system resources or affect their operation.
Passive Attacks
Passive attacks (Figure 1.2a) are in the nature of eavesdropping on, or monitoring 
of, transmissions. The goal of the opponent is to obtain information that is being 
transmitted. Two types of passive attacks are the release of message contents and 
traffic analysis.
The release of message contents is easily understood. A telephone conver-
sation, an electronic mail message, and a transferred file may contain sensitive or 
confidential information. We would like to prevent an opponent from learning the 
contents of these transmissions.
Threat
A potential for violation of security, which exists when there is a circumstance, capability, action, 
or event that could breach security and cause harm. That is, a threat is a possible danger that might 
exploit a vulnerability.
Attack
An assault on system security that derives from an intelligent threat. That is, an intelligent act that 
is a deliberate attempt (especially in the sense of a method or technique) to evade security services 
and violate the security policy of a system.
Table 1.1 Threats and Attacks (RFC 4949)

A second type of passive attack, traffic analysis, is subtler. Suppose that we 
had a way of masking the contents of messages or other information traffic so that 
opponents, even if they captured the message, could not extract the information 
from the message. The common technique for masking contents is encryption. If we 
had encryption protection in place, an opponent still might be able to observe the 
pattern of these messages. The opponent could determine the location and identity 
of communicating hosts and could observe the frequency and length of messages 
being exchanged. This information might be useful in guessing the nature of the 
communication that was taking place.
Passive attacks are very difficult to detect, because they do not involve any 
alteration of the data. Typically, the message traffic is sent and received in an 
Figure 1.2 Security Attacks
(a) Passive attacks
Alice
(b) Active attacks
Bob
Darth
Bob
Darth
Alice
Internet or
other comms facility
Internet or
other comms facility
1
2
3

apparently normal fashion, and neither the sender nor the receiver is aware that 
a third party has read the messages or observed the traffic pattern. However, it is 
feasible to prevent the success of these attacks, usually by means of encryption. 
Thus, the emphasis in dealing with passive attacks is on prevention rather than 
detection.
Active Attacks
Active attacks (Figure 1.2b) involve some modification of the data stream or the 
creation of a false stream and can be subdivided into four categories: masquerade, 
replay, modification of messages, and denial of service.
A masquerade takes place when one entity pretends to be a different entity 
(path 2 of Figure 1.2b is active). A masquerade attack usually includes one of the 
other forms of active attack. For example, authentication sequences can be captured 
and replayed after a valid authentication sequence has taken place, thus enabling an 
authorized entity with few privileges to obtain extra privileges by impersonating an 
entity that has those privileges.
Replay involves the passive capture of a data unit and its subsequent retrans-
mission to produce an unauthorized effect (paths 1, 2, and 3 active).
Modification of messages simply means that some portion of a legitimate mes-
sage is altered, or that messages are delayed or reordered, to produce an unauthor-
ized effect (paths 1 and 2 active). For example, a message meaning ‚ÄúAllow John 
Smith to read confidential file accounts‚Äù is modified to mean ‚ÄúAllow Fred Brown 
to read confidential file accounts.‚Äù
The denial of service prevents or inhibits the normal use or management of 
communications facilities (path 3 active). This attack may have a specific target; for 
example, an entity may suppress all messages directed to a particular destination 
(e.g., the security audit service). Another form of service denial is the disruption of 
an entire network‚Äîeither by disabling the network or by overloading it with mes-
sages so as to degrade performance.
Active attacks present the opposite characteristics of passive attacks. Whereas 
passive attacks are difficult to detect, measures are available to prevent their success. 
On the other hand, it is quite difficult to prevent active attacks absolutely because 
of the wide variety of potential physical, software, and network vulnerabilities. 
Instead, the goal is to detect active attacks and to recover from any disruption or 
delays caused by them. If the detection has a deterrent effect, it also may  contribute 
to prevention.
 1.4 security services
X.800 defines a security service as a service that is provided by a protocol layer of 
communicating open systems and that ensures adequate security of the systems or 
of data transfers. Perhaps a clearer definition is found in RFC 4949, which provides 
the following definition: A processing or communication service that is provided by

a system to give a specific kind of protection to system resources; security services 
implement security policies and are implemented by security mechanisms.
X.800 divides these services into five categories and fourteen specific services 
(Table 1.2). We look at each category in turn.6
6There is no universal agreement about many of the terms used in the security literature. For example, the 
term integrity is sometimes used to refer to all aspects of information security. The term authentication is 
sometimes used to refer both to verification of identity and to the various functions listed under integrity 
in this chapter. Our usage here agrees with both X.800 and RFC 4949.
AuTHeNTICATION
The assurance that the communicating entity is the 
one that it claims to be.
Peer entity Authentication
Used in association with a logical connection to 
provide confidence in the identity of the entities 
 connected.
Data-Origin Authentication
In a connectionless transfer, provides assurance that 
the source of received data is as claimed.
ACCeSS CONTROL
The prevention of unauthorized use of a resource 
(i.e., this service controls who can have access to a 
resource, under what conditions access can occur, 
and what those accessing the resource are allowed 
to do).
DATA CONFIDeNTIALITy
The protection of data from unauthorized 
 disclosure.
Connection Confidentiality
The protection of all user data on a connection.
Connectionless Confidentiality
The protection of all user data in a single data block.
Selective-Field Confidentiality
The confidentiality of selected fields within the user 
data on a connection or in a single data block.
Traffic-Flow Confidentiality
The protection of the information that might be 
derived from observation of traffic flows.
DATA INTegRITy
The assurance that data received are exactly 
as sent¬†by an authorized entity (i.e., contain no 
 modification, insertion, deletion, or replay).
Connection Integrity with Recovery
Provides for the integrity of all user data on a 
 connection and detects any modification, insertion, 
deletion, or replay of any data within an entire data 
sequence, with recovery attempted.
Connection Integrity without Recovery
As above, but provides only detection without 
 recovery.
Selective-Field Connection Integrity
Provides for the integrity of selected fields within the 
user data of a data block transferred over a connec-
tion and takes the form of determination of whether 
the selected fields have been modified, inserted, 
deleted, or replayed.
Connectionless Integrity
Provides for the integrity of a single connectionless 
data block and may take the form of detection of 
data modification. Additionally, a limited form of 
replay detection may be provided.
Selective-Field Connectionless Integrity
Provides for the integrity of selected fields within a 
single connectionless data block; takes the form of 
determination of whether the selected fields have 
been modified.
NONRePuDIATION
Provides protection against denial by one of the 
entities involved in a communication of having 
 participated in all or part of the communication.
Nonrepudiation, Origin
Proof that the message was sent by the specified 
party.
Nonrepudiation, Destination
Proof that the message was received by the specified 
party.
Table 1.2 Security Services (X.800)

Authentication
The authentication service is concerned with assuring that a communication is 
 authentic. In the case of a single message, such as a warning or alarm signal, the 
function of the authentication service is to assure the recipient that the message 
is from the source that it claims to be from. In the case of an ongoing interaction, 
such as the connection of a terminal to a host, two aspects are involved. First, at the 
time of connection initiation, the service assures that the two entities are authentic 
(i.e.,¬†that each is the entity that it claims to be). Second, the service must assure that 
the connection is not interfered with in such a way that a third party can masquer-
ade as one of the two legitimate parties for the purposes of unauthorized transmis-
sion or reception.
Two specific authentication services are defined in X.800:
‚óÜ
‚ñ† Peer entity authentication: Provides for the corroboration of the identity of a 
peer entity in an association. Two entities are considered peers if they imple-
ment the same protocol in different systems (e.g., two TCP modules in two 
communicating systems). Peer entity authentication is provided for use at the 
establishment of or during the data transfer phase of a connection. It attempts 
to provide confidence that an entity is not performing either a masquerade or 
an unauthorized replay of a previous connection.
‚óÜ
‚ñ† Data origin authentication: Provides for the corroboration of the source of a 
data unit. It does not provide protection against the duplication or modifica-
tion of data units. This type of service supports applications like electronic 
mail, where there are no prior interactions between the communicating 
entities.
Access Control
In the context of network security, access control is the ability to limit and control 
the access to host systems and applications via communications links. To achieve 
this, each entity trying to gain access must first be identified, or authenticated, so 
that access rights can be tailored to the individual.
Data Confidentiality
Confidentiality is the protection of transmitted data from passive attacks. With re-
spect to the content of a data transmission, several levels of protection can be iden-
tified. The broadest service protects all user data transmitted between two users 
over a period of time. For example, when a TCP connection is set up between two 
systems, this broad protection prevents the release of any user data transmitted over 
the TCP connection. Narrower forms of this service can also be defined, including 
the protection of a single message or even specific fields within a message. These 
refinements are less useful than the broad approach and may even be more complex 
and expensive to implement.
The other aspect of confidentiality is the protection of traffic flow from 
 analysis. This requires that an attacker not be able to observe the source and desti-
nation, frequency, length, or other characteristics of the traffic on a communications 
facility.

Data Integrity
As with confidentiality, integrity can apply to a stream of messages, a single mes-
sage, or selected fields within a message. Again, the most useful and straightforward 
approach is total stream protection.
A connection-oriented integrity service deals with a stream of messages 
and assures that messages are received as sent with no duplication, insertion, 
modification, reordering, or replays. The destruction of data is also covered 
under this service. Thus, the connection-oriented integrity service addresses 
both message stream modification and denial of service. On the other hand, a 
connectionless integrity service deals with individual messages without regard to 
any larger context and generally provides protection against message modifica-
tion only.
We can make a distinction between service with and without recovery. Because 
the integrity service relates to active attacks, we are concerned with detection rather 
than prevention. If a violation of integrity is detected, then the service may simply 
report this violation, and some other portion of software or human intervention is 
required to recover from the violation. Alternatively, there are mechanisms avail-
able to recover from the loss of integrity of data, as we will review subsequently. 
The incorporation of automated recovery mechanisms is typically the more attrac-
tive alternative.
Nonrepudiation
Nonrepudiation prevents either sender or receiver from denying a transmitted mes-
sage. Thus, when a message is sent, the receiver can prove that the alleged sender in 
fact sent the message. Similarly, when a message is received, the sender can prove 
that the alleged receiver in fact received the message.
Availability Service
Both X.800 and RFC 4949 define availability to be the property of a system or a 
system resource being accessible and usable upon demand by an authorized sys-
tem entity, according to performance specifications for the system (i.e., a system 
is available if it provides services according to the system design whenever users 
request them). A variety of attacks can result in the loss of or reduction in avail-
ability. Some of these attacks are amenable to automated countermeasures, such 
as authentication and encryption, whereas others require some sort of physical 
action to prevent or recover from loss of availability of elements of a distributed 
system.
X.800 treats availability as a property to be associated with various security 
services. However, it makes sense to call out specifically an availability service. An 
availability service is one that protects a system to ensure its availability. This ser-
vice addresses the security concerns raised by denial-of-service attacks. It depends 
on proper management and control of system resources and thus depends on access 
control service and other security services.

SPeCIFIC SeCuRITy MeCHANISMS
May be incorporated into the appropriate protocol 
layer in order to provide some of the OSI security 
services.
encipherment
The use of mathematical algorithms to transform 
data into a form that is not readily intelligible. The 
transformation and subsequent recovery of the data 
depend on an algorithm and zero or more encryption 
keys.
Digital Signature
Data appended to, or a cryptographic transformation 
of, a data unit that allows a recipient of the data unit 
to prove the source and integrity of the data unit and 
protect against forgery (e.g., by the recipient).
Access Control
A variety of mechanisms that enforce access rights to 
resources.
Data Integrity
A variety of mechanisms used to assure the integrity 
of a data unit or stream of data units.
Authentication exchange
A mechanism intended to ensure the identity of an 
entity by means of information exchange.
Traffic Padding
The insertion of bits into gaps in a data stream to 
frustrate traffic analysis attempts.
Routing Control
Enables selection of particular physically secure 
routes for certain data and allows routing changes, 
especially when a breach of security is suspected.
Notarization
The use of a trusted third party to assure certain 
properties of a data exchange.
PeRvASIve SeCuRITy MeCHANISMS
Mechanisms that are not specific to any particular 
OSI security service or protocol layer.
Trusted Functionality
That which is perceived to be correct with respect 
to some criteria (e.g., as established by a security 
policy).
Security Label
The marking bound to a resource (which may be 
a data unit) that names or designates the security 
 attributes of that resource.
event Detection
Detection of security-relevant events.
Security Audit Trail
Data collected and potentially used to facilitate a 
security audit, which is an independent review and 
examination of system records and activities.
Security Recovery
Deals with requests from mechanisms, such as event 
handling and management functions, and takes 
recovery actions.
Table 1.3 Security Mechanisms (X.800)
 1.5 security mechanisms
Table 1.3 lists the security mechanisms defined in X.800. The mechanisms are di-
vided into those that are implemented in a specific protocol layer, such as TCP 
or an application-layer protocol, and those that are not specific to any particular 
protocol layer or security service. These mechanisms will be covered in the appro-
priate places in the book, so we do not elaborate now except to comment on the

definition of encipherment. X.800 distinguishes between reversible encipherment 
mechanisms and irreversible encipherment mechanisms. A reversible encipherment 
 mechanism is simply an encryption algorithm that allows data to be encrypted and 
subsequently decrypted. Irreversible encipherment mechanisms include hash algo-
rithms and message authentication codes, which are used in digital signature and 
message  authentication applications.
Table 1.4, based on one in X.800, indicates the relationship between security 
services and security mechanisms.
 1.6 FundamentaL security design principLes
Despite years of research and development, it has not been possible to develop 
security design and implementation techniques that systematically exclude security 
flaws and prevent all unauthorized actions. In the absence of such foolproof tech-
niques, it is useful to have a set of widely agreed design principles that can guide 
the development of protection mechanisms. The National Centers of Academic 
Excellence in Information Assurance/Cyber Defense, which is jointly sponsored by 
the U.S. National Security Agency and the U.S. Department of Homeland Security, 
list the following as fundamental security design principles [NCAE13]:
‚óÜ
‚ñ† Economy of mechanism
‚óÜ
‚ñ† Fail-safe defaults
‚óÜ
‚ñ† Complete mediation
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Y
Peer entity authentication
Service
Mechanism
Encipherment
Digital signature
Access control
Data integrity
Authentication exchange
Trafc padding
Routing control
Notarization
Data origin authentication
Access control
Confdentiality
Trafc fow confdentiality
Data integrity
Nonrepudiation
Availability
Table 1.4 Relationship between Security Services and Mechanisms

‚óÜ
‚ñ† Open design
‚óÜ
‚ñ† Separation of privilege
‚óÜ
‚ñ† Least privilege
‚óÜ
‚ñ† Least common mechanism
‚óÜ
‚ñ† Psychological acceptability
‚óÜ
‚ñ† Isolation
‚óÜ
‚ñ† Encapsulation
‚óÜ
‚ñ† Modularity
‚óÜ
‚ñ† Layering
‚óÜ
‚ñ† Least astonishment
The first eight listed principles were initially proposed in [SALT75] and have 
withstood the test of time. In this section, we briefly discuss each principle.
economy of mechanism means that the design of security measures embod-
ied in both hardware and software should be as simple and small as possible. The 
motivation for this principle is that relatively simple, small design is easier to test 
and verify thoroughly. With a complex design, there are many more opportuni-
ties for an adversary to discover subtle weaknesses to exploit that may be difficult 
to spot ahead of time. The more complex the mechanism, the more likely it is to 
possess exploitable flaws. Simple mechanisms tend to have fewer exploitable flaws 
and require less maintenance. Further, because configuration management issues 
are simplified, updating or replacing a simple mechanism becomes a less intensive 
 process. In practice, this is perhaps the most difficult principle to honor. There is a 
constant demand for new features in both hardware and software, complicating the 
security design task. The best that can be done is to keep this principle in mind dur-
ing system design to try to eliminate unnecessary complexity.
Fail-safe default means that access decisions should be based on permission 
rather than exclusion. That is, the default situation is lack of access, and the protec-
tion scheme identifies conditions under which access is permitted. This approach 
exhibits a better failure mode than the alternative approach, where the default is 
to permit access. A design or implementation mistake in a mechanism that gives 
explicit permission tends to fail by refusing permission, a safe situation that can 
be quickly detected. On the other hand, a design or implementation mistake in a 
mechanism that explicitly excludes access tends to fail by allowing access, a failure 
that may long go unnoticed in normal use. For example, most file access systems 
work on this principle and virtually all protected services on client/server systems 
work this way.
Complete mediation means that every access must be checked against the 
access control mechanism. Systems should not rely on access decisions retrieved 
from a cache. In a system designed to operate continuously, this principle requires 
that, if access decisions are remembered for future use, careful consideration should 
be given to how changes in authority are propagated into such local memories. File 
access systems appear to provide an example of a system that complies with this 
principle. However, typically, once a user has opened a file, no check is made to see 
if permissions change. To fully implement complete mediation, every time a user

reads a field or record in a file, or a data item in a database, the system must exercise 
access control. This resource-intensive approach is rarely used.
Open design means that the design of a security mechanism should be open 
rather than secret. For example, although encryption keys must be secret, encryp-
tion algorithms should be open to public scrutiny. The algorithms can then be 
reviewed by many experts, and users can therefore have high confidence in them. 
This is the philosophy behind the National Institute of Standards and Technology 
(NIST) program of standardizing encryption and hash algorithms and has led to the 
widespread adoption of NIST-approved algorithms.
Separation of privilege is defined in [SALT75] as a practice in which multiple 
privilege attributes are required to achieve access to a restricted resource. A good 
example of this is multifactor user authentication, which requires the use of multiple 
techniques, such as a password and a smart card, to authorize a user. The term is 
also now applied to any technique in which a program is divided into parts that are 
limited to the specific privileges they require in order to perform a specific task. This 
is used to mitigate the potential damage of a computer security attack. One example 
of this latter interpretation of the principle is removing high privilege operations 
to another process and running that process with the higher privileges required to 
perform its tasks. Day-to-day interfaces are executed in a lower privileged process.
Least privilege means that every process and every user of the system should 
operate using the least set of privileges necessary to perform the task. A good exam-
ple of the use of this principle is role-based access control, described in Chapter¬†4. 
The system security policy can identify and define the various roles of users or pro-
cesses. Each role is assigned only those permissions needed to perform its functions. 
Each permission specifies a permitted access to a particular resource (such as read 
and write access to a specified file or directory, connect access to a given host and 
port, etc.). Unless a permission is granted explicitly, the user or process should not 
be able to access the protected resource. More generally, any access control system 
should allow each user only the privileges that are authorized for that user. There 
is also a temporal aspect to the least privilege principle. For example, system pro-
grams or administrators who have special privileges should have those privileges 
only when necessary; when they are doing ordinary activities, the privileges should 
be withdrawn. Leaving them in place just opens the door to accidents.
Least common mechanism means that the design should minimize the func-
tions shared by different users, providing mutual security. This principle helps 
reduce the number of unintended communication paths and reduces the amount of 
hardware and software on which all users depend, thus making it easier to verify if 
there are any undesirable security implications.
Psychological acceptability implies that the security mechanisms should not 
interfere unduly with the work of users, while at the same time meeting the needs of 
those who authorize access. If security mechanisms hinder the usability or accessibil-
ity of resources, then users may opt to turn off those mechanisms. Where possible, 
security mechanisms should be transparent to the users of the system or at most 
introduce minimal obstruction. In addition to not being intrusive or burdensome, 
security procedures must reflect the user‚Äôs mental model of protection. If the protec-
tion procedures do not make sense to the user or if the user must translate his image 
of protection into a substantially different protocol, the user is likely to make errors.

Isolation is a principle that applies in three contexts. First, public access 
systems should be isolated from critical resources (data, processes, etc.) to pre-
vent disclosure or tampering. In cases where the sensitivity or criticality of the 
information is high, organizations may want to limit the number of systems on 
which that data is stored and isolate them, either physically or logically. Physical 
isolation may include ensuring that no physical connection exists between an 
organization‚Äôs public access information resources and an organization‚Äôs criti-
cal information. When implementing logical isolation solutions, layers of secu-
rity services and mechanisms should be established between public systems and 
secure systems responsible for protecting critical resources. Second, the processes 
and files of individual users should be isolated from one another except where 
it is explicitly desired. All modern operating systems provide facilities for such 
isolation, so that individual users have separate, isolated process space, mem-
ory space, and file space, with protections for preventing unauthorized access. 
Finally, security mechanisms should be isolated in the sense of preventing access 
to those mechanisms. For example, logical access control may provide a means 
of isolating cryptographic software from other parts of the host system, and for 
protecting cryptographic software from tampering and the keys from replace-
ment or disclosure.
encapsulation can be viewed as a specific form of isolation based on object-
oriented functionality. Protection is provided by encapsulating a collection of pro-
cedures and data objects in a domain of its own so that the internal structure of a 
data object is accessible only to the procedures of the protected subsystem, and the 
procedures may be called only at designated domain entry points.
Modularity in the context of security refers both to the development of secu-
rity functions as separate, protected modules and to the use of a modular architec-
ture for mechanism design and implementation. With respect to the use of separate 
security modules, the design goal here is to provide common security functions and 
services, such as cryptographic functions, as common modules. For example, numer-
ous protocols and applications make use of cryptographic functions. Rather than 
implementing such functions in each protocol or application, a more secure design 
is provided by developing a common cryptographic module that can be invoked by 
numerous protocols and applications. The design and implementation effort can 
then focus on the secure design and implementation of a single cryptographic mod-
ule and including mechanisms to protect the module from tampering. With respect 
to the use of a modular architecture, each security mechanism should be able to 
support migration to new technology or upgrade of new features without requiring 
an entire system redesign. The security design should be modular so that individual 
parts of the security design can be upgraded without the requirement to modify the 
entire system.
Layering refers to the use of multiple, overlapping protection approaches 
addressing the people, technology, and operational aspects of information systems. 
By using multiple, overlapping protection approaches, the failure or circumven-
tion of any individual protection approach will not leave the system unprotected. 
We will see throughout this text that a layering approach is often used to provide 
multiple barriers between an adversary and protected information or services. This 
technique is often referred to as defense in depth.

Least astonishment means that a program or user interface should always 
respond in the way that is least likely to astonish the user. For example, the mecha-
nism for authorization should be transparent enough to a user that the user has a 
good intuitive understanding of how the security goals map to the provided security 
mechanism.
 1.7 attack surFaces and attack trees
In Section 1.3, we provided an overview of the spectrum of security threats and 
attacks facing computer and network systems. Section 11.1 goes into more  detail 
about the nature of attacks and the types of adversaries that present security threats. 
This section elaborates on two concepts that are useful in evaluating and classifying 
threats: attack surfaces and attack trees.
Attack Surfaces
An attack surface consists of the reachable and exploitable vulnerabilities in a 
 system [MANA11, HOWA03]. Examples of attack surfaces are the following:
‚óÜ
‚ñ† Open ports on outward facing Web and other servers, and code listening on 
those ports
‚óÜ
‚ñ† Services available on the inside of a firewall
‚óÜ
‚ñ† Code that processes incoming data, e-mail, XML, office documents, and 
 industry-specific custom data exchange formats
‚óÜ
‚ñ† Interfaces, SQL, and Web forms
‚óÜ
‚ñ† An employee with access to sensitive information vulnerable to a social 
 engineering attack
Attack surfaces can be categorized in the following way:
‚óÜ
‚ñ† Network attack surface: This category refers to vulnerabilities over an enter-
prise network, wide-area network, or the Internet. Included in this category 
are network protocol vulnerabilities, such as those used for a denial-of- service 
attack, disruption of communications links, and various forms of intruder 
attacks.
‚óÜ
‚ñ† Software attack surface: This refers to vulnerabilities in application, utility, 
or operating system code. A particular focus in this category is Web server 
software.
‚óÜ
‚ñ† Human attack surface: This category refers to vulnerabilities created by 
personnel or outsiders, such as social engineering, human error, and trusted 
insiders.
An attack surface analysis is useful for assessing the scale and severity of 
threats to a system. A systematic analysis of points of vulnerability makes develop-
ers and security analysts aware of where security mechanisms are required. Once 
an attack surface is defined, designers may be able to find ways to make the surface

smaller, thus making the task of the adversary more difficult. The attack surface 
also provides guidance on setting priorities for testing, strengthening security mea-
sures, or modifying the service or application.
As illustrated in Figure 1.3, the use of layering, or defense in depth, and attack 
surface reduction complement each other in mitigating security risk.
Attack Trees
An attack tree is a branching, hierarchical data structure that represents a set of 
potential techniques for exploiting security vulnerabilities [MAUW05, MOOR01, 
SCHN99]. The security incident that is the goal of the attack is represented as the 
root node of the tree, and the ways that an attacker could reach that goal are itera-
tively and incrementally represented as branches and subnodes of the tree. Each 
subnode defines a subgoal, and each subgoal may have its own set of further sub-
goals, etc. The final nodes on the paths outward from the root, that is, the leaf nodes, 
represent different ways to initiate an attack. Each node other than a leaf is either 
an AND-node or an OR-node. To achieve the goal represented by an AND-node, 
the subgoals represented by all of that node‚Äôs subnodes must be achieved; and for 
an OR-node, at least one of the subgoals must be achieved. Branches can be labeled 
with values representing difficulty, cost, or other attack attributes, so that alterna-
tive attacks can be compared.
The motivation for the use of attack trees is to effectively exploit the infor-
mation available on attack patterns. Organizations such as CERT publish security 
advisories that have enabled the development of a body of knowledge about both 
general attack strategies and specific attack patterns. Security analysts can use the 
attack tree to document security attacks in a structured form that reveals key vul-
nerabilities. The attack tree can guide both the design of systems and applications, 
and the choice and strength of countermeasures.
Figure 1.4, based on a figure in [DIMI07], is an example of an attack tree 
analysis for an Internet banking authentication application. The root of the tree is 
Figure 1.3 Defense in Depth and Attack Surface
Attack surface
Medium
security risk
High
security risk
Low
security risk
Deep
Layering
Shallow
Small
Large
Medium
security risk

the objective of the attacker, which is to compromise a user‚Äôs account. The shaded 
boxes on the tree are the leaf nodes, which represent events the comprise the 
attacks. Note that in this tree in this example, all the nodes other than leaf nodes 
are OR-nodes. The analysis to generate this tree considered the three components 
involved in authentication:
‚óÜ
‚ñ† user terminal and user (uT/u): These attacks target the user equipment, 
 including the tokens that may be involved, such as smartcards or other pass-
word generators, as well as the actions of the user.
‚óÜ
‚ñ† Communications channel (CC): This type of attack focuses on communication 
links.
‚óÜ
‚ñ† Internet banking server (IBS): These types of attacks are offline attacks against 
the servers that host the Internet banking application.
Figure 1.4 An Attack Tree for Internet Banking Authentication
Bank account compromise
User credential compromise
User credential guessing
UT/U1a User surveillance
UT/U1b Theft of token and
handwritten notes
Malicious software
installation
Vulnerability exploit
UT/U2a Hidden code
UT/U2b Worms
UT/U3a Smartcard analyzers
UT/U2c E-mails with
malicious code
UT/U3b Smartcard reader
manipulator
UT/U3c Brute force attacks
with PIN calculators
CC2 Snifng
UT/U4a Social engineering
IBS3 Web site manipulation
UT/U4b Web page
obfuscation
CC1 Pharming
Redirection of
communication toward
fraudulent site
CC3 Active man-in-the
middle attacks
IBS1 Brute force attacks
User communication
with attacker
Injection of commands
Use of known authenticated
session by attacker
Normal user authentication
with specifed session ID
CC4 Pre-defned session
IDs (session hijacking)
IBS2 Security policy
violation

Five overall attack strategies can be identified, each of which exploits one or 
more of the three components. The five strategies are as follows:
‚óÜ
‚ñ† user credential compromise: This strategy can be used against many elements 
of the attack surface. There are procedural attacks, such as monitoring a  user‚Äôs 
action to observe a PIN or other credential, or theft of the user‚Äôs token or 
handwritten notes. An adversary may also compromise token information 
using a variety of token attack tools, such as hacking the smartcard or using a 
brute force approach to guess the PIN. Another possible strategy is to embed 
malicious software to compromise the user‚Äôs login and password. An adver-
sary may also attempt to obtain credential information via the communication 
channel (sniffing). Finally, an adversary may use various means to engage in 
communication with the target user, as shown in Figure 1.4.
‚óÜ
‚ñ† Injection of commands: In this type of attack, the attacker is able to intercept 
communication between the UT and the IBS. Various schemes can be used to 
be able to impersonate the valid user and so gain access to the banking system.
‚óÜ
‚ñ† user credential guessing: It is reported in [HILT06] that brute force attacks 
against some banking authentication schemes are feasible by sending ran-
dom usernames and passwords. The attack mechanism is based on distributed 
zombie personal computers, hosting automated programs for username- or 
 password-based calculation.
‚óÜ
‚ñ† Security policy violation: For example, violating the bank‚Äôs security policy in 
combination with weak access control and logging mechanisms, an employee 
may cause an internal security incident and expose a customer‚Äôs account.
‚óÜ
‚ñ† use of known authenticated session: This type of attack persuades or forces 
the user to connect to the IBS with a preset session ID. Once the user authen-
ticates to the server, the attacker may utilize the known session ID to send 
packets to the IBS, spoofing the user‚Äôs identity.
Figure 1.4 provides a thorough view of the different types of attacks on an 
Internet banking authentication application. Using this tree as a starting point, secu-
rity analysts can assess the risk of each attack and, using the design principles out-
lined in the preceding section, design a comprehensive security facility. [DIMI07] 
provides a good account of the results of this design effort.
 1.8 a mOdeL FOr netwOrk security
A model for much of what we will be discussing is captured, in very general terms, 
in Figure 1.5. A message is to be transferred from one party to another across some 
sort of Internet service. The two parties, who are the principals in this transaction, 
must cooperate for the exchange to take place. A logical information channel is 
established by defining a route through the Internet from source to destination 
and by the cooperative use of communication protocols (e.g., TCP/IP) by the two 
principals.

Security aspects come into play when it is necessary or desirable to protect the 
information transmission from an opponent who may present a threat to confiden-
tiality, authenticity, and so on. All of the techniques for providing security have two 
components:
1. A security-related transformation on the information to be sent. Examples in-
clude the encryption of the message, which scrambles the message so that it is 
unreadable by the opponent, and the addition of a code based on the contents 
of the message, which can be used to verify the identity of the sender.
2. Some secret information shared by the two principals and, it is hoped,  unknown 
to the opponent. An example is an encryption key used in conjunction with the 
transformation to scramble the message before transmission and unscramble 
it on reception.7
A trusted third party may be needed to achieve secure transmission. For 
example, a third party may be responsible for distributing the secret information 
to the two principals while keeping it from any opponent. Or a third party may be 
needed to arbitrate disputes between the two principals concerning the authenticity 
of a message transmission.
This general model shows that there are four basic tasks in designing a 
 particular security service:
1. Design an algorithm for performing the security-related transformation. The 
algorithm should be such that an opponent cannot defeat its purpose.
2. Generate the secret information to be used with the algorithm.
7Chapter 3 discusses a form of encryption, known as asymmetric encryption, in which only one of the two 
principals needs to have the secret information.
Figure 1.5 Model for Network Security
Information
channel
Security-related
transformation
Sender
Secret
information
Message
Message
Secure
message
Secure
message
Recipient
Opponent
Trusted third party
(e.g., arbiter, distributer
of secret information)
Security-related
transformation
Secret
information

3. Develop methods for the distribution and sharing of the secret information.
4. Specify a protocol to be used by the two principals that make use of the  security 
algorithm and the secret information to achieve a particular security service.
Parts One and Two of this book concentrate on the types of security mech-
anisms and services that fit into the model shown in Figure 1.5. However, there 
are other security-related situations of interest that do not neatly fit this model 
but are considered in this book. A general model of these other situations is illus-
trated by Figure 1.6, which reflects a concern for protecting an information system 
from unwanted access. Most readers are familiar with the concerns caused by the 
existence of hackers who attempt to penetrate systems that can be accessed over 
a network. The hacker can be someone who, with no malign intent, simply gets 
satisfaction from breaking and entering a computer system. The intruder can be a 
disgruntled employee who wishes to do damage or a criminal who seeks to exploit 
computer assets for financial gain (e.g., obtaining credit card numbers or perform-
ing illegal money transfers).
Another type of unwanted access is the placement in a computer system of 
logic that exploits vulnerabilities in the system and that can affect application pro-
grams as well as utility programs, such as editors and compilers. Programs can pres-
ent two kinds of threats:
1. Information access threats: Intercept or modify data on behalf of users who 
should not have access to that data.
2. Service threats: Exploit service flaws in computers to inhibit use by legitimate 
users.
Viruses and worms are two examples of software attacks. Such attacks can be 
introduced into a system by means of a disk that contains the unwanted logic con-
cealed in otherwise useful software. They also can be inserted into a system across a 
network; this latter mechanism is of more concern in network security.
The security mechanisms needed to cope with unwanted access fall into two 
broad categories (see Figure 1.6). The first category might be termed a gatekeeper 
function. It includes password-based login procedures that are designed to deny 
access to all but authorized users and screening logic that is designed to detect and 
reject worms, viruses, and other similar attacks. Once either an unwanted user 
Figure 1.6 Network Access Security Model
Computing resources
 
(processor, memory, I/O)
Data
Processes
Software
Internal security controls
Information system
Gatekeeper
function
Opponent
‚Äîhuman (e.g., hacker)
‚Äîsoftware
         (e.g., virus, worm)
Access channel

or unwanted software gains access, the second line of defense consists of a vari-
ety of internal controls that monitor activity and analyze stored information in an 
attempt to detect the presence of unwanted intruders. These issues are explored in 
Part¬†Three.
 1.9 standards
Many of the security techniques and applications described in this book have 
been specified as standards. Additionally, standards have been developed to 
cover  management practices and the overall architecture of security mechanisms 
and  services. Throughout this book, we describe the most important standards in 
use or being developed for various aspects of cryptography and network security. 
Various organizations have been involved in the development or promotion of 
these  standards. The most important (in the current context) of these organizations 
are as follows.
‚óÜ
‚ñ† National Institute of Standards and Technology: NIST is a U.S. federal agency 
that deals with measurement science, standards, and technology related to 
U.S. government use and to the promotion of U.S. private-sector innovation. 
Despite its national scope, NIST Federal Information Processing Standards 
(FIPS) and Special Publications (SP) have a worldwide impact.
‚óÜ
‚ñ† Internet Society: ISOC is a professional membership society with worldwide 
organizational and individual membership. It provides leadership in address-
ing issues that confront the future of the Internet and is the organization home 
for the groups responsible for Internet infrastructure standards, including the 
Internet Engineering Task Force (IETF) and the Internet Architecture Board 
(IAB). These organizations develop Internet standards and related specifica-
tions, all of which are published as Requests for Comments (RFCs).
A more detailed discussion of these organizations is contained in Appendix C.
 1.10 key terms, review QuestiOns, and prObLems
Key Terms 
access control
active attack
authentication
authenticity
availability
data confidentiality
data integrity
denial of service
encryption
integrity
intruder
masquerade
nonrepudiation
OSI security architecture
passive attack
replay
security attacks
security mechanisms
security services
traffic analysis

Review Questions 
 
1.1 
What is the OSI security architecture?
 
1.2 
Briefly explain masquerade attack with an example.
 
1.3 
What is the difference between security threats and attacks?
 
1.4 
Why are passive attacks difficult to detect and active attacks difficult to prevent?
 
1.5 
Identify the different security attacks prevented by the security mechanisms defined 
in X.800.
 
1.6 
List and briefly define the fundamental security design principles.
 
1.7 
Explain the difference between an attack surface and an attack tree.
Problems 
 
1.1 
Consider an automated teller machine (ATM) in which users provide a personal 
identification number (PIN) and a card for account access. Give examples of confi-
dentiality, integrity, and availability requirements associated with the system. In each 
case, indicate the degree of importance of the requirement.
 
1.2 
Repeat Problem 1.1 for a telephone switching system that routes calls through a 
switching network based on the telephone number requested by the caller.
 
1.3 
Consider a desktop publishing system used to produce documents for various 
organizations.
a. Give an example of a type of publication for which confidentiality of the stored 
data is the most important requirement.
b. Give an example of a type of publication in which data integrity is the most 
 important requirement.
c. Give an example in which system availability is the most important requirement.
 
1.4 
For each of the following assets, assign a low, moderate, or high impact level for the 
loss of confidentiality, availability, and integrity, respectively. Justify your answers.
a. A portal maintained by the Government to provide information regarding its 
 departments and services.
b. A hospital managing the medical records of its patients.
c. A financial organization managing routine administrative information (not 
 privacy-related information).
d. An information system used for large acquisitions in a contracting organization 
that contains both sensitive, pre-solicitation phase contract information and rou-
tine administrative information. Assess the impact for the two data sets separately 
and the information system as a whole.
e. The Examinations department of a University maintains examination  particulars, 
such as question papers of forthcoming examinations, grades obtained, and 
 examiner details. The University‚Äôs administrative department maintains the 
 students‚Äô attendance particulars and internal assessment results. Assess the impact 
for the two data sets separately and the information system as a whole.
 
1.5 
Draw a matrix similar to Table 1.4 that shows the relationship between security 
 attacks and mechanisms.
 
1.6 
Draw a matrix similar to Table 1.4 that shows the relationship between security 
mechanisms and services.
 
1.7 
Develop an attack tree for gaining access to customer account details from the 
 database of a bank.
 
1.8 
Consider a company whose operations are housed in two buildings on the same prop-
erty; one building is headquarters, the other building contains network and computer 
services. The property is physically protected by a fence around the perimeter. The 
only entrance to the property is through the fenced perimeter. In addition to the

perimeter fence, physical security consists of a guarded front gate. The local  networks 
are split between the Headquarters‚Äô LAN and the Network Services‚Äô LAN. Internet 
users connect to the Web server through a firewall. Dial-up users get access to a 
particular server on the Network Services‚Äô LAN. Develop an attack tree in which 
the root node represents disclosure of proprietary secrets. Include physical, social 
 engineering, and technical attacks. The tree may contain both AND and OR nodes. 
Develop a tree that has at least 15 leaf nodes.
 
1.9 
Read all of the classic papers cited in the Recommended Reading section for this 
 chapter, available at the Author Web site at WilliamStallings.com/NetworkSecurity. 
The papers are available at box.com/NetSec6e. Compose a 500‚Äì1000 word paper (or 
8 to 12 slide PowerPoint presentation) that summarizes the key concepts that emerge 
from these papers, emphasizing concepts that are common to most or all of the papers.

Part One: CryPtOgraPhy
Chapter
Symmetric Encryption and 
Message Confidentiality
2.1 
Symmetric Encryption Principles
Cryptography
Cryptanalysis
Feistel Cipher Structure
2.2 
Symmetric Block Encryption Algorithms
Data Encryption Standard
Triple DES
Advanced Encryption Standard
2.3 
Random and Pseudorandom Numbers
The Use of Random Numbers
TRNGs, PRNGs, and PRFs
Algorithm Design
2.4 
Stream Ciphers and RC4
Stream Cipher Structure
The RC4 Algorithm
2.5 
Cipher Block Modes of Operation
Electronic Codebook Mode
Cipher Block Chaining Mode
Cipher Feedback Mode
Counter Mode
2.6 
Key Terms, Review Questions, and Problems

Symmetric encryption, also referred to as conventional encryption, secret-key, or 
single-key encryption, was the only type of encryption in use prior to the develop-
ment of public-key encryption in the late 1970s.1 It remains by far the most widely 
used of the two types of encryption.
This chapter begins with a look at a general model for the symmetric 
 encryption process; this will enable us to understand the context within which the 
 algorithms are used. Then we look at three important block encryption algorithms: 
DES,  triple DES, and AES. This is followed by a discussion of random and pseu-
dorandom number generation. Next, the chapter introduces symmetric stream 
 encryption and describes the widely used stream cipher RC4. Finally, we look at the 
important topic of block cipher modes of operation.
 2.1 Symmetric encryption principleS
A symmetric encryption scheme has five ingredients (Figure 2.1):
‚ñ†
‚ñ† Plaintext: This is the original message or data that is fed into the algorithm 
as¬†input.
‚ñ†
‚ñ† Encryption algorithm: The encryption algorithm performs various substitutions 
and transformations on the plaintext.
‚ñ†
‚ñ† Secret key: The secret key is also input to the algorithm. The exact substitutions 
and transformations performed by the algorithm depend on the key.
1Public-key encryption was first described in the open literature in 1976; the National Security Agency 
(NSA) claims to have discovered it some years earlier.
learning objectiveS
After studying this chapter, you should be able to:
‚óÜ‚ñ†
Present an overview of the main concepts of symmetric cryptography.
‚óÜ‚ñ†
Explain the difference between cryptanalysis and brute-force attack.
‚óÜ‚ñ†
Summarize the functionality of DES.
‚óÜ‚ñ†
Present an overview of AES.
‚óÜ‚ñ†
Explain the concepts of randomness and unpredictability with respect to 
random numbers.
‚óÜ‚ñ†
Understand the differences among true random number generators, 
 pseudorandom number generators, and pseudorandom functions.
‚óÜ‚ñ†
Present an overview of stream ciphers and RC4.
‚óÜ‚ñ†
Compare and contrast ECB, CBC, CFB, and counter modes of operation.

Figure 2.1 Simplified Model of Symmetric Encryption
Plaintext
input
Y = E[K, X]
X = D[K, Y]
X
K
K
Transmitted
ciphertext
Plaintext
output
Secret key shared by
sender and recipient
Secret key shared by
sender and recipient
Encryption algorithm
(e.g., AES)
Decryption algorithm
(reverse of encryption
algorithm)
‚ñ†
‚ñ† Ciphertext: This is the scrambled message produced as output. It depends on 
the plaintext and the secret key. For a given message, two different keys will 
produce two different ciphertexts.
‚ñ†
‚ñ† Decryption algorithm: This is essentially the encryption algorithm run in re-
verse. It takes the ciphertext and the same secret key and produces the original 
plaintext.
There are two requirements for secure use of symmetric encryption:
1. We need a strong encryption algorithm. At a minimum, we would like the algo-
rithm to be such that an opponent who knows the algorithm and has access to 
one or more ciphertexts would be unable to decipher the ciphertext or figure 
out the key. This requirement is usually stated in a stronger form: The oppo-
nent should be unable to decrypt ciphertext or discover the key even if he or 
she is in possession of a number of ciphertexts together with the plaintext that 
produced each ciphertext.
2. Sender and receiver must have obtained copies of the secret key in a secure 
fashion and must keep the key secure. If someone can discover the key and 
knows the algorithm, all communication using this key is readable.
It is important to note that the security of symmetric encryption depends on 
the secrecy of the key, not the secrecy of the algorithm. That is, it is assumed that 
it is impractical to decrypt a message on the basis of the ciphertext plus knowledge 
of the encryption/decryption algorithm. In other words, we do not need to keep the 
algorithm secret; we need to keep only the key secret.
This feature of symmetric encryption is what makes it feasible for widespread 
use. The fact that the algorithm need not be kept secret means that manufactur-
ers can and have developed low-cost chip implementations of data encryption 
 algorithms. These chips are widely available and incorporated into a number of 
products. With the use of symmetric encryption, the principal security problem is 
maintaining the secrecy of the key.

Cryptography
Cryptographic systems are generically classified along three independent 
dimensions:
1. The type of operations used for transforming plaintext to ciphertext. All 
 encryption algorithms are based on two general principles: substitution, in 
which each element in the plaintext (bit, letter, group of bits or letters) is 
mapped into another element; and transposition, in which elements in the 
plaintext are rearranged. The fundamental requirement is that no informa-
tion be lost (i.e., that all operations be reversible). Most systems, referred to 
as product systems, involve multiple stages of substitutions and transpositions.
2. The number of keys used. If both sender and receiver use the same key, the 
system is referred to as symmetric, single-key, secret-key, or conventional 
encryption. If the sender and receiver each use a different key, the system is 
 referred to as asymmetric, two-key, or public-key encryption.
3. The way in which the plaintext is processed. A block cipher processes the 
input one block of elements at a time, producing an output block for each 
input block. A stream cipher processes the input elements continuously, pro-
ducing output one element at a time, as it goes along.
Cryptanalysis
The process of attempting to discover the plaintext or key is known as  cryptanalysis. 
The strategy used by the cryptanalyst depends on the nature of the encryption 
scheme and the information available to the cryptanalyst.
Table 2.1 summarizes the various types of cryptanalytic attacks based on the 
amount of information known to the cryptanalyst. The most difficult problem is 
presented when all that is available is the ciphertext only. In some cases, not even 
the encryption algorithm is known, but in general, we can assume that the opponent 
does know the algorithm used for encryption. One possible attack under these cir-
cumstances is the brute-force approach of trying all possible keys. If the key space 
is very large, this becomes impractical. Thus, the opponent must rely on an analysis 
of the ciphertext itself, generally applying various statistical tests to it. To use this 
approach, the opponent must have some general idea of the type of plaintext that 
is concealed, such as English or French text, an EXE file, a Java source listing, an 
accounting file, and so on.
The ciphertext-only attack is the easiest to defend against because the oppo-
nent has the least amount of information to work with. In many cases, however, 
the analyst has more information. The analyst may be able to capture one or more 
plaintext messages as well as their encryptions. Or the analyst may know that cer-
tain plaintext patterns will appear in a message. For example, a file that is encoded 
in the Postscript format always begins with the same pattern, or there may be a 
standardized header or banner to an electronic funds transfer message, and so on. 
All of these are examples of known plaintext. With this knowledge, the analyst may 
be able to deduce the key on the basis of the way in which the known plaintext is 
transformed.

Type of Attack
Known to Cryptanalyst
Ciphertext only
‚ñ†‚ñ†Encryption algorithm
‚ñ†‚ñ†Ciphertext to be decoded
Known plaintext
‚ñ†‚ñ†Encryption algorithm
‚ñ†‚ñ†Ciphertext to be decoded
‚ñ†‚ñ†One or more plaintext‚Äìciphertext pairs formed with the secret key
Chosen plaintext
‚ñ†‚ñ†Encryption algorithm
‚ñ†‚ñ†Ciphertext to be decoded
‚ñ†‚ñ†‚ñ†Plaintext message chosen by cryptanalyst, together with its corresponding 
 ciphertext generated with the secret key
Chosen ciphertext
‚ñ†‚ñ†Encryption algorithm
‚ñ†‚ñ†Ciphertext to be decoded
‚ñ†‚ñ†‚ñ†Purported ciphertext chosen by cryptanalyst, together with its corresponding 
decrypted plaintext generated with the secret key
Chosen text
‚ñ†‚ñ†Encryption algorithm
‚ñ†‚ñ†Ciphertext to be decoded
‚ñ†‚ñ†‚ñ†Plaintext message chosen by cryptanalyst, together with its corresponding 
 ciphertext generated with the secret key
‚ñ†‚ñ†‚ñ†Purported ciphertext chosen by cryptanalyst, together with its corresponding 
decrypted plaintext generated with the secret key
Table 2.1 Types of Attacks on Encrypted Messages
Closely related to the known-plaintext attack is what might be referred to as a 
probable-word attack. If the opponent is working with the encryption of some gen-
eral prose message, he or she may have little knowledge of what is in the message. 
However, if the opponent is after some very specific information, then parts of the 
message may be known. For example, if an entire accounting file is being transmit-
ted, the opponent may know the placement of certain key words in the header of 
the file. As another example, the source code for a program developed by a corpo-
ration might include a copyright statement in some standardized position.
If the analyst is able somehow to get the source system to insert into the sys-
tem a message chosen by the analyst, then a chosen-plaintext attack is possible. In 
general, if the analyst is able to choose the messages to encrypt, the analyst may 
deliberately pick patterns that can be expected to reveal the structure of the key.
Table 2.1 lists two other types of attack: chosen ciphertext and chosen text. 
These are less commonly employed as cryptanalytic techniques but are nevertheless 
possible avenues of attack.
Only relatively weak algorithms fail to withstand a ciphertext-only attack. 
Generally, an encryption algorithm is designed to withstand a known-plaintext 
attack.
An encryption scheme is computationally secure if the ciphertext generated 
by the scheme meets one or both of the following criteria:
‚ñ†
‚ñ† The cost of breaking the cipher exceeds the value of the encrypted information.
‚ñ†
‚ñ† The time required to break the cipher exceeds the useful lifetime of the 
information.

Unfortunately, it is very difficult to estimate the amount of effort required 
to cryptanalyze ciphertext successfully. However, assuming there are no inherent 
mathematical weaknesses in the algorithm, then a brute-force approach is indicated. 
A brute-force attack involves trying every possible key until an intelligible transla-
tion of the ciphertext into plaintext is obtained. On average, half of all possible keys 
must be tried to achieve success. That is, if there are x different keys, on average 
an attacker would discover the actual key after x/2 tries. It is important to note that 
there is more to a brute-force attack than simply running through all possible keys. 
Unless known plaintext is provided, the analyst must be able to recognize plaintext 
as plaintext. If the message is just plaintext in English, then the result pops out eas-
ily, although the task of recognizing English would have to be automated. If the text 
message has been compressed before encryption, then recognition is more difficult. 
And if the message is some more general type of data, such as a numerical file, and 
this has been compressed, the problem becomes even more difficult to automate. 
Thus, to supplement the brute-force approach, some degree of knowledge about 
the expected plaintext is needed, and some means of automatically distinguishing 
plaintext from garble is also needed.
Feistel Cipher Structure
Many symmetric block encryption algorithms, including DES, have a structure first 
described by Horst Feistel of IBM in 1973 [FEIS73] and shown in Figure 2.2. The 
inputs to the encryption algorithm are a plaintext block of length 2w bits and a key 
K. The plaintext block is divided into two halves, LE0 and RE0. The two halves 
of the data pass through n rounds of processing and then combine to produce the 
 ciphertext block. Each round i has as inputs LEi-1 and REi-1 derived from the pre-
vious round, as well as a subkey Ki derived from the overall K. In general, the sub-
keys Ki are different from K and from each other and are generated from the key 
by a subkey generation algorithm. In Figure 2.2, 16 rounds are used, although any 
number of rounds could be implemented. The right-hand side of Figure 2.2 shows 
the decryption process.
All rounds have the same structure. A substitution is performed on the left 
half of the data. This is done by applying a round function F to the right half of the 
data and then taking the exclusive-OR (XOR) of the output of that function and 
the left half of the data. The round function has the same general structure for each 
round but is parameterized by the round subkey Ki. Following this  substitution, 
a permutation is performed that consists of the interchange of the two halves of 
the¬†data.
The Feistel structure is a particular example of the more general structure 
used by all symmetric block ciphers. In general, a symmetric block cipher consists of 
a sequence of rounds, with each round performing substitutions and permutations 
conditioned by a secret key value. The exact realization of a symmetric block cipher 
depends on the choice of the following parameters and design features.
‚ñ†
‚ñ† Block size: Larger block sizes mean greater security (all other things being 
equal) but reduced encryption/decryption speed. A block size of 128 bits is a 
reasonable trade-off and is nearly universal among recent block cipher designs.

Figure 2.2 Feistel Encryption and Decryption (16 rounds)
Output (ciphertext)
K1
LD0 = RE16
RD0 = LE16
LD2 = RE14
RD2 = LE14
LD14 = RE2
RD14 = LE2
LD16 = RE0
LD17 = RE0
RD16 = LE0
RD17 = LE0
RD1 = LE15
LD1 = RE15
RD15 = LE1
LD15 = RE1
Input (ciphertext)
Output (plaintext)
Round 1
K1
K2
K15
K16
K2
K15
K16
F
LE0
RE0
Input (plaintext)
LE1
RE1
LE2
RE2
F
F
LE14
RE14
LE15
RE15
LE16
RE16
LE17
RE17
F
F
F
F
F
Round 2
Round 15
Round 16
Round 16
Round 15
Round 2
Round 1
‚ñ†
‚ñ† Key size: Larger key size means greater security but may decrease  encryption/
decryption speed. The most common key length in modern algorithms is 
128¬†bits.
‚ñ†
‚ñ† Number of rounds: The essence of a symmetric block cipher is that a single 
round offers inadequate security but that multiple rounds offer increasing 
 security. A typical size is from 10 to 16 rounds.

‚ñ†
‚ñ† Subkey generation algorithm: Greater complexity in this algorithm should 
lead to greater difficulty of cryptanalysis.
‚ñ†
‚ñ† Round function: Again, greater complexity generally means greater resistance 
to cryptanalysis.
There are two other considerations in the design of a symmetric block cipher:
‚ñ†
‚ñ† Fast software encryption/decryption: In many cases, encryption is embedded 
in applications or utility functions in such a way as to preclude a hardware im-
plementation. Accordingly, the speed of execution of the algorithm becomes a 
concern.
‚ñ†
‚ñ† Ease of analysis: Although we would like to make our algorithm as difficult as 
possible to cryptanalyze, there is great benefit in making the algorithm easy 
to analyze. That is, if the algorithm can be concisely and clearly explained, it is 
easier to analyze that algorithm for cryptanalytic vulnerabilities and therefore 
develop a higher level of assurance as to its strength. DES, for example, does 
not have an easily analyzed functionality.
Decryption with a symmetric block cipher is essentially the same as the 
 encryption process. The rule is as follows: Use the ciphertext as input to the algo-
rithm, but use the subkeys Ki in reverse order. That is, use Kn in the first round, 
Kn-1 in the second round, and so on until K1 is used in the last round. This is a nice 
feature, because it means we need not implement two different algorithms‚Äîone for 
encryption and one for decryption.
 2.2 Symmetric block encryption algorithmS
The most commonly used symmetric encryption algorithms are block ciphers. A 
block cipher processes the plaintext input in fixed-sized blocks and produces a 
block of ciphertext of equal size for each plaintext block. This section focuses on 
the three most important symmetric block ciphers: the Data Encryption Standard 
(DES), triple DES (3DES), and the Advanced Encryption Standard (AES).
Data Encryption Standard
Until the introduction of the Advanced Encryption Standard in 2001, the most 
widely used encryption scheme was based on the Data Encryption Standard (DES) 
issued in 1977 as Federal Information Processing Standard 46 (FIPS 46) by the 
National Bureau of Standards, now known as the National Institute of Standards 
and Technology (NIST). The algorithm itself is referred to as the Data Encryption 
Algorithm (DEA).2
2The terminology is a bit confusing. Until recently, the terms DES and DEA could be used interchangeably. 
However, the most recent edition of the DES document includes a specification of the DEA  described 
here plus the triple DEA (3DES) described subsequently. Both DEA and 3DES are part of the Data 
Encryption Standard. Furthermore, until the recent adoption of the official term 3DES, the triple DEA 
algorithm was typically referred to as triple DES and written as 3DES. For the sake of convenience, we 
will use 3DES.

Description of the Algorithm The plaintext is 64 bits in length and the key is 
56 bits in length; longer plaintext amounts are processed in 64-bit blocks. The DES 
structure is a minor variation of the Feistel network shown in Figure 2.2. There are 
16 rounds of processing. From the original 56-bit key, 16 subkeys are generated, one 
of which is used for each round.
The process of decryption with DES is essentially the same as the encryption 
process. The rule is as follows: Use the ciphertext as input to the DES algorithm, but 
use the subkeys Ki in reverse order. That is, use K16 on the first iteration, K15 on the 
second iteration, and so on until K1 is used on the 16th and last iteration.
the strength of Des Concerns about the strength of DES fall into two catego-
ries: concerns about the algorithm itself and concerns about the use of a 56-bit key. 
The first concern refers to the possibility that cryptanalysis is possible by exploiting 
the characteristics of the DES algorithm. Over the years, there have been numerous 
attempts to find and exploit weaknesses in the algorithm, making DES the most-
studied encryption algorithm in existence. Despite numerous approaches, no one 
has so far succeeded in discovering a fatal weakness in DES.3
A more serious concern is key length. With a key length of 56 bits, there are 
256 possible keys, which is approximately 7.2 * 1016 keys. Thus, on the face of it, a 
brute-force attack appears impractical. Assuming that on average half the key space 
has to be searched, a single machine performing one DES encryption per microsec-
ond would take more than a thousand years to break the cipher.
However, the assumption of one encryption per microsecond is overly conser-
vative. DES finally and definitively proved insecure in July 1998, when the Electronic 
Frontier Foundation (EFF) announced that it had broken a DES encryption using 
a special-purpose ‚ÄúDES cracker‚Äù machine that was built for less than $250,000. The 
attack took less than three days. The EFF has published a detailed description of the 
machine, enabling others to build their own cracker [EFF98]. And, of course, hard-
ware prices will continue to drop as speeds increase, making DES virtually worthless.
With current technology, it is not even necessary to use special, purpose-built 
hardware. Rather, the speed of commercial, off-the-shelf processors threaten the 
security of DES. A paper from Seagate Technology [SEAG08] suggests that a rate 
of one billion (109) key combinations per second is reasonable for today‚Äôs multicore 
computers. Recent offerings confirm this. Both Intel and AMD now offer hardware-
based instructions to accelerate the use of AES. Tests run on a contemporary multi-
core Intel machine resulted in an encryption rate of about half a billion [BASU12]. 
Another recent analysis suggests that with contemporary supercomputer technol-
ogy, a rate of 1013 encryptions/s is reasonable [AROR12].
Considering these results, Table 2.2 shows how much time is required for a 
brute-force attack for various key sizes. As can be seen, a single PC can break DES 
in about a year; if multiple PCs work in parallel, the time is drastically shortened. 
And today‚Äôs supercomputers should be able to find a key in about an hour. Key 
sizes of 128 bits or greater are effectively unbreakable using simply a brute-force ap-
proach. Even if we managed to speed up the attacking system by a factor of 1  trillion 
(1012), it would still take over 100,000 years to break a code using a 128-bit key.
3At least, no one has publicly acknowledged such a discovery.

Figure 2.3 Triple DES
E
K1
D
K2
E
K3
P
A
B
C
(a) Encryption
D
K3
E
K2
D
K1
C
B
A
P
(b) Decryption
Fortunately, there are a number of alternatives to DES, the most important of 
which are triple DES and AES, discussed in the remainder of this section.
Triple DES
Triple DES (3DES) was first standardized for use in financial applications in ANSI 
standard X9.17 in 1985. 3DES was incorporated as part of the Data Encryption 
Standard in 1999 with the publication of FIPS 46-3.
3DES uses three keys and three executions of the DES algorithm. The func-
tion follows an encrypt-decrypt-encrypt (EDE) sequence (Figure 2.3a):
C = E(K3, D(K2, E(K1, P)))
where
C = ciphertext
P = plaintext
E[K, X] = encryption of X using key K
D[K, Y] = decryption of Y using key K
Table 2.2 Average Time Required for Exhaustive Key Search
Key Size (bits)
Cipher
Number of 
Alternative Keys
Time Required at 109 
Decryptions/s
Time Required  
at 1013 
Decryptions/s
56
DES
256 ‚âà 7.2 * 1016
255 ns = 1.125 years
1 hour
128
AES
2128 ‚âà 3.4 * 1038
2127 ns = 5.3 * 1021 years
5.3 * 1017 years
168
Triple DES
2168 ‚âà 3.7 * 1050
2167 ns = 5.8 * 1033 years
5.8 * 1029 years
192
AES
2192 ‚âà 6.3 * 1057
2191 ns = 9.8 * 1040 years
9.8 * 1036 years
256
AES
2256 ‚âà 1.2 * 1077
2255 ns = 1.8 * 1060 years
1.8 * 1056 years

Decryption is simply the same operation with the keys reversed (Figure 2.3b):
P = D(K1, E(K2, D(K3, C)))
There is no cryptographic significance to the use of decryption for the second 
stage of 3DES encryption. Its only advantage is that it allows users of 3DES to 
 decrypt data encrypted by users of the older single DES:
C = E(K1, D(K1, E(K1, P))) = E[K, P]
With three distinct keys, 3DES has an effective key length of 168 bits. FIPS 
46-3 also allows for the use of two keys, with K1 = K3; this provides for a key length 
of 112 bits. FIPS 46-3 includes the following guidelines for 3DES.
‚ñ†
‚ñ† 3DES is the FIPS-approved symmetric encryption algorithm of choice.
‚ñ†
‚ñ† The original DES, which uses a single 56-bit key, is permitted under the  standard 
for legacy systems only. New procurements should support 3DES.
‚ñ†
‚ñ† Government organizations with legacy DES systems are encouraged to 
 transition to 3DES.
‚ñ†
‚ñ† It is anticipated that 3DES and the Advanced Encryption Standard (AES) will 
coexist as FIPS-approved algorithms, allowing for a gradual transition to AES.
It is easy to see that 3DES is a formidable algorithm. Because the underlying 
cryptographic algorithm is DEA, 3DES can claim the same resistance to cryptanaly-
sis based on the algorithm as is claimed for DEA. Furthermore, with a 168-bit key 
length, brute-force attacks are effectively impossible.
Ultimately, AES is intended to replace 3DES, but this process will take a 
number of years. NIST anticipates that 3DES will remain an approved algorithm 
(for U.S. government use) for the foreseeable future.
Advanced Encryption Standard
3DES has two attractions that assure its widespread use over the next few years. 
First, with its 168-bit key length, it overcomes the vulnerability to brute-force attack 
of DEA. Second, the underlying encryption algorithm in 3DES is the same as in 
DEA. This algorithm has been subjected to more scrutiny than any other encryp-
tion algorithm over a longer period of time, and no effective cryptanalytic attack 
based on the algorithm rather than brute force has been found. Accordingly, there 
is a high level of confidence that 3DES is very resistant to cryptanalysis. If security 
were the only consideration, then 3DES would be an appropriate choice for a stan-
dardized encryption algorithm for decades to come.
The principal drawback of 3DES is that the algorithm is relatively sluggish 
in software. The original DEA was designed for mid-1970s hardware implementa-
tion and does not produce efficient software code. 3DES, which has three times 
as many rounds as DEA, is correspondingly slower. A secondary drawback is that 
both DEA and 3DES use a 64-bit block size. For reasons of both efficiency and 
 security, a larger block size is desirable.
Because of these drawbacks, 3DES is not a reasonable candidate for long-
term use. As a replacement, NIST in 1997 issued a call for proposals for a new

Advanced Encryption Standard (AES), which should have a security strength equal 
to or better than 3DES and significantly improved efficiency. In addition to these 
general requirements, NIST specified that AES must be a symmetric block cipher 
with a block length of 128 bits and support for key lengths of 128, 192, and 256 bits. 
Evaluation criteria included security, computational efficiency, memory require-
ments, hardware and software suitability, and flexibility.
In a first round of evaluation, 15 proposed algorithms were accepted. A sec-
ond round narrowed the field to five algorithms. NIST completed its evaluation 
process and published a final standard (FIPS PUB 197) in November of 2001. NIST 
selected Rijndael as the proposed AES algorithm. The two researchers who devel-
oped and submitted Rijndael for the AES are both cryptographers from Belgium: 
Dr. Joan Daemen and Dr. Vincent Rijmen.
overview of the Algorithm AES uses a block length of 128 bits and a key length 
that can be 128, 192, or 256 bits. In the description of this section, we assume a key 
length of 128 bits, which is likely to be the one most commonly implemented.
The input to the encryption and decryption algorithms is a single 128-bit 
block. In FIPS PUB 197, this block is depicted as a square matrix of bytes. This 
block is copied into the State array, which is modified at each stage of encryption or 
decryption. After the final stage, State is copied to an output matrix. Similarly, the 
128-bit key is depicted as a square matrix of bytes. This key is then expanded into an 
array of key schedule words: Each word is four bytes and the total key schedule is 
44 words for the 128-bit key. The ordering of bytes within a matrix is by column. So, 
for example, the first four bytes of a 128-bit plaintext input to the encryption cipher 
occupy the first column of the in matrix, the second four bytes occupy the second 
column, and so on. Similarly, the first four bytes of the expanded key, which form a 
word, occupy the first column of the w matrix.
The following comments give some insight into AES.
1. One noteworthy feature of this structure is that it is not a Feistel structure. 
Recall that in the classic Feistel structure, half of the data block is used to 
modify the other half of the data block, and then the halves are swapped. AES 
does not use a Feistel structure but processes the entire data block in parallel 
during each round using substitutions and permutation.
2. The key that is provided as input is expanded into an array of forty-four 32-bit 
words, w[i]. Four distinct words (128 bits) serve as a round key for each round.
3. Four different stages are used, one of permutation and three of substitution 
(Figure 2.4):
‚ñ†
‚ñ† Substitute bytes: Uses a table, referred to as an S-box,4 to perform a byte-
by-byte substitution of the block.
‚ñ†
‚ñ† Shift rows: A simple permutation that is performed row by row.
4The term S-box, or substitution box, is commonly used in the description of symmetric ciphers to refer to 
a table used for a table-lookup type of substitution mechanism.

‚ñ†
‚ñ† Mix columns: A substitution that alters each byte in a column as a function 
of all of the bytes in the column.
‚ñ†
‚ñ† Add round key: A simple bitwise XOR of the current block with a portion 
of the expanded key.
4. The structure is quite simple. For both encryption and decryption, the cipher 
begins with an Add Round Key stage, followed by nine rounds that each 
 includes all four stages, followed by a tenth round of three stages. Figure 2.5 
depicts the structure of a full encryption round.
Figure 2.4 AES Encryption and Decryption
Add round key
w[4, 7]
Plaintext
(16 bytes)
Plaintext
(16 bytes)
Substitute bytes
Expand key
Shift rows
Mix columns
Round 1
Round 9
Round 10
Add round key
‚Ä¢
‚Ä¢
‚Ä¢
Substitute bytes
Shift rows
Mix columns
Add round key
Substitute bytes
Shift rows
Add round key
Ciphertext
(16 bytes)
(a) Encryption
Key
(16 bytes)
Add round key
Inverse sub bytes
Inverse shift rows
Inverse mix cols
Round 10
Round 9
Round 1
Add round key
‚Ä¢
‚Ä¢
‚Ä¢
Inverse sub bytes
Inverse shift rows
Inverse mix cols
Add round key
Inverse sub bytes
Inverse shift rows
Add round key
Ciphertext
(16 bytes)
(b) Decryption
w[36, 39]
w[40, 43]
w[0, 3]

S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
S
SubBytes
State
State
State
State
State
ShiftRows
MixColumns
AddRoundKey
M
M
M
M
r0
r1
r2
r3
r4
r5
r6
r7
r8
r9
r10
r11
r12
r13
r14
r15
Figure 2.5 AES Encryption Round

5. Only the Add Round Key stage makes use of the key. For this reason, the 
 cipher begins and ends with an Add Round Key stage. Any other stage,  applied 
at the beginning or end, is reversible without knowledge of the key and so 
would add no security.
6. The Add Round Key stage by itself would not be formidable. The other three 
stages together scramble the bits, but by themselves, they would provide no 
security because they do not use the key. We can view the cipher as alternat-
ing operations of XOR encryption (Add Round Key) of a block, followed by 
scrambling of the block (the other three stages), followed by XOR encryption, 
and so on. This scheme is both efficient and highly secure.
7. Each stage is easily reversible. For the Substitute Byte, Shift Row, and Mix 
Columns stages, an inverse function is used in the decryption algorithm. For 
the Add Round Key stage, the inverse is achieved by XORing the same round 
key to the block, using the result that A ‚äï B ‚äï B = A.
8. As with most block ciphers, the decryption algorithm makes use of the 
 expanded key in reverse order. However, the decryption algorithm is not 
identical to the encryption algorithm. This is a consequence of the particular 
 structure of AES.
9. Once it is established that all four stages are reversible, it is easy to verify 
that decryption does recover the plaintext. Figure 2.4 lays out encryption 
and decryption going in opposite vertical directions. At each horizontal point 
(e.g., the dashed line in the figure), State is the same for both encryption and 
decryption.
10. The final round of both encryption and decryption consists of only three stages. 
Again, this is a consequence of the particular structure of AES and is required 
to make the cipher reversible.
 2.3 random and pSeudorandom numberS
Random numbers play an important role in the use of encryption for various 
 network security applications. We provide an overview in this section. The topic is 
examined in more detail in Appendix E.
The Use of Random Numbers
A number of network security algorithms based on cryptography make use of 
 random numbers. For example,
‚ñ†
‚ñ† Generation of keys for the RSA public-key encryption algorithm (described in 
Chapter 3) and other public-key algorithms.
‚ñ†
‚ñ† Generation of a stream key for symmetric stream cipher (discussed in the 
 following section).

‚ñ†
‚ñ† Generation of a symmetric key for use as a temporary session key. This func-
tion is used in a number of networking applications, such as Transport Layer 
Security (Chapter 5), Wi-Fi (Chapter 6), e-mail security (Chapter 7), and IP 
security (Chapter 8).
‚ñ†
‚ñ† In a number of key distribution scenarios, such as Kerberos (Chapter 4), 
 random numbers are used for handshaking to prevent replay attacks.
These applications give rise to two distinct and not necessarily compatible 
 requirements for a sequence of random numbers: randomness and unpredictability.
rAnDomness Traditionally, the concern in the generation of a sequence of allegedly 
random numbers has been that the sequence of numbers be random in some well-
defined statistical sense. The following criteria are used to validate that a sequence 
of numbers is random.
‚ñ†
‚ñ† Uniform distribution: The distribution of bits in the sequence should be 
 uniform; that is, the frequency of occurrence of ones and zeros should be 
approximately the same.
‚ñ†
‚ñ† Independence: No one subsequence in the sequence can be inferred from 
the¬†others.
Although there are well-defined tests for determining that a sequence of num-
bers matches a particular distribution, such as the uniform distribution, there is no 
such test to ‚Äúprove‚Äù independence. Rather, a number of tests can be applied to 
demonstrate if a sequence does not exhibit independence. The general strategy is 
to apply a number of such tests until the confidence that independence exists is 
 sufficiently strong.
In the context of our discussion, the use of a sequence of numbers that appear 
statistically random often occurs in the design of algorithms related to cryptogra-
phy. For example, a fundamental requirement of the RSA public-key encryption 
scheme discussed in Chapter 3 is the ability to generate prime numbers. In gen-
eral, it is difficult to determine if a given large number N is prime. A brute-force 
approach would be to divide N by every odd integer less than 2N. If N is on the 
order, say, of 10150 (a not uncommon occurrence in public-key cryptography), such 
a brute-force approach is beyond the reach of human analysts and their computers. 
However, a number of effective algorithms exist that test the primality of a num-
ber by using a sequence of randomly chosen integers as input to relatively simple 
computations. If the sequence is sufficiently long (but far, far less than 210150), 
the primality of a number can be determined with near certainty. This type of 
approach, known as randomization, crops up frequently in the design of algorithms. 
In essence, if a problem is too hard or time-consuming to solve exactly, a simpler, 
shorter approach based on randomization is used to provide an answer with any 
desired level of confidence.
UnpreDictAbility In applications such as reciprocal authentication and session key 
generation, the requirement is not so much that the sequence of numbers be statisti-
cally random but that the successive members of the sequence are unpredictable. 
With ‚Äútrue‚Äù random sequences, each number is statistically independent of other

numbers in the sequence and therefore unpredictable. However, as is discussed 
shortly, true random numbers are not always used; rather, sequences of numbers 
that appear to be random are generated by some algorithm. In this latter case, 
care must be taken that an opponent not be able to predict future elements of the 
 sequence on the basis of earlier elements.
TRNGs, PRNGs, and PRFs
Cryptographic applications typically make use of algorithmic techniques for ran-
dom number generation. These algorithms are deterministic and therefore produce 
sequences of numbers that are not statistically random. However, if the algorithm is 
good, the resulting sequences will pass many reasonable tests of randomness. Such 
numbers are referred to as pseudorandom numbers.
You may be somewhat uneasy about the concept of using numbers generated 
by a deterministic algorithm as if they were random numbers. Despite what might 
be called philosophical objections to such a practice, it generally works. That is, 
under most circumstances, pseudorandom numbers will perform as well as if they 
were random for a given use. The phrase ‚Äúas well as‚Äù is unfortunately subjective, but 
the use of pseudorandom numbers is widely accepted. The same principle applies 
in statistical application, in which a statistician takes a sample of a population and 
assumes that the results will be approximately the same as if the whole population 
were measured.
Figure 2.6 contrasts a true random number generator (TRNG) with two 
forms of pseudorandom number generators. A TRNG takes as input a source that 
is effectively random; the source is often referred to as an entropy source. In es-
sence, the entropy source is drawn from the physical environment of the computer 
Figure 2.6 Random and Pseudorandom Number Generators
Conversion
to binary
Source of
true
randomness
Random
bit stream
(a) TRNG
TRNG = true random number generator
PRNG = pseudorandom number generator
PRF = pseudorandom function
Deterministic
algorithm
Seed
Pseudorandom
bit stream
(b) PRNG
Deterministic
algorithm
Seed
Pseudorandom
value
(c) PRF
Context-
specifc
values

and could include things such as keystroke timing patterns, disk electrical activ-
ity, mouse movements, and instantaneous values of the system clock. The source, 
or combination of sources, serves as input to an algorithm that produces random 
binary output. The TRNG may simply involve conversion of an analog source to a 
binary output. The TRNG may involve additional processing to overcome any bias 
in the source.
In contrast, a PRNG takes as input a fixed value, called the seed, and produces 
a sequence of output bits using a deterministic algorithm. Typically, as shown in 
Figure 2.6, there is some feedback path by which some of the results of the algo-
rithm are fed back as input as additional output bits are produced. The important 
thing to note is that the output bit stream is determined solely by the input value or 
values, so that an adversary who knows the algorithm and the seed can reproduce 
the entire bit stream.
Figure 2.6 shows two different forms of PRNGs, based on application.
‚ñ†
‚ñ† Pseudorandom number generator: An algorithm that is used to produce an 
open-ended sequence of bits is referred to as a PRNG. A common application 
for an open-ended sequence of bits is as input to a symmetric stream cipher, as 
discussed in the following section.
‚ñ†
‚ñ† Pseudorandom function (PRF): A PRF is used to produce a pseudorandom 
string of bits of some fixed length. Examples are symmetric encryption keys 
and nonces. Typically, the PRF takes as input a seed plus some context specific 
values, such as a user ID or an application ID. A number of examples of PRFs 
will be seen throughout this book.
Other than the number of bits produced, there is no difference between a 
PRNG and a PRF. The same algorithms can be used in both applications. Both 
 require a seed and both must exhibit randomness and unpredictability. Furthermore, 
a PRNG application may also employ context-specific input.
Algorithm Design
Cryptographic PRNGs have been the subject of much research over the years, 
and a wide variety of algorithms have been developed. These fall roughly into two 
categories:
‚ñ†
‚ñ† Purpose-built algorithms: These are algorithms designed specifically and 
solely for the purpose of generating pseudorandom bit streams. Some of these 
algorithms are used for a variety of PRNG applications; several of these are 
described in the next section. Others are designed specifically for use in a 
stream cipher. The most important example of the latter is RC4, described in 
the next section.
‚ñ†
‚ñ† Algorithms based on existing cryptographic algorithms: Cryptographic algo-
rithms have the effect of randomizing input. Indeed, this is a requirement of 
such algorithms. For example, if a symmetric block cipher produced ciphertext 
that had certain regular patterns in it, it would aid in the process of crypt-
analysis. Thus, cryptographic algorithms can serve as the core of PRNGs. Three

broad categories of cryptographic algorithms are commonly used to create 
PRNGs:
‚ÄîSymmetric block ciphers
‚ÄîAsymmetric ciphers
‚ÄîHash functions and message authentication codes
Any of these approaches can yield a cryptographically strong PRNG. 
A¬† purpose-built algorithm may be provided by an operating system for general use. 
For applications that already use certain cryptographic algorithms for encryption or 
authentication, it makes sense to re-use the same code for the PRNG. Thus, all of 
these approaches are in common use.
 2.4 Stream cipherS and rc4
A block cipher processes the input one block of elements at a time, producing an 
output block for each input block. A stream cipher processes the input elements 
continuously, producing output one element at a time as it goes along. Although 
block ciphers are far more common, there are certain applications in which a stream 
cipher is more appropriate. Examples are given subsequently in this book. In this 
section, we look at perhaps the most popular symmetric stream cipher, RC4. We 
begin with an overview of stream cipher structure, and then examine RC4.
Stream Cipher Structure
A typical stream cipher encrypts plaintext one byte at a time, although a stream 
cipher may be designed to operate on one bit at a time or on units larger than a 
byte at a time. Figure 2.7 is a representative diagram of stream cipher structure. 
In this structure, a key is input to a pseudorandom bit generator that produces a 
Figure 2.7 Stream Cipher Diagram
Pseudorandom byte
generator
(key stream generator)
Plaintext
byte stream
M
Key
K
Key
K
k
Plaintext
byte stream
M
Ciphertext
byte stream
C
ENCRYPTION
Pseudorandom byte
generator
(key stream generator)
DECRYPTION
k

stream of 8-bit numbers that are apparently random. The pseudorandom stream is 
unpredictable without knowledge of the input key and has an apparently random 
character. The output of the generator, called a keystream, is combined one byte at 
a time with the plaintext stream using the bitwise exclusive-OR (XOR) operation. 
For example, if the next byte generated by the generator is 01101100 and the next 
plaintext byte is 11001100, then the resulting ciphertext byte is
11001100 plaintext
‚äï 01101100 key stream
10100000 ciphertext
Decryption requires the use of the same pseudorandom sequence:
10100000 ciphertext
‚äï 01101100 key stream
11001100 plaintext
[KUMA97] lists the following important design considerations for a stream 
cipher.
1. The encryption sequence should have a large period. A pseudorandom num-
ber generator uses a function that produces a deterministic stream of bits that 
eventually repeats. The longer the period of repeat, the more difficult it will be 
to do cryptanalysis.
2. The keystream should approximate the properties of a true random number 
stream as close as possible. For example, there should be an approximately 
equal number of 1s and 0s. If the keystream is treated as a stream of bytes, 
then all of the 256 possible byte values should appear approximately equally 
often. The more random-appearing the keystream is, the more randomized the 
ciphertext is, making cryptanalysis more difficult.
3. Note from Figure 2.7 that the output of the pseudorandom number genera-
tor is conditioned on the value of the input key. To guard against brute-force 
 attacks, the key needs to be sufficiently long. The same considerations as apply 
for block ciphers are valid here. Thus, with current technology, a key length of 
at least 128 bits is desirable.
With a properly designed pseudorandom number generator, a stream cipher 
can be as secure as block cipher of comparable key length. A potential advantage 
of a stream cipher is that stream ciphers that do not use block ciphers as a building 
block are typically faster and use far less code than do block ciphers. The example 
in this chapter, RC4, can be implemented in just a few lines of code. In recent years, 
this advantage has diminished with the introduction of AES, which is quite efficient 
in software. Furthermore, hardware acceleration techniques are now available for 
AES. For example, the Intel AES Instruction Set has machine instructions for one 
round of encryption and decryption and key generation. Using the hardware in-
structions results in speedups of about an order of magnitude compared to pure 
software implementations [XU10].

One advantage of a block cipher is that you can reuse keys. In contrast, if two 
plaintexts are encrypted with the same key using a stream cipher, then cryptanalysis 
is often quite simple [DAWS96]. If the two ciphertext streams are XORed together, 
the result is the XOR of the original plaintexts. If the plaintexts are text strings, 
credit card numbers, or other byte streams with known properties, then cryptanaly-
sis may be successful.
For applications that require encryption/decryption of a stream of data (such 
as over a data-communications channel or a browser/Web link), a stream cipher 
might be the better alternative. For applications that deal with blocks of data (such 
as file transfer, e-mail, and database), block ciphers may be more appropriate. 
However, either type of cipher can be used in virtually any application.
The RC4 Algorithm
RC4 is a stream cipher designed in 1987 by Ron Rivest for RSA Security. It is 
a variable key-size stream cipher with byte-oriented operations. The algorithm 
is based on the use of a random permutation. Analysis shows that the period of 
the cipher is overwhelmingly likely to be greater than 10100 [ROBS95a]. Eight 
to sixteen machine operations are required per output byte, and the cipher can 
be expected to run very quickly in software. RC4 is used in the Secure Sockets 
Layer/Transport Layer Security (SSL/TLS) standards that have been defined 
for communication between Web browsers and servers. It is also used in the 
Wired Equivalent Privacy (WEP) protocol and the newer WiFi Protected Access 
(WPA) protocol that are part of the IEEE 802.11 wireless LAN standard. RC4 
was kept as a trade secret by RSA Security. In September 1994, the RC4 algo-
rithm was anonymously posted on the Internet on the Cypherpunks anonymous 
remailers list.
The RC4 algorithm is remarkably simple and quite easy to explain. A vari-
able-length key of from 1 to 256 bytes (8 to 2048 bits) is used to initialize a 256-byte 
state vector S, with elements S[0], S[1],¬†.¬†.¬†.¬†, S[255]. At all times, S contains a per-
mutation of all 8-bit numbers from 0 through 255. For encryption and decryption, 
a byte k (see Figure 2.7) is generated from S by selecting one of the 255 entries in a 
systematic fashion. As each value of k is generated, the entries in S are once again 
permuted.
initiAlizAtion of s To begin, the entries of S are set equal to the values from 0 
through 255 in ascending order; that is, S[0] = 0, S[1] = 1, c , S[255] = 255. A 
temporary vector, T, is also created. If the length of the key K is 256 bytes, then 
K is transferred to T. Otherwise, for a key of length keylen bytes, the first keylen 
 elements of T are copied from K, and then K is repeated as many times as necessary 
to fill out T. These preliminary operations can be summarized as:
/* Initialization */
for i = 0 to 255 do
S[i] = i;
T[i] = K[i mod keylen];

Next we use T to produce the initial permutation of S. This involves starting 
with S[0] and going through to S[255] and, for each S[i], swapping S[i] with another 
byte in S according to a scheme dictated by T[i]:
/* Initial Permutation of S */
j = 0;
for i = 0 to 255 do
j = (j + S[i] + T[i]) mod 256;
Swap (S[i], S[j]);
Because the only operation on S is a swap, the only effect is a permutation.  
S still contains all the numbers from 0 through 255.
streAm generAtion Once the S vector is initialized, the input key is no longer 
used. Stream generation involves cycling through all the elements of S[i] and, for 
each S[i], swapping S[i] with another byte in S according to a scheme dictated by the 
current configuration of S. After S[255] is reached, the process continues, starting 
over again at S[0]:
/* Stream Generation */
i, j = 0;
while (true)
i = (i + 1) mod 256;
j = (j + S[i]) mod 256;
Swap (S[i], S[j]);
t = (S[i] + S[j]) mod 256;
k = S[t];
To encrypt, XOR the value k with the next byte of plaintext. To decrypt, XOR 
the value k with the next byte of ciphertext.
Figure 2.8 illustrates the RC4 logic.
strength of rc4 A number of papers have been published analyzing meth-
ods of attacking RC4 (e.g., [KNUD98], [FLUH00], [MANT01]). None of these 
 approaches is practical against RC4 with a reasonable key length, such as 
128¬†bits. A more serious problem is reported in [FLUH01]. The authors dem-
onstrate that the WEP protocol, intended to provide confidentiality on 802.11 
wireless LAN networks, is vulnerable to a particular attack approach. In essence, 
the problem is not with RC4 itself but the way in which keys are generated for 
use as input to RC4. This particular problem does not appear to be relevant to 
other  applications using RC4 and can be remedied in WEP by changing the way 
in which keys are generated. This problem points out the difficulty in designing 
a secure system that involves both cryptographic functions and protocols that 
make use of them.

253 254 255
4
3
2
1
0
S
T
S
(a) Initial state of S and T
(b) Initial permutation of S
Swap
T
K
T[i]
j = j + S[i] + T[i]
t = S[i] + S[j]
S[i]
S[j]
keylen
i
S
(c) Stream generation
Swap
j = j + S[i]
S[i]
S[j]
S[t]
k
i
Figure 2.8 RC4

2.5 cipher block modeS of operation
A symmetric block cipher processes one block of data at a time. In the case 
of DES and 3DES, the block length is b = 64 bits; for AES, the block length is 
b = 128 bits. For longer amounts of plaintext, it is necessary to break the plain-
text into b-bit blocks (padding the last block if necessary). To apply a block cipher 
in a variety of applications, five modes of operation have been defined by NIST 
(Special Publication 800-38A). The five modes are intended to cover virtually all 
of the possible applications of encryption for which a block cipher could be used. 
These modes are intended for use with any symmetric block cipher, including triple 
DES and AES. The most important modes are described briefly in the remainder 
of this section.
Electronic Codebook Mode
The simplest way to proceed is using what is known as electronic codebook (ECB) 
mode, in which plaintext is handled b bits at a time and each block of plaintext is 
encrypted using the same key. The term codebook is used because, for a given key, 
there is a unique ciphertext for every b-bit block of plaintext. Therefore, one can 
imagine a gigantic codebook in which there is an entry for every possible b-bit plain-
text pattern showing its corresponding ciphertext.
With ECB, if the same b-bit block of plaintext appears more than once in 
the message, it always produces the same ciphertext. Because of this, for lengthy 
messages, the ECB mode may not be secure. If the message is highly structured, 
it may be possible for a cryptanalyst to exploit these regularities. For example, 
if it is known that the message always starts out with certain predefined fields, 
then the cryptanalyst may have a number of known plaintext‚Äìciphertext pairs to 
work with. If the message has repetitive elements with a period of repetition a 
multiple of b bits, then these elements can be identified by the analyst. This may 
help in the analysis or may provide an opportunity for substituting or rearranging 
blocks.
To overcome the security deficiencies of ECB, we would like a technique in 
which the same plaintext block, if repeated, produces different ciphertext blocks.
Cipher Block Chaining Mode
In the cipher block chaining (CBC) mode (Figure 2.9), the input to the encryption 
algorithm is the XOR of the current plaintext block and the preceding ciphertext 
block; the same key is used for each block. In effect, we have chained together the 
processing of the sequence of plaintext blocks. The input to the encryption func-
tion for each plaintext block bears no fixed relationship to the plaintext block. 
Therefore, repeating patterns of b bits are not exposed.
For decryption, each cipher block is passed through the decryption algorithm. 
The result is XORed with the preceding ciphertext block to produce the plaintext 
block. To see that this works, we can write
Cj = E(K, [Cj-1 ‚äï Pj])

where E[K, X] is the encryption of plaintext X using key K, and ‚äï  is the exclusive-
OR operation. Then
 D(K, Cj) = D(K, E(K, [Cj-1 ‚äï Pj]))
 D(K, Cj) = Cj-1 ‚äï Pj
 Cj-1 ‚äï D(K, Cj) = Cj-1 ‚äï Cj-1 ‚äï Pj = Pj
which verifies Figure 2.9b.
To produce the first block of ciphertext, an initialization vector (IV) is XORed 
with the first block of plaintext. On decryption, the IV is XORed with the output of 
the decryption algorithm to recover the first block of plaintext.
The IV must be known to both the sender and receiver. For maximum secu-
rity, the IV should be protected as well as the key. This could be done by sending 
the IV using ECB encryption. One reason for protecting the IV is as follows: If an 
opponent is able to fool the receiver into using a different value for IV, then the 
opponent is able to invert selected bits in the first block of plaintext. To see this, 
consider the following:
 C1 = E(K, [IV ‚äï P1])
 P1 = IV ‚äï D(K, C1)
Figure 2.9 Cipher Block Chaining (CBC) Mode
C1
P1
Encrypt
IV
K
P2
C2
Encrypt
K
PN
CN
CN‚Äì1
Encrypt
K
(a) Encryption
P1
C1
Decrypt
IV
K
C2
P2
Decrypt
K
CN
PN
CN‚Äì1
Decrypt
K
(b) Decryption

Now use the notation that X[j] denotes the jth bit of the b-bit quantity X. Then
P1[i] = IV[i] ‚äï D(K, C1)[i]
Then, using the properties of XOR, we can state
P1[i]‚Ä≤ = IV[i]‚Ä≤ ‚äï D(K, C1)[i]
where the prime notation denotes bit complementation. This means that if an 
 opponent can predictably change bits in IV, the corresponding bits of the received 
value of P1 can be changed.
Cipher Feedback Mode
It is possible to convert any block cipher into a stream cipher by using the cipher 
feedback (CFB) mode. A stream cipher eliminates the need to pad a message to be 
an integral number of blocks. It also can operate in real time. Thus, if a character 
stream is being transmitted, each character can be encrypted and transmitted im-
mediately using a character-oriented stream cipher.
One desirable property of a stream cipher is that the ciphertext be of the same 
length as the plaintext. Thus, if 8-bit characters are being transmitted, each char-
acter should be encrypted using 8 bits. If more than 8 bits are used, transmission 
capacity is wasted.
Figure 2.10 depicts the CFB scheme. In the figure, it is assumed that the unit 
of transmission is s bits; a common value is s = 8. As with CBC, the units of plain-
text are chained together, so that the ciphertext of any plaintext unit is a function of 
all the preceding plaintext.
First, consider encryption. The input to the encryption function is a b-bit shift 
register that is initially set to some initialization vector (IV). The leftmost (most 
significant) s bits of the output of the encryption function are XORed with the first 
unit of plaintext P1 to produce the first unit of ciphertext C1, which is then transmit-
ted. In addition, the contents of the shift register are shifted left by s bits, and C1 
is placed in the rightmost (least significant) s bits of the shift register. This process 
continues until all plaintext units have been encrypted.
For decryption, the same scheme is used, except that the received ciphertext 
unit is XORed with the output of the encryption function to produce the plaintext 
unit. Note that it is the encryption function that is used, not the decryption function. 
This is easily explained. Let Ss(X) be defined as the most significant s bits of X. Then
C1 = P1 ‚äï Ss[E(K, IV)]
Therefore,
P1 = C1 ‚äï Ss[E(K, IV)]
The same reasoning holds for subsequent steps in the process.
Counter Mode
Although interest in the counter mode (CTR) has increased recently, with applica-
tions to ATM (asynchronous transfer mode) network security and IPSec (IP security), 
this mode was proposed early on (e.g., [DIFF79]).

Figure 2.11 depicts the CTR mode. A counter equal to the plaintext block 
size is used. The only requirement stated in NIST Special Publication 800-38A is 
that the counter value must be different for each plaintext block that is encrypted. 
Typically, the counter is initialized to some value and then incremented by 1 for 
each subsequent block (modulo 2b, where b is the block size). For encryption, the 
counter is encrypted and then XORed with the plaintext block to produce the ci-
phertext block; there is no chaining. For decryption, the same sequence of counter 
values is used, with each encrypted counter XORed with a ciphertext block to re-
cover the corresponding plaintext block.
Figure 2.10 s-bit Cipher Feedback (CFB) Mode
C1
IV
P1
Encrypt
Select
s bits
Discard
b‚Äìs bits
K
(a) Encryption
CN‚Äì1
(b) Decryption
s bits
s bits
s bits
C2
P2
Encrypt
Select
s bits
Discard
b‚Äìs bits
K
s bits
s bits
b‚Äìs bits
Shift register
s bits
CN
PN
Encrypt
Select
s bits
Discard
b‚Äìs bits
K
s bits
s bits
b‚Äìs bits
Shift register
P1
IV
C1
Encrypt
Select
s bits 
Discard
b‚Äìs bits
K
CN‚Äì1
s bits
C2
s bits
CN
s bits
s bits
s bits
P2
Encrypt
Select
s bits
Discard
b‚Äìs bits
K
s bits
b‚Äìs bits
Shift register
s bits
b‚Äìs bits
Shift register
s bits
PN
Encrypt
Select
s bits
Discard
b‚Äìs bits
K

[LIPM00] lists the following advantages of CTR mode.
‚ñ†
‚ñ† Hardware efficiency: Unlike the chaining modes, encryption (or decryption) in 
CTR mode can be done in parallel on multiple blocks of plaintext or cipher-
text. For the chaining modes, the algorithm must complete the computation 
on one block before beginning on the next block. This limits the maximum 
throughput of the algorithm to the reciprocal of the time for one execution of 
block encryption or decryption. In CTR mode, the throughput is only limited 
by the amount of parallelism that is achieved.
‚ñ†
‚ñ† Software efficiency: Similarly, because of the opportunities for parallel execu-
tion in CTR mode, processors that support parallel features (such as aggres-
sive pipelining, multiple instruction dispatch per clock cycle, a large number of 
registers, and SIMD instructions) can be effectively utilized.
Figure 2.11 Counter (CTR) Mode
(a) Encryption
P1
C1
Counter 1
Encrypt
K
Counter 2
Counter N
P2
PN
C2
Encrypt
K
CN
Encrypt
K
(b) Decryption
C1
P1
Counter 1
Encrypt
K
Counter 2
Counter N
C2
CN
P2
Encrypt
K
PN
Encrypt
K

‚ñ†
‚ñ† Preprocessing: The execution of the underlying encryption algorithm does not 
depend on input of the plaintext or ciphertext. Therefore, if sufficient memory 
is available and security is maintained, preprocessing can be used to prepare 
the output of the encryption boxes that feed into the XOR functions in Figure 
2.11. When the plaintext or ciphertext input is presented, then the only compu-
tation is a series of XORs. Such a strategy greatly enhances throughput.
‚ñ†
‚ñ† Random access: The ith block of plaintext or ciphertext can be processed in 
random-access fashion. With the chaining modes, block Ci cannot be com-
puted until the i - 1 prior block are computed. There may be applications in 
which a ciphertext is stored, and it is desired to decrypt just one block; for such 
applications, the random access feature is attractive.
‚ñ†
‚ñ† Provable security: It can be shown that CTR is at least as secure as the other 
modes discussed in this section.
‚ñ†
‚ñ† Simplicity: Unlike ECB and CBC modes, CTR mode requires only the im-
plementation of the encryption algorithm and not the decryption algorithm. 
This matters most when the decryption algorithm differs substantially from 
the  encryption algorithm, as it does for AES. In addition, the decryption key 
scheduling need not be implemented.
 2.6 key termS, review QueStionS, and problemS
Review Questions 
 
2.1 
What is symmetric encryption? What are the two requirements for secure use of  
symmetric encryption?
 
2.2 
What is cryptanalysis? Summarize the various types of cryptanalytic attacks on  
encrypted messages.
 
2.3 
List the parameters of a symmetric block cipher for greater security.
 
2.4 
What is a block cipher? Name the important symmetric block ciphers.
 
2.5 
Describe the data encryption algorithm for 64-bit length plaintext and 56-bit length key.
 
2.6 
Describe the encryption and decryption of triple DES.
 
2.7 
What are the advantages and disadvantages of triple DES?
 
2.8 
List the important design criteria for a stream cipher.
Key Terms 
Advanced Encryption 
Standard (AES)
block cipher
brute-force attack
cipher block chaining (CBC) 
mode
cipher feedback (CFB) mode
ciphertext
counter mode (CTR)
cryptanalysis
cryptography
Data Encryption Standard 
(DES)
decryption
electronic codebook (ECB) 
mode
encryption
end-to-end encryption
Feistel cipher
key distribution
keystream
link encryption
plaintext
session key
stream cipher
subkey
symmetric encryption
triple DES (3DES)

Problems 
 
2.1 
This problem uses a real-world example of a symmetric cipher, from an old U.S. 
Special Forces manual (public domain). The document, filename SpecialForces.pdf, 
is available at box.com/NetSec6e.
a. Using the two keys (memory words) cryptographic and network security, encrypt 
the following message:
Be at the third pillar from the left outside the lyceum theatre tonight at 
seven. If you are distrustful bring two friends.
Make reasonable assumptions about how to treat redundant letters and 
excess letters in the memory words and how to treat spaces and punctuation. Indi-
cate what your assumptions are. Note: The message is from the Sherlock Holmes 
novel The Sign of Four.
b. Decrypt the ciphertext. Show your work.
c. Comment on when it would be appropriate to use this technique and what its 
advantages are.
 
2.2 
Consider a very simple symmetric block encryption algorithm in which 64-bit blocks 
of plaintext are encrypted using a 128-bit key. Encryption is defined as
C = (P ‚äï K1) √Ñ K0
 
 
where C = ciphertext, K = secret key, K0 = leftmost 64 bits of K, K1 = rightmost 
64 bits of K, ‚äï = bitwise exclusive OR, and √Ñ  is addition mod 264.
a. Show the decryption equation. That is, show the equation for P as a function of C, 
K0, and K1.
b. Suppose an adversary has access to two sets of plaintexts and their corresponding 
ciphertexts and wishes to determine K. We have the two equations:
 
C = (P ‚äï K1) √Ñ K0; C‚Ä≤ = (P‚Ä≤ ‚äï K1) √Ñ K0
First, derive an equation in one unknown (e.g., K0). Is it possible to proceed further
to solve for K0?
 
2.3 
Perhaps the simplest ‚Äúserious‚Äù symmetric block encryption algorithm is the Tiny 
Encryption Algorithm (TEA). TEA operates on 64-bit blocks of plaintext using a 
128-bit key. The plaintext is divided into two 32-bit blocks (L0, R0), and the key is 
divided into four 32-bit blocks (K0, K1, K2, K3). Encryption involves repeated applica-
tion of a pair of rounds, defined as follows for rounds i and i + 1:
 Li = Ri-1
 Ri = Li-1 √Ñ F(Ri-1, K0, K1, di)
 Li+1 = Ri
 Ri+1 = Li √Ñ F(Ri, K2, K3, di+1)
 
 
where F is defined as
 
F(M, Kj, Kk, di) = ((M 6 6 4) √Ñ Kj) ‚äï ((M W 5) √Ñ Kk) ‚äï (M √Ñ di) 
 
 
and where the logical shift of x by y bits is denoted by x 6 6 y, the logical right shift 
of x by y bits is denoted by x W y, and di is a sequence of predetermined constants.
a. Comment on the significance and benefit of using the sequence of constants.
b. Illustrate the operation of TEA using a block diagram or flow chart type of 
 depiction.

c. If only one pair of rounds is used, then the ciphertext consists of the 64-bit block 
(L2, R2). For this case, express the decryption algorithm in terms of equations.
d. Repeat part (c) using an illustration similar to that used for part (b).
 
2.4 
Is the DES decryption the inverse of DES encryption? Justify your answer.
 
2.5 
Consider a Feistel cipher composed of 14 rounds with block length 128 bits and key 
length 128 bits. Suppose that, for a given k, the key scheduling algorithm determines 
values for the first seven round keys, k1, k2, c, k8, and then sets
 
k8 = k7, k9 = k6, k10 = k5, c, k14 = k1 
 
 
Suppose you have a ciphertext c. Explain how, with access to an encryption oracle, 
you can decrypt c and determine m using just a single oracle query. This shows that 
such a cipher is vulnerable to a chosen plaintext attack. (An encryption oracle can be 
thought of as a device that, when given a plaintext, returns the corresponding cipher-
text. The internal details of the device are not known to you, and you cannot break 
open the device. You can only gain information from the oracle by making queries to 
it and observing its responses.)
 
2.6 
For any block cipher, the fact that it is a nonlinear function is crucial to its security. To 
see this, suppose that we have a linear block cipher EL that encrypts 256-bit blocks 
of plaintext into 256-bit blocks of ciphertext. Let EL(k, m) denote the encryption of a 
256-bit message m under a key k (the actual bit length of k is irrelevant). Thus,
 
EL(k, [m1 ‚äï m2]) = EL(k, m1) ‚äï EL(k, m2) for all 256@bit patterns m1, m2 
 
 
Describe how, with 256 chosen ciphertexts, an adversary can decrypt any ciphertext 
without knowledge of the secret key k. (A ‚Äúchosen ciphertext‚Äù means that an adver-
sary has the ability to choose a ciphertext and then obtain its decryption. Here, you 
have 256 plaintext‚Äìciphertext pairs to work with, and you have the ability to choose 
the value of the ciphertexts.)
 
2.7 
Suppose you have a true random bit generator where each bit in the generated stream 
has the same probability of being a 0 or 1 as any other bit in the stream and that the 
bits are not correlated; that is, the bits are generated from identical independent dis-
tribution. However, the bit stream is biased. The probability of a 1 is 0.5 - d and the 
probability of a 0 is 0.5 + d where 0 6 d 6 0.5. A simple deskewing algorithm is as 
follows: Examine the bit stream as a sequence of nonoverlapping pairs. Discard all 00 
and 11 pairs. Replace each 01 pair with 0 and each 10 pair with 1.
a. What is the probability of occurrence of each pair in the original sequence?
b. What is the probability of occurrence of 0 and 1 in the modified sequence?
c. What is the expected number of input bits to produce x output bits?
d. Suppose that the algorithm uses overlapping successive bit pairs instead of non-
overlapping successive bit pairs. That is, the first output bit is based on input bits 1 
and 2, the second output bit is based on input bits 2 and 3, and so on. What can you 
say about the output bit stream?
 
2.8 
Another approach to deskewing is to consider the bit stream as a sequence of non-
overlapping groups of n bits each and output the parity of each group. That is, if a 
group contains an odd number of ones, the output is 1; otherwise the output is 0.
a. Express this operation in terms of a basic Boolean function.
b. Assume, as in the Problem 2.7, that the probability of a 1 is 0.5 - d. If each group 
consists of 2 bits, what is the probability of an output of 1?
c. If each group consists of 3 bits, what is the probability of an output of 1?
d. Generalize the result to find the probability of an output of 1 for input groups of 
n bits.

2.9 
Is it appropriate to reuse keys in RC4? Why or why not?
 
2.10 
RC4 has a secret internal state which is a permutation of all the possible values of the 
vector S and the two indices i and j.
a. Using a straightforward scheme to store the internal state, how many bits are used?
b. Suppose we think of it from the point of view of how much information is repre-
sented by the state. In that case, we need to determine how many different states 
there are, then take the log to the base 2 to find out how many bits of information 
this represents. Using this approach, how many bits would be needed to represent 
the state?
 
2.11 
Alice and Bob agree to communicate privately via e-mail using a scheme based on 
RC4, but they want to avoid using a new secret key for each transmission. Alice and 
Bob privately agree on a 128-bit key k. To encrypt a message m consisting of a string 
of bits, the following procedure is used.
1. Choose a random 80-bit value v
2. Generate the ciphertext c = RC4(v}k) ‚äï m
3. Send the bit string (v}c)
 
 
 
a. Suppose Alice uses this procedure to send a message m to Bob. Describe how 
Bob can recover the message m from (v} c) using k.
 
 
 
b. If an adversary observes several values (v1} c1), (v2} c2), c transmitted 
 between Alice and Bob, how can he or she determine when the same key 
stream has been used to encrypt two messages?
 
2.12 
With the ECB mode, if there is an error in a block of the transmitted ciphertext, 
only the corresponding plaintext block is affected. However, in the CBC mode, this 
error propagates. For example, an error in the transmitted C1 (Figure 2.9) obviously 
 corrupts P1 and P2.
a. Are any blocks beyond P2 affected?
b. Suppose that there is a bit error in the source version of P1. Through how many 
ciphertext blocks is this error propagated? What is the effect at the receiver?
 
2.13 
Is it possible to perform decryption operations in parallel on multiple blocks of 
 ciphertext in CBC mode? How about encryption?
 
2.14 
Why should the IV in CBC be protected?
 
2.15 
CBC-Pad is a block cipher mode of operation used in the RC5 block cipher, but it 
could be used in any block cipher. CBC-Pad handles plaintext of any length. The 
ciphertext is longer than the plaintext by at most the size of a single block. Padding is 
used to assure that the plaintext input is a multiple of the block length. It is assumed 
that the original plaintext is an integer number of bytes. This plaintext is padded at 
the end by from 1 to bb bytes, where bb equals the block size in bytes. The pad bytes 
are all the same and set to a byte that represents the number of bytes of padding. For 
example, if there are 8 bytes of padding, each byte has the bit pattern 00001000. 
Why not allow zero bytes of padding? That is, if the original plaintext is an integer 
multiple of the block size, why not refrain from padding?
 
2.16 
Padding may not always be appropriate. For example, one might wish to store the 
encrypted data in the same memory buffer that originally contained the plaintext. In 
that case, the ciphertext must be the same length as the original plaintext. A mode for 
that purpose is the ciphertext stealing (CTS) mode. Figure 2.12a shows an implemen-
tation of this mode.
a. Explain how it works.
b. Describe how to decrypt Cn-1 and Cn.

Figure 2.12 Block Cipher Modes for Plaintext not a Multiple of Block Size
IV
P1
C1
K
K
K
K
+
+
+
+
PN‚Äì2
CN‚Äì2
CN‚Äì3
Encrypt
Encrypt
Encrypt
Encrypt
Encrypt
Encrypt
(a) Ciphertext stealing mode
(b) Alternative method
Encrypt
CN
X
PN‚Äì1
CN‚Äì1
PN 00‚Ä¶0
IV
P1
(bb bits)
C1
(bb bits)
K
K
K
K
+
+
+
+
PN‚Äì2
(bb bits)
CN‚Äì2
(bb bits)
CN‚Äì3
Select
leftmost
j bits
PN‚Äì1
(bb bits)
CN‚Äì1
(bb bits)
PN
(  j bits )
CN
( j bits )
Encrypt
 
2.17 
Figure 2.12b shows an alternative to CTS for producing ciphertext of equal length to 
the plaintext when the plaintext is not an integer multiple of the block size.
a. Explain the algorithm.
b. Explain why CTS is preferable to this approach illustrated in Figure 2.12b.
 
2.18 
If a bit error occurs in the transmission of a ciphertext character in 8-bit CFB mode, 
how far does the error propagate?

78
Public-Key Cryptography and 
Message Authentication
Chapter
3.1 
Approaches to Message Authentication
Authentication Using Conventional Encryption
Message Authentication without Message Encryption
3.2 
Secure Hash Functions
Hash Function Requirements
Security of Hash Functions
Simple Hash Functions
The SHA Secure Hash Function
SHA-3
3.3 
Message Authentication Codes
HMAC
MACs Based on Block Ciphers
3.4 
Public-Key Cryptography Principles
Public-Key Encryption Structure
Applications for Public-Key Cryptosystems
Requirements for Public-Key Cryptography
3.5 
Public-Key Cryptography Algorithms
The RSA Public-Key Encryption Algorithm
Diffie‚ÄìHellman Key Exchange
Other Public-Key Cryptography Algorithms
3.6 
Digital Signatures
Digital Signature Generation and Verification
RSA Digital Signature Algorithm
3.7 
Key Terms, Review Questions, and Problems

In addition to message confidentiality, message authentication is an important network 
security function. This chapter examines three aspects of message authentication. First, 
we look at the use of message authentication codes and hash functions to provide mes-
sage authentication. Then we look at public-key encryption principles and two specific 
public-key algorithms. These algorithms are useful in the exchange of conventional 
encryption keys. Then we look at the use of public-key encryption to produce digital 
signatures, which provides an enhanced form of message authentication.
 3.1 ApproAches to MessAge AuthenticAtion
Encryption protects against passive attack (eavesdropping). A different require-
ment is to protect against active attack (falsification of data and transactions). 
Protection against such attacks is known as message authentication.
A message, file, document, or other collection of data is said to be authentic 
when it is genuine and comes from its alleged source. Message authentication is a 
procedure that allows communicating parties to verify that received messages are 
authentic.1 The two important aspects are to verify that the contents of the mes-
sage have not been altered and that the source is authentic. We may also wish to 
verify a message‚Äôs timeliness (it has not been artificially delayed and replayed) and 
sequence relative to other messages flowing between two parties. All of these con-
cerns come under the category of data integrity as described in Chapter 1.
1For simplicity, for the remainder of this chapter, we refer to message authentication. By this we mean 
both authentication of transmitted messages and of stored data (data authentication).
LeArning objectives
After studying this chapter, you should be able to:
‚óÜ‚óÜ
Define the term message authentication code.
‚óÜ‚óÜ
List and explain the requirements for a message authentication code.
‚óÜ‚óÜ
Explain why a hash function used for message authentication needs to be 
secured.
‚óÜ‚óÜ
Understand the differences among preimage resistant, second preimage 
resistant, and collision resistant properties.
‚óÜ‚óÜ
Understand the operation of SHA-512.
‚óÜ‚óÜ
Present an overview of HMAC.
‚óÜ‚óÜ
Present an overview of the basic principles of public-key cryptosystems.
‚óÜ‚óÜ
Explain the two distinct uses of public-key cryptosystems.
‚óÜ‚óÜ
Present an overview of the RSA algorithm.
‚óÜ‚óÜ
Define Diffie‚ÄìHellman key exchange.
‚óÜ‚óÜ
Understand the man-in-the-middle attack.

Authentication Using Conventional Encryption
It would seem possible to perform authentication simply by the use of symmetric 
encryption. If we assume that only the sender and receiver share a key (which is as it 
should be), then only the genuine sender would be able to encrypt a message success-
fully for the other participant, provided the receiver can recognize a valid message. 
Furthermore, if the message includes an error-detection code and a sequence num-
ber, the receiver is assured that no alterations have been made and that sequencing 
is proper. If the message also includes a timestamp, the receiver is assured that the 
message has not been delayed beyond that normally expected for network transit.
In fact, symmetric encryption alone is not a suitable tool for data authentica-
tion. To give one simple example, in the ECB mode of encryption, if an attacker 
reorders the blocks of ciphertext, then each block will still decrypt successfully. 
However, the reordering may alter the meaning of the overall data sequence. 
Although sequence numbers may be used at some level (e.g., each IP packet), it is 
typically not the case that a separate sequence number will be associated with each 
b-bit block of plaintext. Thus, block reordering is a threat.
Message Authentication without Message Encryption
In this section, we examine several approaches to message authentication that do 
not rely on encryption. In all of these approaches, an authentication tag is generated 
and appended to each message for transmission. The message itself is not encrypted 
and can be read at the destination independent of the authentication function at the 
destination.
Because the approaches discussed in this section do not encrypt the message, 
message confidentiality is not provided. As was mentioned, message encryption by 
itself does not provide a secure form of authentication. However, it is possible to 
combine authentication and confidentiality in a single algorithm by encrypting a 
message plus its authentication tag. Typically, however, message authentication is 
provided as a separate function from message encryption. [DAVI89] suggests three 
situations in which message authentication without confidentiality is preferable:
1. There are a number of applications in which the same message is broadcast to 
a number of destinations. Two examples are notification to users that the net-
work is now unavailable and an alarm signal in a control center. It is cheaper 
and more reliable to have only one destination responsible for monitoring au-
thenticity. Thus, the message must be broadcast in plaintext with an associated 
message authentication tag. The responsible system performs authentication. If 
a violation occurs, the other destination systems are alerted by a general alarm.
2. Another possible scenario is an exchange in which one side has a heavy load and 
cannot afford the time to decrypt all incoming messages. Authentication is car-
ried out on a selective basis with messages being chosen at random for checking.
3. Authentication of a computer program in plaintext is an attractive service. 
The computer program can be executed without having to decrypt it every 
time, which would be wasteful of processor resources. However, if a message 
authentication tag were attached to the program, it could be checked when-
ever assurance is required of the integrity of the program.

Thus, there is a place for both authentication and encryption in meeting secu-
rity requirements.
Message authentication code One authentication technique involves the use of 
a secret key to generate a small block of data, known as a message  authentication 
code (MAC), that is appended to the message. This technique assumes that two 
communicating parties, say A and B, share a common secret key KAB. When A 
has a message to send to B, it calculates the message authentication code as a func-
tion of the message and the key: MACM = F(KAB, M). The message plus code are 
transmitted to the intended recipient. The recipient performs the same calcula-
tion on the received message, using the same secret key, to generate a new mes-
sage authentication code. The received code is compared to the calculated code 
(Figure¬†3.1). If we assume that only the receiver and the sender know the identity 
of the secret key, and if the received code matches the calculated code, then the fol-
lowing statements¬†apply:
1. The receiver is assured that the message has not been altered. If an attacker 
alters the message but does not alter the code, then the receiver‚Äôs calcula-
tion of the code will differ from the received code. Because the attacker is 
assumed not to know the secret key, the attacker cannot alter the code to cor-
respond to the alterations in the message.
Figure 3.1 Message Authentication Using a Message Authentication Code
MAC
algorithm
MAC
algorithm
MAC
K
K
Compare
Message
Transmit

2. The receiver is assured that the message is from the alleged sender. Because 
no one else knows the secret key, no one else could prepare a message with a 
proper code.
3. If the message includes a sequence number (such as is used with HDLC and 
TCP), then the receiver can be assured of the proper sequence, because an 
attacker cannot successfully alter the sequence number.
A number of algorithms could be used to generate the code. The NIST speci-
fication, FIPS PUB 113, recommends the use of DES. DES is used to generate an 
encrypted version of the message, and the last number of bits of ciphertext are used 
as the code. A 16- or 32-bit code is typical.
The process just described is similar to encryption. One difference is that the 
authentication algorithm need not be reversible, as it must for decryption. Because 
of the mathematical properties of the authentication function, it is less vulnerable to 
being broken than encryption.
one-Way hash Function An alternative to the message authentication code is 
the one-way hash function. As with the message authentication code, a hash func-
tion accepts a variable-size message M as input and produces a fixed-size message 
digest H(M) as output. Unlike the MAC, a hash function does not take a secret key 
as input. To authenticate a message, the message digest is sent with the message in 
such a way that the message digest is authentic.
Figure 3.2 illustrates three ways in which the message can be authenticated. 
The message digest can be encrypted using conventional encryption (part a); 
if it is  assumed that only the sender and receiver share the encryption key, then 
 authenticity is assured. The message digest can be encrypted using public-key 
 encryption (part b); this is explained in Section 3.5. The public-key approach has 
two  advantages: (1) It provides a digital signature as well as message authentication. 
(2)¬†It¬†does not require the distribution of keys to communicating parties.
These two approaches also have an advantage over approaches that encrypt 
the entire message in that less computation is required. Nevertheless, there has 
been interest in developing a technique that avoids encryption altogether. Several 
reasons for this interest are pointed out in [TSUD92]:
‚óÜ
‚ñ† Encryption software is quite slow. Even though the amount of data to be en-
crypted per message is small, there may be a steady stream of messages into 
and out of a system.
‚óÜ
‚ñ† Encryption hardware costs are nonnegligible. Low-cost chip implementations 
of DES are available, but the cost adds up if all nodes in a network must have 
this capability.
‚óÜ
‚ñ† Encryption hardware is optimized toward large data sizes. For small blocks of 
data, a high proportion of the time is spent in initialization/invocation overhead.
‚óÜ
‚ñ† An encryption algorithm may be protected by a patent.
Figure 3.2c shows a technique that uses a hash function but no encryption 
for¬† message authentication. This technique assumes that two communicating 
 parties, say A and B, share a common secret value SAB. When A has a message to 
send to¬†B, it calculates the hash function over the concatenation of the secret value

and the¬†message: MDM = H(SAB }M).2 It then sends [M }MDM] to B. Because B 
possesses SAB, it can recompute H(SAB }M) and verify MDM. Because the secret 
value itself is not sent, it is not possible for an attacker to modify an intercepted 
message. As long as the secret value remains secret, it is also not possible for an at-
tacker to generate a false message.
2} denotes concatenation.
Figure 3.2 Message Authentication Using a One-Way Hash Function
Source A
Destination B
Message
Message
Compare
Message
H
H
E
(a) Using conventional encryption
K
D
K
Message
Message
Message
Compare
Message
H
H
H
E
S
S
(b) Using public-key encryption
(c) Using secret value
PRa
PUa
Compare
H
D
Message
Message

A variation on the third technique, called HMAC, is the one adopted 
for IP  security (described in Chapter 9); it also has been specified for SNMPv3 
(Chapter¬†13).
 3.2 secure hAsh Functions
The one-way hash function, or secure hash function, is important not only in mes-
sage authentication but in digital signatures. In this section, we begin with a discus-
sion of requirements for a secure hash function. Then we look at the most important 
hash function, SHA.
Hash Function Requirements
The purpose of a hash function is to produce a ‚Äúfingerprint‚Äù of a file, message, or 
other block of data. To be useful for message authentication, a hash function H 
must have the following properties:
1. H can be applied to a block of data of any size.
2. H produces a fixed-length output.
3. H(x) is relatively easy to compute for any given x, making both hardware and 
software implementations practical.
4. For any given code h, it is computationally infeasible to find x such that 
H(x) = h. A hash function with this property is referred to as one-way or 
preimage resistant.3
5. For any given block x, it is computationally infeasible to find y ‚â† x with 
H(y) = H(x). A hash function with this property is referred to as second 
 pre-image resistant. This is sometimes referred to as weak collision resistant.
6. It is computationally infeasible to find any pair (x, y) such that H(x) = H(y). 
A hash function with this property is referred to as collision resistant. This is 
sometimes referred to as strong collision resistant.
The first three properties are requirements for the practical application of a 
hash function to message authentication. The fourth property, preimage resistant, 
is the ‚Äúone-way‚Äù property: It is easy to generate a code given a message, but virtu-
ally impossible to generate a message given a code. This property is important if the 
authentication technique involves the use of a secret value (Figure 3.2c). The secret 
value itself is not sent; however, if the hash function is not one way, an attacker can 
easily discover the secret value: If the attacker can observe or intercept a transmis-
sion, the attacker obtains the message M and the hash code C = H(SAB }M). The 
attacker then inverts the hash function to obtain SAB }M = H-1(C). Because the 
attacker now has both M and SAB }M, it is a trivial matter to recover SAB.
The second preimage resistant property guarantees that it is impossible to find 
an alternative message with the same hash value as a given message. This prevents 
3For f(x) = y, x is said to be a preimage of y. Unless f is one-to-one, there may be multiple preimage 
values for a given y.

forgery when an encrypted hash code is used (Figures 3.2a and b). If this prop-
erty were not true, an attacker would be capable of the following sequence: First, 
observe or intercept a message plus its encrypted hash code; second, generate an 
unencrypted hash code from the message; third, generate an alternate message with 
the same hash code.
A hash function that satisfies the first five properties in the preceding list is 
referred to as a weak hash function. If the sixth property is also satisfied, then it is 
referred to as a strong hash function. The sixth property, collision resistant, protects 
against a sophisticated class of attack known as the birthday attack. Details of this 
attack are beyond the scope of this book. The attack reduces the strength of an  
m-bit hash function from 2m to 2m/2. See [STAL13] for details.
In addition to providing authentication, a message digest also provides data 
integrity. It performs the same function as a frame check sequence: If any bits in the 
message are accidentally altered in transit, the message digest will be in error.
Security of Hash Functions
As with symmetric encryption, there are two approaches to attacking a secure hash 
function: cryptanalysis and brute-force attack. As with symmetric encryption algo-
rithms, cryptanalysis of a hash function involves exploiting logical weaknesses in the 
algorithm.
The strength of a hash function against brute-force attacks depends solely on 
the length of the hash code produced by the algorithm. For a hash code of length n, 
the level of effort required is proportional to the following:
Preimage resistant
2n
Second preimage resistant
2n
Collision resistant
2n/2
If collision resistance is required (and this is desirable for a general-purpose 
secure hash code), then the value 2n/2 determines the strength of the hash code 
against brute-force attacks. Van Oorschot and Wiener [VANO94] presented a de-
sign for a $10 million collision search machine for MD5, which has a 128-bit hash 
length, that could find a collision in 24 days. Thus, a 128-bit code may be viewed as 
inadequate. The next step up, if a hash code is treated as a sequence of 32 bits, is a 
160-bit hash length. With a hash length of 160 bits, the same search machine would 
require over four thousand years to find a collision. With today‚Äôs technology, the 
time would be much shorter, so that 160 bits now appears suspect.
Simple Hash Functions
All hash functions operate using the following general principles. The input (mes-
sage, file, etc.) is viewed as a sequence of n-bit blocks. The input is processed one 
block at a time in an iterative fashion to produce an n-bit hash function.
One of the simplest hash functions is the bit-by-bit exclusive-OR (XOR) of 
every block. This can be expressed as
Ci = bi1 ‚äï bi2 ‚äï g ‚äï bim

where
Ci = ith bit of the hash code, 1 ‚Ä¶ i ‚Ä¶ n
m = number of n-bit blocks in the input
bij = ith bit in jth block
‚äï = XOR operation
Figure 3.3 illustrates this operation; it produces a simple parity for each bit 
position and is known as a longitudinal redundancy check. It is reasonably effective 
for random data as a data integrity check. Each n-bit hash value is equally likely. 
Thus, the probability that a data error will result in an unchanged hash value is 2-n. 
With more predictably formatted data, the function is less effective. For example, in 
most normal text files, the high-order bit of each octet is always zero. So if a 128-bit 
hash value is used, instead of an effectiveness of 2-128, the hash function on this type 
of data has an effectiveness of 2-112.
A simple way to improve matters is to perform a 1-bit circular shift, or rotation, 
on the hash value after each block is processed. The procedure can be summarized as
1. Initially set the n-bit hash value to zero.
2. Process each successive n-bit block of data:
a. Rotate the current hash value to the left by one bit.
b. XOR the block into the hash value.
This has the effect of ‚Äúrandomizing‚Äù the input more completely and overcoming 
any regularities that appear in the input.
Although the second procedure provides a good measure of data integrity, 
it is virtually useless for data security when an encrypted hash code is used with a 
plaintext message, as in Figures 3.2a and b. Given a message, it is an easy matter to 
produce a new message that yields that hash code: Simply prepare the desired alter-
nate message and then append an n-bit block that forces the combined new message 
plus block to yield the desired hash code.
Although a simple XOR or rotated XOR (RXOR) is insufficient if only the 
hash code is encrypted, you may still feel that such a simple function could be¬† useful 
when the message as well as the hash code are encrypted. But one must be care-
ful. A¬†technique originally proposed by the National Bureau of Standards used 
Figure 3.3 Simple Hash Function Using Bitwise XOR
bit 1
Block 1
Block 2
Block m
Hash code
b11
b21
bn1
bn2
bnm
b22
b2m
b12
b1m
C1
C2
Cn
bit 2
bit n

the simple XOR applied to 64-bit blocks of the message and then an encryption 
of the entire message using the cipher block chaining (CBC) mode. We can define 
the scheme as follows: Given a message consisting of a sequence of 64-bit blocks 
X1, X2, c , XN, define the hash code C as the block-by-block XOR or all blocks 
and append the hash code as the final block:
C = XN+1 = X1 ‚äï X2 ‚äï g ‚äï XN
Next, encrypt the entire message plus hash code using CBC mode to produce the 
encrypted message Y1, Y2, c , YN+1. [JUEN85] points out several ways in which 
the ciphertext of this message can be manipulated in such a way that it is not detect-
able by the hash code. For example, by the definition of CBC (Figure 2.9), we have
 X1 = IV ‚äï D(K, Y1)
 Xi = Yi-1 ‚äï D(K, Yi)
 XN+1 = YN ‚äï D(K, YN+1)
But XN+1 is the hash code:
 XN+1 = X1 ‚äï X2 ‚äï g ‚äï XN
 = [IV ‚äï D(K, Y1)] ‚äï [Y1 ‚äï D(K, Y2)] ‚äï g ‚äï [YN-1 ‚äï D(K, YN)]
Because the terms in the preceding equation can be XORed in any order, it follows 
that the hash code would not change if the ciphertext blocks were permuted.
The SHA Secure Hash Function
In recent years, the most widely used hash function has been the Secure Hash 
Algorithm (SHA). Indeed, because virtually every other widely used hash function 
had been found to have substantial cryptanalytic weaknesses, SHA was more or 
less the last remaining standardized hash algorithm by 2005. SHA was developed 
by the National Institute of Standards and Technology (NIST) and published as a 
federal information processing standard (FIPS 180) in 1993. When weaknesses were 
discovered in SHA (now known as SHA-0), a revised version was issued as FIPS 
180-1 in 1995 and is referred to as SHA-1. The actual standards document is entitled 
‚ÄúSecure Hash Standard.‚Äù SHA is based on the hash function MD4, and its design 
closely models MD4.
SHA-1 produces a hash value of 160 bits. In 2002, NIST produced a revised 
version of the standard, FIPS 180-2, that defined three new versions of SHA with 
hash value lengths of 256, 384, and 512 bits known as SHA-256, SHA-384, and 
SHA-512, respectively. Collectively, these hash algorithms are known as SHA-2. 
These new versions have the same underlying structure and use the same types of 
modular arithmetic and logical binary operations as SHA-1. A revised document 
was issued as FIP PUB 180-3 in 2008, which added a 224-bit version (Table 3.1). 
SHA-1 and SHA-2 are also specified in RFC 6234, which essentially duplicates the 
material in FIPS 180-3 but adds a C code implementation.
In 2005, NIST announced the intention to phase out approval of SHA-1 and 
move to a reliance on SHA-2 by 2010. Shortly thereafter, a research team  described 
an attack in which two separate messages could be found that deliver the same 
SHA-1 hash using 269 operations, far fewer than the 280 operations previously

thought needed to find a collision with an SHA-1 hash [WANG05]. This result 
should hasten the transition to SHA-2.
In this section, we provide a description of SHA-512. The other versions are 
quite similar.
The algorithm takes as input a message with a maximum length of less than 
2128 bits and produces as output a 512-bit message digest. The input is processed in 
1024-bit blocks. Figure 3.4 depicts the overall processing of a message to produce a 
digest. The processing consists of the following steps.
Step 1 Append padding bits: The message is padded so that its length is congruent 
to 896 modulo 1024 [length K 896 (mod 1024)]. Padding is always added, 
even if the message is already of the desired length. Thus, the number of 
padding bits is in the range of 1 to 1024. The padding consists of a single 1 
bit followed by the necessary number of 0 bits.
Step 2 Append length: A block of 128 bits is appended to the message. This block 
is treated as an unsigned 128-bit integer (most significant byte first) and 
contains the length of the original message (before the padding).
The outcome of the first two steps yields a message that is an integer 
multiple of 1024 bits in length. In Figure 3.4, the expanded message is repre-
sented as the sequence of 1024-bit blocks M1, M2, c , MN, so that the total 
length of the expanded message is N * 1024 bits.
Step 3 Initialize hash buffer: A 512-bit buffer is used to hold intermediate and final 
results of the hash function. The buffer can be represented as eight 64-bit 
registers (a, b, c, d, e, f, g, h). These registers are initialized to the following 
64-bit integers (hexadecimal values):
a = 6A09E667F3BCC908  e = 510E527FADE682D1
b = BB67AE8584CAA73B   f = 9B05688C2B3E6C1F
c = 3C6EF372FE94F82B      g = 1F83D9ABFB41BD6B
d = A54FF53A5F1D36F1     h = 5BE0CD19137E2179
These values are stored in big-endian format, which is the most significant byte of 
a word in the low-address (leftmost) byte position. These words were obtained by 
taking the first sixty-four bits of the fractional parts of the square roots of the first 
eight prime numbers.
SHA-1
SHA-224
SHA-256
SHA-384
SHA-512
Message Digest Size
160
224
256
384
512
Message Size
6264
6264
6264
62128
62128
Block Size
512
512
512
1024
1024
Word Size
32
32
32
64
64
Number of Steps
80
64
64
80
80
Note: All sizes are measured in bits.
Table 3.1 Comparison of SHA Parameters

Step 4 Process message in 1024-bit (128-word) blocks: The heart of the algorithm 
is a module that consists of 80 rounds; this module is labeled F in Figure 3.4. 
The logic is illustrated in Figure 3.5.
Each round takes as input the 512-bit buffer value abcdefgh and 
updates the contents of the buffer. At input to the first round, the buffer 
has the value of the intermediate hash value, Hi-1. Each round t makes 
use of a 64-bit value Wt derived from the current 1024-bit block being pro-
cessed (Mi). Each round also makes use of an additive constant Kt, where 
0 ‚Ä¶ t ‚Ä¶ 79 indicates one of the 80 rounds. These words represent the first 
64 bits of the fractional parts of the cube roots of the first 80 prime numbers. 
The constants provide a ‚Äúrandomized‚Äù set of 64-bit patterns, which should 
eliminate any regularities in the input data.
The output of the 80th round is added to the input to the first round 
(Hi-1) to produce Hi. The addition is done independently for each of the 
eight words in the buffer with each of the corresponding words in Hi-1, 
using addition modulo 264.
Step 5 Output: After all N 1024-bit blocks have been processed, the output from 
the Nth stage is the 512-bit message digest.
Figure 3.4 Message Digest Generation Using SHA-512
N √ó 1024 bits
M1
M2
H2
MN
Message
hash code
1024 bits
1024 bits
1024 bits
L bits
L
128 bits
512 bits
512 bits
512 bits
1000000..0
+ = word-by-word addition mod 264
H1
F
+
IV = H0
F
+
F
+
HN

The SHA-512 algorithm has the property that every bit of the hash code is 
a function of every bit of the input. The complex repetition of the basic function 
F¬†produces results that are well mixed; that is, it is unlikely that two messages cho-
sen at random, even if they exhibit similar regularities, will have the same hash 
code. Unless there is some hidden weakness in SHA-512, which has not so far been 
published, the difficulty of coming up with two messages having the same message 
 digest is on the order of 2256 operations, while the difficulty of finding a message 
with a given digest is on the order of 2512 operations.
SHA-3
SHA-2, particularly the 512-bit version, would appear to provide unassailable secu-
rity. However, SHA-2 shares the same structure and mathematical operations as its 
predecessors, and this is a cause for concern. Because it would take years to find a 
suitable replacement for SHA-2, should it become vulnerable, NIST announced in 
2007 a competition to produce the next-generation NIST hash function, which is to 
be called SHA-3. Following are the basic requirements that must be satisfied by any 
candidate for SHA-3:
Figure 3.5 SHA-512 Processing of a Single 1024-Bit Block
64
Mi
Wt
Hi
Hi‚Äì1
W0
W79
Kt
K0
K79
a
b
c
Round 0
d
e
f
g
h
a
b
c
Round t
d
e
f
g
h
Message
schedule
a
b
c
Round 79
d
e
f
g
h
+
+
+
+
+
+
+
+

1. It must be possible to replace SHA-2 with SHA-3 in any application by a sim-
ple drop-in substitution. Therefore, SHA-3 must support hash value lengths 
of 224, 256, 384, and 512 bits.
2. SHA-3 must preserve the online nature of SHA-2. That is, the algorithm must 
process comparatively small blocks (512 or 1024 bits) at a time instead of 
requiring that the entire message be buffered in memory before processing it.
In 2012, NIST selected a winning submission and formally published SHA-3. 
A detailed presentation of SHA-3 is provided in Chapter 15.
 3.3 MessAge AuthenticAtion codes
HMAC
In recent years, there has been increased interest in developing a MAC derived 
from a cryptographic hash code, such as SHA-1. The motivations for this interest 
are as follows:
‚óÜ
‚ñ† Cryptographic hash functions generally execute faster in software than con-
ventional encryption algorithms such as DES.
‚óÜ
‚ñ† Library code for cryptographic hash functions is widely available.
A hash function such as SHA-1 was not designed for use as a MAC and cannot 
be used directly for that purpose because it does not rely on a secret key. There have 
been a number of proposals for the incorporation of a secret key into an existing hash 
algorithm. The approach that has received the most support is HMAC [BELL96a, 
BELL96b]. HMAC has been issued as RFC 2104, has been chosen as the mandatory-
to-implement MAC for IP Security, and is used in other Internet protocols, such as 
Transport Layer Security (TLS) and Secure Electronic Transaction (SET).
hMac design objectives RFC 2104 lists the following design objectives for HMAC.
‚óÜ
‚ñ† To use, without modifications, available hash functions. In particular, hash 
functions that perform well in software, and for which code is freely and 
widely available
‚óÜ
‚ñ† To allow for easy replaceability of the embedded hash function in case faster 
or more secure hash functions are found or required
‚óÜ
‚ñ† To preserve the original performance of the hash function without incurring 
a significant degradation
‚óÜ
‚ñ† To use and handle keys in a simple way
‚óÜ
‚ñ† To have a well-understood cryptographic analysis of the strength of the au-
thentication mechanism based on reasonable assumptions on the embedded 
hash function
The first two objectives are important to the acceptability of HMAC. HMAC 
treats the hash function as a ‚Äúblack box.‚Äù This has two benefits. First, an existing  
 implementation of a hash function can be used as a module in implementing HMAC.

In this way, the bulk of the HMAC code is prepackaged and ready to use without 
modification. Second, if it is ever desired to replace a given hash function in an 
HMAC implementation, all that is required is to remove the existing hash function 
module and drop in the new module. This could be done if a faster hash function 
were desired. More important, if the security of the embedded hash function were 
compromised, the security of HMAC could be retained simply by replacing the 
 embedded hash function with a more secure one.
The last design objective in the preceding list is, in fact, the main advantage 
of HMAC over other proposed hash-based schemes. HMAC can be proven secure 
provided that the embedded hash function has some reasonable cryptographic 
strengths. We return to this point later in this section, but first we examine the struc-
ture of HMAC.
hMac algorithM Figure 3.6 illustrates the overall operation of HMAC. The 
 following terms are defined:
H = embedded hash function (e.g., SHA-1)
M =  message input to HMAC (including the padding specified in the 
 embedded hash function)
Figure 3.6 HMAC Structure
K+
Si
So
Y0
Y1
YL‚Äì1
b bits
b bits
b bits
b bits
ipad
K+
opad
Hash
IV
n bits
n bits
pad to b bits
Hash
IV
n bits
n bits
HMAC(K, M)
H(Si || M)

Yi = ith block of M, 0 ‚Ä¶ i ‚Ä¶ (L - 1)
L = number of blocks in M
b = number of bits in a block
n = length of hash code produced by embedded hash function
K =  secret key; if key length is greater than b, the key is input to the hash 
function to produce an n-bit key; recommended length is 7 n
K+ = K padded with zeros on the left so that the result is b bits in length
ipad = 00110110 (36 in hexadecimal) repeated b/8 times
opad = 01011100 (5C in hexadecimal) repeated b/8 times
Then HMAC can be expressed as
 
HMAC(K, M) = H[(K+ ‚äï opad) }H[(K+ ‚äï ipad) }M]] 
In words, HMAC is defined as follows:
1. Append zeros to the left end of K to create a b-bit string K+ (e.g., if K is of 
length 160 bits and b = 512, then K will be appended with 44 zero bytes).
2. XOR (bitwise exclusive-OR) K+ with ipad to produce the b-bit block Si.
3. Append M to Si.
4. Apply H to the stream generated in step 3.
5. XOR K+ with opad to produce the b-bit block So.
6. Append the hash result from step 4 to So.
7. Apply H to the stream generated in step 6 and output the result.
Note that the XOR with ipad results in flipping one-half of the bits of K. 
Similarly, the XOR with opad results in flipping one-half of the bits of K, but a dif-
ferent set of bits. In effect, by passing Si and So through the hash algorithm, we have 
pseudorandomly generated two keys from K.
HMAC should execute in approximately the same time as the embedded hash 
function for long messages. HMAC adds three executions of the basic hash function 
(for Si, So, and the block produced from the inner hash).
MACs Based on Block Ciphers
In this section, we look at several MACs based on the use of a block cipher.
cipher-based Message authentication code (cMac) The Cipher-based 
Message Authentication Code mode of operation is for use with AES and triple 
DES. It is specified in SP 800-38B.
First, let us consider the operation of CMAC when the message is an integer 
multiple n of the cipher block length b. For AES, b = 128, and for triple DES, 
b = 64. The message is divided into n blocks (M1, M2, c , Mn). The algorithm 
makes use of a k-bit encryption key K and an n-bit key, K1. For AES, the key size 
k is 128, 192, or 256 bits; for triple DES, the key size is 112 or 168 bits. CMAC is 
calculated as follows (Figure 3.7).

C1 = E(K, M1)
 C2 = E(K, [M2 ‚äï C1])
 C3 = E(K, [M3 ‚äï C2])
#
#
#
 Cn = E(K, [MN ‚äï Cn-1 ‚äï K1])
 T = MSBTlen(Cn)
where
T    = message authentication code, also referred to as the tag
Tlen    = bit length of T
MSBs(X) = the s leftmost bits of the bit string X
If the message is not an integer multiple of the cipher block length, then 
the final block is padded to the right (least significant bits) with a 1 and as many 
Figure 3.7 Cipher-Based Message Authentication Code (CMAC)
Encrypt
K
K
K
T
Encrypt
Encrypt
MSB(Tlen)
M1
K1
K2
M2
Mn
(a) Message length is integer multiple of block size
Encrypt
K
K
K
T
Encrypt
Encrypt
MSB(Tlen)
10...0
(b) Message length is not integer multiple of block size
b
k
Mn
M1
M2

0s as necessary so that the final block is also of length b. The CMAC opera-
tion then proceeds as before, except that a different n-bit key K2 is used instead 
of¬†K1.
To generate the two n-bit keys, the block cipher is applied to the block that 
consists entirely of 0 bits. The first subkey is derived from the resulting ciphertext 
by a left shift of one bit and, conditionally, by XORing a constant that depends 
on the block size. The second subkey is derived in the same manner from the first 
subkey.
counter With cipher block chaining-Message authentication code The 
Counter with Cipher Block Chaining-Message Authentication Code (CCM) mode 
of operation, defined in SP 800-38C, is referred to as an authenticated encryption 
mode. ‚ÄúAuthenticated encryption‚Äù is a term used to describe encryption systems 
that simultaneously protect confidentiality and authenticity (integrity) of commu-
nications. Many applications and protocols require both forms of security, but until 
recently the two services have been designed separately.
The key algorithmic ingredients of CCM are the AES encryption algorithm 
(Section 2.2), the CTR mode of operation (Section 2.5), and the CMAC authenti-
cation algorithm. A single key K is used for both encryption and MAC algorithms. 
The input to the CCM encryption process consists of three elements.
1. Data that will be both authenticated and encrypted. This is the plaintext mes-
sage P of data block.
2. Associated data A that will be authenticated but not encrypted. An example 
is a protocol header that must be transmitted in the clear for proper protocol 
operation but which needs to be authenticated.
3. A nonce N that is assigned to the payload and the associated data. This is a 
unique value that is different for every instance during the lifetime of a pro-
tocol association and is intended to prevent replay attacks and certain other 
types of attacks.
Figure 3.8 illustrates the operation of CCM. For authentication, the input 
 includes the nonce, the associated data, and the plaintext. This input is formatted 
as a sequence of blocks B0 through Br. The first block contains the nonce plus some 
formatting bits that indicate the lengths of the N, A, and P elements. This is  followed 
by zero or more blocks that contain A, followed by zero or more blocks that contain 
P. The resulting sequence of blocks serves as input to the CMAC algorithm, which 
produces a MAC value with length Tlen, which is less than or equal to the block 
length (Figure 3.8a).
For encryption, a sequence of counters is generated that must be independent 
of the nonce. The authentication tag is encrypted in CTR mode using the single 
counter Ctr0. The Tlen most significant bits of the output are XORed with the tag 
to produce an encrypted tag. The remaining counters are used for the CTR mode 
encryption of the plaintext (Figure 2.11). The encrypted plaintext is concatenated 
with the encrypted tag to form the ciphertext output (Figure 3.8b).

3.4 pubLic-Key cryptogrAphy principLes
Of equal importance to conventional encryption is public-key encryption, which 
finds use in message authentication and key distribution. This section looks first 
at the basic concept of public-key encryption and takes a preliminary look at key 
distribution issues. Section 3.5 examines the two most important public-key algo-
rithms: RSA and Diffie‚ÄìHellman. Section 3.6 introduces digital signatures.
Figure 3.8 Counter with Cipher Block Chaining-Message Authentication Code
(a) Authentication
(b) Encryption
B0
Ctr0
B1
B2
Br
Tag
Tag
Nonce
Plaintext
Plaintext
Ciphertext
Ass. Data
K
CMAC
MSB(Tlen)
K
CTR
Ctr1, Ctr2, ..., Ctrm
Encrypt
K

Public-Key Encryption Structure
Public-key encryption, first publicly proposed by Diffie and Hellman in 1976 
[DIFF76], is the first truly revolutionary advance in encryption in literally thou-
sands of years. Public-key algorithms are based on mathematical functions rather 
than on simple operations on bit patterns, such as are used in symmetric encryption 
algorithms. More important, public-key cryptography is asymmetric, involving the 
use of two separate keys‚Äîin contrast to the symmetric conventional encryption, 
which uses only one key. The use of two keys has profound consequences in the 
areas of confidentiality, key distribution, and authentication.
Before proceeding, we should first mention several common misconcep-
tions concerning public-key encryption. One is that public-key encryption is more 
 secure from cryptanalysis than conventional encryption. In fact, the security of any 
 encryption scheme depends on (1) the length of the key and (2) the computational 
work involved in breaking a cipher. There is nothing in principle about either con-
ventional or public-key encryption that makes one superior to another from the 
point of view of resisting cryptanalysis. A second misconception is that public-key 
encryption is a general-purpose technique that has made conventional encryp-
tion obsolete. On the contrary, because of the computational overhead of current 
public-key encryption schemes, there seems no foreseeable likelihood that conven-
tional encryption will be abandoned. Finally, there is a feeling that key distribution 
is trivial when using public-key encryption, compared to the rather cumbersome 
handshaking involved with key distribution centers for conventional encryption. 
In fact, some form of protocol is needed, often involving a central agent, and the 
procedures involved are no simpler or any more efficient than those required for 
conventional encryption.
A public-key encryption scheme has six ingredients (Figure 3.9a).
‚óÜ
‚ñ† Plaintext: This is the readable message or data that is fed into the algorithm 
as input.
‚óÜ
‚ñ† Encryption algorithm: The encryption algorithm performs various transfor-
mations on the plaintext.
‚óÜ
‚ñ† Public and private key: This is a pair of keys that have been selected so that if 
one is used for encryption, the other is used for decryption. The exact trans-
formations performed by the encryption algorithm depend on the public or 
private key that is provided as input.
‚óÜ
‚ñ† Ciphertext: This is the scrambled message produced as output. It depends on 
the plaintext and the key. For a given message, two different keys will produce 
two different ciphertexts.
‚óÜ
‚ñ† Decryption algorithm: This algorithm accepts the ciphertext and the match-
ing key and produces the original plaintext.
As the names suggest, the public key of the pair is made public for others to 
use, while the private key is known only to its owner. A general-purpose public-key 
cryptographic algorithm relies on one key for encryption and a different but related 
key for decryption.

The essential steps are the following:
1. Each user generates a pair of keys to be used for the encryption and decryp-
tion of messages.
2. Each user places one of the two keys in a public register or other accessible 
file. This is the public key. The companion key is kept private. As Figure¬†3.9a 
Figure 3.9 Public-Key Cryptography
Plaintext
input
Bobs‚Äôs
public key
ring
Transmitted
ciphertext
Y = E[PUa, X]
X =
D[PRa, Y]
Plaintext
output
Encryption algorithm
(e.g., RSA)
Decryption algorithm
Joy
Mike
Mike
Bob
Ted
Alice
X
Plaintext
input
Transmitted
ciphertext
Y = E[PRb, X]
X =
D[PUb, Y]
Plaintext
output
Encryption algorithm
(e.g., RSA)
X
Alice‚Äôs public
key
Bob‚Äôs public
key
PUa
PUb
Bob‚Äôs private
key
PRb
PRa
Alice‚Äôs private
key
Decryption algorithm
Alice‚Äôs
public key
ring
Joy
Ted
(a) Encryption with public key
Bob
Alice
(b) Encryption with private key
Bob
Alice

suggests, each user maintains a collection of public keys obtained from 
others.
3. If Bob wishes to send a private message to Alice, Bob encrypts the message 
using Alice‚Äôs public key.
4. When Alice receives the message, she decrypts it using her private key. No 
other recipient can decrypt the message because only Alice knows Alice‚Äôs 
private key.
With this approach, all participants have access to public keys, and private 
keys are generated locally by each participant and therefore need never be distrib-
uted. As long as a user protects his or her private key, incoming communication is 
secure. At any time, a user can change the private key and publish the companion 
public key to replace the old public key.
The key used in conventional encryption is typically referred to as a secret 
key. The two keys used for public-key encryption are referred to as the public key 
and the private key. Invariably, the private key is kept secret, but it is referred 
to as a private key rather than a secret key to avoid confusion with conventional 
encryption.
Applications for Public-Key Cryptosystems
Before proceeding, we need to clarify one aspect of public-key cryptosystems that 
is otherwise likely to lead to confusion. Public-key systems are characterized by 
the use of a cryptographic type of algorithm with two keys, one held private and 
one available publicly. Depending on the application, the sender uses either the 
sender‚Äôs private key, the receiver‚Äôs public key, or both to perform some type of 
cryptographic function. In broad terms, we can classify the use of public-key crypto-
systems into three categories:
‚óÜ
‚ñ† Encryption/decryption: The sender encrypts a message with the recipient‚Äôs 
public key.
‚óÜ
‚ñ† Digital signature: The sender ‚Äúsigns‚Äù a message with its private key. Signing 
is achieved by a cryptographic algorithm applied to the message or to a small 
block of data that is a function of the message.
‚óÜ
‚ñ† Key exchange: Two sides cooperate to exchange a session key. Several dif-
ferent approaches are possible, involving the private key(s) of one or both 
parties.
Some algorithms are suitable for all three applications, whereas others can be 
used only for one or two of these applications. Table 3.2 indicates the applications 
supported by the algorithms discussed in this chapter: RSA and Diffie‚ÄìHellman. 
This table also includes the Digital Signature Standard (DSS) and elliptic-curve 
cryptography, also mentioned later in this chapter.
One general observation can be made at this point. Public-key algorithms 
 require considerably more computation than symmetric algorithms for compara-
ble security and a comparable plaintext length. Accordingly, public-key algorithms 
are¬†used only for short messages or data blocks, such as to encrypt a secret key 
or¬†PIN.

Requirements for Public-Key Cryptography
The cryptosystem illustrated in Figure 3.9 depends on a cryptographic algorithm 
based on two related keys. Diffie and Hellman postulated this system without dem-
onstrating that such algorithms exist. However, they did lay out the conditions that 
such algorithms must fulfill [DIFF76]:
1. It is computationally easy for a party B to generate a pair (public key PUb, 
private key PRb).
2. It is computationally easy for a sender A, knowing the public key and the 
message to be encrypted, M, to generate the corresponding ciphertext:
C = E(PUb, M)
3. It is computationally easy for the receiver B to decrypt the resulting cipher-
text using the private key to recover the original message:
M = D(PRb, C) = D[PRb, E(PUb, M)]
4. It is computationally infeasible for an opponent, knowing the public key, 
PUb, to determine the private key, PRb.
5. It is computationally infeasible for an opponent, knowing the public key, 
PUb, and a ciphertext, C, to recover the original message, M.
We can add a sixth requirement that, although useful, is not necessary for all 
public-key applications.
1. Either of the two related keys can be used for encryption, with the other used 
for decryption.
M = D[PUb, E(PRb, M)] = D[PRb, E(PUb, M)]
 3.5 pubLic-Key cryptogrAphy ALgorithMs
Two widely used public-key algorithms are RSA and Diffie‚ÄìHellman. We look at 
both of these in this section and then briefly introduce two other algorithms.4
4This section uses some elementary concepts from number theory. For a review, see Appendix A.
Algorithm
Encryption/Decryption
Digital Signature
Key Exchange
RSA
Yes
Yes
Yes
Diffie‚ÄìHellman
No
No
Yes
DSS
No
Yes
No
Elliptic curve
Yes
Yes
Yes
Table 3.2 Applications for Public-Key Cryptosystems

The RSA Public-Key Encryption Algorithm
One of the first public-key schemes was developed in 1977 by Ron Rivest, Adi 
Shamir, and Len Adleman at MIT and first published in 1978 [RIVE78]. The RSA 
scheme has until recently reigned supreme as the most widely accepted and imple-
mented approach to public-key encryption. Currently, both RSA and elliptic-curve 
cryptography are widely used. RSA is a block cipher in which the plaintext and 
ciphertext are integers between 0 and n - 1 for some n.
basic rsa encryption and decryption Encryption and decryption are of the fol-
lowing form period for some plaintext block M and ciphertext block C:
 C = Me mod n
 M = Cd mod n = (Me)d mod n = Med mod n
Both sender and receiver must know the values of n and e, and only the  receiver 
knows the value of d. This is a public-key encryption algorithm with a  public key of 
KU = {e, n} and a private key of KR = {d, n}. For this algorithm to be satisfactory 
for public-key encryption, the following requirements must be met.
1. It is possible to find values of e, d, n such that Med mod n = M for all M 6 n.
2. It is relatively easy to calculate Me and Cd for all values of M 6 n.
3. It is infeasible to determine d given e and n.
The first two requirements are easily met. The third requirement can be met 
for large values of e and n.
Figure 3.10 summarizes the RSA algorithm. Begin by selecting two prime 
numbers p and q and calculating their product n, which is the modulus for encryp-
tion and decryption. Next, we need the quantity f(n), referred to as the Euler  totient 
of n, which is the number of positive integers less than n and relatively prime to n. 
Then select an integer e that is relatively prime to f(n) [i.e., the greatest  common 
divisor of e and f(n) is 1]. Finally, calculate d as the multiplicative inverse of e, 
modulo f(n). It can be shown that d and e have the desired properties.
Suppose that user A has published its public key and that user B wishes to 
send the message M to A. Then B calculates C = Me (mod n) and transmits C. On 
receipt of this ciphertext, user A decrypts by calculating M = Cd (mod n).
An example, from [SING99], is shown in Figure 3.11. For this example, the 
keys were generated as follows:
1. Select two prime numbers, p = 17 and q = 11.
2. Calculate n = pq = 17 * 11 = 187.
3. Calculate f(n) = (p - 1)(q - 1) = 16 * 10 = 160.
4. Select e such that e is relatively prime to f(n) = 160 and less than f(n); we 
choose e = 7.
5. Determine d such that de mod 160 = 1 and d 6 160. The correct value is 
d = 23, because 23 * 7 = 161 = (1 * 160) + 1.
The resulting keys are public key PU = {7, 187} and private key PR = 
{23, 187}. The example shows the use of these keys for a plaintext input of M = 88.

For encryption, we need to calculate C = 887 mod 187. Exploiting the properties of 
modular arithmetic, we can do this as follows:
 887 mod 187 = [(884 mod 187) * (882 mod 187) * (881 mod 187)] mod 187
 881 mod 187 = 88 882 mod 187 = 7744 mod 187 = 77
 884 mod 187 = 59,969,536 mod 187 = 132
 887 mod 187 = (88 * 77 * 132) mod 187 = 894,432 mod 187 = 11
Figure 3.10 The RSA Algorithm
Select p, q
Select integer e
Calculate n = p √ó q
Calculate d
Calculate f(n) = (p ‚Äì 1)(q ‚Äì 1)
p and q both prime, p ‚â† q
Key Generation
Encryption
gcd (f(n), e) = 1; 1 < e < f(n)
Public key
de mod f(n) = 1
KU = {e, n}
Plaintext:
M < n
Ciphertext:
C = Me (mod n)
Private key
KR = {d, n}
Decryption
Ciphertext:
C
Plaintext:
M = Cd (mod n) 
Figure 3.11 Example of RSA Algorithm
Encryption
plaintext
88
plaintext
88
ciphertext
11
88  mod 187 = 11
PU = 7, 187
Decryption
7
11    mod 187 = 88
PR = 23, 187
23

For decryption, we calculate M = 1123 mod 187:
 1123 mod 187 = [(111 mod 187) * (112 mod 187) * (114 mod 187) *
(118 mod 187) * (118 mod 187)] mod 187
 111 mod 187 = 11
 112 mod 187 = 121
 114 mod 187 = 14,641 mod 187 = 55
 118 mod 187 = 214,358,881 mod 187 = 33
 1123 mod 187 = (11 * 121 * 55 * 33 * 33) mod 187
 = 79,720,245 mod 187 = 88
security considerations The security of RSA depends on it being used in such a 
way as to counter potential attacks. Four possible attack approaches are as follows:
‚óÜ
‚ñ† Mathematical attacks: There are several approaches, all equivalent in effort 
to factoring the product of two primes. The defense against mathematical 
 attacks is to use a large key size. Thus, the larger the number of bits in d, the 
better. However, because the calculations involved, both in key  generation 
and in  encryption/decryption, are complex, the larger the size of the key, 
the slower the system will run. SP 800-131A (Transitions: Recommendation 
for Transitioning the Use of Cryptographic Algorithms and Key Lengths, 
November 2015) recommends the use of a 2048-bit key size. A recent report 
from the European Union Agency for Network and Information Security 
(Algorithms, key size and parameters report‚Äî2014, November 2014) recom-
mends a 3072-bit key length. Either of these lengths should provide adequate 
security for a considerable time into the future.
‚óÜ
‚ñ† Timing attacks: These depend on the running time of the decryption algo-
rithm. Various approaches to mask the time required so as to thwart attempts 
to deduce key size have been suggested, such as introducing a random delay.
‚óÜ
‚ñ† Chosen ciphertext attacks: This type of attack exploits properties of the RSA 
algorithm by selecting blocks of data that, when processed using the target‚Äôs 
private key, yield information needed for cryptanalysis. These attacks can be 
thwarted by suitable padding of the plaintext.
To counter sophisticated chosen ciphertext attacks, RSA Security Inc., a 
leading RSA vendor and former holder of the RSA patent, recommends modify-
ing the plaintext using a procedure known as optimal asymmetric encryption pad-
ding (OAEP). A full discussion of the threats and OAEP are beyond our scope; 
see [POIN02] for an introduction and [BELL94a] for a thorough analysis. Here, we 
simply summarize the OAEP procedure.
Figure 3.12 depicts OAEP encryption. As a first step, the message M to be 
encrypted is padded. A set of optional parameters, P, is passed through a hash func-
tion, H. The output is then padded with zeros to get the desired length in the overall 
data block (DB). Next, a random seed is generated and passed through another hash 
function, called the mask generating function (MGF). The resulting hash value is bit-
by-bit XORed with DB to produce a maskedDB. The maskedDB is in turn passed

through the MGF to form a hash that is XORed with the seed to produce the masked 
seed. The concatenation of the maskedseed and the maskedDB forms the encoded 
message EM. Note that the EM includes the padded message masked by the seed, 
and the seed masked by the maskedDB. The EM is then encrypted using RSA.
Diffie‚ÄìHellman Key Exchange
The first published public-key algorithm appeared in the seminal paper by Diffie 
and Hellman that defined public-key cryptography [DIFF76] and is generally 
 referred to as the Diffie‚ÄìHellman key exchange. A number of commercial products 
employ this key exchange technique.
The purpose of the algorithm is to enable two users to securely exchange a 
secret key that then can be used for subsequent encryption of messages. The algo-
rithm itself is limited to the exchange of the keys.
The Diffie‚ÄìHellman algorithm depends for its effectiveness on the difficulty of 
computing discrete logarithms. Briefly, we can define the discrete logarithm in the 
following way. First, we define a primitive root of a prime number p as one whose 
Figure 3.12 Encryption Using Optimal Asymmetric Encryption Padding (OAEP)
seed
maskedseed
DB
maskedDB
M
EM
padding
H(P)
MGF
MGF
P
P = encoding parameters
M = message to be encoded
H = hash function
DB = data block
MGF = mask generating function
EM = encoded message

powers generate all the integers from 1 to p - 1. That is, if a is a primitive root of 
the prime number p, then the numbers
a mod p, a2 mod p, c , ap-1 mod p
are distinct and consist of the integers from 1 through p - 1 in some permutation.
For any integer b less than p and a primitive root a of prime number p, one can 
find a unique exponent i such that
b = ai mod p 0 ‚Ä¶ i ‚Ä¶ (p - 1)
The exponent i is referred to as the discrete logarithm, or index, of b for the base a, 
mod p. We denote this value as dloga, p(b).5
the algorithM With this background, we can define the Diffie‚ÄìHellman key 
 exchange, which is summarized in Figure 3.13. For this scheme, there are two pub-
licly known numbers: a prime number q and an integer a that is a primitive root 
of q. Suppose the users A and B wish to exchange a key. User A selects a random 
integer XA 6 q and computes YA = aXA mod q. Similarly, user B independently 
selects a random integer XB 6 q and computes YB = aXB mod q. Each side keeps 
the X value private and makes the Y value available publicly to the other side. 
User A computes the key as K = (YB)XA mod q and user B computes the key as 
K = (YA)XB mod q. These two calculations produce identical results:
 K = (YB)XA mod q
 = (aXB mod q)XA mod q
 = (aXB)XA mod q
 = aXBXA mod q
 = (aXA)XB mod q
 = (aXA mod q)XB mod q
 = (YA)XB mod q
The result is that the two sides have exchanged a secret value. Furthermore, 
because XA and XB are private, an adversary only has the following ingredients to 
work with: q, a, YA, and YB. Thus, the adversary is forced to take a discrete loga-
rithm to determine the key. For example, to determine the private key of user B, an 
adversary must compute
XB = dloga,q(YB)
The adversary can then calculate the key K in the same manner as user B does.
The security of the Diffie‚ÄìHellman key exchange lies in the fact that, while 
it is relatively easy to calculate exponentials modulo a prime, it is very difficult 
to calculate discrete logarithms. For large primes, the latter task is considered 
infeasible.
5Many texts refer to the discrete logarithm as the index. There is no generally agreed notation for this 
concept, much less an agreed name.

Figure 3.13 The Diffie‚ÄìHellman Key Exchange
Alice
Bob
Alice and Bob share a
prime q and A, such that
A < q and A is a primitive
root of q
Alice generates a private
key XA such that XA < q
Alice calculates a public
key YA = AXA mod q
Alice receives Bob‚Äôs
public key YB in plaintext
Alice calculates shared
secret key K = (YB)XA mod q
Bob calculates shared
secret key K = (YA)XB mod q
Bob receives Alice‚Äôs
public key YA in plaintext
Bob calculates a public
key YB = AXB mod q
Bob generates a private
key XB such that XB < q
Alice and Bob share a
prime q and A, such that
A < q and A is a primitive
root of q
YA
YB
Here is an example. Key exchange is based on the use of the prime number 
q = 353 and a primitive root of 353, in this case a = 3. A and B select secret keys 
XA = 97 and XB = 233, respectively. Each computes its public key:
 A computes YA = 397 mod 353 = 40. B computes YB = 3233 mod 353 = 248.
After they exchange public keys, each can compute the common secret key:
 A computes K = (YB)XA mod 353 = 24897 mod 353 = 160.
 B computes K = (YA)XB mod 353 = 40233 mod 353 = 160.
We assume an attacker would have available the following information:
q = 353; a = 3; YA = 40; YB = 248
In this simple example, it would be possible to determine the secret key 160 by 
brute force. In particular, an attacker E can determine the common key by discover-
ing a solution to the equation 3a mod 353 = 40 or the equation 3b mod 353 = 248. 
The brute-force approach is to calculate powers of 3 modulo 353, stopping when 
the result equals either 40 or 248. The desired answer is reached with the exponent 
value of 97, which provides 397 mod 353 = 40.
With larger numbers, the problem becomes impractical.

key exchange protocols Figure 3.13 shows a simple protocol that makes use of 
the Diffie‚ÄìHellman calculation. Suppose that user A wishes to set up a connection 
with user B and use a secret key to encrypt messages on that connection. User A 
can generate a one-time private key XA, calculate YA, and send that to user B. User 
B responds by generating a private value XB, calculating YB, and sending YB to user 
A. Both users can now calculate the key. The necessary public values q and a would 
need to be known ahead of time. Alternatively, user A could pick values for q and 
include those in the first message.
As an example of another use of the Diffie‚ÄìHellman algorithm, suppose that 
a group of users (e.g., all users on a LAN) each generate a long-lasting private 
value XA and calculate a public value YA. These public values, together with global 
public values for q and a, are stored in some central directory. At any time, user 
B can access user A‚Äôs public value, calculate a secret key, and use that to send an 
encrypted message to user A. If the central directory is trusted, then this form 
of communication provides both confidentiality and a degree of authentication. 
Because only A and B can determine the key, no other user can read the mes-
sage (confidentiality). Recipient A knows that only user B could have created a 
message using this key (authentication). However, the technique does not protect 
against replay attacks.
Man-in-the-Middle attack The protocol depicted in Figure 3.13 is insecure 
against a man-in-the-middle attack. Suppose Alice and Bob wish to exchange keys, 
and Darth is the adversary. The attack proceeds as follows (Figure 3.14):
1. Darth prepares for the attack by generating two random private keys XD1 
and XD2, and then computing the corresponding public keys YD1 and YD2.
2. Alice transmits YA to Bob.
3. Darth intercepts YA and transmits YD1 to Bob. Darth also calculates 
K2 = (YA)XD2 mod q.
4. Bob receives YD1 and calculates K1 = (YD1)XB mod q.
5. Bob transmits YB to Alice.
6. Darth intercepts YB and transmits YD2 to Alice. Darth calculates 
K1 = (YB)XD1 mod q.
7. Alice receives YD2 and calculates K2 = (YD2)XA mod q.
At this point, Bob and Alice think that they share a secret key. Instead Bob 
and Darth share secret key K1, and Alice and Darth share secret key K2. All future 
communication between Bob and Alice is compromised in the following way:
1. Alice sends an encrypted message M: E(K2, M).
2. Darth intercepts the encrypted message and decrypts it to recover M.
3. Darth sends Bob E(K1, M) or E(K1, M‚Ä≤), where M‚Ä≤ is any message. In the 
first case, Darth simply wants to eavesdrop on the communication without 
altering it. In the second case, Darth wants to modify the message going to 
Bob.

The key exchange protocol is vulnerable to such an attack because it does not 
authenticate the participants. This vulnerability can be overcome with the use of 
digital signatures and public-key certificates; these topics are explored later in this 
chapter and in Chapter 4.
Other Public-Key Cryptography Algorithms
Two other public-key algorithms have found commercial acceptance: DSS and 
 elliptic-curve cryptography.
digital signature standard The National Institute of Standards and Technology 
(NIST) has published Federal Information Processing Standard FIPS PUB 186, 
known as the Digital Signature Standard (DSS). The DSS makes use of the SHA-1 
and presents a new digital signature technique, the Digital Signature Algorithm 
Figure 3.14 Man-in-the-Middle Attack
Alice
Darth
Bob
Private key XA
Public key
YA = AXA mod q 
Private key XB
Public key
YB = AXB mod q 
Private keys XD1, XD2
Public keys
YD1 = AXD1 mod q
YD2 = AXD2 mod q
YA 
Secret key
K2 = (YA)XD2 mod q
Secret key
K1 = (YB)XD1 mod q
Secret key
K1 = (YD1)XB mod q
Secret key
K2 = (YD2)XA mod q
Alice and Darth
share K2
Bob and Darth
share K1
YD2 
YD1 
YB

(DSA). The DSS was originally proposed in 1991 and revised in 1993 in response to 
public feedback concerning the security of the scheme. There was a further minor 
revision in 1996. The DSS uses an algorithm that is designed to provide only the 
digital signature function. Unlike RSA, it cannot be used for encryption or key 
exchange.
elliptic-curve cryptography The vast majority of the products and standards 
that use public-key cryptography for encryption and digital signatures use RSA. 
The bit length for secure RSA use has increased over recent years, and this has 
put a heavier processing load on applications using RSA. This burden has ramifica-
tions, especially for electronic commerce sites that conduct large numbers of secure 
transactions. Recently, a competing system has begun to challenge RSA: elliptic 
curve cryptography (ECC). Already, ECC is showing up in standardization efforts, 
including the IEEE P1363 Standard for Public-Key Cryptography.
The principal attraction of ECC compared to RSA is that it appears to offer 
equal security for a far smaller bit size, thereby reducing processing overhead. On 
the other hand, although the theory of ECC has been around for some time, it is 
only recently that products have begun to appear and that there has been sustained 
cryptanalytic interest in probing for weaknesses. Thus, the confidence level in ECC 
is not yet as high as that in RSA.
ECC is fundamentally more difficult to explain than either RSA or Diffie‚Äì
Hellman, and a full mathematical description is beyond the scope of this book. 
The¬†technique is based on the use of a mathematical construct known as the elliptic 
curve.
 3.6 digitAL signAtures
NIST FIPS PUB 186-4 [Digital Signature Standard (DSS), July 2013] defines a digi-
tal signature as follows: The result of a cryptographic transformation of data that, 
when properly implemented, provides a mechanism for verifying origin authentica-
tion, data integrity, and signatory non-repudiation.
Thus, a digital signature is a data-dependent bit pattern, generated by an agent 
as a function of a file, message, or other form of data block. Another agent can ac-
cess the data block and its associated signature and verify that (1) the data block has 
been signed by the alleged signer and that (2) the data block has not been altered 
since the signing. Further, the signer cannot repudiate the signature.
FIPS 186-4 specifies the use of one of three digital signature algorithms:
‚óÜ
‚ñ† Digital Signature Algorithm (DSA): The original NIST-approved algorithm, 
which is based on the difficulty of computing discrete logarithms.
‚óÜ
‚ñ† RSA Digital Signature Algorithm: Based on the RSA public-key algorithm.
‚óÜ
‚ñ† Elliptic Curve Digital Signature Algorithm (ECDSA): Based on elliptic-
curve cryptography.
In this section, we provide a brief overview of the digital signature process, 
then describe the RSA digital signature algorithm.

Digital Signature Generation and Verification
Figure 3.15 is a generic model of the process of making and using digital signatures. 
All of the digital signature schemes in FIPS 186-4 have this structure. Suppose that 
Bob wants to send a message to Alice. Although it is not important that the mes-
sage be kept as a secret, he wants Alice to be certain that the message is indeed 
from him. For this purpose, Bob uses a secure hash function, such as SHA-512, to 
generate a hash value for the message. That hash value, together with Bob‚Äôs private 
key, serve as input to a digital signature generation algorithm that produces a short 
block that functions as a digital signature. Bob sends the message with the signa-
ture attached. When Alice receives the message plus signature, she (1) calculates 
Figure 3.15 Simplified Depiction of Essential Elements of Digital Signature Process
Bob
Alice
Bob‚Äôs
signature
for M
Message M
Cryptographic
hash
function
Digital
signature
generation
algorithm
Digital
signature
verifcation
algorithm
h
Message M
Cryptographic
hash
function
h
S
Message M
S
Return
signature
valid or not valid
Bob‚Äôs
private
key
(a) Bob signs a message
(b) Alice verifes the signature
Bob‚Äôs
public
key

a hash value for the message and (2) provides the hash value and Bob‚Äôs public key 
as inputs to a digital signature verification algorithm. If the algorithm returns the 
result that the signature is valid, Alice is assured that the message must have been 
signed by Bob. No one else has Bob‚Äôs private key and therefore no one else could 
have created a signature that could be verified for this message with Bob‚Äôs public 
key. In addition, it is impossible to alter the message without access to Bob‚Äôs pri-
vate key, so the message is authenticated both in terms of source and in terms of 
data integrity.
It is important to emphasize that the encryption process just described does 
not provide confidentiality. That is, the message being sent is safe from alteration, 
but not safe from eavesdropping. This is obvious in the case of a signature based 
on a portion of the message, because the rest of the message is transmitted in 
the clear. Even in the case of complete encryption, there is no protection of con-
fidentiality because any observer can decrypt the message by using the sender‚Äôs 
public¬†key.
RSA Digital Signature Algorithm
The essence of the RSA digital signature algorithm is to encrypt the hash of the 
message to be signed using RSA. However, as with the use of RSA for encryption of 
keys or short messages, the RSA digital signature algorithm first modifies the hash 
value to enhance security. There are several approaches to this, one of which is the 
RSA Probabilistic Signature Scheme (RSA-PSS). RSA-PSS is the latest of the RSA 
schemes and the one that RSA Laboratories recommends as the most secure of the 
RSA digital signature schemes. We provide a brief overview here; for more detail 
see [STAL16].
Figure 3.16 illustrates the RSS-PSS signature generation process. The steps 
are as follows:
1. Generate a hash value, or message digest, mHash from the message M to be 
signed.
2. Pad mHash with a constant value padding1 and pseudorandom value salt to 
form M'
3. Generate hash value H from M'.
4. Generate a block DB consisting of a constant value padding 2 and salt.
5. Use the mask generating function MGF, which produces a randomized out-
put from input H of the same length as DB.
6. Create the encoded message (EM) block by padding H with the hexadecimal 
constant BC and the XOR of H and DB.
7. Encrypt EM with RSA using the signer‚Äôs private key.
The objective with this algorithm is to make it more difficult for an adversary 
to find another message that maps to the same message digest as a given message or 
to find two messages that map to the same message digest. Because the salt changes 
with every use, signing the same message twice using the same private key will yield 
two different signatures. This is an added measure of security.

3.7 Key terMs, review Questions, And probLeMs
Hash
Hash
MGF
M
Signature
mHash
salt
padding1
bc
maskedDB
salt
padding2
M≈ì =
DB =
EM =
H
E
signer‚Äôs
private key
Figure 3.16 RSA-PSS Encoding and Signature Generation
Key Terms 
authenticated encryption
Diffie‚ÄìHellman key  
exchange
digital signature
Digital Signature Standard 
(DSS)
elliptic-curve cryptography 
(ECC)
HMAC
key exchange
MD5
message authentication
message authentication code 
(MAC)
message digest
one-way hash function
private key
public key
public-key certificate
public-key encryption
RSA
secret key
secure hash function
SHA-1
strong collision resistant
weak collision resistant
Review Questions 
 
3.1 
List three approaches to message authentication.
 
3.2 
What is a message authentication code?

3.3 
What is a one-way hash function? How is it different from message authentication code?
 
3.4 
List the properties of a strong hash function.
 
3.5 
Compare SHA-1 and SHA-2 with respect to SHA parameters.
 
3.6 
List the design objectives for HMAC.
 
3.7 
Differentiate between public-key cryptosystem and symmetric encryption algorithm.
 
3.8 
List the public-key cryptography algorithm used in digital signatures, encryption/
decryption, and key exchange.
 
3.9 
In what way is the Diffie‚ÄìHellman key exchange algorithm insecure against a  
man-in-the-middle attack?
Problems 
 
3.1 
Consider a 32-bit hash function defined as the concatenation of two 16-bit functions: 
XOR and RXOR, which are defined in Section 3.2 as ‚Äútwo simple hash functions.‚Äù
a. Will this checksum detect all errors caused by an odd number of error bits? 
 Explain.
b. Will this checksum detect all errors caused by an even number of error bits? If not, 
characterize the error patterns that will cause the checksum to fail.
c. Comment on the effectiveness of this function for use as a hash function for 
 authentication.
 
3.2 
Suppose H(m) is a collision-resistant hash function that maps a message of arbitrary 
bit length into an n-bit hash value. Is it true that, for all messages x, x‚Ä≤ with x [ x‚Ä≤, we 
have H(x) ‚â† H(x‚Ä≤)? Explain your answer.
 
3.3 
State the value of the padding field in SHA-512 if the length of the message is
a. 4987 bits
b. 4199 bits
c. 1227 bits
 
3.4 
State the value of the length field in SHA-512 if the length of the message is
a. 3967 bits
b. 3968 bits
c. 3969 bits
 
3.5 
a. Consider the following hash function. Messages are in the form of a sequence 
of decimal numbers, M = (a1, a2, c , at). The hash value h is calculated as 
¬¢ a
t
i=1
ai‚â§mod n, for some predefined value n. Does this hash function satisfy any 
of the requirements for a hash function listed in Section 3.2? Explain your answer.
b. Repeat part (a) for the hash function h = ¬¢ a
t
i=1
(ai)2‚â§mod n.
c. Calculate the hash function of part (b) for M = (237, 632, 913, 423, 349) and 
n = 757.
 
3.6 
This problem introduces a hash function similar in spirit to SHA that operates on let-
ters instead of binary data. It is called the toy tetragraph hash (tth).6 Given a message 
consisting of a sequence of letters, tth produces a hash value consisting of four let-
ters. First, tth divides the message into blocks of 16 letters, ignoring spaces, punctua-
tion, and capitalization. If the message length is not divisible by 16, it is padded out 
with¬†nulls. A four-number running total is maintained that starts out with the value 
(0, 0, 0, 0); this is input to the compression function for processing the first block. The 
compression function consists of two rounds. Round 1: Get the next block of text and 
arrange it as a row-wise 4 block of text and covert it to numbers (A = 0, B = 1, etc.). 
For example, for the block ABCDEFGHIJKLMNOP, we have
6I thank William K. Mason of the magazine staff of The Cryptogram for providing this example.

A
B
C
D
E
F
G
H
I
J
K
L
M
N
O
P
0
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Then, add each column mod 26 and add the result to the running total, mod 26. In this 
example, the running total is (24, 2, 6, 10). Round 2: Using the matrix from round 1, 
rotate the first row left by 1, second row left by 2, third row left by 3, and reverse the 
order of the fourth row. In our example:
B
C
D
A
G
H
E
F
L
I
J
K
P
O
N
M
1
2
3
0
6
7
4
5
11
8
9
10
15
14
13
12
Now, add each column mod 26 and add the result to the running total. The new run-
ning total is (5, 7, 9, 11). This running total is now the input into the first round of the 
compression function for the next block of text. After the final block is processed, 
 convert the final running total to letters. For example, if the message is ABCDE 
FGHIJKLMNOP, then the hash is FHJL.
a. Draw figures comparable to Figures 3.4 and 3.5 to depict the overall tth logic and 
the compression function logic.
b. Calculate the hash function for the 48-letter message ‚ÄúI leave twenty million dol-
lars to my friendly cousin Bill.‚Äù
c. To demonstrate the weakness of tth, find a 48-letter block that produces the same 
hash as that just derived. Hint: Use lots of A‚Äôs.
 
3.7 
It is possible to use a hash function to construct a block cipher with a structure similar 
to DES. Because a hash function is one way and a block cipher must be reversible (to 
decrypt), how is it possible?
 
3.8 
Now consider the opposite problem: Use an encryption algorithm to construct a one-
way hash function. Consider using RSA with a known key. Then process a message 
consisting of a sequence of blocks as follows: Encrypt the first block, XOR the result 
with the second block and encrypt again, and so on. Show that this scheme is not 
secure by solving the following problem. Given a two-block message B1, B2, and its 
hash, we have
 
RSAH(B1, B2) = RSA(RSA(B1) ‚äï B2) 
Given an arbitrary block C1, choose C2 so that RSAH(C1, C2) = RSAH(B1, B2). 
Thus, the hash function does not satisfy weak collision resistance.
 
3.9 
One of the most widely used MACs, referred to as the Data Authentication 
Algorithm, is based on DES. The algorithm is both a FIPS publication (FIPS PUB 
113) and an ANSI standard (X9.17). The algorithm can be defined as using the cipher 
block chaining (CBC) mode of operation of DES with an initialization vector of zero 
(Figure 2.9). The data (e.g., message, record, file, or program) to be authenticated is 
grouped into contiguous 64-bit blocks: P1, P2, c , PN. If necessary, the final block is 
padded on the right with 0s to form a full 64-bit block. The MAC consists of either 
the entire ciphertext block CN or the leftmost M bits of the block with 16 ‚Ä¶ M ‚Ä¶ 64. 
Show that the same result can be produced using the cipher feedback mode.
 
3.10 
In this problem, we will compare the security services that are provided by digital 
signatures (DS) and message authentication codes (MAC). We assume that Oscar

is able to observe all messages sent from Alice to Bob and vice versa. Oscar has no 
knowledge of any keys but the public one in case of DS. State whether and how (i) 
DS and (ii) MAC protect against each attack. The value auth(x) is computed with 
a DS or a MAC algorithm, respectively.
a. (Message integrity) Alice sends a message x = ‚ÄúTransfer $1000 to Mark‚Äù 
in the clear and also sends auth(x) to Bob. Oscar intercepts the message and 
replaces ‚ÄúMark‚Äù with ‚ÄúOscar.‚Äù Will Bob detect this?
b. (Replay) Alice sends a message x = ‚ÄúTransfer $1000 to Oscar‚Äù in the 
clear and also sends auth(x) to Bob. Oscar observes the message and signature 
and sends them 100 times to Bob. Will Bob detect this?
c. (Sender Authentication with cheating third party) Oscar claims that he sent some 
message x with a valid auth(x) to Bob, but Alice claims the same. Can Bob clear 
the question in either case?
d. (Authentication with Bob cheating) Bob claims that he received a message x with 
a valid signature auth(x) from Alice (e.g., ‚ÄúTransfer $1000 from Alice to Bob‚Äù) 
but Alice claims she has never sent it. Can Alice clear this question in either case?
 
3.11 
Figure 3.17 shows an alternative means of implementing HMAC.
a. Describe the operation of this implementation.
b. What potential benefit does this implementation have over that shown in Figure 3.6?
Figure 3.17 Efficient Implementation of HMAC
b bits
b bits
b bits
Precomputed
Computed per message
Hash
IV
n bits
b bits
n bits
Pad to b bits
n bits
n bits
HMAC(K, M)
f
IV
b bits
f
f
Si
So
Y0
Y1
ipad
K+
K+
opad
YL‚Äì1
H(Si || M)

3.12 
In this problem, we demonstrate that for CMAC, a variant that XORs the second 
key after applying the final encryption doesn‚Äôt work. Let us consider this for the 
case of the message being an integer multiple of the block size. Then the variant 
can be expressed as VMAC(K, M) = CBC(K, M) ‚äï K1. Now suppose an adver-
sary is able to ask for the MACs of three messages: the message 0 = 0n, where n is 
the cipher block size; the message 1 = 1n; and the message 1} 0. As a result of these 
three queries, the adversary gets T0 = CBC(K, 0) ‚äï K1; T1 = CBC(K, 1) ‚äï K1 and 
T2 = CBC(K, [CBC(K, 1)]) ‚äï K1. Show that the adversary can compute the correct 
MAC for the (unqueried) message 0} (T0 ‚äï T1).
 
3.13 
Prior to the discovery of any specific public-key schemes, such as RSA, an existence 
proof was developed whose purpose was to demonstrate that public-key encryption is 
possible in theory. Consider the functions f1(x1) = z1; f2(x2, y2) = z2; f3(x3, y3) = z3, 
where all values are integers with 1 ‚Ä¶ xi, yi, zi ‚Ä¶ N. Function f1 can be represented 
by a vector M1 of length N in which the kth entry is the value of f1(k). Similarly, f2 
and f3 can be represented by N * N matrices M2 and M3. The intent is to represent 
the encryption/decryption process by table lookups for tables with very large values 
of N. Such tables would be impractically huge but in principle could be constructed. 
The scheme works as follows: Construct M1 with a random permutation of all inte-
gers between 1 and N; that is, each integer appears exactly once in M1. Construct M2 
so that each row contains a random permutation of the first N integers. Finally, fill in 
M3 to satisfy the condition:
 
f3(f2(f1(k), p), k) = p for all k, p with 1 ‚Ä¶ k, p ‚Ä¶ N 
In words,
1.  M1 takes an input k and produces an output x.
2. M2 takes inputs x and p giving output z.
3. M3 takes inputs z and k and produces p.
The three tables, once constructed, are made public.
a. It should be clear that it is possible to construct M3 to satisfy the preceding condi-
tion. As an example, fill in M3 for the following simple case:
5
5
2
3
4
1
4
4
2
5
1
3
M1 =
2
M2 =
1
3
2
4
5
M3 =
3
3
1
4
2
5
1
2
5
3
4
1
Convention: The ith element of M1 corresponds to k = i. The ith row of M2 cor-
responds to x = i; the jth column of M2 corresponds to p = j. The ith row of M3 
corresponds to z = i; the jth column of M3 corresponds to k = j.
b. Describe the use of this set of tables to perform encryption and decryption 
 between two users.
c. Argue that this is a secure scheme.
 
3.14 
Perform encryption and decryption using the RSA algorithm (Figure 3.10) for the 
following:
a. p = 3; q = 11, e = 7; M = 2
b. p = 5; q = 11, e = 3; M = 5
c. p = 7; q = 11, e = 17; M = 2
d. p = 11; q = 13, e = 11; M = 3
e. p = 17; q = 11, e = 7; M = 88
Hint: Decryption is not as hard as you think; use some finesse.
 
3.15 
In a public-key system using RSA, you intercept the ciphertext C = 16 sent to a user 
whose public key is e = 6, n = 40. What is the plaintext M?

3.16 
In an RSA system, the public key of a given user is e = 7, n = 137. What is the  
private key of this user?
 
3.17 
Suppose we have a set of blocks encoded with the RSA algorithm and we don‚Äôt have 
the private key. Assume n = pq, e is the public key. Suppose also someone tells us 
they know one of the plaintext blocks has a common factor with n. Does this help us 
in any way?
 
3.18 
Show how RSA can be represented by matrices M1, M2, and M3 of Problem 3.4.
 
3.19 
Consider the following scheme.
1.  Pick an odd number, E.
2.   Pick two prime numbers, P and Q, where (P - 1)(Q - 1) is relatively prime to E.
3.  Multiply P and Q to get N.
5.  Calculate D =
(P - 1)(Q - 1)(E + 1) + 1
E
Is this scheme equivalent to RSA? Show why or why not.
 
3.20 
Suppose Bob uses the RSA cryptosystem with a very large modulus n for which the 
factorization cannot be found in a reasonable amount of time. Suppose Alice sends 
a message to Bob by representing each alphabetic character as an integer between 
0 and 25 (A S 0, c , Z S 25), and then encrypting each number separately using 
RSA with large e and large n. Is this method secure? If not, describe the most effi-
cient attack against this encryption method.
 
3.21 
Consider a Diffie‚ÄìHellman scheme with a common prime q = 353 and a primitive 
root a = 3.
a. If user A has public key YA = 40, what is A‚Äôs private key XA?
b. If user B has public key YB = 248, what is the shared secret key K?



4.1 
Remote User Authentication Principles
The NIST Model for Electronic User Authentication
Means of Authentication
4.2 
Symmetric Key Distribution Using Symmetric Encryption
4.3 
Kerberos
Kerberos Version 4
Kerberos Version 5
4.4 
Key Distribution Using Asymmetric Encryption
Public-Key Certificates
Public-Key Distribution of Secret Keys
4.5 
X.509 Certificates
Certificates
X.509 Version 3
4.6 
Public-Key Infrastructure
PKIX Management Functions
PKIX Management Protocols
4.7 
Federated Identity Management
Identity Management
Identity Federation
4.8 
Key Terms, Review Questions, and Problems
Chapter
Key Distribution and User 
Authentication
Part two: Network Security aPPlicatioNS

This chapter covers two important related concepts. First is the complex topic of cryp-
tographic key distribution, involving cryptographic, protocol, and management con-
siderations. This chapter gives the reader a feel for the issues involved and provides a 
broad survey of the various aspects of key management and distribution.
This chapter also examines some of the authentication functions that have been 
developed to support network-based user authentication. The chapter includes a detail 
discussion of one of the earliest and also one of the most widely used key distribution 
and user authentication services: Kerberos. Next, the chapter looks at key distribution 
schemes that rely on asymmetric encryption. This is followed by a discussion of X.509 
certificates and public-key infrastructure. Finally, the concept of federated identity 
management is introduced.
 4.1 Remote UseR AUthenticAtion PRinciPles
In most computer security contexts, user authentication is the fundamental build-
ing block and the primary line of defense. User authentication is the basis for most 
types of access control and for user accountability. RFC 4949 (Internet Security 
Glossary) defines user authentication as the process of verifying an identity claimed 
by or for a system entity. This process consists of two steps:
‚ñ†
‚ñ† Identification step: Presenting an identifier to the security system. (Identifiers 
should be assigned carefully, because authenticated identities are the basis for 
other security services, such as access control service.)
‚ñ†
‚ñ† Verification step: Presenting or generating authentication information that 
corroborates the binding between the entity and the identifier.
For example, user Alice Toklas could have the user identifier ABTOKLAS. 
This information needs to be stored on any server or computer system that Alice 
wishes to use and could be known to system administrators and other users. A¬†typi-
cal item of authentication information associated with this user ID is a password, 
leARning objectives
After studying this chapter, you should be able to:
‚óÜ‚ñ†
Understand the issues involved in the use of symmetric encryption to 
 distribute symmetric keys.
‚óÜ‚ñ†
Give a presentation on Kerberos.
‚óÜ‚ñ†
Explain the differences between versions 4 and 5 of Kerberos.
‚óÜ‚ñ†
Understand the issues involved in the use of asymmetric encryption to 
distribute symmetric keys.
‚óÜ‚ñ†
List and explain the elements in an X.509 certificate.
‚óÜ‚ñ†
Present an overview of public-key infrastructure concepts.
‚óÜ‚ñ†
Understand the need for a federated identity management system.

which is kept secret (known only to Alice and to the system). If no one is able 
to obtain or guess Alice‚Äôs password, then the combination of Alice‚Äôs user ID and 
password enables administrators to set up Alice‚Äôs access permissions and audit her 
activity. Because Alice‚Äôs ID is not secret, system users can send her e-mail, but 
because her password is secret, no one can pretend to be Alice.
In essence, identification is the means by which a user provides a claimed iden-
tity to the system; user authentication is the means of establishing the validity of 
the claim. Note that user authentication is distinct from message authentication. As 
 defined in Chapter 3, message authentication is a procedure that allows communicat-
ing parties to verify that the contents of a received message have not been altered and 
that the source is authentic. This chapter is concerned solely with user authentication.
The NIST Model for Electronic User Authentication
NIST SP 800-63-2 (Electronic Authentication Guideline, August 2013) defines elec-
tronic user authentication as the process of establishing confidence in user identities 
that are presented electronically to an information system. Systems can use the authen-
ticated identity to determine if the authenticated individual is authorized to perform 
particular functions, such as database transactions or access to system resources. In 
many cases, the authentication and transaction or other authorized function take 
place across an open network such as the Internet. Equally, authentication and subse-
quent authorization can take place locally, such as across a local area network.
SP 800-63-2 defines a general model for user authentication that involves 
a number of entities and procedures. We discuss this model with reference to 
Figure¬†4.1.
The initial requirement for performing user authentication is that the user must 
be registered with the system. The following is a typical sequence for registration. An 
applicant applies to a registration authority (RA) to become a subscriber of a cre-
dential service provider (CSP). In this model, the RA is a trusted entity that estab-
lishes and vouches for the identity of an applicant to a CSP. The CSP then engages 
Figure 4.1 The NIST SP 800-63-2 E-Authentication Architectural Model
Registration
authority (RA)
Registration, credential issuance,
and maintenance
E-Authentication using
token and credential
Identity proofng
User registration
Token, credential
Registration/issuance
Authenticated session
Authenticated protocol
Exchange
Authenticated
assertion
Registration
Confrmation
Token/credential
Validation
Relying
party (RP)
Verifer
Subscriber/
claimant
Credential
service
provider (RA)

in an exchange with the subscriber. Depending on the details of the overall authen-
tication system, the CSP issues some sort of electronic credential to the subscriber. 
The credential is a data structure that authoritatively binds an identity and additional 
attributes to a token possessed by a subscriber, and can be verified when presented 
to the verifier in an authentication transaction. The token could be an encryption 
key or an encrypted password that identifies the subscriber. The token may be issued 
by the CSP, generated directly by the subscriber, or provided by a third party. The 
token and credential may be used in subsequent authentication events.
Once a user is registered as a subscriber, the actual authentication process can 
take place between the subscriber and one or more systems that perform authen-
tication and, subsequently, authorization. The party to be authenticated is called a 
claimant and the party verifying that identity is called a verifier. When a claimant 
successfully demonstrates possession and control of a token to a verifier through an 
authentication protocol, the verifier can verify that the claimant is the subscriber 
named in the corresponding credential. The verifier passes on an assertion about the 
identity of the subscriber to the relying party (RP). That assertion includes identity 
information about a subscriber, such as the subscriber name, an identifier assigned 
at registration, or other subscriber attributes that were verified in the registration 
process. The RP can use the authenticated information provided by the verifier to 
make access control or authorization decisions.
An implemented system for authentication will differ from or be more com-
plex than this simplified model, but the model illustrates the key roles and functions 
needed for a secure authentication system.
Means of Authentication
There are four general means of authenticating a user‚Äôs identity, which can be used 
alone or in combination:
‚ñ†
‚ñ† Something the individual knows: Examples include a password, a personal 
identification number (PIN), or answers to a prearranged set of questions.
‚ñ†
‚ñ† Something the individual possesses: Examples include cryptographic keys, 
electronic keycards, smart cards, and physical keys. This type of authenticator 
is referred to as a token.
‚ñ†
‚ñ† Something the individual is (static biometrics): Examples include recognition 
by fingerprint, retina, and face.
‚ñ†
‚ñ† Something the individual does (dynamic biometrics): Examples include 
 recognition by voice pattern, handwriting characteristics, and typing rhythm.
All of these methods, properly implemented and used, can provide secure 
user authentication. However, each method has problems. An adversary may be 
able to guess or steal a password. Similarly, an adversary may be able to forge or 
steal a token. A user may forget a password or lose a token. Furthermore, there 
is a significant administrative overhead for managing password and token infor-
mation on systems and securing such information on systems. With respect to bio-
metric authenticators, there are a variety of problems, including dealing with false 
positives and false negatives, user acceptance, cost, and convenience. For network-
based user authentication, the most important methods involve cryptographic keys 
and something the individual knows, such as a password.

4.2 symmetRic Key DistRibUtion Using 
symmetRic¬†encRyPtion
For symmetric encryption to work, the two parties to an exchange must share the 
same key, and that key must be protected from access by others. Furthermore, fre-
quent key changes are usually desirable to limit the amount of data compromised if 
an attacker learns the key. Therefore, the strength of any cryptographic system rests 
with the ‚Äúkey distribution technique,‚Äù a term that refers to the means of delivering 
a key to two parties that wish to exchange data, without allowing others to see the 
key. Key distribution can be achieved in a number of ways. For two parties A and B, 
there are the following options:
1. A key could be selected by A and physically delivered to B.
2. A third party could select the key and physically deliver it to A and B.
3. If A and B have previously and recently used a key, one party could transmit 
the new key to the other, using the old key to encrypt the new key.
4. If A and B each have an encrypted connection to a third party C, C could 
deliver a key on the encrypted links to A and B.
Options 1 and 2 call for manual delivery of a key. For link encryption, this is 
a reasonable requirement, because each link encryption device is only going to be 
exchanging data with its partner on the other end of the link. However, for end-to-
end encryption over a network, manual delivery is awkward. In a distributed sys-
tem, any given host or terminal may need to engage in exchanges with many other 
hosts and terminals over time. Thus, each device needs a number of keys supplied 
dynamically. The problem is especially difficult in a wide-area  distributed system.
Option 3 is a possibility for either link encryption or end-to-end encryption, but 
if an attacker ever succeeds in gaining access to one key, then all subsequent keys are 
revealed. Even if frequent changes are made to the link encryption keys, these should 
be done manually. To provide keys for end-to-end encryption, option 4 is preferable.
For option 4, two kinds of keys are used:
‚ñ†
‚ñ† Session key: When two end systems (hosts, terminals, etc.) wish to communi-
cate, they establish a logical connection (e.g., virtual circuit). For the duration of 
that logical connection, called a session, all user data are encrypted with a one-
time session key. At the conclusion of the session the session key is destroyed.
‚ñ†
‚ñ† Permanent key: A permanent key is a key used between entities for the 
 purpose of distributing session keys.
A necessary element of option 4 is a key distribution center (KDC). The KDC 
determines which systems are allowed to communicate with each other. When 
 permission is granted for two systems to establish a connection, the key distribution 
center provides a one-time session key for that connection.
In general terms, the operation of a KDC proceeds as follows:
1. When host A wishes to set up a connection to host B, it transmits a 
 connection-request packet to the KDC. The communication between A and 
the KDC is encrypted using a master key shared only by A and the KDC.

2. If the KDC approves the connection request, it generates a unique one-time 
session key. It encrypts the session key using the permanent key it shares with 
A and delivers the encrypted session key to A. Similarly, it encrypts the ses-
sion key using the permanent key it shares with B and delivers the encrypted 
session key to B.
3. A and B can now set up a logical connection and exchange messages and 
data, all encrypted using the temporary session key.
The automated key distribution approach provides the flexibility and dynamic 
characteristics needed to allow a number of users to access a number of servers and 
for the servers to exchange data with each other. The most widely used application 
that implements this approach is Kerberos, described in the next section.
 4.3 KeRbeRos
Kerberos is a key distribution and user authentication service developed at MIT. 
The problem that Kerberos addresses is this: Assume an open distributed environ-
ment in which users at workstations wish to access services on servers distributed 
throughout the network. We would like for servers to be able to restrict access to 
authorized users and to be able to authenticate requests for service. In this envi-
ronment, a workstation cannot be trusted to identify its users correctly to network 
services. In particular, the following three threats exist:
1. A user may gain access to a particular workstation and pretend to be another 
user operating from that workstation.
2. A user may alter the network address of a workstation so that the requests 
sent from the altered workstation appear to come from the impersonated 
workstation.
3. A user may eavesdrop on exchanges and use a replay attack to gain entrance 
to a server or to disrupt operations.
In any of these cases, an unauthorized user may be able to gain access to services 
and data that he or she is not authorized to access. Rather than building elaborate 
authentication protocols at each server, Kerberos provides a centralized authentica-
tion server whose function is to authenticate users to servers and servers to users. 
Kerberos relies exclusively on symmetric encryption, making no use of public-key 
encryption.
Two versions of Kerberos are in use. Version 4 [MILL88, STEI88] implemen-
tations still exist, although this version is being phased out. Version 5 [KOHL94] 
corrects some of the security deficiencies of version 4 and has been issued as a pro-
posed Internet Standard (RFC 4120).
Because of the complexity of Kerberos, it is best to start with a description 
of version 4. This enables us to see the essence of the Kerberos strategy without 
considering some of the details required to handle subtle security threats. Then, we 
examine version 5.

Kerberos Version 4
Version 4 of Kerberos makes use of DES, in a rather elaborate protocol, to provide 
the authentication service. Viewing the protocol as a whole, it is difficult to see the 
need for the many elements contained therein. Therefore, we adopt a strategy used 
by Bill Bryant [BRYA88] and build up to the full protocol by looking first at sev-
eral hypothetical dialogues. Each successive dialogue adds additional complexity to 
counter security vulnerabilities revealed in the preceding dialogue.
After examining the protocol, we look at some other aspects of version 4.
A Simple AuthenticAtion DiAlogue In an unprotected network environment, 
any client can apply to any server for service. The obvious security risk is that of 
 impersonation. An opponent can pretend to be another client and obtain unauthor-
ized privileges on server machines. To counter this threat, servers must be able to 
confirm the identities of clients who request service. Each server can be required to 
undertake this task for each client/server interaction, but in an open environment, 
this places a substantial burden on each server.
An alternative is to use an authentication server (AS) that knows the pass-
words of all users and stores these in a centralized database. In addition, the AS 
shares a unique secret key with each server. These keys have been distributed physi-
cally or in some other secure manner. Consider the following hypothetical 
dialogue:1
(1) C S AS: IDC }PC }IDV
(2) AS S C: Ticket
(3) C S V: IDC }Ticket
Ticket = E(Kv, [IDC }ADC }IDV])
where
 C = client
 AS = authentication server
 V = server
 IDC = identifier of user on C
 IDV = identifier of V
 PC = password of user on C
 ADC = network address of C
 Kv = secret encryption key shared by AS and V
In this scenario, the user logs on to a workstation and requests access to server V. 
The client module C in the user‚Äôs workstation requests the user‚Äôs password and 
1The portion to the left of the colon indicates the sender and receiver, the portion to the right indicates 
the contents of the message, and the symbol }  indicates concatenation.

then sends a message to the AS that includes the user‚Äôs ID, the server‚Äôs ID, and 
the user‚Äôs password. The AS checks its database to see if the user has supplied 
the proper password for this user ID and whether this user is permitted access to 
server V. If both tests are passed, the AS accepts the user as authentic and must 
now convince the server that this user is authentic. To do so, the AS creates a ticket 
that contains the user‚Äôs ID and network address and the server‚Äôs ID. This ticket is 
encrypted using the secret key shared by the AS and this server. This ticket is then 
sent back to C. Because the ticket is encrypted, it cannot be altered by C or by an 
opponent.
With this ticket, C can now apply to V for service. C sends a message to V con-
taining C‚Äôs ID and the ticket. V decrypts the ticket and verifies that the user ID in 
the ticket is the same as the unencrypted user ID in the message. If these two match, 
the server considers the user authenticated and grants the requested service.
Each of the ingredients of message (3) is significant. The ticket is encrypted to 
prevent alteration or forgery. The server‚Äôs ID (IDV) is included in the ticket so that 
the server can verify that it has decrypted the ticket properly. IDC is included in the 
ticket to indicate that this ticket has been issued on behalf of C. Finally, ADC serves 
to counter the following threat. An opponent could capture the ticket transmitted 
in message (2), then use the name IDC, and transmit a message of form (3) from 
another workstation. The server would receive a valid ticket that matches the user 
ID and grant access to the user on that other workstation. To prevent this attack, 
the AS includes in the ticket the network address from which the original request 
came. Now the ticket is valid only if it is transmitted from the same workstation that 
initially requested the ticket.
A more Secure AuthenticAtion DiAlogue Although the foregoing scenario 
solves some of the problems of authentication in an open network environment, 
problems remain. Two in particular stand out. First, we would like to minimize the 
number of times that a user has to enter a password. Suppose each ticket can be 
used only once. If user C logs on to a workstation in the morning and wishes to 
check his or her mail at a mail server, C must supply a password to get a ticket for 
the mail server. If C wishes to check the mail several times during the day, each 
attempt requires reentering the password. We can improve matters by saying that 
tickets are reusable. For a single logon session, the workstation can store the mail-
server ticket after it is received and use it on behalf of the user for multiple accesses 
to the mail server.
However, under this scheme, it remains the case that a user would need a new 
ticket for every different service. If a user wished to access a print server, a mail 
server, a file server, and so on, the first instance of each access would require a new 
ticket and hence require the user to enter the password.
The second problem is that the earlier scenario involved a plaintext transmis-
sion of the password [message (1)]. An eavesdropper could capture the password 
and use any service accessible to the victim.
To solve these additional problems, we introduce a scheme for avoiding plain-
text passwords and a new server, known as the ticket-granting server (TGS). The 
new (but still hypothetical) scenario is as follows.

Once per user logon session:
(1) C S AS: IDC }IDtgs
(2) AS S C: E(Kc, Tickettgs)
Once per type of service:
(3) C S TGS: IDC }IDV }Tickettgs
(4) TGS S C: Ticketv
Once per service session:
(5) C S V: IDC }Ticketv
Tickettgs = E(Ktgs, [IDC }ADC }IDtgs }TS1 }Lifetime1])
Ticketv = E(Kv, [IDC }ADC }IDv }TS2 }Lifetime2])
The new service, TGS, issues tickets to users who have been authenticated to 
AS. Thus, the user first requests a ticket-granting ticket (Tickettgs) from the AS. The 
client module in the user workstation saves this ticket. Each time the user  requires 
access to a new service, the client applies to the TGS, using the ticket to authenti-
cate itself. The TGS then grants a ticket for the particular service. The client saves 
each service-granting ticket and uses it to authenticate its user to a server each time 
a particular service is requested. Let us look at the details of this scheme:
1. The client requests a ticket-granting ticket on behalf of the user by sending 
its user‚Äôs ID to the AS, together with the TGS ID, indicating a request to use 
the TGS service.
2. The AS responds with a ticket that is encrypted with a key that is derived 
from the user‚Äôs password (KC), which is already stored at the AS. When this 
response arrives at the client, the client prompts the user for his or her pass-
word, generates the key, and attempts to decrypt the incoming message. If the 
correct password is supplied, the ticket is successfully recovered.
Because only the correct user should know the password, only the correct user 
can recover the ticket. Thus, we have used the password to obtain credentials from 
Kerberos without having to transmit the password in plaintext. The ticket itself 
consists of the ID and network address of the user and the ID of the TGS. This 
corresponds to the first scenario. The idea is that the client can use this ticket to 
request multiple service-granting tickets. So the ticket-granting ticket is to be reus-
able. However, we do not wish an opponent to be able to capture the ticket and 
use it. Consider the following scenario: An opponent captures the login ticket and 
waits until the user has logged off his or her workstation. Then the opponent either 
gains access to that workstation or configures his workstation with the same net-
work address as that of the victim. The opponent would be able to reuse the ticket 
to spoof the TGS. To counter this, the ticket includes a timestamp, indicating the 
date and time at which the ticket was issued, and a lifetime, indicating the length 
of time for which the ticket is valid (e.g., eight hours). Thus, the client now has a 
reusable ticket and need not bother the user for a password for each new service

request. Finally, note that the ticket-granting ticket is encrypted with a secret key 
known only to the AS and the TGS. This prevents alteration of the ticket. The ticket 
is reencrypted with a key based on the user‚Äôs password. This assures that the ticket 
can be recovered only by the correct user, providing the authentication.
Now that the client has a ticket-granting ticket, access to any server can be 
obtained with steps 3 and 4.
3. The client requests a service-granting ticket on behalf of the user. For this 
purpose, the client transmits a message to the TGS containing the user‚Äôs ID, 
the ID of the desired service, and the ticket-granting ticket.
4. The TGS decrypts the incoming ticket using a key shared only by the AS 
and the TGS (Ktgs) and verifies the success of the decryption by the presence 
of its ID. It checks to make sure that the lifetime has not expired. Then it 
compares the user ID and network address with the incoming information to 
authenticate the user. If the user is permitted access to the server V, the TGS 
issues a ticket to grant access to the requested service.
The service-granting ticket has the same structure as the ticket-granting ticket. 
Indeed, because the TGS is a server, we would expect that the same elements are 
needed to authenticate a client to the TGS and to authenticate a client to an appli-
cation server. Again, the ticket contains a timestamp and lifetime. If the user wants 
access to the same service at a later time, the client can simply use the previously 
acquired service-granting ticket and need not bother the user for a password. Note 
that the ticket is encrypted with a secret key (Kv) known only to the TGS and the 
server, preventing alteration.
Finally, with a particular service-granting ticket, the client can gain access to 
the corresponding service with step 5.
5. The client requests access to a service on behalf of the user. For this purpose, 
the client transmits a message to the server containing the user‚Äôs ID and the ser-
vice-granting ticket. The server authenticates by using the contents of the ticket.
This new scenario satisfies the two requirements of only one password query 
per user session and protection of the user password.
the VerSion 4 AuthenticAtion DiAlogue Although the foregoing scenario 
enhances security compared to the first attempt, two additional problems remain. 
The heart of the first problem is the lifetime associated with the ticket-granting 
ticket. If this lifetime is very short (e.g., minutes), then the user will be repeatedly 
asked for a password. If the lifetime is long (e.g., hours), then an opponent has a 
greater opportunity for replay. An opponent could eavesdrop on the network and 
capture a copy of the ticket-granting ticket and then wait for the legitimate user to 
log out. Then the opponent could forge the legitimate user‚Äôs network address and 
send the message of step (3) to the TGS. This would give the opponent unlimited 
access to the resources and files available to the legitimate user.
Similarly, if an opponent captures a service-granting ticket and uses it before it 
expires, the opponent has access to the corresponding service.
Thus, we arrive at an additional requirement. A network service (the TGS or 
an application service) must be able to prove that the person using a ticket is the 
same person to whom that ticket was issued.

(1) C u AS
IDc }IDtgs}TS1
(2) AS u C
E(Kc, [Kc,tgs}IDtgs}TS2 }Lifetime2 }Tickettgs])
  Tickettgs = E(Ktgs, [Kc, tgs }IDC}ADC}IDtgs}TS2 }Lifetime2])
(a) Authentication Service Exchange to obtain ticket-granting ticket
(3) C u TGS
IDv }Tickettgs}Authenticatorc
(4) TGS u C
E(Kc, tgs, [Kc, v }IDv }TS4 }Ticketv])
Tickettgs = E(Ktgs, [Kc, tgs }IDC}ADC}IDtgs}TS2 }Lifetime2])
Ticketv = E(Kv, [Kc, v }IDC}ADC}IDv }TS4 }Lifetime4])
Authenticatorc = E(Kc, tgs, [IDC}ADC}TS3])
(b) Ticket-Granting Service Exchange to obtain service-granting ticket
(5) C u V
Ticketv }Authenticatorc
(6) V u C
E(Kc, v, [TS5 + 1])(for mutual authentication)
Ticketv = E(Kv, [Kc, v }IDC}ADC}IDv }TS4 }Lifetime4])
Authenticatorc = E(Kc, v, [IDC}ADC}TS5])
(c) Client/Server Authentication Exchange to obtain service
Table 4.1 Summary of Kerberos Version 4 Message Exchanges
The second problem is that there may be a requirement for servers to authenti-
cate themselves to users. Without such authentication, an opponent could sabotage 
the configuration so that messages to a server were directed to another  location. 
The false server then would be in a position to act as a real server, capture any 
 information from the user, and deny the true service to the user.
We examine these problems in turn and refer to Table 4.1, which shows the 
actual Kerberos protocol. Figure 4.2 provides simplified overview.
First, consider the problem of captured ticket-granting tickets and the need 
to determine that the ticket presenter is the same as the client for whom the ticket 
was issued. The threat is that an opponent will steal the ticket and use it before it 
expires. To get around this problem, let us have the AS provide both the client and 
the TGS with a secret piece of information in a secure manner. Then the client can 
prove its identity to the TGS by revealing the secret information, again in a secure 
manner. An efficient way of accomplishing this is to use an encryption key as the 
secure information; this is referred to as a session key in Kerberos.
Table 4.1a shows the technique for distributing the session key. As before, the 
client sends a message to the AS requesting access to the TGS. The AS responds 
with a message, encrypted with a key derived from the user‚Äôs password (KC), that 
contains the ticket. The encrypted message also contains a copy of the session key, 
KC,tgs, where the subscripts indicate that this is a session key for C and TGS. Because 
this session key is inside the message encrypted with KC, only the user‚Äôs client can 
read it. The same session key is included in the ticket, which can be read only by 
the TGS. Thus, the session key has been securely delivered to both C and the TGS.
Note that several additional pieces of information have been added to this 
first phase of the dialogue. Message (1) includes a timestamp, so that the AS knows 
that the message is timely. Message (2) includes several elements of the ticket in a

form accessible to C. This enables C to confirm that this ticket is for the TGS and to 
learn its expiration time.
Armed with the ticket and the session key, C is ready to approach the TGS. 
As before, C sends the TGS a message that includes the ticket plus the ID of the 
requested service [message (3) in Table 4.1b]. In addition, C transmits an authentica-
tor, which includes the ID and address of C‚Äôs user and a timestamp. Unlike the ticket, 
which is reusable, the authenticator is intended for use only once and has a very short 
lifetime. The TGS can decrypt the ticket with the key that it shares with the AS. This 
ticket indicates that user C has been provided with the session key KC,tgs. In effect, 
the ticket says, ‚ÄúAnyone who uses KC,tgs must be C.‚Äù The TGS uses the session key to 
decrypt the authenticator. The TGS can then check the name and address from the 
authenticator with that of the ticket and with the network address of the incoming 
message. If all match, then the TGS is assured that the sender of the ticket is indeed 
the ticket‚Äôs real owner. In effect, the authenticator says, ‚ÄúAt time TS3, I hereby use 
KC,tgs.‚Äù Note that the ticket does not prove anyone‚Äôs identity but is a way to distribute 
keys securely. It is the authenticator that proves the client‚Äôs identity. Because the 
Figure 4.2 Overview of Kerberos
Authentication
server
Ticket-
granting
server (TGS)
Host/
application
server
Request ticket-
granting ticket
Once per
user logon
session
1. User logs on to
workstation and
requests service on host
3. Workstation prompts
user for password to decrypt
incoming message, then
send ticket and
authenticator that contains
user‚Äôs name, network
address, and time to TGS
Ticket + session key
Request service-
granting ticket
Ticket + session key
Once per
type of service
4. TGS decrypts ticket and
authenticator, verifes request, and
then creates ticket for requested
application server.
Kerberos
5. Workstation sends
ticket and authenticator
to host
6. Host verifes that
ticket and authenticator
match, and then grants access
to service. If mutual
authentication is
required, server returns
an authenticator
Request service
Provide server
authenticator
Once per
service session
2. AS verifes user‚Äôs access right in
database, and creates ticket-granting ticket
and session key. Results are encrypted
using key derived from user‚Äôs password

authenticator can be used only once and has a short lifetime, the threat of an oppo-
nent stealing both the ticket and the authenticator for presentation later is countered.
The reply from the TGS in message (4) follows the form of message (2). The 
message is encrypted with the session key shared by the TGS and C and includes 
a session key to be shared between C and the server V, the ID of V, and the time-
stamp of the ticket. The ticket itself includes the same session key.
C now has a reusable service-granting ticket for V. When C presents this ticket, 
as shown in message (5), it also sends an authenticator. The server can decrypt the 
ticket, recover the session key, and decrypt the authenticator.
If mutual authentication is required, the server can reply as shown in message 
(6) of Table 4.1. The server returns the value of the timestamp from the authentica-
tor, incremented by 1, and encrypted in the session key. C can decrypt this message 
to recover the incremented timestamp. Because the message was encrypted by the 
session key, C is assured that it could have been created only by V. The contents of 
the message assure C that this is not a replay of an old reply.
Finally, at the conclusion of this process, the client and server share a secret 
key. This key can be used to encrypt future messages between the two or to exchange 
a new random session key for that purpose.
Figure 4.3 illustrates the Kerberos exchanges among the parties. Table 4.2 
summarizes the justification for each of the elements in the Kerberos protocol.
Figure 4.3 Kerberos Exchanges
Client
Client authentication
IDc || IDtgs || TS1
Tickettgs, server ID, and client authentication
IDv || Tickettgs || Authenticatorc
Shared key and ticket
E(Kc,tgs, [Kc,v || IDv || TS4 || Ticketv])
Ticketv and client authentication
Ticketv || Authenticatorc
Service granted
E(Kc,v, [TS5 + 1])
Shared key and ticket
E(Kc, [Kc,tgs || IDtgs || TS2 ||
Lifetime2 || Tickettgs])
Authentication
server (AS)
Ticket-granting
server (AS)
Service
provider

Message (1)
Client requests ticket-granting ticket.
IDC
Tells AS identity of user from this client.
IDtgs
Tells AS that user requests access to TGS.
TS1
Allows AS to verify that client‚Äôs clock is synchronized with that of AS.
Message (2)
AS returns ticket-granting ticket.
Kc
Encryption is based on user‚Äôs password, enabling AS and client to verify 
 password, and protecting contents of message (2).
Kc,tgs
Copy of session key accessible to client created by AS to permit secure 
exchange between client and TGS without requiring them to share a 
 permanent key.
IDtgs
Confirms that this ticket is for the TGS.
TS2
Informs client of time this ticket was issued.
Lifetime2
Informs client of the lifetime of this ticket.
Tickettgs
Ticket to be used by client to access TGS.
(a) Authentication Service Exchange
Message (3)
Client requests service-granting ticket.
IDV
Tells TGS that user requests access to server V.
Tickettgs
Assures TGS that this user has been authenticated by AS.
Authenticatorc
Generated by client to validate ticket.
Message (4)
TGS returns service-granting ticket.
Kc,tgs
Key shared only by C and TGS protects contents of message (4).
Kc,v
Copy of session key accessible to client created by TGS to permit secure 
exchange between client and server without requiring them to share 
a¬† permanent¬†key.
IDV
Confirms that this ticket is for server V.
TS4
Informs client of time this ticket was issued.
TicketV
Ticket to be used by client to access server V.
Tickettgs
Reusable so that user does not have to reenter password.
Ktgs
Ticket is encrypted with key known only to AS and TGS, to prevent 
 tampering.
Kc,tgs
Copy of session key accessible to TGS used to decrypt authenticator, thereby 
authenticating ticket.
IDC
Indicates the rightful owner of this ticket.
ADC
Prevents use of ticket from workstation other than one that initially requested 
the ticket.
IDtgs
Assures server that it has decrypted ticket properly.
TS2
Informs TGS of time this ticket was issued.
Lifetime2
Prevents replay after ticket has expired.
Authenticatorc
Assures TGS that the ticket presenter is the same as the client for whom the 
ticket was issued has very short lifetime to prevent replay.
Table 4.2 Rationale for the Elements of the Kerberos Version 4 Protocol

Kc,tgs
Authenticator is encrypted with key known only to client and TGS, to prevent 
tampering.
IDC
Must match ID in ticket to authenticate ticket.
ADC
Must match address in ticket to authenticate ticket.
TS3
Informs TGS of time this authenticator was generated.
(b) Ticket-Granting Service Exchange
Message (5)
Client requests service.
TicketV
Assures server that this user has been authenticated by AS.
Authenticatorc
Generated by client to validate ticket.
Message (6)
Optional authentication of server to client.
Kc,v
Assures C that this message is from V.
TS5 + 1
Assures C that this is not a replay of an old reply.
Ticketv
Reusable so that client does not need to request a new ticket from TGS for 
each access to the same server.
Kv
Ticket is encrypted with key known only to TGS and server, to prevent 
 tampering.
Kc,v
Copy of session key accessible to client; used to decrypt authenticator, thereby 
authenticating ticket.
IDC
Indicates the rightful owner of this ticket.
ADC
Prevents use of ticket from workstation other than one that initially requested 
the ticket.
IDV
Assures server that it has decrypted ticket properly.
TS4
Informs server of time this ticket was issued.
Lifetime4
Prevents replay after ticket has expired.
Authenticatorc
Assures server that the ticket presenter is the same as the client for whom the 
ticket was issued; has very short lifetime to prevent replay.
Kc,v
Authenticator is encrypted with key known only to client and server, to 
 prevent tampering.
IDC
Must match ID in ticket to authenticate ticket.
ADc
Must match address in ticket to authenticate ticket.
TS5
Informs server of time this authenticator was generated.
(c) Client/Server Authentication Exchange
KerberoS reAlmS AnD multiple Kerberi A full-service Kerberos environment 
consisting of a Kerberos server, a number of clients, and a number of application 
servers requires the following:
1. The Kerberos server must have the user ID and hashed passwords of all par-
ticipating users in its database. All users are registered with the Kerberos 
server.
2. The Kerberos server must share a secret key with each server. All servers are 
registered with the Kerberos server.

Such an environment is referred to as a Kerberos realm. The concept of realm 
can be explained as follows. A Kerberos realm is a set of managed nodes that share 
the same Kerberos database. The Kerberos database resides on the Kerberos mas-
ter computer system, which should be kept in a physically secure room. A read-only 
copy of the Kerberos database might also reside on other Kerberos computer sys-
tems. However, all changes to the database must be made on the master computer 
system. Changing or accessing the contents of a Kerberos database requires the 
Kerberos master password. A related concept is that of a Kerberos principal, which 
is a service or user that is known to the Kerberos system. Each Kerberos principal is 
identified by its principal name. Principal names consist of three parts: a service or 
user name, an instance name, and a realm name.
Networks of clients and servers under different administrative organizations 
typically constitute different realms. That is, it generally is not practical or does not 
conform to administrative policy to have users and servers in one administrative 
domain registered with a Kerberos server elsewhere. However, users in one realm 
may need access to servers in other realms, and some servers may be willing to pro-
vide service to users from other realms, provided that those users are authenticated.
Kerberos Version 5
Kerberos version 5 is specified in RFC 4120 and provides a number of improve-
ments over version 4 [KOHL94]. To begin, we provide an overview of the changes 
from version 4 to version 5 and then look at the version 5 protocol.
DifferenceS between VerSionS 4 AnD 5 Version 5 is intended to address the 
 limitations of version 4 in two areas: environmental shortcomings and technical 
 deficiencies. We briefly summarize the improvements in each area. Kerberos  version 
4 did not fully address the need to be of general purpose. This led to the following 
 environmental shortcomings.
1. Encryption system dependence: Version 4 requires the use of DES. Export 
restriction on DES as well as doubts about the strength of DES were thus of 
concern. In version 5, ciphertext is tagged with an encryption-type identifier so 
that any encryption technique may be used. Encryption keys are tagged with 
a type and a length, allowing the same key to be used in different algorithms 
and allowing the specification of different variations on a given algorithm.
2. Internet protocol dependence: Version 4 requires the use of Internet Protocol 
(IP) addresses. Other address types, such as the ISO network address, are not 
accommodated. Version 5 network addresses are tagged with type and length, 
allowing any network address type to be used.
3. Message byte ordering: In version 4, the sender of a message employs a byte 
ordering of its own choosing and tags the message to indicate least significant 
byte in lowest address or most significant byte in lowest address. This tech-
nique works but does not follow established conventions. In version 5, all mes-
sage structures are defined using Abstract Syntax Notation One (ASN.1) and 
Basic Encoding Rules (BER), which provide an unambiguous byte ordering.
4. Ticket lifetime: Lifetime values in version 4 are encoded in an 8-bit quantity 
in units of five minutes. Thus, the maximum lifetime that can be expressed

is 28 * 5 = 1280 minutes (a little over 21 hours). This may be inadequate 
for some applications (e.g., a long-running simulation that requires valid 
Kerberos credentials throughout execution). In version 5, tickets include an 
explicit start time and end time, allowing tickets with arbitrary lifetimes.
5. Authentication forwarding: Version 4 does not allow credentials issued to 
one client to be forwarded to some other host and used by some other client. 
This capability would enable a client to access a server and have that server 
access another server on behalf of the client. For example, a client issues a 
request to a print server that then accesses the client‚Äôs file from a file server, 
using the client‚Äôs credentials for access. Version 5 provides this capability.
6. Interrealm authentication: In version 4, interoperability among N realms 
 requires on the order of N2 Kerberos-to-Kerberos relationships, as described 
earlier. Version 5 supports a method that requires fewer relationships, as 
 described shortly.
Apart from these environmental limitations, there are technical  deficiencies 
in¬†the version 4 protocol itself. Most of these deficiencies were documented in 
[BELL90], and version 5 attempts to address these. The deficiencies are the following.
1. Double encryption: Note in Table 4.1 [messages (2) and (4)] that tickets pro-
vided to clients are encrypted twice‚Äîonce with the secret key of the  target 
server and then again with a secret key known to the client. The second 
 encryption is not necessary and is computationally wasteful.
2. PCBC encryption: Encryption in version 4 makes use of a nonstandard 
mode of DES known as propagating cipher block chaining (PCBC).2 It has 
been demonstrated that this mode is vulnerable to an attack involving the 
interchange of ciphertext blocks [KOHL89]. PCBC was intended to provide 
an integrity check as part of the encryption operation. Version 5 provides 
explicit integrity mechanisms, allowing the standard CBC mode to be used 
for encryption. In particular, a checksum or hash code is attached to the mes-
sage prior to encryption using CBC.
3. Session keys: Each ticket includes a session key that is used by the client to 
encrypt the authenticator sent to the service associated with that ticket. In 
addition, the session key subsequently may be used by the client and the 
server to protect messages passed during that session. However, because the 
same ticket may be used repeatedly to gain service from a particular server, 
there is the risk that an opponent will replay messages from an old session to 
the client or the server. In version 5, it is possible for a client and server to 
negotiate a subsession key, which is to be used only for that one connection. 
A new access by the client would result in the use of a new subsession key.
4. Password attacks: Both versions are vulnerable to a password attack. The mes-
sage from the AS to the client includes material encrypted with a key based 
on the client‚Äôs password.3 An opponent can capture this message and attempt 
2This is described in Appendix F.
3Appendix F describes the mapping of passwords to encryption keys.

to decrypt it by trying various passwords. If the result of a test  decryption 
is of the proper form, then the opponent has discovered the  client‚Äôs pass-
word and may subsequently use it to gain authentication credentials from 
Kerberos. This is the same type of password attack described in Chapter 10, 
with the same kinds of countermeasures being applicable. Version 5 does pro-
vide a mechanism known as preauthentication, which should make password 
attacks more difficult, but it does not prevent them.
the VerSion 5 AuthenticAtion DiAlogue Table 4.3 summarizes the basic version 
5 dialogue. This is best explained by comparison with version 4 (Table 4.1).
First, consider the authentication service exchange. Message (1) is a client 
 request for a ticket-granting ticket. As before, it includes the ID of the user and the 
TGS. The following new elements are added:
‚ñ†
‚ñ† Realm: Indicates realm of user.
‚ñ†
‚ñ† Options: Used to request that certain flags be set in the returned ticket.
‚ñ†
‚ñ† Times: Used by the client to request the following time settings in the ticket:
from: the desired start time for the requested ticket
till: the requested expiration time for the requested ticket
rtime: requested renew-till time
‚ñ†
‚ñ† Nonce: A random value to be repeated in message (2) to assure that the 
 response is fresh and has not been replayed by an opponent.
(1) C u AS
Options }IDc }Realmc }IDtgs}Times }Nonce1
(2) AS u C
Realmc }IDC}Tickettgs}E(Kc, [Kc, tgs }Times }Nonce1 }Realmtgs}IDtgs])
Tickettgs = E(Ktgs, [Flags }Kc,tgs}Realmc }IDC}ADC}Times])
(a) Authentication Service Exchange to obtain ticket-granting ticket
(3) C u TGS
Options }IDv }Times } }Nonce2 }Tickettgs}Authenticatorc
(4) TGS u C
Realmc }IDC}Ticketv }E(Kc,tgs, [Kc,v }Times }Nonce2 }Realmv }IDv])
Tickettgs = E(Ktgs, [Flags }Kc,tgs}Realmc }IDC}ADC}Times])
Ticketv = E(Kv, [Flags }Kc,v }Realmc }IDC}ADC}Times])
Authenticatorc = E(Kc, tgs, [IDC}Realmc }TS1])
(b) Ticket-Granting Service Exchange to obtain service-granting ticket
(5) C u V
Options }Ticketv }Authenticatorc
(6) V u C
EKc, v[TS2 }Subkey }Seq#]
Ticketv = E(Kv, [Flags }Kc, v }Realmc }IDC}ADC}Times])
Authenticatorc = E(Kc,v, [IDC}Realmc }TS2 }Subkey }Seq#])
(c) Client/Server Authentication Exchange to obtain service
Table 4.3 Summary of Kerberos Version 5 Message Exchanges

Message (2) returns a ticket-granting ticket, identifying information for the 
client, and a block encrypted using the encryption key based on the user‚Äôs password. 
This block includes the session key to be used between the client and the TGS, times 
specified in message (1), the nonce from message (1), and TGS identifying informa-
tion. The ticket itself includes the session key, identifying information for the client, 
the requested time values, and flags that reflect the status of this ticket and the 
requested options. These flags introduce significant new functionality to version 5.  
For now, we defer a discussion of these flags and concentrate on the overall struc-
ture of the version 5 protocol.
Let us now compare the ticket-granting service exchange for versions 4 and 5. 
We see that message (3) for both versions includes an authenticator, a ticket, and 
the name of the requested service. In addition, version 5 includes requested times 
and options for the ticket and a nonce‚Äîall with functions similar to those of mes-
sage (1). The authenticator itself is essentially the same as the one used in version 4.
Message (4) has the same structure as message (2). It returns a ticket plus 
information needed by the client, with the information encrypted using the session 
key now shared by the client and the TGS.
Finally, for the client/server authentication exchange, several new features 
 appear in version 5. In message (5), the client may request as an option that mutual 
authentication is required. The authenticator includes several new fields:
‚ñ†
‚ñ† Subkey: The client‚Äôs choice for an encryption key to be used to protect this 
specific application session. If this field is omitted, the session key from the 
ticket (KC,V) is used.
‚ñ†
‚ñ† Sequence number: An optional field that specifies the starting sequence num-
ber to be used by the server for messages sent to the client during this session. 
Messages may be sequence numbered to detect replays.
If mutual authentication is required, the server responds with message (6). 
This message includes the timestamp from the authenticator. Note that in version 4, 
the timestamp was incremented by one. This is not necessary in version 5, because 
the nature of the format of messages is such that it is not possible for an opponent 
to create message (6) without knowledge of the appropriate encryption keys. The 
subkey field, if present, overrides the subkey field, if present, in message (5). The 
optional sequence number field specifies the starting sequence number to be used 
by the client.
 4.4 Key DistRibUtion Using AsymmetRic encRyPtion
One of the major roles of public-key encryption is to address the problem of key 
distribution. There are actually two distinct aspects to the use of public-key encryp-
tion in this regard.
‚ñ†
‚ñ† The distribution of public keys.
‚ñ†
‚ñ† The use of public-key encryption to distribute secret keys.
We examine each of these areas in turn.

Public-Key Certificates
On the face of it, the point of public-key encryption is that the public key is public. 
Thus, if there is some broadly accepted public-key algorithm, such as RSA, any 
participant can send his or her public key to any other participant or broadcast the 
key to the community at large. Although this approach is convenient, it has a major 
weakness. Anyone can forge such a public announcement. That is, some user could 
pretend to be user A and send a public key to another participant or broadcast such 
a public key. Until such time as user A discovers the forgery and alerts other partici-
pants, the forger is able to read all encrypted messages intended for A and can use 
the forged keys for authentication.
The solution to this problem is the public-key certificate. In essence, a certifi-
cate consists of a public key plus a user ID of the key owner, with the whole block 
signed by a trusted third party. Typically, the third party is a certificate authority 
(CA) that is trusted by the user community, such as a government agency or a finan-
cial institution. A user can present his or her public key to the authority in a secure 
manner and obtain a certificate. The user can then publish the certificate. Anyone 
needing this user‚Äôs public key can obtain the certificate and verify that it is valid by 
way of the attached trusted signature. Figure 4.4 illustrates the process.
One scheme has become universally accepted for formatting public-key certifi-
cates: the X.509 standard. X.509 certificates are used in most network security appli-
cations, including IP security, secure sockets layer (SSL), and S/MIME‚Äîall of which 
are discussed in subsequent chapters. X.509 is examined in detail in the next section.
Public-Key Distribution of Secret Keys
With conventional encryption, a fundamental requirement for two parties to com-
municate securely is that they share a secret key. Suppose Bob wants to create a 
messaging application that will enable him to exchange e-mail securely with anyone 
Figure 4.4 Public-Key Certificate Use
Unsigned certifcate:
contains user ID,
user‚Äôs public key
Signed certifcate
Hash code of incoming
unsigned certifcate
Return
signature valid
or not valid
Generate hash
code of unsigned
certifcate
Generate signature as
a function of hash code
using CA‚Äôs private key
H
H
Bob‚Äôs ID
information
CA
information
Bob‚Äôs public key
G
V
Verifty incoming signature
as a function of incoming
hash code using CA‚Äôs public key
Use certifcate to
verify Bob‚Äôs public key
Create signed
digital certifcate

who has access to the Internet or to some other network that the two of them share. 
Suppose Bob wants to do this using conventional encryption. With conventional 
encryption, Bob and his correspondent, say, Alice, must come up with a way to share 
a unique secret key that no one else knows. How are they going to do that? If Alice 
is in the next room from Bob, Bob could generate a key and write it down on a piece 
of paper or store it on a diskette and hand it to Alice. But if Alice is on the other side 
of the continent or the world, what can Bob do? He could encrypt this key using con-
ventional encryption and e-mail it to Alice, but this means that Bob and Alice must 
share a secret key to encrypt this new secret key. Furthermore, Bob and everyone 
else who uses this new e-mail package faces the same problem with every potential 
correspondent: Each pair of correspondents must share a unique secret key.
One approach is the use of Diffie‚ÄìHellman key exchange. This approach is 
indeed widely used. However, it suffers the drawback that, in its simplest form, 
Diffie‚ÄìHellman provides no authentication of the two communicating partners.
A powerful alternative is the use of public-key certificates. When Bob wishes 
to communicate with Alice, Bob can do the following:
1. Prepare a message.
2. Encrypt that message using conventional encryption with a one-time conven-
tional session key.
3. Encrypt the session key using public-key encryption with Alice‚Äôs public key.
4. Attach the encrypted session key to the message and send it to Alice.
Only Alice is capable of decrypting the session key and therefore of recover-
ing the original message. If Bob obtained Alice‚Äôs public key by means of Alice‚Äôs 
public-key certificate, then Bob is assured that it is a valid key.
 4.5 X.509 ceRtificAtes
ITU-T recommendation X.509 is part of the X.500 series of recommendations that 
define a directory service. The directory is, in effect, a server or distributed set 
of servers that maintains a database of information about users. The information 
includes a mapping from user name to network address, as well as other attributes 
and information about the users.
X.509 defines a framework for the provision of authentication services by the 
X.500 directory to its users. The directory may serve as a repository of public-key 
certificates. Each certificate contains the public key of a user and is signed with the 
private key of a trusted certification authority. In addition, X.509 defines alternative 
authentication protocols based on the use of public-key certificates.
X.509 is an important standard because the certificate structure and authenti-
cation protocols defined in X.509 are used in a variety of contexts. For example, the 
X.509 certificate format is used in S/MIME (Chapter 8), IP Security (Chapter 9), 
and SSL/TLS (Chapter 6).
X.509 was initially issued in 1988. The standard was subsequently revised 
in 1993 to address some of the security concerns documented in [IANS90] and 
[MITC90]. The standard is currently at version 7, issued in 2012.

X.509 is based on the use of public-key cryptography and digital signatures. 
The standard does not dictate the use of a specific digital signature algorithm nor a 
specific hash function. Figure 4.5 illustrates the overall X.509 scheme for generation 
of a public-key certificate. The certificate for Bob‚Äôs public key includes unique iden-
tifying information for Bob, Bob‚Äôs public key, and identifying information about 
the CA, plus other information as explained subsequently. This information is then 
signed by computing a hash value of the information and generating a digital signa-
ture using the hash value and the CA‚Äôs private key.
Certificates
The heart of the X.509 scheme is the public-key certificate associated with each user. 
These user certificates are assumed to be created by some trusted certification author-
ity (CA) and placed in the directory by the CA or by the user. The directory server 
itself is not responsible for the creation of public keys or for the certification function; 
it merely provides an easily accessible location for users to obtain certificates.
Figure 4.5a shows the general format of a certificate, which includes the fol-
lowing elements.
‚ñ†
‚ñ† Version: Differentiates among successive versions of the certificate format; 
the default is version 1. If the Issuer Unique Identifier or Subject Unique 
Identifier are present, the value must be version 2. If one or more extensions 
Figure 4.5 X.509 Formats
Certifcate
serial number
Version
Issuer name
Signature
algorithm
identifer
Subject name
Extensions
Issuer unique
identifer
Subject unique
identifer
Algorithm
Parameters
Not before
Algorithms
Parameters
Key
Algorithms
Parameters
Signature of certifcate
(a) X.509 certifcate
Not after
Subject‚Äôs
public key
info
Signature
Period of
validity
Version 1
Version 2
Version 3
All
versions
Issuer name
This update date
Next update date
Signature
algorithm
identifer
Algorithm
Parameters
User certifcate serial #
(b) Certifcate revocation list
Revocation date
Algorithms
Parameters
Signature of certifcate
Signature
Revoked
certifcate
User certifcate serial #
Revocation date
Revoked
certifcate

are present, the version must be version 3. Although the X.509 specification is 
currently at version 7, no changes have been made to the fields that make up 
the certificate since version 3.
‚ñ†
‚ñ† Serial number: An integer value, unique within the issuing CA, that is unam-
biguously associated with this certificate.
‚ñ†
‚ñ† Signature algorithm identifier: The algorithm used to sign the certificate, 
 together with any associated parameters. Because this information is  repeated 
in the Signature field at the end of the certificate, this field has little, if any, 
utility.
‚ñ†
‚ñ† Issuer name: X.500 name of the CA that created and signed this certificate.
‚ñ†
‚ñ† Period of validity: Consists of two dates: the first and last on which the certifi-
cate is valid.
‚ñ†
‚ñ† Subject name: The name of the user to whom this certificate refers. That is, 
this certificate certifies the public key of the subject who holds the corre-
sponding private key.
‚ñ†
‚ñ† Subject‚Äôs public-key information: The public key of the subject, plus an iden-
tifier of the algorithm for which this key is to be used, together with any 
 associated parameters.
‚ñ†
‚ñ† Issuer unique identifier: An optional bit string field used to identify uniquely 
the issuing CA in the event the X.500 name has been reused for different 
entities.
‚ñ†
‚ñ† Subject unique identifier: An optional bit string field used to identify uniquely 
the subject in the event the X.500 name has been reused for different entities.
‚ñ†
‚ñ† Extensions: A set of one or more extension fields. Extensions were added in 
version 3 and are discussed later in this section.
‚ñ†
‚ñ† Signature: Covers all of the other fields of the certificate. One component of 
this field is the digital signature applied to the other fields of the certificate. 
This field includes the signature algorithm identifier.
The unique identifier fields were added in version 2 to handle the possible 
reuse of subject and/or issuer names over time. These fields are rarely used.
The standard uses the following notation to define a certificate:
CA V A W
= CA {V, SN, AI, CA, UCA, A, UA, Ap, TA}
where
Y V X W = the certificate of user X issued by certification authority Y
Y {I} = the signing of I by Y; consists of I with an encrypted hash code 
appended
V = version of the certificate
SN = serial number of the certificate
AI = identifier of the algorithm used to sign the certificate
CA = name of certificate authority
UCA = optional unique identifier of the CA

A = name of user A
UA = optional unique identifier of the user A
Ap = public key of user A
TA = period of validity of the certificate
The CA signs the certificate with its private key. If the corresponding public 
key is known to a user, then that user can verify that a certificate signed by the CA 
is valid. This is the typical digital signature approach, as illustrated in Figure 3.15.
obtAining A uSer‚ÄôS certificAte User certificates generated by a CA have the fol-
lowing characteristics:
‚ñ†
‚ñ† Any user with access to the public key of the CA can verify the user public 
key that was certified.
‚ñ†
‚ñ† No party other than the certification authority can modify the certificate 
without this being detected.
Because certificates are unforgeable, they can be placed in a directory without the 
need for the directory to make special efforts to protect them.
If all users subscribe to the same CA, then there is a common trust of that CA. 
All user certificates can be placed in the directory for access by all users. In addi-
tion, a user can transmit his or her certificate directly to other users. In either case, 
once B is in possession of A‚Äôs certificate, B has confidence that messages it encrypts 
with A‚Äôs public key will be secure from eavesdropping and that messages signed 
with A‚Äôs private key are unforgeable.
If there is a large community of users, it may not be practical for all users to 
subscribe to the same CA. Because it is the CA that signs certificates, each par-
ticipating user must have a copy of the CA‚Äôs own public key to verify signatures. 
This public key must be provided to each user in an absolutely secure way (with 
respect to integrity and authenticity) so that the user has confidence in the associ-
ated certificates. Thus, with many users, it may be more practical for there to be a 
number of CAs, each of which securely provides its public key to some fraction of 
the users.
Now suppose that A has obtained a certificate from certification authority X1 
and B has obtained a certificate from CA X2. If A does not securely know the public 
key of X2, then B‚Äôs certificate, issued by X2, is useless to A. A can read B‚Äôs certificate, 
but A cannot verify the signature. However, if the two CAs have securely exchanged 
their own public keys, the following procedure will enable A to obtain B‚Äôs public key.
1. A obtains (from the directory) the certificate of X2 signed by X1. Because A 
securely knows X1>s public key, A can obtain X2>s public key from its certifi-
cate and verify it by means of X1>s signature on the certificate.
2. A then goes back to the directory and obtains the certificate of B signed by 
X2. Because A now has a trusted copy of X2>s public key, A can verify the 
signature and securely obtain B‚Äôs public key.
A has used a chain of certificates to obtain B‚Äôs public key. In the notation of 
X.509, this chain is expressed as
X1 V X2 W X2 V B W

In the same fashion, B can obtain A‚Äôs public key with the reverse chain:
X2 V X1 W X1 V A W
This scheme need not be limited to a chain of two certificates. An arbitrarily 
long path of CAs can be followed to produce a chain. A chain with N elements 
would be expressed as
X1 V X2 W X2 V X3 W cXN V B W
In this case, each pair of CAs in the chain (Xi, Xi+1) must have created certificates 
for each other.
All of these certificates of CAs by CAs need to appear in the directory, and 
the user needs to know how they are linked to follow a path to another user‚Äôs pub-
lic-key certificate. X.509 suggests that CAs be arranged in a hierarchy so that navi-
gation is straightforward.
Figure 4.6, taken from X.509, is an example of such a hierarchy. The connected 
circles indicate the hierarchical relationship among the CAs; the associated boxes 
indicate certificates maintained in the directory for each CA entry. The directory 
entry for each CA includes two types of certificates:
‚ñ†
‚ñ† Forward certificates: Certificates of X generated by other CAs.
‚ñ†
‚ñ† Reverse certificates: Certificates generated by X that are the certificates of 
other CAs.
Figure 4.6 X.509 Hierarchy: A Hypothetical Example
U
V
W
Y
Z
B
X
C
A
U<<V>>
V<<U>>
V<<W>>
W<<V>>
V<<Y>>
Y<<V>>
W<<X>>
X<<W>>
X<<Z>>
Y<<Z>>
Z<<Y>>
Z<<X>>
X<<C>>
X<<A>>
Z<<B>>

In this example, user A can acquire the following certificates from the direc-
tory to establish a certification path to B:
X V W W W V V W V V Y W Y V Z W Z V B W
When A has obtained these certificates, it can unwrap the certification path in 
sequence to recover a trusted copy of B‚Äôs public key. Using this public key, A can 
send encrypted messages to B. If A wishes to receive encrypted messages back from 
B or to sign messages sent to B, then B will require A‚Äôs public key, which can be 
obtained from the certification path:
Z V Y W Y V V W V V W W W V X W X V A W
B can obtain this set of certificates from the directory or A can provide them 
as part of its initial message to B.
reVocAtion of certificAteS Recall from Figure 4.5 that each certificate includes 
a period of validity, much like a credit card. Typically, a new certificate is issued just 
before the expiration of the old one. In addition, it may be desirable on occasion to 
revoke a certificate before it expires for one of the following reasons.
1. The user‚Äôs private key is assumed to be compromised.
2. The user is no longer certified by this CA. Reasons for this include subject‚Äôs 
name has changed, the certificate is superseded, or the certificate was not 
 issued in conformance with the CA‚Äôs policies.
3. The CA‚Äôs certificate is assumed to be compromised.
Each CA must maintain a list consisting of all revoked but not expired cer-
tificates issued by that CA, including both those issued to users and to other CAs. 
These lists also should be posted on the directory.
Each certificate revocation list (CRL) posted to the directory is signed by the 
issuer and includes (Figure 4.5b) the issuer‚Äôs name, the date the list was created, the 
date the next CRL is scheduled to be issued, and an entry for each revoked certifi-
cate. Each entry consists of the serial number of a certificate and revocation date for 
that certificate. Because serial numbers are unique within a CA, the serial number 
is sufficient to identify the certificate.
When a user receives a certificate in a message, the user must determine 
whether the certificate has been revoked. The user could check the directory each 
time a certificate is received. To avoid the delays (and possible costs) associated 
with directory searches, it is likely that the user would maintain a local cache of cer-
tificates and lists of revoked certificates.
X.509 Version 3
The X.509 version 2 format does not convey all of the information that recent design 
and implementation experience has shown to be needed. [FORD95] lists the follow-
ing requirements not satisfied by version 2:
1. The Subject field is inadequate to convey the identity of a-key owner to a 
public-key user. X.509 names may be relatively short and lacking in obvious 
identification details that may be needed by the user.

2. The Subject field is also inadequate for many applications, which typically 
recognize entities by an Internet e-mail address, a URL, or some other 
Internet-related identification.
3. There is a need to indicate security policy information. This enables a security 
application or function, such as IPSec, to relate an X.509 certificate to a given 
policy.
4. There is a need to limit the damage that can result from a faulty or malicious 
CA by setting constraints on the applicability of a particular certificate.
5. It is important to be able to identify different keys used by the same owner at 
different times. This feature supports key life cycle management, in particular 
the ability to update key pairs for users and CAs on a regular basis or under 
exceptional circumstances.
Rather than continue to add fields to a fixed format, standards developers 
felt that a more flexible approach was needed. Thus, version 3 includes a number 
of optional extensions that may be added to the version 2 format. Each extension 
consists of an extension identifier, a criticality indicator, and an extension value. 
The criticality indicator indicates whether an extension can be safely ignored. If 
the indicator has a value of TRUE and an implementation does not recognize the 
extension, it must treat the certificate as invalid.
The certificate extensions fall into three main categories: key and policy infor-
mation, subject and issuer attributes, and certification path constraints.
Key AnD policy informAtion These extensions convey additional information 
about the subject and issuer keys, plus indicators of certificate policy. A certificate 
policy is a named set of rules that indicates the applicability of a certificate to a par-
ticular community and/or class of application with common security requirements. 
For example, a policy might be applicable to the authentication of electronic data 
interchange (EDI) transactions for the trading of goods within a given price range.
This area includes the following:
‚ñ†
‚ñ† Authority key identifier: Identifies the public key to be used to verify the sig-
nature on this certificate or CRL. Enables distinct keys of the same CA to be 
differentiated. One use of this field is to handle CA key pair updating.
‚ñ†
‚ñ† Subject key identifier: Identifies the public key being certified. Useful for 
subject key pair updating. Also, a subject may have multiple key pairs and, 
correspondingly, different certificates for different purposes (e.g., digital sig-
nature and encryption key agreement).
‚ñ†
‚ñ† Key usage: Indicates a restriction imposed as to the purposes for which, and 
the policies under which, the certified public key may be used. May indicate 
one or more of the following: digital signature, nonrepudiation, key encryp-
tion, data encryption, key agreement, CA signature verification on certifi-
cates, and CA signature verification on CRLs.
‚ñ†
‚ñ† Private-key usage period: Indicates the period of use of the private key cor-
responding to the public key. Typically, the private key is used over a different 
period from the validity of the public key. For example, with digital signature

keys, the usage period for the signing private key is typically shorter than that 
for the verifying public key.
‚ñ†
‚ñ† Certificate policies: Certificates may be used in environments where multiple 
policies apply. This extension lists policies that the certificate is recognized as 
supporting, together with optional qualifier information.
‚ñ†
‚ñ† Policy mappings: Used only in certificates for CAs issued by other CAs. Policy 
mappings allow an issuing CA to indicate that one or more of that issuer‚Äôs policies 
can be considered equivalent to another policy used in the subject CA‚Äôs domain.
certificAte Subject AnD iSSuer AttributeS These extensions support alternative 
names, in alternative formats, for a certificate subject or certificate issuer and can 
convey additional information about the certificate subject to increase a certificate 
user‚Äôs confidence that the certificate subject is a particular person or entity. For 
example, information such as postal address, position within a corporation, or pic-
ture image may be required.
The extension fields in this area include the following:
‚ñ†
‚ñ† Subject alternative name: Contains one or more alternative names, using any 
of a variety of forms. This field is important for supporting certain applica-
tions, such as electronic mail, EDI, and IPSec, which may employ their own 
name forms.
‚ñ†
‚ñ† Issuer alternative name: Contains one or more alternative names, using any 
of a variety of forms.
‚ñ†
‚ñ† Subject directory attributes: Conveys any desired X.500 directory attribute 
values for the subject of this certificate.
certificAtion pAth conStrAintS These extensions allow constraint specifications 
to be included in certificates issued for CAs by other CAs. The constraints may 
restrict the types of certificates that can be issued by the subject CA or that may 
occur subsequently in a certification chain.
The extension fields in this area include the following:
‚ñ†
‚ñ† Basic constraints: Indicates if the subject may act as a CA. If so, a certification 
path length constraint may be specified.
‚ñ†
‚ñ† Name constraints: Indicates a name space within which all subject names in 
subsequent certificates in a certification path must be located.
‚ñ†
‚ñ† Policy constraints: Specifies constraints that may require explicit certificate 
policy identification or inhibit policy mapping for the remainder of the certi-
fication path.
 4.6 PUblic-Key infRAstRUctURe
RFC 4949 (Internet Security Glossary) defines public-key infrastructure (PKI) 
as the set of hardware, software, people, policies, and procedures needed to cre-
ate, manage, store, distribute, and revoke digital certificates based on asymmet-
ric cryptography. The principal objective for developing a PKI is to enable secure,

convenient, and efficient acquisition of public keys. The Internet Engineering Task 
Force (IETF) Public Key Infrastructure X.509 (PKIX) working group has been the 
driving force behind setting up a formal (and generic) model based on X.509 that is 
suitable for deploying a certificate-based architecture on the Internet. This section 
describes the PKIX model.
Figure 4.7 shows the interrelationship among the key elements of the PKIX 
model. These elements are
‚ñ†
‚ñ† End entity: A generic term used to denote end users, devices (e.g., servers, 
routers), or any other entity that can be identified in the subject field of a 
public key certificate. End entities typically consume and/or support PKI-
related services.
‚ñ†
‚ñ† Certification authority (CA): The issuer of certificates and (usually) certifi-
cate revocation lists (CRLs). It may also support a variety of administrative 
functions, although these are often delegated to one or more registration 
authorities.
‚ñ†
‚ñ† Registration authority (RA): An optional component that can assume a 
 number of administrative functions from the CA. The RA is often associated 
Figure 4.7 PKIX Architectural Model
End entity
Certifcate/CRL retrieval
Certifcate
publication
Certifcate/CRL
publication
CRL
publication
Cross-
certifcation
Certifcate/CRL repository
Certifcate
authority
Registration
authority
Certifcate
authority
Registration,
initialization,
certifcation,
key pair recovery,
key pair update
revocation request
PKI
users
PKI
management
entities
CRL issuer

with the end entity registration process, but can assist in a number of other 
areas as well.
‚ñ†
‚ñ† CRL issuer: An optional component that a CA can delegate to publish CRLs.
‚ñ†
‚ñ† Repository: A generic term used to denote any method for storing certifi-
cates and CRLs so that they can be retrieved by end entities.
PKIX Management Functions
PKIX identifies a number of management functions that potentially need to be sup-
ported by management protocols. These are indicated in Figure 4.7 and include the 
following:
‚ñ†
‚ñ† Registration: This is the process whereby a user first makes itself known to 
a CA (directly, or through an RA), prior to that CA issuing a certificate or 
certificates for that user. Registration begins the process of enrolling in a PKI. 
Registration usually involves some off-line or online procedure for mutual 
authentication. Typically, the end entity is issued one or more shared secret 
keys used for subsequent authentication.
‚ñ†
‚ñ† Initialization: Before a client system can operate securely, it is necessary to 
install key materials that have the appropriate relationship with keys stored 
elsewhere in the infrastructure. For example, the client needs to be securely 
initialized with the public key and other assured information of the trusted 
CA(s) to be used in validating certificate paths.
‚ñ†
‚ñ† Certification: This is the process in which a CA issues a certificate for a user‚Äôs 
public key and returns that certificate to the user‚Äôs client system and/or posts 
that certificate in a repository.
‚ñ†
‚ñ† Key pair recovery: Key pairs can be used to support digital signature creation 
and verification, encryption and decryption, or both. When a key pair is used 
for encryption/decryption, it is important to provide a mechanism to recover 
the necessary decryption keys when normal access to the keying material is 
no longer possible, otherwise it will not be possible to recover the encrypted 
data. Loss of access to the decryption key can result from forgotten pass-
words/PINs, corrupted disk drives, damage to hardware tokens, and so on. 
Key pair recovery allows end entities to restore their encryption/decryption 
key pair from an authorized key backup facility (typically, the CA that issued 
the end entity‚Äôs certificate).
‚ñ†
‚ñ† Key pair update: All key pairs need to be updated regularly (i.e., replaced 
with a new key pair) and new certificates issued. Update is required when the 
certificate lifetime expires and as a result of certificate revocation.
‚ñ†
‚ñ† Revocation request: An authorized person advises a CA of an abnormal situ-
ation requiring certificate revocation. Reasons for revocation include private 
key compromise, change in affiliation, and name change.
‚ñ†
‚ñ† Cross-certification: Two CAs exchange information used in establishing 
a cross-certificate. A cross-certificate is a certificate issued by one CA to 
another CA that contains a CA signature key used for issuing certificates.

PKIX Management Protocols
The PKIX working group has defined two alternative management protocols 
between PKIX entities that support the management functions listed in the pre-
ceding subsection. RFC 2510 defines the certificate management protocols (CMP). 
Within CMP, each of the management functions is explicitly identified by specific 
protocol exchanges. CMP is designed to be a flexible protocol able to accommodate 
a variety of technical, operational, and business models.
RFC 2797 defines certificate management messages over CMS (CMC), where 
CMS refers to RFC 2630, cryptographic message syntax. CMC is built on earlier work 
and is intended to leverage existing implementations. Although all of the PKIX func-
tions are supported, the functions do not all map into specific protocol exchanges.
 4.7 feDeRAteD iDentity mAnAgement
Federated identity management is a relatively new concept dealing with the use of 
a common identity management scheme across multiple enterprises and numerous 
applications and supporting many thousands, even millions, of users. We begin our 
overview with a discussion of the concept of identity management and then examine 
federated identity management.
Identity Management
Identity management is a centralized, automated approach to provide enterprise-
wide access to resources by employees and other authorized individuals. The focus 
of identity management is defining an identity for each user (human or process), 
associating attributes with the identity, and enforcing a means by which a user can 
verify identity. The central concept of an identity management system is the use 
of single sign-on (SSO). SSO enables a user to access all network resources after a 
single authentication.
Typical services provided by a federated identity management system include 
the following:
‚ñ†
‚ñ† Point of contact: Includes authentication that a user corresponds to the user 
name provided, and management of user/server sessions.
‚ñ†
‚ñ† SSO protocol services: Provides a vendor-neutral security token service for 
supporting a single sign on to federated services.
‚ñ†
‚ñ† Trust services: Federation relationships require a trust relationship-based 
federation between business partners. A trust relationship is represented by 
the combination of the security tokens used to exchange information about a 
user, the cryptographic information used to protect these security tokens, and 
optionally the identity mapping rules applied to the information contained 
within this token.
‚ñ†
‚ñ† Key services: Management of keys and certificates.
‚ñ†
‚ñ† Identity services: Services that provide the interface to local data stores, includ-
ing user registries and databases, for identity-related information management.

‚ñ†
‚ñ† Authorization: Granting access to specific services and/or resources based on 
the authentication.
‚ñ†
‚ñ† Provisioning: Includes creating an account in each target system for the user, 
enrollment or registration of user in accounts, establishment of access rights 
or credentials to ensure the privacy and integrity of account data.
‚ñ†
‚ñ† Management: Services related to runtime configuration and deployment.
Note that Kerberos contains a number of elements of an identity management 
system.
Figure 4.8 [LINN06] illustrates entities and data flows in a generic identity 
management architecture. A principal is an identity holder. Typically, this is a human 
user that seeks access to resources and services on the network. User devices, agent 
processes, and server systems may also function as principals. Principals authenti-
cate themselves to an identity provider. The identity provider associates authentica-
tion information with a principal, as well as attributes and one or more identifiers.
Increasingly, digital identities incorporate attributes other than simply 
an identifier and authentication information (such as passwords and biometric 
 information). An attribute service manages the creation and maintenance of such 
attributes. For example, a user needs to provide a shipping address each time an 
order is placed at a new Web merchant, and this information needs to be revised 
when the user moves. Identity management enables the user to provide this infor-
mation once, so that it is maintained in a single place and released to data con-
sumers in accordance with authorization and privacy policies. Users may create 
some of the attributes to be associated with their digital identity, such as address. 
Administrators may also assign attributes to users, such as roles, access permissions, 
and employee information.
Figure 4.8 Generic Identity Management Architecture
Identity
Provider
Attribute
Service
Data
consumer
Principal
Administrator

Data consumers are entities that obtain and employ data maintained and pro-
vided by identity and attribute providers, which are often used to support authoriza-
tion decisions and to collect audit information. For example, a database server or 
file server is a data consumer that needs a client‚Äôs credentials so as to know what 
access to provide to that client.
Identity Federation
Identity federation is, in essence, an extension of identity management to mul-
tiple security domains. Such domains include autonomous internal business units, 
external business partners, and other third-party applications and services. The 
goal is to provide the sharing of digital identities so that a user can be authen-
ticated a single time and then access applications and resources across multiple 
domains. Because these domains are relatively autonomous or independent, no 
centralized control is possible. Rather, the cooperating organizations must form a 
federation based on agreed standards and mutual levels of trust to securely share 
digital identities.
Federated identity management refers to the agreements, standards, and 
technologies that enable the portability of identities, identity attributes, and entitle-
ments across multiple enterprises and numerous applications and supports many 
thousands, even millions, of users. When multiple organizations implement interop-
erable federated identity schemes, an employee in one organization can use a single 
sign-on to access services across the federation with trust relationships associated 
with the identity. For example, an employee may log onto her corporate intranet 
and be authenticated to perform authorized functions and access authorized ser-
vices on that intranet. The employee could then access her health benefits from an 
outside health-care provider without having to reauthenticate.
Beyond SSO, federated identity management provides other capabilities. One 
is a standardized means of representing attributes. Increasingly, digital identities 
incorporate attributes other than simply an identifier and authentication informa-
tion (such as passwords and biometric information). Examples of attributes include 
account numbers, organizational roles, physical location, and file ownership. A user 
may have multiple identifiers; for example, each identifier may be associated with a 
unique role with its own access permissions.
Another key function of federated identity management is identity map-
ping. Different security domains may represent identities and attributes differ-
ently. Furthermore, the amount of information associated with an individual in one 
domain may be more than is necessary in another domain. The federated identity 
management protocols map identities and attributes of a user in one domain to the 
requirements of another domain.
Figure 4.9 illustrates entities and data flows in a generic federated identity 
management architecture.
The identity provider acquires attribute information through dialogue and 
protocol exchanges with users and administrators. For example, a user needs to 
provide a shipping address each time an order is placed at a new Web merchant, 
and this information needs to be revised when the user moves. Identity manage-
ment enables the user to provide this information once, so that it is maintained in a

single place and released to data consumers in accordance with authorization and 
privacy¬†policies.
Service providers are entities that obtain and employ data maintained and pro-
vided by identity providers, often to support authorization decisions and to collect 
audit information. For example, a database server or file server is a data consumer 
that needs a client‚Äôs credentials so as to know what access to provide to that client. 
A service provider can be in the same domain as the user and the identity provider. 
The power of this approach is for federated identity management, in which the ser-
vice provider is in a different domain (e.g., a vendor or supplier network).
StAnDArDS Federated identity management uses a number of standards as the 
building blocks for secure identity exchange across different domains or hetero-
geneous systems. In essence, organizations issue some form of security tickets for 
their users that can be processed by cooperating partners. Identity federation stan-
dards are thus concerned with defining these tickets in terms of content and format, 
Figure 4.9 Federated Identity Operation
User
1
Identity provider
(source domain)
Service provider
(destination domain)
1
End user‚Äôs browser or other application engages
in an authentication dialogue with identity provider
in the same domain. End user also provides attribute
values associated with user‚Äôs identity.
2
Some attributes associated with an identity, such as
allowable roles, may be provided by an administrator
in the same domain.
3
A service provider in a remote domain, which the user
wishes to access, obtains identity information, 
authentication information, and associated attributes
from the identity provider in the source domain.
4
Service provider opens session with remote user and
enforces access control restrictions based on user‚Äôs
identity and attributes.
Administrator
2
3
4

providing protocols for exchanging tickets, and performing a number of manage-
ment tasks. These tasks include configuring systems to perform attribute transfers 
and identity mapping, and performing logging and auditing functions. The key stan-
dards are as follows:
‚ñ†
‚ñ† The Extensible Markup Language (XML): A markup language uses sets of 
embedded tags or labels to characterize text elements within a document so 
as to indicate their appearance, function, meaning, or context. XML docu-
ments appear similar to HTML (Hypertext Markup Language) documents 
that are visible as Web pages, but provide greater functionality. XML includes 
strict definitions of the data type of each field, thus supporting database for-
mats and semantics. XML provides encoding rules for commands that are 
used to transfer and update data objects.
‚ñ†
‚ñ† The Simple Object Access Protocol (SOAP): A minimal set of conventions 
for invoking code using XML over HTTP. It enables applications to request 
services from one another with XML-based requests and receive responses 
as data formatted with XML. Thus, XML defines data objects and structures, 
and SOAP provides a means of exchanging such data objects and performing 
remote procedure calls related to these objects. See [ROS06] for an informa-
tive discussion.
‚ñ†
‚ñ† WS-Security: A set of SOAP extensions for implementing message integrity 
and confidentiality in Web services. To provide for secure exchange of SOAP 
messages among applications, WS-Security assigns security tokens to each 
message for use in authentication.
‚ñ†
‚ñ† Security Assertion Markup Language (SAML): An XML-based language 
for the exchange of security information between online business partners. 
SAML conveys authentication information in the form of assertions about 
subjects. Assertions are statements about the subject issued by an authorita-
tive entity.
The challenge with federated identity management is to integrate multiple 
technologies, standards, and services to provide a secure, user-friendly utility. The 
key, as in most areas of security and networking, is the reliance on a few mature 
standards widely accepted by industry. Federated identity management seems to 
have reached this level of maturity.
exAmpleS To get some feel for the functionality of identity federation, we look 
at three scenarios, taken from [COMP06]. In the first scenario (Figure 4.10a), 
Workplace.com contracts with Health.com to provide employee health benefits. 
An employee uses a Web interface to sign on to Workplace.com and goes through 
an authentication procedure there. This enables the employee to access autho-
rized services and resources at Workplace.com. When the employee clicks on a 
link to access health benefits, her browser is redirected to Health.com. At the same 
time, the Workplace.com software passes the user‚Äôs identifier to Health.com in a 
secure manner. The two organizations are part of a federation that cooperatively 
exchanges user identifiers. Health.com maintains user identities for every employee 
at Workplace.com and associates with each identity health-benefits information

and access rights. In this example, the linkage between the two companies is based 
on account information and user participation is browser based.
Figure 4.10b shows a second type of browser-based scheme. PartsSupplier.
com is a regular supplier of parts to Workplace.com. In this case, a role-based 
access control (RBAC) scheme is used for access to information. An engineer of 
Workplace.com authenticates at the employee portal at Workplace.com and clicks 
on a link to access information at PartsSupplier.com. Because the user is authen-
ticated in the role of an engineer, he is taken to the technical documentation and 
troubleshooting portion of PartsSupplier.com‚Äôs Web site without having to sign 
on. Similarly, an employee in a purchasing role signs on at Workplace.com and is 
authorized, in that role, to place purchases at PartsSupplier.com without having to 
authenticate to PartsSupplier.com. For this scenario, PartsSupplier.com does not 
have identity information for individual employees at Workplace.com. Rather, the 
linkage between the two federated partners is in terms of roles.
Figure 4.10 Federated Identity Scenarios
User store
(a) Federation based on account linking
(c) Chained Web Services
Workplace.com
(employee portal)
Name
Joe
Jane
Ravi
ID
1213
1410
1603
User store
Name
Joe
Jane
Ravi
ID
1213
1410
1603
Health.com
User store
(b) Federation based on roles
Name
Joe
Jane
Ravi
ID
1213
1410
1603
Dept
Eng
Purch
Purch
User store
Role
Engineer
Purchaser
Authentication
Website access
End user
(employee)
User ID
Workplace.com
(procurement
application)
PinSupplies.com
(Purchasing Web
service)
Authentication
Procurement
request
End user
(employee)
SOAP message
Eship.com
(shipping Web
service)
SOAP message
Workplace.com
(employee portal)
PartsSupplier.com
Authentication
Website access
End user
(employee)
Role

The scenario illustrated in Figure 4.10c can be referred to as document based 
rather than browser based. In this third example, Workplace.com has a purchasing 
agreement with PinSupplies.com, and PinSupplies.com has a business relationship 
with E-Ship.com. An employee of Workplace.com signs on and is authenticated to 
make purchases. The employee goes to a procurement application that provides a 
list of Workplace.com‚Äôs suppliers and the parts that can be ordered. The user clicks 
on the PinSupplies button and is presented with a purchase order Web page (HTML 
page). The employee fills out the form and clicks the submit button. The procure-
ment application generates an XML/SOAP document that it inserts into the enve-
lope body of an XML-based message. The procurement application then inserts the 
user‚Äôs credentials in the envelope header of the message, together with Workplace.
com‚Äôs organizational identity. The procurement application posts the message to the 
PinSupplies.com‚Äôs purchasing Web service. This service authenticates the incoming 
message and processes the request. The purchasing Web service then sends a SOAP 
message its shipping partner to fulfill the order. The message includes a PinSupplies.
com security token in the envelope header and the list of items to be shipped as well 
as the end user‚Äôs shipping information in the envelope body. The shipping Web ser-
vice authenticates the request and processes the shipment order.
 4.8 Key teRms, Review QUestions, AnD PRoblems
Key Terms 
authentication
authentication server (AS)
federated identity 
management
identity management
Kerberos
Kerberos realm
key distribution
key distribution center (KDC)
key management
master key
mutual authentication
nonce
one-way authentication
propagating cipher block 
chaining (PCBC) mode
public-key certificate
public-key directory
realm
replay attack
ticket
ticket-granting server (TGS)
timestamp
X.509 certificate
Review Questions 
 
4.1 
Explain the operation of a key distribution center.
 
4.2 
What are the advantages of the automated key distribution approach?
 
4.3 
What is Kerberos?
 
4.4 
Identify the security threats that exist in an open distributed network.
 
4.5 
In the context of Kerberos, what is a realm?
 
4.6 
What are the ingredients of a authentication server‚Äôs ticket? Explain the significance 
of each.
 
4.7 
List the environmental shortcomings and technical deficiencies of Kerberos version 4.
 
4.8 
Identify the weakness of a public key distribution with a public key algorithm. How 
can it be fixed?
 
4.9 
What is a X.509 certificate?
 
4.10 
Explain the different fields of the public-key certificate of the X.509 scheme.

4.11 
What is public-key infrastructure?
 
4.12 
What are the key elements of the PKIX model?
 
4.13 
Name the PKIX certificate management protocols.
 
4.14 
What is federated identity management?
Problems 
 
4.1 
‚ÄúWe are under great pressure, Holmes.‚Äù Detective Lestrade looked nervous. ‚ÄúWe 
have learned that copies of sensitive government documents are stored in computers 
of one foreign embassy here in London. Normally these documents exist in electronic 
form only on a selected few government computers that satisfy the most stringent 
security requirements. However, sometimes they must be sent through the network 
connecting all government computers. But all messages in this network are encrypted 
using a top secret encryption algorithm certified by our best crypto experts. Even the 
NSA and the KGB are unable to break it. And now these documents have appeared 
in hands of diplomats of a small, otherwise insignificant, country. And we have no 
idea how it could happen.‚Äù
‚ÄúBut you do have some suspicion who did it, do you?‚Äù asked Holmes.
‚ÄúYes, we did some routine investigation. There is a man who has legal access 
to one of the government computers and has frequent contacts with diplomats from 
the embassy. But the computer he has access to is not one of the trusted ones where 
these documents are normally stored. He is the suspect, but we have no idea how he 
could obtain copies of the documents. Even if he could obtain a copy of an encrypted 
document, he couldn‚Äôt decrypt it.‚Äù
‚ÄúHmm, please describe the communication protocol used on the network.‚Äù 
Holmes opened his eyes, thus proving that he had followed Lestrade‚Äôs talk with an 
attention that contrasted with his sleepy look.
‚ÄúWell, the protocol is as follows. Each node N of the network has been assigned 
a unique secret key Kn. This key is used to secure communication between the node 
and a trusted server. That is, all the keys are stored also on the server. User A, wish-
ing to send a secret message M to user B, initiates the following protocol:
1. A generates a random number R and sends to the server his name A, destina-
tion B, and E(Ka, R).
2. Server responds by sending E(Kb, R) to A.
3. A sends E(R, M) together with E(Kb, R) to B.
4. B knows Kb, thus decrypts E(Kb, R) to get R and will subsequently use R to 
decrypt E(R, M) to get M.
You see that a random key is generated every time a message has to be sent. I admit 
the man could intercept messages sent between the top secret trusted nodes, but I see 
no way he could decrypt them.‚Äù
‚ÄúWell, I think you have your man, Lestrade. The protocol isn‚Äôt secure because 
the server doesn‚Äôt authenticate users who send him a request. Apparently designers 
of the protocol have believed that sending E(Kx, R) implicitly authenticates user X as 
the sender, as only X (and the server) knows Kx. But you know that E(Kx, R) can be 
intercepted and later replayed. Once you understand where the hole is, you will be 
able to obtain enough evidence by monitoring the man‚Äôs use of the computer he has 
access to. Most likely he works as follows: After intercepting E(Ka, R) and E(R, M) 
(see steps 1 and 3 of the protocol), the man, let‚Äôs denote him as Z, will continue by 
pretending to be A and¬†.¬†.¬†.¬†
Finish the sentence for Holmes.

4.2 
There are three typical ways to use nonces as challenges. Suppose Na is a nonce gen-
erated by A, A and B share key K, and f() is a function (such as increment). The three 
usages are
Usage 1
Usage 2
Usage 3
(1) A S B: Na
(1) A S B: E(K, Na)
(1) A S B: E(K, Na)
(2) B S A: E(K, Na)
(2) B S A: Na
(2) B S A: E(K, f(Na))
Describe situations for which each usage is appropriate.
 
4.3 
Show that a random error in one block of ciphertext is propagated to all subsequent 
blocks of plaintext in PCBC mode (see Figure F.2 in Appendix F).
 
4.4 
Suppose that, in PCBC mode, blocks Ci and Ci+1 are interchanged during transmis-
sion. Show that this affects only the decrypted blocks Pi and Pi+1 but not subsequent 
blocks.
 
4.5 
In addition to providing a standard for public-key certificate formats, X.509 specifies 
an authentication protocol. The original version of X.509 contains a security flaw. 
The essence of the protocol is
A S B: A {tA, rA, IDB}
B S A: B {tB, rB, IDA, rA}
A S B: A {rB}
where tA and tB are timestamps, rA and rB are nonces, and the notation X {Y}  indicates 
that the message Y is transmitted, encrypted, and signed by X.
The text of X.509 states that checking timestamps tA and tB is optional for 
three-way authentication. But consider the following example: Suppose A and B 
have used the preceding protocol on some previous occasion, and that opponent C 
has intercepted the preceding three messages. In addition, suppose that timestamps 
are not used and are all set to 0. Finally, suppose C wishes to impersonate A to B.
C initially sends the first captured message to B:
 
C S B: A {0, rA, IDB} 
B responds, thinking it is talking to A but is actually talking to C:
 
B S C: B {0, r B
= , IDA, rA} 
C meanwhile causes A to initiate authentication with C by some means. As a result, 
A sends C the following:
 
A S C: A {0, r A
= , IDC} 
C responds to A using the same nonce provided to C by B.
 
C S A: C {0, r B
= , IDA, r A
= } 
A responds with
 
A S C: A {r B
= } 
This is exactly what C needs to convince B that it is talking to A, so C now repeats the 
incoming message back out to B.
 
C S B: A {r B
= } 
So B will believe it is talking to A, whereas it is actually talking to C. Suggest a simple 
solution to this problem that does not involve the use of timestamps.

4.6 
Consider a one-way authentication technique based on asymmetric encryption:
A S B: IDA
B S A: R1
A S B: E(PRa, R1)
a. Explain the protocol.
b. What type of attack is this protocol susceptible to?
 
4.7 
Consider a one-way authentication technique based on asymmetric encryption:
A S B: IDA
B S A: E(PUa, R2)
A S B: R2
a. Explain the protocol.
b. What type of attack is this protocol susceptible to?
 
4.8 
In Kerberos, how do servers verify the authenticity of the client using the ticket?
 
4.9 
In Kerberos, how does an authentication server protect a ticket from being altered by 
the client or opponent?
 
4.10 
How is ticket reuse by an opponent prevented in Kerberos?
 
4.11 
What is the purpose of a session key in Kerberos? How is it distributed by the AS?
 
4.12 
The 1988 version of X.509 lists properties that RSA keys must satisfy to be secure, 
given current knowledge about the difficulty of factoring large numbers. The discus-
sion concludes with a constraint on the public exponent and the modulus n:
It must be ensured that e 7 log2(n) to prevent attack by taking the 
 eth root mod n to disclose the plaintext.
Although the constraint is correct, the reason given for requiring it is incorrect. What 
is wrong with the reason given and what is the correct reason?
 
4.13 
Find at least one intermediate certification authority‚Äôs certificate and one trusted 
root certification authority‚Äôs certificate on your computer (e.g., in the browser). Print 
screenshots of both the general and details tab for each certificate.
 
4.14 
NIST defines the term ‚Äúcryptoperiod‚Äù as the time span during which a specific key is 
authorized for use or in which the keys for a given system or application may remain 
in effect. One document on key management uses the following time diagram for a 
shared secret key.
Originator Usage Period
Recipient Usage Period
Cryptoperiod
Explain the overlap by giving an example application in which the originator‚Äôs usage 
period for the shared secret key begins before the recipient‚Äôs usage period and also 
ends before the recipient‚Äôs usage period.

4.15 
Consider the following protocol, designed to let A and B decide on a fresh, shared 
session key KAB
=
. We assume that they already share a long-term key KAB.
1. A S B: A, NA
2. B S A: E(KAB, [NA, KAB
=
])
3. A S B: E(KAB
=
, NA)
a. We first try to understand the protocol designer‚Äôs reasoning:
‚ñ†‚ñ†
Why would A and B believe after the protocol ran that they share KAB
=
 
with the other party?
‚ñ†‚ñ†
Why would they believe that this shared key is fresh?
In both cases, you should explain both the reasons of both A and B, so your answer 
should complete the following sentences.
A believes that she shares KAB
=
 with B since¬†.¬†.¬†.¬†
B believes that he shares KAB
=
 with A since¬†.¬†.¬†.¬†
A believes that KAB
=
 is fresh since¬†.¬†.¬†.¬†
B believes that KAB
=
 is fresh since¬†.¬†.¬†.¬†
b. Assume now that A starts a run of this protocol with B. However, the connec-
tion is intercepted by the adversary C. Show how C can start a new run of the 
protocol using reflection, causing A to believe that she has agreed on a fresh 
key with B (in spite of the fact that she has only been communicating with C). 
Thus, in particular, the belief in (a) is false.
c. Propose a modification of the protocol that prevents this attack.
 
4.16 
List the different management functions of the PKIX model.
 
4.17 
Explain the entities and data flows of generic identity management architecture.
 
4.18 
Consider the following protocol:
A S KDC: IDA } IDB} N1
KDC S A: E(Ka, [KS } IDB} N1} E(Kb, [KS } IDA]))
A S B:  E(Kb, [KS } IDA])
B S A:  E(KS, N2)A S B:  E(KS, f(N2))
a. Explain the protocol.
b. Can you think of a possible attack on this protocol? Explain how it can be done.
c. Mention a possible technique to get around the attack‚Äînot a detailed mechanism, 
just the basics of the idea.
Note: The remaining problems deal with a cryptographic product developed by IBM, 
which is briefly described in a document at this book‚Äôs Web site in IBMCrypto.pdf. 
Try these problems after reviewing the document.
 
4.19 
What is the effect of adding the instruction EMKi?
 
EMKi: X S E(KMHi, X) i = 0, 1 
 
4.20 
Suppose N different systems use the IBM Cryptographic Subsystem with host master 
keys KMH[i](i = 1, 2, c , N). Devise a method for communicating between sys-
tems without requiring the system to either share a common host master key or to 
divulge their individual host master keys. Hint: Each system needs three variants of 
its host master key.
 
4.21 
The principal objective of the IBM Cryptographic Subsystem is to protect transmis-
sions between a terminal and the processing system. Devise a procedure, perhaps 
adding instructions, which will allow the processor to generate a session key KS and 
distribute it to Terminal i and Terminal j without having to store a key-equivalent 
variable in the host.

160
5.1 
Network Access Control
Elements of a Network Access Control System
Network Access Enforcement Methods
5.2 
Extensible Authentication Protocol
Authentication Methods
EAP Exchanges
5.3 
IEEE 802.1X Port-Based Network Access Control
5.4 
Cloud Computing
Cloud Computing Elements
Cloud Computing Reference Architecture
5.5 
Cloud Security Risks and Countermeasures
5.6 
Data Protection in the Cloud
5.7 
Cloud Security as a Service
5.8 
Addressing Cloud Computing Security Concerns
5.9 
Key Terms, Review Questions, and Problems
 Chapter
Network Access Control 
and¬†Cloud Security

This chapter begins our discussion of network security, focusing on two key topics: 
 network access control and cloud security. We begin with an overview of network 
 access control systems, summarizing the principal elements and techniques involved 
in such a system. Next, we discuss the Extensible Authentication Protocol and IEEE 
802.1X, two widely implemented standards that are the foundation of many network 
access control systems.
The remainder of the chapter deals with cloud security. We begin with an 
 overview of cloud computing, and follow this with a discussion of cloud security 
issues.
 5.1 Network Access coNtrol
Network access control (NAC) is an umbrella term for managing access to a 
 network. NAC authenticates users logging into the network and determines what 
data they can access and actions they can perform. NAC also examines the health of 
the user‚Äôs computer or mobile device (the endpoints).
Elements of a Network Access Control System
NAC systems deal with three categories of components:
‚ñ†
‚ñ† Access requestor (AR): The AR is the node that is attempting to access the 
network and may be any device that is managed by the NAC system, including 
workstations, servers, printers, cameras, and other IP-enabled devices. ARs are 
also referred to as supplicants, or simply, clients.
‚ñ†
‚ñ† Policy server: Based on the AR‚Äôs posture and an enterprise‚Äôs defined policy, 
the policy server determines what access should be granted. The policy server 
often relies on backend systems, including antivirus, patch management, or a 
user directory, to help determine the host‚Äôs condition.
leArNiNg objectives
After studying this chapter, you should be able to:
‚óÜ‚ñ†
Discuss the principal elements of a network access control system.
‚óÜ‚ñ†
Discuss the principal network access enforcement methods.
‚óÜ‚ñ†
Present an overview of the Extensible Authentication Protocol.
‚óÜ‚ñ†
Understand the operation and role of the IEEE 802.1X Port-Based 
 Network Access Control mechanism.
‚óÜ‚ñ†
Present an overview of cloud computing concepts.
‚óÜ‚ñ†
Understand the unique security issues related to cloud computing.

‚ñ†
‚ñ† Network access server (NAS): The NAS functions as an access control point 
for users in remote locations connecting to an enterprise‚Äôs internal network. 
Also called a media gateway, a remote access server (RAS), or a policy server, 
an NAS may include its own authentication services or rely on a separate 
 authentication service from the policy server.
Figure 5.1 is a generic network access diagram. A variety of different ARs 
seek access to an enterprise network by applying to some type of NAS. The first 
step is generally to authenticate the AR. Authentication typically involves some 
sort of secure protocol and the use of cryptographic keys. Authentication may be 
performed by the NAS, or the NAS may mediate the authentication process. In the 
latter case, authentication takes place between the supplicant and an authentication 
server that is part of the policy server or that is accessed by the policy server.
The authentication process serves a number of purposes. It verifies a suppli-
cant‚Äôs claimed identity, which enables the policy server to determine what access 
privileges, if any, the AR may have. The authentication exchange may result in the 
Figure 5.1 Network Access Control Context
Supplicants
Network access servers
Authentication
server
DHCP
server
VLAN
server
Policy
server
Patch
management
Network
resources
Quarantine
network
Antivirus
Antispyware
Enterprise network

establishment of session keys to enable future secure communication between the 
supplicant and resources on the enterprise network.
Typically, the policy server or a supporting server will perform checks on the 
AR to determine if it should be permitted interactive remote access  connectivity. 
These checks‚Äîsometimes called health, suitability, screening, or assessment 
checks‚Äîrequire software on the user‚Äôs system to verify compliance with certain 
 requirements from the organization‚Äôs secure configuration baseline. For example, 
the user‚Äôs antimalware software must be up-to-date, the operating system must 
be fully patched, and the remote computer must be owned and controlled by the 
 organization. These checks should be performed before granting the AR access to 
the enterprise  network. Based on the results of these checks, the organization can 
determine whether the remote computer should be permitted to use interactive 
 remote access. If the user has acceptable authorization credentials but the remote 
computer does not pass the health check, the user and remote computer should be 
denied network access or have limited access to a quarantine network so that autho-
rized personnel can fix the security deficiencies. Figure 5.1 indicates that the quar-
antine portion of the enterprise network consists of the policy server and related 
AR suitability servers. There may also be application servers that do not require the 
normal security threshold be met.
Once an AR has been authenticated and cleared for a certain level of access 
to the enterprise network, the NAS can enable the AR to interact with resources in 
the enterprise network. The NAS may mediate every exchange to enforce a security 
policy for this AR, or may use other methods to limit the privileges of the AR.
Network Access Enforcement Methods
Enforcement methods are the actions that are applied to ARs to regulate access 
to the enterprise network. Many vendors support multiple enforcement methods 
simultaneously, allowing the customer to tailor the configuration by using one or a 
combination of methods. The following are common NAC enforcement methods.
‚ñ†
‚ñ† IEEE 802.1X: This is a link layer protocol that enforces authorization before 
a port is assigned an IP address. IEEE 802.1X makes use of the Extensible 
Authentication Protocol for the authentication process. Sections 5.2 and 5.3 
cover the Extensible Authentication Protocol and IEEE 802.1X, respectively.
‚ñ†
‚ñ† Virtual local area networks (VLANs): In this approach, the enterprise net-
work, consisting of an interconnected set of LANs, is segmented logically into 
a number of virtual LANs.1 The NAC system decides to which of the network‚Äôs 
VLANs it will direct an AR, based on whether the device needs security reme-
diation, Internet access only, or some level of network access to enterprise 
 resources. VLANs can be created dynamically and VLAN membership, of 
both enterprise servers and ARs, may overlap. That is, an enterprise server or 
an AR may belong to more than one VLAN.
1A VLAN is a logical subgroup within a LAN that is created via software rather than manually  moving 
cables in the wiring closet. It combines user stations and network devices into a single unit regardless 
of¬†the physical LAN segment they are attached to and allows traffic to flow more efficiently within 
 populations of mutual interest. VLANs are implemented in port-switching hubs and LAN switches.

‚ñ†
‚ñ† Firewall: A firewall provides a form of NAC by allowing or denying network 
traffic between an enterprise host and an external user. Firewalls are discussed 
in Chapter 12.
‚ñ†
‚ñ† DHCP management: The Dynamic Host Configuration Protocol (DHCP) is 
an Internet protocol that enables dynamic allocation of IP addresses to hosts. 
A DHCP server intercepts DHCP requests and assigns IP addresses instead. 
Thus, NAC enforcement occurs at the IP layer based on subnet and IP assign-
ment. A DCHP server is easy to install and configure, but is subject to various 
forms of IP spoofing, providing limited security.
There are a number of other enforcement methods available from vendors. 
The ones in the preceding list are perhaps the most common, and IEEE 802.1X is by 
far the most commonly implemented solution.
 5.2 exteNsible AutheNticAtioN Protocol
The Extensible Authentication Protocol (EAP), defined in RFC 3748, acts as a 
framework for network access and authentication protocols. EAP provides a set of 
protocol messages that can encapsulate various authentication methods to be used 
between a client and an authentication server. EAP can operate over a variety of 
network and link level facilities, including point-to-point links, LANs, and other 
networks, and can accommodate the authentication needs of the various links and 
networks. Figure 5.2 illustrates the protocol layers that form the context for EAP.
Authentication Methods
EAP supports multiple authentication methods. This is what is meant by referring 
to EAP as extensible. EAP provides a generic transport service for the exchange of 
 authentication information between a client system and an authentication server. 
The basic EAP transport service is extended by using a specific authentication proto-
col, or method, that is installed in both the EAP client and the authentication server.
Figure 5.2 EAP Layered Context
Authentication
methods
EAP
layer
Data link
layer
Extensible Authentication Protocol (EAP)
IEEE 802.1X
EAP over LAN (EAPOL)
EAP-
TLS
EAP-
TTLS
EAP-
PSK
EAP-
IKEv2
PPP
802.3
Ethernet
802.11
WLAN
Other
Other

Numerous methods have been defined to work over EAP. The following are 
commonly supported EAP methods:
‚ñ†
‚ñ† EAP-TLS (EAP Transport Layer Security): EAP-TLS (RFC 5216) defines 
how the TLS protocol (described in Chapter 6) can be encapsulated in EAP 
messages. EAP-TLS uses the handshake protocol in TLS, not its encryption 
method. Client and server authenticate each other using digital certificates. 
Client generates a pre-master secret key by encrypting a random number with 
the server‚Äôs public key and sends it to the server. Both client and server use the 
pre-master to generate the same secret key.
‚ñ†
‚ñ† EAP-TTLS (EAP Tunneled TLS): EAP-TTLS is like EAP-TLS, except only 
the server has a certificate to authenticate itself to the client first. As in EAP-
TLS, a secure connection (the ‚Äútunnel‚Äù) is established with secret keys, but 
that connection is used to continue the authentication process by authenti-
cating the client and possibly the server again using any EAP method or 
legacy method such as PAP (Password Authentication Protocol) and CHAP 
(Challenge-Handshake Authentication Protocol). EAP-TTLS is defined in 
RFC 5281.
‚ñ†
‚ñ† EAP-GPSK (EAP Generalized Pre-Shared Key): EAP-GPSK, defined in 
RFC 5433, is an EAP method for mutual authentication and session key deri-
vation using a Pre-Shared Key (PSK). EAP-GPSK specifies an EAP method 
based on pre-shared keys and employs secret key-based cryptographic algo-
rithms. Hence, this method is efficient in terms of message flows and com-
putational costs, but requires the existence of pre-shared keys between each 
peer and EAP server. The set up of these pairwise secret keys is part of the 
peer registration, and thus, must satisfy the system preconditions. It provides 
a protected communication channel when mutual authentication is success-
ful for both parties to communicate over and is designed for authentication 
over insecure networks such as IEEE 802.11. EAP-GPSK does not require 
any public-key cryptography. The EAP method protocol exchange is done in a 
minimum of four messages.
‚ñ†
‚ñ† EAP-IKEv2: It is based on the Internet Key Exchange protocol version 2 
(IKEv2), which is described in Chapter 9. It supports mutual authentication 
and session key establishment using a variety of methods. EAP-TLS is defined 
in RFC 5106.
EAP Exchanges
Whatever method is used for authentication, the authentication information and 
authentication protocol information are carried in EAP messages.
RFC 3748 defines the goal of the exchange of EAP messages to be successful 
authentication. In the context of RFC 3748, successful authentication is an  exchange 
of EAP messages, as a result of which the authenticator decides to allow access 
by the peer, and the peer decides to use this access. The authenticator‚Äôs decision 
 typically involves both authentication and authorization aspects; the peer may 
 successfully authenticate to the authenticator, but access may be denied by the 
 authenticator due to policy reasons.

Figure 5.3 indicates a typical arrangement in which EAP is used. The follow-
ing components are involved:
‚ñ†
‚ñ† EAP peer: Client computer that is attempting to access a network.
‚ñ†
‚ñ† EAP authenticator: An access point or NAS that requires EAP authentication 
prior to granting access to a network.
‚ñ†
‚ñ† Authentication server: A server computer that negotiates the use of a  specific 
EAP method with an EAP peer, validates the EAP peer‚Äôs credentials, and 
 authorizes access to the network. Typically, the authentication server is a 
Remote Authentication Dial-In User Service (RADIUS) server.
The authentication server functions as a backend server that can authenti-
cate peers as a service to a number of EAP authenticators. The EAP authentica-
tor then makes the decision of whether to grant access. This is referred to as the 
EAP¬† pass-through mode. Less commonly, the authenticator takes over the role of 
the EAP server; that is, only two parties are involved in the EAP execution.
As a first step, a lower-level protocol, such as PPP (point-to-point protocol) 
or IEEE 802.1X, is used to connect to the EAP authenticator. The software entity 
in the EAP peer that operates at this level is referred to as the supplicant. EAP 
 messages containing the appropriate information for a chosen EAP method are 
then exchanged between the EAP peer and the authentication server.
EAP messages may include the following fields:
‚ñ†
‚ñ† Code: Identifies the Type of EAP message. The codes are Request (1), 
Response (2), Success (3), and Failure (4).
‚ñ†
‚ñ† Identifier: Used to match Responses with Requests.
‚ñ†
‚ñ† Length: Indicates the length, in octets, of the EAP message, including the 
Code, Identifier, Length, and Data fields.
Figure 5.3 EAP Protocol Exchanges
Method
EAP peer/
authenticator
EAP layer
Lower layer
EAP
authenticator
EAP layer
Lower layer
Method
EAP peer/
authenticator
EAP layer
Lower layer
RADIUS
EAP
messages
EAP
messages
802.1X,
PPP
EAP peer
EAP authenticator
Authentication server
(RADIUS)

‚ñ†
‚ñ† Data: Contains information related to authentication. Typically, the Data field 
consists of a Type subfield, indicating the type of data carried, and a Type-Data 
field.
The Success and Failure messages do not include a Data field.
The EAP authentication exchange proceeds as follows. After a lower-level 
exchange that established the need for an EAP exchange, the authenticator sends a 
Request to the peer to request an identity, and the peer sends a Response with the 
identity information. This is followed by a sequence of Requests by the authentica-
tor and Responses by the peer for the exchange of authentication information. The 
information exchanged and the number of Request‚ÄìResponse exchanges needed 
depend on the authentication method. The conversation continues until either 
(1)¬†the authenticator determines that it cannot authenticate the peer and transmits 
an EAP Failure or (2) the authenticator determines that successful authentication 
has occurred and transmits an EAP Success.
Figure 5.4 gives an example of an EAP exchange. Not shown in the figure is a 
message or signal sent from the EAP peer to the authenticator using some protocol 
other than EAP and requesting an EAP exchange to grant network access. One 
protocol used for this purpose is IEEE 802.1X, discussed in the next section. The 
first pair of EAP Request and Response messages is of Type identity, in which the 
authenticator requests the peer‚Äôs identity, and the peer returns its claimed identity 
in the Response message. This Response is passed through the authenticator to the 
authentication server. Subsequent EAP messages are exchanged between the peer 
and the authentication server.
Figure 5.4 EAP Message Flow in Pass-Through Mode
EAP peer
EAP-Response/Identity
EAP-Request/Identity
EAP authenticator
Authentication server
(RADIUS)
EAP-Response/Auth
EAP-Request/Auth
EAP-Response/Auth
EAP-Request/Auth
EAP-Success/Failure

Upon receiving the identity Response message from the peer, the server 
 selects an EAP method and sends the first EAP message with a Type field related 
to an authentication method. If the peer supports and accepts the selected EAP 
method, it replies with the corresponding Response message of the same type. 
Otherwise, the peer sends a NAK, and the EAP server either selects another EAP 
method or aborts the EAP execution with a failure message. The selected EAP 
method determines the number of Request/Response pairs. During the exchange 
the appropriate authentication information, including key material, is exchanged. 
The exchange ends when the server determines that authentication has succeeded 
or that no further attempt can be made and authentication has failed.
 5.3 ieee 802.1x Port-bAsed Network Access coNtrol
IEEE 802.1X Port-Based Network Access Control was designed to provide access 
control functions for LANs. Table 5.1 briefly defines key terms used in the IEEE 
802.11 standard. The terms supplicant, network access point, and authentication 
Authenticator
An entity at one end of a point-to-point LAN segment that facilities authentication of the entity to the other 
end of the link.
Authentication exchange
The two-party conversation between systems performing an authentication process.
Authentication process
The cryptographic operations and supporting data frames that perform the actual authentication.
Authentication server (AS)
An entity that provides an authentication service to an authenticator. This service determines, from the 
 credentials provided by supplicant, whether the supplicant is authorized to access the services provided by 
the¬†system in which the authenticator resides.
Authentication transport
The datagram session that actively transfers the authentication exchange between two systems.
Bridge port
A port of an IEEE 802.1D or 802.1Q bridge.
Edge port
A bridge port attached to a LAN that has no other bridges attached to it.
Network access port
A point of attachment of a system to a LAN. It can be a physical port, such as a single LAN MAC attached to 
a physical LAN segment, or a logical port, for example, an IEEE 802.11 association between a station and an 
access point.
Port access entity (PAE)
The protocol entity associated with a port. It can support the protocol functionality associated with the 
authenticator, the supplicant, or both.
Supplicant
An entity at one end of a point-to-point LAN segment that seeks to be authenticated by an authenticator 
attached to the other end of that link.
Table 5.1 Terminology Related to IEEE 802.1X

server correspond to the EAP terms peer, authenticator, and authentication server, 
respectively.
Until the AS authenticates a supplicant (using an authentication protocol), 
the authenticator only passes control and authentication messages between the sup-
plicant and the AS; the 802.1X control channel is unblocked, but the 802.11 data 
channel is blocked. Once a supplicant is authenticated and keys are provided, the 
authenticator can forward data from the supplicant, subject to predefined access 
control limitations for the supplicant to the network. Under these circumstances, 
the data channel is unblocked.
As indicated in Figure 5.5, 802.1X uses the concepts of controlled and uncon-
trolled ports. Ports are logical entities defined within the authenticator and refer to 
physical network connections. Each logical port is mapped to one of these two types 
of physical ports. An uncontrolled port allows the exchange of protocol data units 
(PDUs) between the supplicant and the AS, regardless of the authentication state 
of the supplicant. A controlled port allows the exchange of PDUs between a sup-
plicant and other systems on the network only if the current state of the supplicant 
authorizes such an exchange.
The essential element defined in 802.1X is a protocol known as EAPOL (EAP 
over LAN). EAPOL operates at the network layers and makes use of an IEEE 802 
LAN, such as Ethernet or Wi-Fi, at the link level. EAPOL enables a supplicant to 
communicate with an authenticator and supports the exchange of EAP packets for 
authentication.
Figure 5.5 802.1X Access Control
Supplicant
Network
access point
Uncontrolled
port
Controlled
port
Authentication server
Network or Internet

The most common EAPOL packets are listed in Table 5.2. When the supplicant 
first connects to the LAN, it does not know the MAC address of the  authenticator. 
Actually it doesn‚Äôt know whether there is an authenticator present at all. By send-
ing an EAPOL-Start packet to a special group-multicast address  reserved for IEEE 
802.1X authenticators, a supplicant can determine whether an authenticator is pres-
ent and let it know that the supplicant is ready. In many cases, the authenticator will 
already be notified that a new device has connected from some hardware notifica-
tion. For example, a hub knows that a cable is plugged in before the device sends 
any data. In this case the authenticator may preempt the Start message with its own 
message. In either case the authenticator sends an  EAP-Request Identity message 
encapsulated in an EAPOL-EAP packet. The EAPOL-EAP is the EAPOL frame 
type used for transporting EAP packets.
The authenticator uses the EAP-Key packet to send cryptographic keys to the 
supplicant once it has decided to admit it to the network. The EAP-Logoff packet 
type indicates that the supplicant wishes to be disconnected from the network.
The EAPOL packet format includes the following fields:
‚ñ†
‚ñ† Protocol version: version of EAPOL.
‚ñ†
‚ñ† Packet type: indicates start, EAP, key, logoff, etc.
‚ñ†
‚ñ† Packet body length: If the packet includes a body, this field indicates the body 
length.
‚ñ†
‚ñ† Packet body: The payload for this EAPOL packet. An example is an EAP 
packet.
Figure 5.6 shows an example of exchange using EAPOL. In Chapter 7, we 
examine the use of EAP and EAPOL in the context of IEEE 802.11 wireless LAN 
security.
 5.4 cloud comPutiNg
There is an increasingly prominent trend in many organizations to move a substan-
tial portion of or even all information technology (IT) operations to an Internet-
connected infrastructure known as enterprise cloud computing. This  section  provides 
an overview of cloud computing. For a more detailed treatment, see [STAL16b].
Frame Type
Definition
EAPOL-EAP
Contains an encapsulated EAP packet.
EAPOL-Start
A supplicant can issue this packet instead of waiting for 
a¬†challenge from the authenticator.
EAPOL-Logoff
Used to return the state of the port to unauthorized when 
the supplicant is finished using the network.
EAPOL-Key
Used to exchange cryptographic keying information.
Table 5.2 Common EAPOL Frame Types

Cloud Computing Elements
NIST defines cloud computing, in NIST SP 800-145 (The NIST Definition of Cloud 
Computing), as follows:
Figure 5.6 Example Timing Diagram for IEEE 802.1X
EAP peer
EAPOL-Start
EAPOL-EAP (EAP-Request/Identity)
EAPOL-EAP (EAP-Response/Identity)
EAP authenticator
Authentication server
(RADIUS)
EAPOL-Logof
EAPOL-EAP (EAP-Response/Auth)
EAPOL-EAP (EAP-Request/Auth)
EAPOL-EAP (EAP-Response/Auth)
EAPOL-EAP (EAP-Request/Auth)
EAPOL-EAP (EAP-Success)
Cloud computing: A model for enabling ubiquitous, convenient, on-demand net-
work access to a shared pool of configurable computing resources (e.g., networks, 
servers, storage, applications, and services) that can be rapidly provisioned and 
 released with minimal management effort or service provider interaction. This 
cloud model promotes availability and is composed of five essential characteris-
tics, three service models, and four deployment models.
The definition refers to various models and characteristics, whose relationship is 
 illustrated in Figure 5.7. The essential characteristics of cloud computing include the 
following:
‚ñ†
‚ñ† Broad network access: Capabilities are available over the network and 
 accessed through standard mechanisms that promote use by heterogeneous

thin or thick client platforms (e.g., mobile phones, laptops, and PDAs) as well 
as other traditional or cloud-based software services.
‚ñ†
‚ñ† Rapid elasticity: Cloud computing gives you the ability to expand and reduce 
resources according to your specific service requirement. For example, you 
may need a large number of server resources for the duration of a specific task. 
You can then release these resources upon completion of the task.
‚ñ†
‚ñ† Measured service: Cloud systems automatically control and optimize resource 
use by leveraging a metering capability at some level of abstraction appropri-
ate to the type of service (e.g., storage, processing, bandwidth, and active user 
accounts). Resource usage can be monitored, controlled, and reported, provid-
ing transparency for both the provider and consumer of the utilized service.
‚ñ†
‚ñ† On-demand self-service: A consumer can unilaterally provision computing 
 capabilities, such as server time and network storage, as needed automati-
cally without requiring human interaction with each service provider. Because 
the service is on demand, the resources are not permanent parts of your IT 
infrastructure.
‚ñ†
‚ñ† Resource pooling: The provider‚Äôs computing resources are pooled to serve 
multiple consumers using a multi-tenant model, with different physical and 
virtual resources dynamically assigned and reassigned according to consumer 
demand. There is a degree of location independence in that the customer 
Figure 5.7 Cloud Computing Elements
Broad
Network Access
Resource Pooling
Rapid
Elasticity
Essential
Characteristics
Service
Models
Deployment
Models
Measured
Service
On-Demand
Self-Service
Public
Private
Hybrid
Community
Software as a Service (SaaS)
Platform as a Service (PaaS)
Infrastructure as a Service (IaaS)

generally has no control or knowledge of the exact location of the provided 
resources, but may be able to specify location at a higher level of abstraction 
(e.g., country, state, or data center). Examples of resources include storage, 
processing, memory, network bandwidth, and virtual machines. Even private 
clouds tend to pool resources between different parts of the same organization.
NIST defines three service models, which can be viewed as nested service 
alternatives:
‚ñ†
‚ñ† Software as a service (SaaS): The capability provided to the consumer is to use 
the provider‚Äôs applications running on a cloud infrastructure. The applications 
are accessible from various client devices through a thin client interface such as 
a Web browser. Instead of obtaining desktop and server licenses for software 
products it uses, an enterprise obtains the same functions from the cloud service. 
SaaS saves the complexity of software installation, maintenance, upgrades, and 
patches. Examples of services at this level are Gmail, Google‚Äôs e-mail  service, 
and Salesforce.com, which helps firms keep track of their customers.
‚ñ†
‚ñ† Platform as a service (PaaS): The capability provided to the consumer is to 
 deploy onto the cloud infrastructure consumer-created or acquired applica-
tions created using programming languages and tools supported by the pro-
vider. PaaS often provides middleware-style services such as database and 
component services for use by applications. In effect, PaaS is an operating 
 system in the cloud.
‚ñ†
‚ñ† Infrastructure as a service (IaaS): The capability provided to the consumer is 
to provision processing, storage, networks, and other fundamental computing 
resources where the consumer is able to deploy and run arbitrary software, 
which can include operating systems and applications. IaaS enables custom-
ers to combine basic computing services, such as number crunching and data 
 storage, to build highly adaptable computer systems.
NIST defines four deployment models:
‚ñ†
‚ñ† Public cloud: The cloud infrastructure is made available to the general public 
or a large industry group and is owned by an organization selling cloud ser-
vices. The cloud provider is responsible both for the cloud infrastructure and 
for the control of data and operations within the cloud.
‚ñ†
‚ñ† Private cloud: The cloud infrastructure is operated solely for an organization. 
It may be managed by the organization or a third party and may exist on prem-
ise or off premise. The cloud provider (CP) is responsible only for the infra-
structure and not for the control.
‚ñ†
‚ñ† Community cloud: The cloud infrastructure is shared by several organizations 
and supports a specific community that has shared concerns (e.g., mission,  security 
requirements, policy, and compliance considerations). It may be managed by the 
organizations or a third party and may exist on premise or off premise.
‚ñ†
‚ñ† Hybrid cloud: The cloud infrastructure is a composition of two or more clouds 
(private, community, or public) that remain unique entities but are bound 
 together by standardized or proprietary technology that enables data and 
 application portability (e.g., cloud bursting for load balancing between clouds).

Figure 5.8 illustrates the typical cloud service context. An enterprise maintains 
workstations within an enterprise LAN or set of LANs, which are connected by a 
router through a network or the Internet to the cloud service provider. The cloud 
service provider maintains a massive collection of servers, which it manages with a 
variety of network management, redundancy, and security tools. In the  figure, the 
cloud infrastructure is shown as a collection of blade servers, which is a common 
architecture.
Cloud Computing Reference Architecture
NIST SP 500-292 (NIST Cloud Computing Reference Architecture) establishes a 
 reference architecture, described as follows:
Figure 5.8 Cloud Computing Context
Router
Servers
LAN
switch
Cloud
service
provider
Network
or Internet
Router
LAN
switch
Enterprise
(Cloud user)
 The NIST cloud computing reference architecture focuses on the requirements 
of ‚Äúwhat‚Äù cloud services provide, not a ‚Äúhow to‚Äù design solution and implemen-
tation. The reference architecture is intended to facilitate the understanding of 
the operational intricacies in cloud computing. It does not represent the system 
architecture of a specific cloud computing system; instead it is a tool for describ-
ing, discussing, and developing a system-specific architecture using a common 
framework of reference.

NIST developed the reference architecture with the following objectives 
in¬†mind:
‚ñ†
‚ñ† to illustrate and understand the various cloud services in the context of an 
overall cloud computing conceptual model
‚ñ†
‚ñ† to provide a technical reference for consumers to understand, discuss, catego-
rize, and compare cloud services
‚ñ†
‚ñ† to facilitate the analysis of candidate standards for security, interoperability, 
and portability and reference implementations
The reference architecture, depicted in Figure 5.9, defines five major actors in 
terms of the roles and responsibilities:
‚ñ†
‚ñ† Cloud consumer: A person or organization that maintains a business relation-
ship with, and uses service from, cloud providers.
‚ñ†
‚ñ† Cloud provider: A person, organization, or entity responsible for making a 
 service available to interested parties.
‚ñ†
‚ñ† Cloud auditor: A party that can conduct independent assessment of cloud 
 services, information system operations, performance, and security of the 
cloud implementation.
‚ñ†
‚ñ† Cloud broker: An entity that manages the use, performance, and delivery of 
cloud services, and negotiates relationships between CPs and cloud consumers.
‚ñ†
‚ñ† Cloud carrier: An intermediary that provides connectivity and transport of 
cloud services from CPs to cloud consumers.
The roles of the cloud consumer and provider have already been discussed. To 
summarize, a cloud provider can provide one or more of the cloud services to meet 
IT and business requirements of cloud consumers. For each of the three service 
Figure 5.9 NIST Cloud Computing Reference Architecture
Cloud
consumer
Cloud
auditor
Service
intermediation
Service
aggregation
Service
arbitrage
Cloud
broker
Cloud provider 
Security
audit
Performance
audit
Privacy
impact audit
SaaS
Service layer
Service orchestration
Cloud
service
management
PaaS
Hardware
Physical resource layer
Facility
Resource abstraction
and control layer
IaaS
Business
support
Provisioning/
confguration
Portability/
interoperability
Security
Privacy
Cloud carrier

models (SaaS, PaaS, IaaS), the CP provides the storage and processing  facilities 
needed to support that service model, together with a cloud interface for cloud 
 service consumers. For SaaS, the CP deploys, configures, maintains, and  updates 
the operation of the software applications on a cloud infrastructure so that the 
 services are provisioned at the expected service levels to cloud consumers. The 
consumers of SaaS can be organizations that provide their members with access to 
software  applications, end users who directly use software applications, or software 
 application administrators who configure applications for end users.
For PaaS, the CP manages the computing infrastructure for the platform and 
runs the cloud software that provides the components of the platform, such as run-
time software execution stack, databases, and other middleware components. Cloud 
consumers of PaaS can employ the tools and execution resources provided by CPs to 
develop, test, deploy, and manage the applications hosted in a cloud environment.
For IaaS, the CP acquires the physical computing resources underlying the 
 service, including the servers, networks, storage, and hosting infrastructure. The 
IaaS cloud consumer in turn uses these computing resources, such as a virtual 
 computer, for their fundamental computing needs.
The cloud carrier is a networking facility that provides connectivity and trans-
port of cloud services between cloud consumers and CPs. Typically, a CP will set up 
service level agreements (SLAs) with a cloud carrier to provide services consistent 
with the level of SLAs offered to cloud consumers, and may require the cloud carrier 
to provide dedicated and secure connections between cloud consumers and¬†CPs.
A cloud broker is useful when cloud services are too complex for a cloud con-
sumer to easily manage. Three areas of support can be offered by a cloud broker:
‚ñ†
‚ñ† Service intermediation: These are value-added services, such as identity man-
agement, performance reporting, and enhanced security.
‚ñ†
‚ñ† Service aggregation: The broker combines multiple cloud services to meet 
consumer needs not specifically addressed by a single CP, or to optimize per-
formance or minimize cost.
‚ñ†
‚ñ† Service arbitrage: This is similar to service aggregation except that the services 
being aggregated are not fixed. Service arbitrage means a broker has the flexibil-
ity to choose services from multiple agencies. The cloud broker, for example, can 
use a credit-scoring service to measure and select an agency with the best score.
A cloud auditor can evaluate the services provided by a CP in terms of secu-
rity controls, privacy impact, performance, and so on. The auditor is an independent 
entity that can assure that the CP conforms to a set of standards.
 5.5 cloud security risks ANd couNtermeAsures
In general terms, security controls in cloud computing are similar to the security 
controls in any IT environment. However, because of the operational models and 
technologies used to enable cloud service, cloud computing may present risks that 
are specific to the cloud environment. The essential concept in this regard is that 
the enterprise loses a substantial amount of control over resources, services, and 
 applications but must maintain accountability for security and privacy policies.

The Cloud Security Alliance [CSA10] lists the following as the top cloud- 
specific security threats, together with suggested countermeasures:
‚ñ†
‚ñ† Abuse and nefarious use of cloud computing: For many CPs, it is relatively 
easy to register and begin using cloud services, some even offering free limited 
trial periods. This enables attackers to get inside the cloud to conduct various 
attacks, such as spamming, malicious code attacks, and denial of service. PaaS 
providers have traditionally suffered most from this kind of attacks; however, 
recent evidence shows that hackers have begun to target IaaS vendors as well. 
The burden is on the CP to protect against such attacks, but cloud service cli-
ents must monitor activity with respect to their data and resources to detect 
any malicious behavior.
Countermeasures include (1) stricter initial registration and valida-
tion  processes; (2) enhanced credit card fraud monitoring and coordination; 
(3)¬†comprehensive introspection of customer network traffic; and (4) monitor-
ing public blacklists for one‚Äôs own network blocks.
‚ñ†
‚ñ† Insecure interfaces and APIs: CPs expose a set of software interfaces or APIs 
that customers use to manage and interact with cloud services. The security 
and availability of general cloud services are dependent upon the security of 
these basic APIs. From authentication and access control to encryption and 
activity monitoring, these interfaces must be designed to protect against both 
accidental and malicious attempts to circumvent policy.
Countermeasures include (1) analyzing the security model of CP inter-
faces; (2) ensuring that strong authentication and access controls are imple-
mented in concert with encrypted transmission; and (3) understanding the 
 dependency chain associated with the API.
‚ñ†
‚ñ† Malicious insiders: Under the cloud computing paradigm, an organization 
 relinquishes direct control over many aspects of security and, in doing so, con-
fers an unprecedented level of trust onto the CP. One grave concern is the 
risk of malicious insider activity. Cloud architectures necessitate certain roles 
that are extremely high risk. Examples include CP system administrators and 
 managed security service providers.
Countermeasures include the following: (1) enforce strict supply chain 
management and conduct a comprehensive supplier assessment; (2) specify 
human resource requirements as part of legal contract; (3) require transpar-
ency into overall information security and management practices, as well as 
compliance reporting; and (4) determine security breach notification processes.
‚ñ†
‚ñ† Shared technology issues: IaaS vendors deliver their services in a scalable way 
by sharing infrastructure. Often, the underlying components that make up this 
infrastructure (CPU caches, GPUs, etc.) were not designed to offer strong iso-
lation properties for a multi-tenant architecture. CPs typically approach this 
risk by the use of isolated virtual machines for individual clients. This approach 
is still vulnerable to attack, by both insiders and outsiders, and so can only be a 
part of an overall security strategy.
Countermeasures include the following: (1) implement security best 
practices for installation/configuration; (2) monitor environment for unauthor-
ized changes/activity; (3) promote strong authentication and access control

for  administrative access and operations; (4) enforce SLAs for  patching and 
 vulnerability remediation; and (5) conduct vulnerability scanning and 
 configuration audits.
‚ñ†
‚ñ† Data loss or leakage: For many clients, the most devastating impact from a 
security breach is the loss or leakage of data. We address this issue in the next 
subsection.
Countermeasures include the following: (1) implement strong API 
 access  control; (2) encrypt and protect integrity of data in transit; (3) analyze 
data protection at both design and run time; and (4) implement strong key 
 generation, storage and management, and destruction practices.
‚ñ†
‚ñ† Account or service hijacking: Account or service hijacking, usually with stolen 
credentials, remains a top threat. With stolen credentials, attackers can often 
access critical areas of deployed cloud computing services, allowing them to 
compromise the confidentiality, integrity, and availability of those services.
Countermeasures include the following: (1) prohibit the sharing of 
 account credentials between users and services; (2) leverage strong two- factor 
authentication techniques where possible; (3) employ proactive monitor-
ing to detect unauthorized activity; and (4) understand CP security policies 
and¬†SLAs.
‚ñ†
‚ñ† Unknown risk profile: In using cloud infrastructures, the client necessarily 
cedes control to the CP on a number of issues that may affect security. Thus 
the client must pay attention to and clearly define the roles and responsibili-
ties involved for managing risks. For example, employees may deploy applica-
tions and data resources at the CP without observing the normal policies and 
procedures for privacy, security, and oversight.
Countermeasures include (1) disclosure of applicable logs and data; 
(2)¬† partial/full disclosure of infrastructure details (e.g., patch levels and 
 firewalls); and (3) monitoring and alerting on necessary information.
Similar lists have been developed by the European Network and Information 
Security Agency [ENIS09] and NIST [JANS11].
 5.6 dAtA ProtectioN iN the cloud
As can be seen from the previous section, there are numerous aspects to cloud 
 security and numerous approaches to providing cloud security measures. 
A¬† further example is seen in the NIST guidelines for cloud security, specified in 
SP-800-14 and listed in Table 5.3. Thus, the topic of cloud security is well  beyond 
the scope of this chapter. In this section, we focus on one specific element of 
cloud security.
There are many ways to compromise data. Deletion or alteration of records 
without a backup of the original content is an obvious example. Unlinking a record 
from a larger context may render it unrecoverable, as can storage on unreliable 
media. Loss of an encoding key may result in effective destruction. Finally, unau-
thorized parties must be prevented from gaining access to sensitive data.

Governance
Extend organizational practices pertaining to the policies, procedures, and standards used for application 
development and service provisioning in the cloud, as well as the design, implementation, testing, use, and 
monitoring of deployed or engaged services.
Put in place audit mechanisms and tools to ensure organizational practices are followed throughout the 
system life cycle.
Compliance
Understand the various types of laws and regulations that impose security and privacy obligations on the 
 organization and potentially impact cloud computing initiatives, particularly those involving data location, 
 privacy and security controls, records management, and electronic discovery requirements.
Review and assess the cloud provider‚Äôs offerings with respect to the organizational requirements to be met 
and ensure that the contract terms adequately meet the requirements.
Ensure that the cloud provider‚Äôs electronic discovery capabilities and processes do not compromise the 
 privacy or security of data and applications.
Trust
Ensure that service arrangements have sufficient means to allow visibility into the security and privacy 
 controls and processes employed by the cloud provider, and their performance over time.
Establish clear, exclusive ownership rights over data.
Institute a risk management program that is flexible enough to adapt to the constantly evolving and 
 shifting risk landscape for the life cycle of the system.
Continuously monitor the security state of the information system to support ongoing risk management 
decisions.
Architecture
Understand the underlying technologies that the cloud provider uses to provision services, including the 
 implications that the technical controls involved have on the security and privacy of the system, over the full 
system life cycle and across all system components.
Identity and access management
Ensure that adequate safeguards are in place to secure authentication, authorization, and other identity and 
access management functions, and are suitable for the organization.
Software isolation
Understand virtualization and other logical isolation techniques that the cloud provider employs in its 
 multi-tenant software architecture, and assess the risks involved for the organization.
Data protection
Evaluate the suitability of the cloud provider‚Äôs data management solutions for the organizational data 
 concerned and the ability to control access to data, to secure data while at rest, in transit, and in use, and to 
sanitize data.
Take into consideration the risk of collating organizational data with those of other organizations whose 
threat profiles are high or whose data collectively represent significant concentrated value.
Fully understand and weigh the risks involved in cryptographic key management with the facilities 
 available in the cloud environment and the processes established by the cloud provider.
Availability
Understand the contract provisions and procedures for availability, data backup and recovery, and disaster 
recovery, and ensure that they meet the organization‚Äôs continuity and contingency planning requirements.
Ensure that during an intermediate or prolonged disruption or a serious disaster, critical operations 
can¬†be¬†immediately resumed, and that all operations can be eventually reinstituted in a timely and organized 
manner.
Incident response
Understand the contract provisions and procedures for incident response and ensure that they meet the 
requirements of the organization.
Table 5.3 NIST Guidelines on Security and Privacy Issues and Recommendations

Ensure that the cloud provider has a transparent response process in place and sufficient mechanisms to 
share information during and after an incident.
Ensure that the organization can respond to incidents in a coordinated fashion with the cloud provider in 
accordance with their respective roles and responsibilities for the computing environment.
Table 5.3 Continued
The threat of data compromise increases in the cloud, due to the number of 
and interactions between risks and challenges that are either unique to the cloud or 
more dangerous because of the architectural or operational characteristics of the 
cloud environment.
Database environments used in cloud computing can vary significantly. Some 
providers support a multi-instance model, which provides a unique DBMS running 
on a virtual machine instance for each cloud subscriber. This gives the subscriber 
complete control over role definition, user authorization, and other administrative 
tasks related to security. Other providers support a multi-tenant model, which pro-
vides a predefined environment for the cloud subscriber that is shared with other 
tenants, typically through tagging data with a subscriber identifier. Tagging gives 
the appearance of exclusive use of the instance, but relies on the CP to establish and 
maintain a sound secure database environment.
Data must be secured while at rest, in transit, and in use, and access to the 
data must be controlled. The client can employ encryption to protect data in  transit, 
though this involves key management responsibilities for the CP. The client can 
enforce access control techniques but, again, the CP is involved to some extent 
 depending on the service model used.
For data at rest, the ideal security measure is for the client to encrypt the data-
base and only store encrypted data in the cloud, with the CP having no access to the 
encryption key. So long as the key remains secure, the CP has no ability to read the 
data, although corruption and other denial-of-service attacks remain a risk.
A straightforward solution to the security problem in this context is to  encrypt 
the entire database and not provide the encryption/decryption keys to the service 
provider. This solution by itself is inflexible. The user has little ability to access 
 individual data items based on searches or indexing on key parameters, but rather 
would have to download entire tables from the database, decrypt the tables, and 
work with the results. To provide more flexibility, it must be possible to work with 
the database in its encrypted form.
An example of such an approach, depicted in Figure 5.10, is reported in 
[DAMI05] and [DAMI03]. A similar approach is described in [HACI02]. Four enti-
ties are involved:
‚ñ†
‚ñ† Data owner: An organization that produces data to be made available for 
 controlled release, either within the organization or to external users.
‚ñ†
‚ñ† User: Human entity that presents requests (queries) to the system. The user 
could be an employee of the organization who is granted access to the data-
base via the server, or a user external to the organization who, after authenti-
cation, is granted access.
‚ñ†
‚ñ† Client: Frontend that transforms user queries into queries on the encrypted 
data stored on the server.

‚ñ†
‚ñ† Server: An organization that receives the encrypted data from a data owner 
and makes them available for distribution to clients. The server could in fact 
be owned by the data owner but, more typically, is a facility owned and main-
tained by an external provider. For our discussion, the server is a cloud server.
Before continuing this discussion, we need to define some database terms. 
In¬†relational database parlance, the basic building block is a relation, which is a flat 
table. Rows are referred to as tuples, and columns are referred to as attributes. 
A¬†primary key is defined to be a portion of a row used to uniquely identify a row in 
a table; the primary key consists of one or more column names.2 For example, in 
an¬†employee table, the employee ID is sufficient to uniquely identify a row in a 
 particular table.
Let us first examine the simplest possible arrangement based on this scenario. 
Suppose that each individual item in the database is encrypted separately, all using 
the same encryption key. The encrypted database is stored at the server, but the 
server does not have the encryption key. Thus, the data are secure at the server. 
Even if someone were able to hack into the server‚Äôs system, all he or she would have 
access to is encrypted data. The client system does have a copy of the encryption 
key. A user at the client can retrieve a record from the database with the following 
sequence:
1. The user issues a query for fields from one or more records with a specific 
value of the primary key.
2Note that a primary key has nothing to do with cryptographic keys. A primary key in a database is a 
means of indexing into the database.
Figure 5.10 An Encryption Scheme for a Cloud-Based Database
Query
processor
1. Original query
Metadata
4. Plaintext
result
2. Transformed
query
3. Encrypted
result
Client
User
Data owner
Cloud
server
Encrypt/
Decrypt
Query
executor
Metadata
Metadata
Encrypted
database
Database

2. The query processor at the client encrypts the primary key, modifies the query 
accordingly, and transmits the query to the server.
3. The server processes the query using the encrypted value of the primary key 
and returns the appropriate record or records.
4. The query processor decrypts the data and returns the results.
This method is certainly straightforward but is quite limited. For example, sup-
pose the Employee table contains a salary attribute and the user wishes to  retrieve 
all records for salaries less than $70K. There is no obvious way to do this, because 
the attribute value for salary in each record is encrypted. The set of encrypted  values 
does not preserve the ordering of values in the original attribute.
There are a number of ways to extend the functionality of this approach. For 
example, an unencrypted index value can be associated with a given attribute and 
the table can be partitioned based on these index values, enabling a user to retrieve 
a certain portion of the table. The details of such schemes are beyond our scope. 
See¬†[STAL15] for more detail.
 5.7 cloud security As A service
The term Security as a Service (SecaaS) has generally meant a package of security 
services offered by a service provider that offloads much of the security respon-
sibility from an enterprise to the security service provider. Among the services 
typically provided are authentication, antivirus, antimalware/-spyware, intrusion 
detection, and security event management. In the context of cloud computing, 
cloud security as a service, designated SecaaS, is a segment of the SaaS offering 
of a CP.
The Cloud Security Alliance defines SecaaS as the provision of security 
 applications and services via the cloud either to cloud-based infrastructure and soft-
ware or from the cloud to the customers‚Äô on-premise systems [CSA11b]. The Cloud 
Security Alliance has identified the following SecaaS categories of service:
‚ñ†
‚ñ† Identity and access management
‚ñ†
‚ñ† Data loss prevention
‚ñ†
‚ñ† Web security
‚ñ†
‚ñ† E-mail security
‚ñ†
‚ñ† Security assessments
‚ñ†
‚ñ† Intrusion management
‚ñ†
‚ñ† Security information and event management
‚ñ†
‚ñ† Encryption
‚ñ†
‚ñ† Business continuity and disaster recovery
‚ñ†
‚ñ† Network security
In this section, we examine these categories with a focus on security of the 
cloud-based infrastructure and services (Figure 5.11).

Identity and access management (IAM) includes people, processes, and 
 systems that are used to manage access to enterprise resources by assuring that the 
identity of an entity is verified, and then granting the correct level of access based 
on this assured identity. One aspect of identity management is identity provision-
ing, which has to do with providing access to identified users and subsequently 
 deprovisioning, or deny access, to users when the client enterprise designates such 
users as no  longer having access to enterprise resources in the cloud. Another  aspect 
of identity management is for the cloud to participate in the federated identity 
management scheme (see Chapter 4) used by the client enterprise. Among other 
 requirements, the cloud service provider (CSP) must be able to exchange identity 
attributes with the enterprise‚Äôs chosen identity provider.
The access management portion of IAM involves authentication and access 
control services. For example, the CSP must be able to authenticate users in a 
trustworthy manner. The access control requirements in SPI environments include 
establishing trusted user profile and policy information, using it to control access 
within the cloud service, and doing this in an auditable way.
Data loss prevention (DLP) is the monitoring, protecting, and verifying the 
security of data at rest, in motion, and in use. Much of DLP can be implemented 
Figure 5.11 Elements of Cloud Security as a Service
Cloud service clients and adversaries
Identity and access management
Network security
Data loss
prevention
Web security
Intrusion
management
Encryption
Email security
Security assessments
Security information and
 
event management
Business continuity and
 
disaster recovery

by the cloud client, such as discussed in Section 5.6. The CSP can also provide DLP 
services, such as implementing rules about what functions can be performed on data 
in various contexts.
Web security is real-time protection offered either on premise through soft-
ware/appliance installation or via the cloud by proxying or redirecting Web traffic 
to the CP. This provides an added layer of protection on top of things like antivi-
ruses to prevent malware from entering the enterprise via activities such as Web 
browsing. In addition to protecting against malware, a cloud-based Web security 
service might include usage policy enforcement, data backup, traffic control, and 
Web access control.
A CSP may provide a Web-based e-mail service, for which security measures 
are needed. E-mail security provides control over inbound and outbound e-mail, 
 protecting the organization from phishing, malicious attachments, enforcing corporate 
polices such as acceptable use and spam prevention. The CSP may also incorporate 
digital signatures on all e-mail clients and provide optional e-mail encryption.
Security assessments are third-part audits of cloud services. While this service 
is outside the province of the CSP, the CSP can provide tools and access points to 
facilitate various assessment activities.
Intrusion management encompasses intrusion detection, prevention, and 
 response. The core of this service is the implementation of intrusion detection sys-
tems (IDSs) and intrusion prevention systems (IPSs) at entry points to the cloud 
and on servers in the cloud. An IDS is a set of automated tools designed to detect 
unauthorized access to a host system. We discuss this in Chapter 11. An IPS incor-
porates IDS functionality but also includes mechanisms designed to block traffic 
from intruders.
Security information and event management (SIEM) aggregates (via push or 
pull mechanisms) log and event data from virtual and real networks, applications, 
and systems. This information is then correlated and analyzed to provide real-time 
reporting and alerting on information/events that may require intervention or other 
type of response. The CSP typically provides an integrated service that can put 
 together information from a variety of sources both within the cloud and within the 
client enterprise network.
Encryption is a pervasive service that can be provided for data at rest in the 
cloud, e-mail traffic, client-specific network management information, and iden-
tity information. Encryption services provided by the CSP involve a range of com-
plex issues, including key management, how to implement virtual private network 
(VPN) services in the cloud, application encryption, and data content access.
Business continuity and disaster recovery comprise measures and mechanisms 
to ensure operational resiliency in the event of any service interruptions. This is 
an area where the CSP, because of economies of scale, can offer obvious benefits 
to a cloud service client [WOOD10]. The CSP can provide backup at multiple 
 locations, with reliable failover and disaster recovery facilities. This service must 
include a¬†flexible infrastructure, redundancy of functions and hardware, monitored 
 operations, geographically distributed data centers, and network survivability.
Network security consists of security services that allocate access, distribute, 
monitor, and protect the underlying resource services. Services include perimeter 
and server firewalls and denial-of-service protection. Many of the other services

listed in this section, including intrusion management, identity and access man-
agement, data loss protection, and Web security, also contribute to the network 
 security service.
 5.8 AddressiNg cloud comPutiNg security coNcerNs
Numerous documents have been developed to guide businesses thinking about the 
security issues associated with cloud computing. In addition to SP 800-144, which 
provides overall guidance, NIST has issued SP 800-146 (Cloud Computing Synopsis 
and Recommendations, May 2012). NIST‚Äôs recommendations systematically con-
sider each of the major types of cloud services consumed by businesses including 
Software as a Service (SaaS), Infrastructure as a Service (IaaS), and Platform as 
a Service (PaaS). While security issues vary somewhat depending on the type of 
cloud service, there are multiple NIST recommendations that are independent of 
service type. Not surprisingly, NIST recommends selecting cloud providers that 
support strong encryption, have appropriate redundancy mechanisms in place, 
 employ authentication mechanisms, and offer subscribers sufficient visibility about 
mechanisms used to protect subscribers from other subscribers and the provider. 
SP¬†800-146 also lists the overall security controls that are relevant in a cloud com-
puting environment and that must be assigned to the different cloud actors. These 
are shown in Table 5.4.
As more businesses incorporate cloud services into their enterprise net-
work infrastructures, cloud computing security will persist as an important issue. 
Examples of cloud computing security failures have the potential to have a chilling 
effect on business interest in cloud services and this is inspiring service providers 
to be serious about incorporating security mechanisms that will allay concerns of 
potential subscribers. Some service providers have moved their operations to Tier 4 
data centers to address user concerns about availability and redundancy. Because so 
many businesses remain reluctant to embrace cloud computing in a big way, cloud 
service providers will have to continue to work hard to convince potential  customers 
that computing support for core business processes and mission critical applications 
can be moved safely and securely to the cloud.
Technical
Operational
Management
Access Control
Audit and Accountability
Identification and Authentication
System and Communication 
Protection
Awareness and Training
Configuration and Management
Contingency Planning
Incident Response
Maintenance
Media Protection
Physical and Environmental 
Protection
Personnel Security System and 
Information Integrity
Certification, Accreditation, and 
Security Assessment
Planning Risk Assessment
System and Services Acquisition
Table 5.4 Control Functions and Classes

Key Terms 
access requestor (AR)
authentication server
cloud
cloud auditor
cloud broker
cloud carrier
cloud computing
cloud consumer
cloud provider
community cloud
Dynamic Host Configuration 
Protocol (DHCP)
EAP authenticator
EAP-GPSK
EAP-IKEv2
EAP over LAN (EAPOL)
EAP method
EAP pass-through mode
EAP peer
EAP-TLS
EAP-TTLS
Extensible Authentication 
Protocol (EAP)
firewall
IEEE 802.1X
media gateway
Network Access Control 
(NAC)
Network Access Server 
(NAS)
Platform as a Service (PaaS)
policy server
private cloud
public cloud
Remote Access Server (RAS)
Security as a Service (SecaaS)
Software as a Service (SaaS)
supplicant
Virtual Local Area Network 
(VLAN)
 5.9 key terms, review QuestioNs, ANd Problems
Review Questions 
 
5.1 
Provide a brief definition of network access control.
 
5.2 
What is an EAP?
 
5.3 
List and briefly define four EAP authentication methods.
 
5.4 
What is DHCP? How useful is it to help achieve security of IP addresses?
 
5.5 
Why is EAPOL an essential element of IEEE 802.1X?
 
5.6 
What are the essential characteristics of cloud computing?
 
5.7 
List and briefly define the deployment models of cloud computing.
 
5.8 
What is the cloud computing reference architecture?
 
5.9 
Describe some of the main cloud-specific security threats.
Problems 
 
5.1 
Investigate the network access control scheme used at your school or place of 
 employment. Draw a diagram and describe the principal components.
 
5.2 
Figure 5.3 suggests that EAP can be described in the context of a four-layer model. 
Indicate the functions and formats of each of the four layers. You may need to refer 
to RFC 3748.
 
5.3 
List some commonly used cloud-based data services. Explore and compare these 
 services based on their use of encryption, flexibility, efficiency, speed, and ease of 
use. Study security breaches on these services in recent past. What changes were 
made by the services after these attacks?

6.1 
Web Security Considerations
Web Security Threats
Web Traffic Security Approaches
6.2 
Transport Layer Security
TLS Architecture
TLS Record Protocol
Change Cipher Spec Protocol
Alert Protocol
Handshake Protocol
Cryptographic Computations
Heartbeat Protocol
SSL/TLS Attacks
TLSv1.3
6.3 
HTTPS
Connection Initiation
Connection Closure
6.4 
Secure Shell (SSH)
Transport Layer Protocol
User Authentication Protocol
Connection Protocol
6.5 
Key Terms, Review Questions, and Problems
Chapter
Transport-Level Security

Virtually all businesses, most government agencies, and many individuals now have 
Web sites. The number of individuals and companies with Internet access is expanding 
rapidly and all of these have graphical Web browsers. As a result, businesses are enthu-
siastic about setting up facilities on the Web for electronic commerce. But the reality 
is that the Internet and the Web are extremely vulnerable to compromises of various 
sorts. As businesses wake up to this reality, the demand for secure Web services grows.
The topic of Web security is a broad one and can easily fill a book. In this chap-
ter, we begin with a discussion of the general requirements for Web security and then 
focus on three standardized schemes that are becoming increasingly important as part 
of Web commerce and that focus on security at the transport layer: SSL/TLS, HTTPS, 
and SSH.
 6.1 Web Security conSiderationS
The World Wide Web is fundamentally a client/server application running over the 
Internet and TCP/IP intranets. As such, the security tools and approaches discussed 
so far in this book are relevant to the issue of Web security. However, the following 
characteristics of Web usage suggest the need for tailored security tools:
‚ñ†
‚ñ† Although Web browsers are very easy to use, Web servers are relatively easy 
to configure and manage, and Web content is increasingly easy to develop, the 
underlying software is extraordinarily complex. This complex software may 
hide many potential security flaws. The short history of the Web is filled with 
examples of new and upgraded systems, properly installed, that are vulnerable 
to a variety of security attacks.
‚ñ†
‚ñ† A Web server can be exploited as a launching pad into the corporation‚Äôs or 
agency‚Äôs entire computer complex. Once the Web server is subverted, an 
attacker may be able to gain access to data and systems not part of the Web 
itself but connected to the server at the local site.
Learning objectiveS
After studying this chapter, you should be able to:
‚óÜ‚ñ†
Summarize Web security threats and Web traffic security approaches.
‚óÜ‚ñ†
Present an overview of Transport Layer Security (TLS).
‚óÜ‚ñ†
Understand the differences between Secure Sockets Layer and Transport 
Layer Security.
‚óÜ‚ñ†
Compare the pseudorandom function used in Transport Layer Security 
with those discussed earlier in the book.
‚óÜ‚ñ†
Present an overview of HTTPS (HTTP over SSL).
‚óÜ‚ñ†
Present an overview of Secure Shell (SSH).

‚ñ†
‚ñ† Casual and untrained (in security matters) users are common clients for Web-
based services. Such users are not necessarily aware of the security risks that 
exist and do not have the tools or knowledge to take effective countermeasures.
Web Security Threats
Table 6.1 provides a summary of the types of security threats faced when using the 
Web. One way to group these threats is in terms of passive and active attacks. Passive 
attacks include eavesdropping on network traffic between browser and server and 
gaining access to information on a Web site that is supposed to be restricted. Active 
attacks include impersonating another user, altering messages in transit between 
client and server, and altering information on a Web site.
Another way to classify Web security threats is in terms of the location of the 
threat: Web server, Web browser, and network traffic between browser and server. 
Issues of server and browser security fall into the category of computer system secu-
rity; Part Six of this book addresses the issue of system security in general but is also 
applicable to Web system security. Issues of traffic security fall into the category of 
network security and are addressed in this chapter.
Web Traffic Security Approaches
A number of approaches to providing Web security are possible. The various 
approaches that have been considered are similar in the services they provide and, 
to some extent, in the mechanisms that they use, but they differ with respect to their 
scope of applicability and their relative location within the TCP/IP protocol stack.
Threats
Consequences
Countermeasures
Integrity
‚Ä¢ÔøΩ Modification of user data
‚Ä¢ÔøΩ Trojan horse browser
‚Ä¢ÔøΩ Modification of memory
‚Ä¢ÔøΩ Modification of message 
 traffic in transit
‚Ä¢ÔøΩ Loss of information
‚Ä¢ÔøΩ Compromise of machine
‚Ä¢ÔøΩ Vulnerability to all other 
threats
Cryptographic 
checksums
Confidentiality
‚Ä¢ÔøΩ Eavesdropping on the net
‚Ä¢ÔøΩ Theft of info from server
‚Ä¢ÔøΩ Theft of data from client
‚Ä¢ÔøΩ Info about network 
configuration
‚Ä¢ÔøΩ Info about which client talks 
to server
‚Ä¢ÔøΩ Loss of information
‚Ä¢ÔøΩ Loss of privacy
Encryption, Web 
proxies
Denial of 
Service
‚Ä¢ÔøΩ Killing of user threads
‚Ä¢ÔøΩ Flooding machine with bogus 
requests
‚Ä¢ÔøΩ Filling up disk or memory
‚Ä¢ÔøΩ Isolating machine by DNS 
attacks
‚Ä¢ÔøΩ Disruptive
‚Ä¢ÔøΩ Annoying
‚Ä¢ÔøΩ Prevent user from getting work 
done
Difficult to prevent
Authentication
‚Ä¢ÔøΩ Impersonation of legitimate 
users
‚Ä¢ÔøΩ Data forgery
‚Ä¢ÔøΩ Misrepresentation of user
‚Ä¢ÔøΩ Belief that false information 
is valid
Cryptographic 
techniques
Table 6.1 A Comparison of Threats on the Web

Figure 6.1 illustrates this difference. One way to provide Web security is 
to use IP security (IPsec) (Figure 6.1a). The advantage of using IPsec is that it is 
transparent to end users and applications and provides a general-purpose solution. 
Furthermore, IPsec includes a filtering capability so that only selected traffic need 
incur the overhead of IPsec processing.
Another relatively general-purpose solution is to implement security just 
above TCP (Figure 6.1b). The foremost example of this approach is the Secure 
Sockets Layer (SSL) and the follow-on Internet standard known as Transport 
Layer Security (TLS). At this level, there are two implementation choices. For full 
generality, SSL (or TLS) could be provided as part of the underlying protocol suite 
and therefore be transparent to applications. Alternatively, TLS can be embedded 
in specific packages. For example, virtually all browsers come equipped with TLS, 
and most Web servers have implemented the protocol.
Application-specific security services are embedded within the particular 
application. Figure 6.1c shows examples of this architecture. The advantage of this 
approach is that the service can be tailored to the specific needs of a given application.
 6.2 tranSport Layer Security
One of the most widely used security services is Transport Layer Security (TSL); 
the current version is Version 1.2, defined in RFC 5246. TLS is an Internet stan-
dard that evolved from a commercial protocol known as Secure Sockets Layer 
(SSL). Although SSL implementations are still around, it has been deprecated by 
IETF and is disabled by most corporations offering TLS software. TLS is a general-
purpose service implemented as a set of protocols that rely on TCP. At this level, 
there are two implementation choices. For full generality, TLS could be provided 
as part of the underlying protocol suite and therefore be transparent to applica-
tions. Alternatively, TLS can be embedded in specific packages. For example, most 
browsers come equipped with TLS, and most Web servers have implemented the 
protocol.
TLS Architecture
TLS is designed to make use of TCP to provide a reliable end-to-end secure ser-
vice. TLS is not a single protocol but rather two layers of protocols, as illustrated in 
Figure 6.2.
Figure 6.1 Relative Location of Security Facilities in the TCP/IP Protocol Stack
SMTP
HTTP
TCP
IP/IPSec
(a) Network level
FTP
SMTP
HTTP
TCP
SSL or TLS
IP
(b) Transport level
FTP
IP
S/MIME
HTTP
Kerberos
UDP
SMTP
(c) Application level
TCP

The TLS Record Protocol provides basic security services to various higher-
layer protocols. In particular, the Hypertext Transfer Protocol (HTTP), which 
provides the transfer service for Web client/server interaction, can operate on top 
of TLS. Three higher-layer protocols are defined as part of TLS: the Handshake 
Protocol; the Change Cipher Spec Protocol; and the Alert Protocol. These TLS-
specific protocols are used in the management of TLS exchanges and are examined 
later in this section. A fourth protocol, the Heartbeat Protocol, is defined in a sepa-
rate RFC and is also discussed subsequently in this section.
Two important TLS concepts are the TLS session and the TLS connection, 
which are defined in the specification as follows:
‚ñ†
‚ñ† Connection: A connection is a transport (in the OSI layering model definition) 
that provides a suitable type of service. For TLS, such connections are peer-to-
peer relationships. The connections are transient. Every connection is associ-
ated with one session.
‚ñ†
‚ñ† Session: A TLS session is an association between a client and a server. Sessions 
are created by the Handshake Protocol. Sessions define a set of cryptographic 
security parameters, which can be shared among multiple connections. Sessions 
are used to avoid the expensive negotiation of new security parameters for 
each connection.
Between any pair of parties (applications such as HTTP on client and server), 
there may be multiple secure connections. In theory, there may also be multiple 
simultaneous sessions between parties, but this feature is not used in practice.
There are a number of states associated with each session. Once a session is 
 established, there is a current operating state for both read and write (i.e., receive 
and send). In addition, during the Handshake Protocol, pending read and write 
states are created. Upon successful conclusion of the Handshake Protocol, the 
pending states become the current states.
A session state is defined by the following parameters:
‚ñ†
‚ñ† Session identifier: An arbitrary byte sequence chosen by the server to identify 
an active or resumable session state.
‚ñ†
‚ñ† Peer certificate: An X509.v3 certificate of the peer. This element of the state 
may be null.
Figure 6.2 TLS Protocol Stack
IP
TCP
Record protocol
Handshake
protocol
Change
cipher spec
protocol
Alert
protocol
HTTP
Heartbeat
protocol

‚ñ†
‚ñ† Compression method: The algorithm used to compress data prior to encryption.
‚ñ†
‚ñ† Cipher spec: Specifies the bulk data encryption algorithm (such as null, AES, 
etc.) and a hash algorithm (such as MD5 or SHA-1) used for MAC calculation. 
It also defines cryptographic attributes such as the hash_size.
‚ñ†
‚ñ† Master secret: 48-byte secret shared between the client and server.
‚ñ†
‚ñ† Is resumable: A flag indicating whether the session can be used to initiate new 
connections.
A connection state is defined by the following parameters:
‚ñ†
‚ñ† Server and client random: Byte sequences that are chosen by the server and 
client for each connection.
‚ñ†
‚ñ† Server write MAC secret: The secret key used in MAC operations on data sent 
by the server.
‚ñ†
‚ñ† Client write MAC secret: The symmetric key used in MAC operations on data 
sent by the client.
‚ñ†
‚ñ† Server write key: The symmetric encryption key for data encrypted by the 
server and decrypted by the client.
‚ñ†
‚ñ† Client write key: The symmetric encryption key for data encrypted by the 
 client and decrypted by the server.
‚ñ†
‚ñ† Initialization vectors: When a block cipher in CBC mode is used, an initial-
ization vector (IV) is maintained for each key. This field is first initialized by 
the TLS Handshake Protocol. Thereafter, the final ciphertext block from each 
 record is preserved for use as the IV with the following record.
‚ñ†
‚ñ† Sequence numbers: Each party maintains separate sequence numbers for 
transmitted and received messages for each connection. When a party sends or 
receives a ‚Äúchange cipher spec message,‚Äù the appropriate sequence number is 
set to zero. Sequence numbers may not exceed 264 - 1.
TLS Record Protocol
The TLS Record Protocol provides two services for TLS connections:
‚ñ†
‚ñ† Confidentiality: The Handshake Protocol defines a shared secret key that is 
used for conventional encryption of TLS payloads.
‚ñ†
‚ñ† Message Integrity: The Handshake Protocol also defines a shared secret key 
that is used to form a message authentication code (MAC).
Figure 6.3 indicates the overall operation of the TLS Record Protocol. The 
Record Protocol takes an application message to be transmitted, fragments the data 
into manageable blocks, optionally compresses the data, applies a MAC, encrypts, 
adds a header, and transmits the resulting unit in a TCP segment. Received data 
are decrypted, verified, decompressed, and reassembled before being delivered to 
higher-level users.
The first step is fragmentation. Each upper-layer message is fragmented into 
blocks of 214 bytes (16,384 bytes) or less. Next, compression is optionally applied. 
Compression must be lossless and may not increase the content length by more than

1024 bytes.1 In TLSv2, no compression algorithm is specified, so the default com-
pression algorithm is null.
The next step in processing is to compute a message authentication code over 
the compressed data. TLS makes use of the HMAC algorithm defined in RFC 2104. 
Recall from Chapter 3 that HMAC is defined as
 
HMACK(M) = H[(K+ ‚äï opad) ‚Äò H[(K+ ‚äï ipad) ‚Äò M]] 
where
H     = embedded hash function (for TLS, either MD5 or SHA-1)
M     = message input to HMAC
K+    = secret key padded with zeros on the left so that the result is equal to 
the block length of the hash code (for MD5 and SHA-1, block 
length = 512 bits)
ipad = 00110110 (36 in hexadecimal) repeated 64 times (512 bits)
opad = 01011100 (5C in hexadecimal) repeated 64 times (512 bits)
For TLS, the MAC calculation encompasses the fields indicated in the 
 following expression:
HMAC_hash(MAC_write_secret, seq_num ‚Äò TLSCompressed.type ‚Äò 
TLSCompressed.version ‚Äò TLSCompressed.length ‚Äò TLSCompressed.fragment)
The MAC calculation covers all of the fields XXX, plus the field 
TLSCompressed.version, which is the version of the protocol being employed.
Next, the compressed message plus the MAC are encrypted using symmetric 
encryption. Encryption may not increase the content length by more than 1024¬†bytes, 
1Of course, one hopes that compression shrinks rather than expands the data. However, for very short 
blocks, it is possible, because of formatting conventions, that the compression algorithm will actually pro-
vide output that is longer than the input.
Figure 6.3 TLS Record Protocol Operation
Application data
Fragment
Compress
Add MAC
Encrypt
Append TLS
record header

so that the total length may not exceed 214 + 2048. The following encryption algo-
rithms are permitted:
Block Cipher
Stream Cipher
Algorithm
Key Size
Algorithm
Key Size
AES
3DES
128, 256
168
RC4-128
128
For stream encryption, the compressed message plus the MAC are encrypted. 
Note that the MAC is computed before encryption takes place and that the MAC is 
then encrypted along with the plaintext or compressed plaintext.
For block encryption, padding may be added after the MAC prior to encryp-
tion. The padding is in the form of a number of padding bytes followed by a one-
byte indication of the length of the padding. The padding can be any amount that 
results in a total that is a multiple of the cipher‚Äôs block length, up to a maximum 
of 255 bytes. For example, if the cipher block length is 16 bytes (e.g., AES) and if 
the plaintext (or compressed text if compression is used) plus MAC plus padding 
length byte is 79 bytes long, then the padding length (in bytes) can be 1, 17, 33, and 
so on, up to 161. At a padding length of 161, the total length is 79 + 161 = 240. A 
variable padding length may be used to frustrate attacks based on an analysis of 
the lengths of exchanged messages.
The final step of TLS Record Protocol processing is to prepend a header con-
sisting of the following fields:
‚ñ†
‚ñ† Content Type (8 bits): The higher-layer protocol used to process the enclosed 
fragment.
‚ñ†
‚ñ† Major Version (8 bits): Indicates major version of TLS in use. For TLSv2, the 
value is 3.
‚ñ†
‚ñ† Minor Version (8 bits): Indicates minor version in use. For TLSv2, the value is 1.
‚ñ†
‚ñ† Compressed Length (16 bits): The length in bytes of the plaintext fragment 
(or compressed fragment if compression is used). The maximum value is 
214 + 2048.
The content types that have been defined are change_cipher_spec, 
alert, handshake, and application_data. The first three are the TLS-
specific protocols, discussed next. Note that no distinction is made among the vari-
ous applications (e.g., HTTP) that might use TLS; the content of the data created by 
such applications is opaque to TLS. 
Figure 6.4 illustrates the TLS record format.
Change Cipher Spec Protocol
The Change Cipher Spec Protocol is one of the four TLS-specific protocols that use 
the TLS Record Protocol, and it is the simplest. This protocol consists of a single 
message (Figure 6.5a), which consists of a single byte with the value 1. The sole pur-
pose of this message is to cause the pending state to be copied into the current state, 
which updates the cipher suite to be used on this connection.

Alert Protocol
The Alert Protocol is used to convey TLS-related alerts to the peer entity. As with 
other applications that use TLS, alert messages are compressed and encrypted, as 
specified by the current state.
Each message in this protocol consists of two bytes (Figure 6.5b). The first 
byte takes the value warning (1) or fatal (2) to convey the severity of the message. 
If the level is fatal, TLS immediately terminates the connection. Other connections 
on the same session may continue, but no new connections on this session may 
be established. The second byte contains a code that indicates the specific alert. 
The¬† following alerts are always fatal:
‚ñ†
‚ñ† unexpected_message: An inappropriate message was received.
‚ñ†
‚ñ† bad_record_mac: An incorrect MAC was received.
‚ñ†
‚ñ† decompression_failure: The decompression function received improper input 
(e.g., unable to decompress or decompress to greater than maximum allowable 
length).
‚ñ†
‚ñ† handshake_failure: Sender was unable to negotiate an acceptable set of secu-
rity parameters given the options available.
‚ñ†
‚ñ† illegal_parameter: A field in a handshake message was out of range or incon-
sistent with other fields.
Figure 6.5 TLS Record Protocol Payload
1
(a) Change cipher spec protocol
1 byte
Type
(c) Handshake protocol
1 byte
Length
3 bytes
Content
√ö 0 bytes
(d) Other upper-layer protocol (e.g., HTTP)
Opaque content
√ö 1 byte
Level
(b) Alert protocol
1 byte 1 byte
Alert
Figure 6.4 TLS Record Format
Content
type
Major
version
Minor
version
Compressed
length
Plaintext
(optionally
compressed)
MAC (0, 16, or 20 bytes)
Encrypted

‚ñ†
‚ñ† decryption_failed: A ciphertext decrypted in an invalid way; either it was not 
an even multiple of the block length or its padding values, when checked, were 
incorrect.
‚ñ†
‚ñ† record_overflow: A TLS record was received with a payload (ciphertext) 
whose length exceeds 214 + 2048 bytes, or the ciphertext decrypted to a length 
of greater than 214 + 1024 bytes.
‚ñ†
‚ñ† unknown_ca: A valid certificate chain or partial chain was received, but the 
certificate was not accepted because the CA certificate could not be located or 
could not be matched with a known, trusted CA.
‚ñ†
‚ñ† access_denied: A valid certificate was received, but when access control was 
applied, the sender decided not to proceed with the negotiation.
‚ñ†
‚ñ† decode_error: A message could not be decoded, because either a field was out 
of its specified range or the length of the message was incorrect.
‚ñ†
‚ñ† export_restriction: A negotiation not in compliance with export restrictions on 
key length was detected.
‚ñ†
‚ñ† protocol_version: The protocol version the client attempted to negotiate is 
recognized but not supported.
‚ñ†
‚ñ† insufficient_security: Returned instead of handshake_failure when a negotia-
tion has failed specifically because the server requires ciphers more secure 
than those supported by the client.
‚ñ†
‚ñ† internal_error: An internal error unrelated to the peer or the correctness of 
the protocol makes it impossible to continue.
The remaining alerts are the following.
‚ñ†
‚ñ† close_notify: Notifies the recipient that the sender will not send any more mes-
sages on this connection. Each party is required to send a close_notify alert 
before closing the write side of a connection.
‚ñ†
‚ñ† bad_certificate: A received certificate was corrupt (e.g., contained a signature 
that did not verify).
‚ñ†
‚ñ† unsupported_certificate: The type of the received certificate is not supported.
‚ñ†
‚ñ† certificate_revoked: A certificate has been revoked by its signer.
‚ñ†
‚ñ† certificate_expired: A certificate has expired.
‚ñ†
‚ñ† certificate_unknown: Some other unspecified issue arose in processing the 
certificate, rendering it unacceptable.
‚ñ†
‚ñ† decrypt_error: A handshake cryptographic operation failed, including being 
unable to verify a signature, decrypt a key exchange, or validate a finished 
message.
‚ñ†
‚ñ† user_canceled: This handshake is being canceled for some reason unrelated to 
a protocol failure.
‚ñ†
‚ñ† no_renegotiation: Sent by a client in response to a hello request or by the 
server in response to a client hello after initial handshaking. Either of these 
messages would normally result in renegotiation, but this alert indicates that 
the sender is not able to renegotiate. This message is always a warning.

Handshake Protocol
The most complex part of TLS is the Handshake Protocol. This protocol allows 
the server and client to authenticate each other and to negotiate an encryption and 
MAC algorithm and cryptographic keys to be used to protect data sent in a TLS 
record. The Handshake Protocol is used before any application data is transmitted.
The Handshake Protocol consists of a series of messages exchanged by client 
and server. All of these have the format shown in Figure 6.5c. Each message has 
three fields:
‚ñ†
‚ñ† Type (1 byte): Indicates one of 10 messages. Table 6.2 lists the defined message 
types.
‚ñ†
‚ñ† Length (3 bytes): The length of the message in bytes.
‚ñ†‚ñ†
Content (# 0 bytes): The parameters associated with this message; these are 
listed in Table 6.2.
Figure 6.6 shows the initial exchange needed to establish a logical connection 
between client and server. The exchange can be viewed as having four phases.
Phase 1. establish security caPabilities Phase 1 initiates a logical connection 
and establishes the security capabilities that will be associated with it. The exchange 
is initiated by the client, which sends a client_hello message with the following 
parameters:
‚ñ†
‚ñ† Version: The highest TLS version understood by the client.
‚ñ†
‚ñ† Random: A client-generated random structure consisting of a 32-bit timestamp 
and 28 bytes generated by a secure random number generator. These values 
serve as nonces and are used during key exchange to prevent replay attacks.
‚ñ†
‚ñ† Session ID: A variable-length session identifier. A nonzero value indicates that 
the client wishes to update the parameters of an existing connection or to cre-
ate a new connection on this session. A zero value indicates that the client 
wishes to establish a new connection on a new session.
Message Type
Parameters
hello_request
null
client_hello
version, random, session id, cipher suite, compression method
server_hello
version, random, session id, cipher suite, compression method
certificate
chain of X.509v3 certificates
server_key_exchange
parameters, signature
certificate_request
type, authorities
server_done
null
certificate_verify
signature
client_key_exchange
parameters, signature
finished
hash value
Table 6.2 TLS Handshake Protocol Message Types

‚ñ†
‚ñ† CipherSuite: This is a list that contains the combinations of cryptographic 
algorithms supported by the client, in decreasing order of preference. Each 
element of the list (each cipher suite) defines both a key exchange algorithm 
and a CipherSpec; these are discussed subsequently.
‚ñ†
‚ñ† Compression Method: This is a list of the compression methods the client 
supports.
After sending the client_hello message, the client waits for the server_
hello  message, which contains the same parameters as the client_hello 
Figure 6.6 Handshake Protocol Action
Client
Server
Phase 1
Establish security capabilities, including
protocol version, session ID, cipher suite,
compression method, and initial random
numbers.
Phase 2
Server may send certifcate, key exchange,
and request certifcate. Server signals end
of hello message phase.
Phase 3
Client sends certifcate if requested. Client
sends key exchange. Client may send
certifcate verifcation.
Phase 4
Change cipher suite and fnish
handshake protocol.
Note: Shaded transfers are
optional or situation-dependent
messages that are not always sent.
fnished
change_cipher_spec
fnished
change_cipher_spec
certifcate_verify
client_key_exchange
certifcate
server_hello_done
certifcate_request
server_key_exchange
certifcate
server_hello
client_hello
Time

message. For the server_hello message, the following conventions apply. The 
 Version field contains the lowest of the version suggested by the client and the highest 
supported by the server. The Random field is generated by the server and is indepen-
dent of the client‚Äôs Random field. If the SessionID field of the client was nonzero, the 
same value is used by the server; otherwise the server‚Äôs SessionID field contains the 
value for a new session. The CipherSuite field contains the single cipher suite selected 
by the server from those proposed by the client. The Compression field contains the 
compression method selected by the server from those proposed by the client.
The first element of the Ciphersuite parameter is the key exchange method 
(i.e., the means by which the cryptographic keys for conventional encryption and 
MAC are exchanged). The following key exchange methods are supported.
‚ñ†
‚ñ† RSA: The secret key is encrypted with the receiver‚Äôs RSA public key. A public-
key certificate for the receiver‚Äôs key must be made available.
‚ñ†
‚ñ† Fixed Diffie‚ÄìHellman: This is a Diffie‚ÄìHellman key exchange in which the 
server‚Äôs certificate contains the Diffie‚ÄìHellman public parameters signed 
by the certificate authority (CA). That is, the public-key certificate contains 
the Diffie‚ÄìHellman public-key parameters. The client provides its Diffie‚Äì
Hellman public-key parameters either in a certificate, if client authentication 
is  required, or in a key exchange message. This method results in a fixed  secret 
key between two peers based on the Diffie‚ÄìHellman calculation using the 
fixed public keys.
‚ñ†
‚ñ† Ephemeral Diffie‚ÄìHellman: This technique is used to create ephemeral (tem-
porary, one-time) secret keys. In this case, the Diffie‚ÄìHellman public keys are 
exchanged and signed using the sender‚Äôs private RSA or DSS key. The receiver 
can use the corresponding public key to verify the signature. Certificates are used 
to authenticate the public keys. This would appear to be the most secure of the 
three Diffie‚ÄìHellman options because it results in a temporary, authenticated key.
‚ñ†
‚ñ† Anonymous Diffie‚ÄìHellman: The base Diffie‚ÄìHellman algorithm is used 
with no authentication. That is, each side sends its public Diffie‚ÄìHellman 
 parameters to the other with no authentication. This approach is vulnerable to 
man-in-the-middle attacks, in which the attacker conducts anonymous Diffie‚Äì
Hellman with both parties.
Following the definition of a key exchange method is the CipherSpec, which 
includes the following fields:
‚ñ†
‚ñ† CipherAlgorithm: Any of the algorithms mentioned earlier: RC4, RC2, DES, 
3DES, DES40, or IDEA
‚ñ†
‚ñ† MACAlgorithm: MD5 or SHA-1
‚ñ†
‚ñ† CipherType: Stream or Block
‚ñ†
‚ñ† IsExportable: True or False
‚ñ†
‚ñ† HashSize: 0, 16 (for MD5), or 20 (for SHA-1) bytes
‚ñ†
‚ñ† Key Material: A sequence of bytes that contain data used in generating the 
write keys
‚ñ†
‚ñ† IV Size: The size of the Initialization Value for Cipher Block Chaining (CBC) 
encryption

Phase 2. server authentication and Key exchange The server begins this 
phase by sending its certificate if it needs to be authenticated; the message con-
tains one or a chain of X.509 certificates. The certificate message is required for 
any agreed-on key exchange method except anonymous Diffie‚ÄìHellman. Note 
that if fixed Diffie‚ÄìHellman is used, this certificate message functions as the serv-
er‚Äôs key exchange message because it contains the server‚Äôs public Diffie‚ÄìHellman 
parameters.
Next, a server_key_exchange message may be sent if it is required. It is not 
required in two instances: (1) The server has sent a certificate with fixed Diffie‚Äì
Hellman parameters; or (2) RSA key exchange is to be used. The server_key_ 
exchange message is needed for the following:
‚ñ†
‚ñ† Anonymous Diffie‚ÄìHellman: The message content consists of the two global 
Diffie‚ÄìHellman values (a prime number and a primitive root of that number) 
plus the server‚Äôs public Diffie‚ÄìHellman key (see Figure 10.1).
‚ñ†
‚ñ† Ephemeral Diffie‚ÄìHellman: The message content includes the three Diffie‚Äì
Hellman parameters provided for anonymous Diffie‚ÄìHellman plus a signature 
of those parameters.
‚ñ†
‚ñ† RSA key exchange (in which the server is using RSA but has a signature-only 
RSA key): Accordingly, the client cannot simply send a secret key encrypted 
with the server‚Äôs public key. Instead, the server must create a temporary RSA 
public/private key pair and use the server_key_exchange message to send the 
public key. The message content includes the two parameters of the temporary 
RSA public key (exponent and modulus; see Figure 9.5) plus a signature of 
those parameters.
Some further details about the signatures are warranted. As usual, a signature 
is created by taking the hash of a message and encrypting it with the sender‚Äôs private 
key. In this case, the hash is defined as
 
hash(ClientHello.random ‚Äò ServerHello.random ‚Äò ServerParams) 
So the hash covers not only the Diffie‚ÄìHellman or RSA parameters but also the 
two nonces from the initial hello messages. This ensures against replay attacks and 
misrepresentation. In the case of a DSS signature, the hash is performed using the 
SHA-1 algorithm. In the case of an RSA signature, both an MD5 and an SHA-1 
hash are calculated, and the concatenation of the two hashes (36 bytes) is encrypted 
with the server‚Äôs private key.
Next, a nonanonymous server (server not using anonymous Diffie‚ÄìHellman) 
can request a certificate from the client. The certificate_request message includes 
two parameters: certificate_type and certificate_authorities. The certificate type 
 indicates the public-key algorithm and its use:
‚ñ†
‚ñ† RSA, signature only
‚ñ†
‚ñ† DSS, signature only
‚ñ†
‚ñ† RSA for fixed Diffie‚ÄìHellman; in this case the signature is used only for 
authentication, by sending a certificate signed with RSA
‚ñ†
‚ñ† DSS for fixed Diffie‚ÄìHellman; again, used only for authentication

The second parameter in the certificate_request message is a list of the distin-
guished names of acceptable certificate authorities.
The final message in phase 2, and one that is always required, is the server_
done message, which is sent by the server to indicate the end of the server hello and 
associated messages. After sending this message, the server will wait for a client 
response. This message has no parameters.
Phase 3. client authentication and Key exchange Upon receipt of the 
server_done message, the client should verify that the server provided a valid 
certificate (if required) and check that the server_hello parameters are accept-
able. If all is satisfactory, the client sends one or more messages back to the server.
If the server has requested a certificate, the client begins this phase by send-
ing a certificate message. If no suitable certificate is available, the client sends a 
no_certificate alert instead.
Next is the client_key_exchange message, which must be sent in this phase. 
The content of the message depends on the type of key exchange, as follows:
‚ñ†
‚ñ† RSA: The client generates a 48-byte pre-master secret and encrypts with the 
public key from the server‚Äôs certificate or temporary RSA key from a server_
key_exchange message. Its use to compute a master secret is explained later.
‚ñ†
‚ñ† Ephemeral or Anonymous Diffie‚ÄìHellman: The client‚Äôs public Diffie‚ÄìHellman 
parameters are sent.
‚ñ†
‚ñ† Fixed Diffie‚ÄìHellman: The client‚Äôs public Diffie‚ÄìHellman parameters were 
sent in a certificate message, so the content of this message is null.
Finally, in this phase, the client may send a certificate_verify message to pro-
vide explicit verification of a client certificate. This message is only sent following 
any client certificate that has signing capability (i.e., all certificates except those 
containing fixed Diffie‚ÄìHellman parameters). This message signs a hash code based 
on the preceding messages, defined as
CertificateVerify.signature.md5_hash
 MD5(handshake_messages);
Certificate.signature.sha_hash
 SHA(handshake_messages);
where handshake_messages refers to all Handshake Protocol messages sent or 
received starting at client_hello but not including this message. If the user‚Äôs 
private key is DSS, then it is used to encrypt the SHA-1 hash. If the user‚Äôs private 
key is RSA, it is used to encrypt the concatenation of the MD5 and SHA-1 hashes. 
In either case, the purpose is to verify the client‚Äôs ownership of the private key for 
the client certificate. Even if someone is misusing the client‚Äôs certificate, he or she 
would be unable to send this message.
Phase 4. Finish Phase 4 completes the setting up of a secure connection. The  client 
sends a change_cipher_spec message and copies the pending CipherSpec into the 
current CipherSpec. Note that this message is not considered part of the Handshake 
Protocol but is sent using the Change Cipher Spec Protocol. The client then imme-
diately sends the finished message under the new algorithms, keys, and secrets.

The¬†finished message verifies that the key exchange and authentication processes 
were successful. The content of the finished message is:
PRF(master_secret, finished_label, MD5(handshake_messages) ‚Äò SHA@1
(handshake_messages))
where finished_label is the string ‚Äúclient finished‚Äù for the client and ‚Äúserver 
finished‚Äù for the server.
In response to these two messages, the server sends its own change_ cipher_
spec message, transfers the pending to the current CipherSpec, and sends its fin-
ished message. At this point, the handshake is complete and the client and server 
may begin to exchange application-layer data.
Cryptographic Computations
Two further items are of interest: (1) the creation of a shared master secret by 
means of the key exchange; and (2) the generation of cryptographic parameters 
from the master secret.
Master secret creation The shared master secret is a one-time 48-byte value 
(384 bits) generated for this session by means of secure key exchange. The creation 
is in two stages. First, a pre_master_secret is exchanged. Second, the  master_
secret is calculated by both parties. For pre_master_secret exchange, there 
are two possibilities.
‚ñ†
‚ñ† RSA: A 48-byte pre_master_secret is generated by the client, encrypted with 
the server‚Äôs public RSA key, and sent to the server. The server decrypts the 
ciphertext using its private key to recover the pre_master_secret.
‚ñ†
‚ñ† Diffie‚ÄìHellman: Both client and server generate a Diffie‚ÄìHellman public key. 
After these are exchanged, each side performs the Diffie‚ÄìHellman calculation 
to create the shared pre_master_secret.
Both sides now compute the master_secret as
master_secret =
 PRF(pre_master_secret, ‚Äúmaster secret‚Äù, ClientHello.random ‚Äò ServerHello 
.random)
where ClientHello.random and ServerHello.random are the two nonce 
values exchanged in the initial hello messages.
The algorithm is performed until 48 bytes of pseudorandom output are pro-
duced. The calculation of the key block material (MAC secret keys, session encryp-
tion keys, and IVs) is defined as
key_block =
 PRF(SecurityParameters.master_secret, ‚Äúkey expansion‚Äù,
SecurityParameters.server_random ‚Äò SecurityParameters.client_random)
until enough output has been generated.

generation oF cryPtograPhic ParaMeters CipherSpecs require a client write 
MAC secret, a server write MAC secret, a client write key, a server write key, a 
client write IV, and a server write IV, which are generated from the master secret 
in that order. These parameters are generated from the master secret by hashing 
the master secret into a sequence of secure bytes of sufficient length for all needed 
parameters.
The generation of the key material from the master secret uses the same for-
mat for generation of the master secret from the pre-master secret as
key_block = MD5(master_secret ‚Äò SHA(=A> ‚Äò master_secret ‚Äò
ServerHello.random ‚Äò ClientHello.random)) ‚Äò 
MD5(master_secret ‚Äò SHA(=BB> ‚Äò master_secret ‚Äò 
ServerHello.random ‚Äò ClientHello.random)) ‚Äò 
MD5(master_secret ‚Äò SHA(=CCC> ‚Äò master_secret ‚Äò 
ServerHello.random ‚Äò ClientHello.random)) ‚Äò c
until enough output has been generated. The result of this algorithmic structure is a 
pseudorandom function. We can view the master_secret as the pseudorandom 
seed value to the function. The client and server random numbers can be viewed as 
salt values to complicate cryptanalysis (see Chapter 11 for a discussion of the use of 
salt values).
PseudorandoM Function TLS makes use of a pseudorandom function referred 
to as PRF to expand secrets into blocks of data for purposes of key generation or 
validation. The objective is to make use of a relatively small, shared secret value but 
to generate longer blocks of data in a way that is secure from the kinds of attacks 
made on hash functions and MACs. The PRF is based on the data expansion func-
tion (Figure 6.7) given as
 
P_hash(secret, seed) = HMAC_hash(secret, A(1) ‚Äò seed) ‚Äò
                                          HMAC_hash(secret, A(2) ‚Äò seed) ‚Äò
                                         HMAC_hash(secret, A(3) ‚Äò seed) ‚Äò
 
where A() is defined as
A(0) = seed
A(i) = HMAC_hash(secret, A(i - 1))
The data expansion function makes use of the HMAC algorithm with either MD5 
or SHA-1 as the underlying hash function. As can be seen, P_hash can be iterated 
as many times as necessary to produce the required quantity of data. For example, if 
P_SHA256 was used to generate 80 bytes of data, it would have to be iterated three 
times (through A(3)), producing 96 bytes of data of which the last 16 would be dis-
carded. In this case, P_MD5 would have to be iterated four times, producing exactly 
64 bytes of data. Note that each iteration involves two executions of HMAC, each 
of which in turn involves two executions of the underlying hash algorithm.

To make PRF as secure as possible, it uses two hash algorithms in a way that 
should guarantee its security if either algorithm remains secure. PRF is defined as
 
PRF(secret, label, seed) = P_6hash7(secret, label ‚Äò seed) 
PRF takes as input a secret value, an identifying label, and a seed value and 
produces an output of arbitrary length.
Heartbeat Protocol
In the context of computer networks, a heartbeat is a periodic signal generated by 
hardware or software to indicate normal operation or to synchronize other parts of 
a system. A heartbeat protocol is typically used to monitor the availability of a pro-
tocol entity. In the specific case of TLS, a Heartbeat protocol was defined in 2012 in 
RFC 6250 (Transport Layer Security (TLS) and Datagram Transport Layer Security 
(DTLS) Heartbeat Extension).
Figure 6.7 TLS Function P_hash(secret, seed)
Secret
Seed
Seed
A(1)
HMAC
Secret
Secret
Length = hash size
Secret
Seed
A(2)
HMAC
HMAC
Secret
Seed
A(3)
HMAC
HMAC
Secret
HMAC

The Heartbeat protocol runs on top of the TLS Record Protocol and con-
sists of two message types: heartbeat_request and heartbeat_response. 
The use of the Heartbeat protocol is established during Phase 1 of the Handshake 
protocol (Figure 6.6). Each peer indicates whether it supports heartbeats. If heart-
beats are supported, the peer indicates whether it is willing to receive heartbeat_ 
request messages and respond with heartbeat_response messages or only 
willing to send heartbeat_request messages.
A heartbeat_request message can be sent at any time. Whenever a re-
quest message is received, it should be answered promptly with a corresponding 
heartbeat_response message. The heartbeat_request message includes 
payload length, payload, and padding fields. The payload is a random content 
between 16 bytes and 64 Kbytes in length. The corresponding heartbeat_ 
response message must include an exact copy of the received payload. The pad-
ding is also random content. The padding enables the sender to perform a path 
MTU (maximum transfer unit) discovery operation, by sending requests with in-
creasing padding until there is no answer anymore, because one of the hosts on 
the path cannot handle the message.
The heartbeat serves two purposes. First, it assures the sender that the recipi-
ent is still alive, even though there may not have been any activity over the under-
lying TCP connection for a while. Second, the heartbeat generates activity across 
the connection during idle periods, which avoids closure by a firewall that does not 
tolerate idle connections.
The requirement for the exchange of a payload was designed into the Heartbeat 
protocol to support its use in a connectionless version of TLS known as Datagram 
Transport Layer Security (DTLS). Because a connectionless service is subject 
to packet loss, the payload enables the requestor to match response messages to 
request messages. For simplicity, the same version of the Heartbeat protocol is used 
with both TLS and DTLS. Thus, the payload is required for both TLS and DTLS.
SSL/TLS ATTACKS
Since the first introduction of SSL in 1994, and the subsequent standardization of 
TLS, numerous attacks have been devised against these protocols. The appearance 
of each attack has necessitated changes in the protocol, the encryption tools used, or 
some aspect of the implementation of SSL and TLS to counter these threats.
attacK categories We can group the attacks into four general categories:
‚ñ†
‚ñ† Attacks on the handshake protocol: As early as 1998, an approach to com-
promising the handshake protocol based on exploiting the formatting and 
implementation of the RSA encryption scheme was presented [BLEI98]. As 
 countermeasures were implemented the attack was refined and adjusted to not 
only thwart the countermeasures but also speed up the attack [e.g., BARD12].
‚ñ†
‚ñ† Attacks on the record and application data protocols: A number of vulnerabili-
ties have been discovered in these protocols, leading to patches to counter the 
new threats. As a recent example, in 2011, researchers Thai Duong and Juliano 
Rizzo demonstrated a proof of concept called BEAST (Browser Exploit Against 
SSL/TLS) that turned what had been considered only a theoretical vulnerability

into a practical attack [GOOD11]. BEAST leverages a type of cryptographic 
attack called a chosen-plaintext attack. The attacker mounts the attack by 
choosing a guess for the plaintext that is associated with a known ciphertext. The 
researchers developed a practical algorithm for launching successful attacks. 
Subsequent patches were able to thwart this attack. The authors of the BEAST 
attack are also the creators of the 2012 CRIME (Compression Ratio Info-leak 
Made Easy) attack, which can allow an attacker to recover the content of web 
cookies when data compression is used along with TLS [GOOD12]. When used 
to recover the content of secret authentication cookies, it allows an attacker to 
perform session hijacking on an authenticated web session.
‚ñ†
‚ñ† Attacks on the PKI: Checking the validity of X.509 certificates is an activity 
subject to a variety of attacks, both in the context of SSL/TLS and elsewhere. 
For example, [GEOR12] demonstrated that commonly used libraries for 
SSL/TLS suffer from vulnerable certificate validation implementations. The 
 authors revealed weaknesses in the source code of OpenSSL, GnuTLS, JSSE, 
ApacheHttpClient, Weberknecht, cURL, PHP, Python and applications built 
upon or with these products.
‚ñ†
‚ñ† Other attacks: [MEYE13] lists a number of attacks that do not fit into any of 
the preceding categories. One example is an attack announced in 2011 by the 
German hacker group The Hackers Choice, which is a DoS attack [KUMA11]. 
The attack creates a heavy processing load on a server by overwhelming the 
target with SSL/TLS handshake requests. Boosting system load is done by 
establishing new connections or using renegotiation. Assuming that the major-
ity of computation during a handshake is done by the server, the attack creates 
more system load on the server than on the source device, leading to a DoS. 
The server is forced to continuously recompute random numbers and keys.
The history of attacks and countermeasures for SSL/TLS is representative of 
that for other Internet-based protocols. A ‚Äúperfect‚Äù protocol and a ‚Äúperfect‚Äù imple-
mentation strategy are never achieved. A constant back-and-forth between threats 
and countermeasures determines the evolution of Internet-based protocols.
TLSv1.3
In 2014, the IETF TLS working group began work on a version 1.3 of TLS. The 
primary aim is to improve the security of TLS. As of this writing, TLSv1.3 is still 
in a draft stage, but the final standard is likely to be very close to the current draft. 
Among the significant changes from version 1.2 are the following:
‚ñ†
‚ñ† TLSv1.3 removes support for a number of options and functions. Remov-
ing code that implements functions no longer needed reduces the chances 
of potentially dangerous coding errors and reduces the attack surface. The 
deleted items include:
‚ÄìCompression
‚ÄìCiphers that do not offer authenticated encryption
‚ÄìStatic RSA and DH key exchange
‚Äì32-bit timestamp as part of the Random parameter in the client_hello 
message

‚ÄìRenegotiation
‚ÄìChange Cipher Spec Protocol
‚ÄìRC4
‚ÄìUse of MD5 and SHA-224 hashes with signatures
‚ñ†
‚ñ† TLSv1.3 uses Diffie‚ÄìHellman or Elliptic Curve Diffie‚ÄìHellman for key 
exchange and does not permit RSA. The danger with RSA is that if the private 
key is compromised, all handshakes using these cipher suites will be compro-
mised. With DH or ECDH, a new key is negotiated for each handshake.
‚ñ†
‚ñ† TLSv1.3 allows for a ‚Äú1 round trip time‚Äù handshake by changing the order of 
message sent with establishing a secure connection. The client sends a  Client 
Key Exchange message containing its cryptographic parameters for key estab-
lishment before a cipher suite has been negotiated. This enables a server 
to  calculate keys for encryption and authentication before sending its first 
response. Reducing the number of packets sent during this handshake phase 
speeds up the process and reduces the attack surface.
These changes should improve the efficiency and security of TLS.
 6.3 HttpS
HTTPS (HTTP over SSL) refers to the combination of HTTP and SSL to imple-
ment secure communication between a Web browser and a Web server. The HTTPS 
capability is built into all modern Web browsers. Its use depends on the Web server 
supporting HTTPS communication. For example, some search engines do not sup-
port HTTPS.
The principal difference seen by a user of a Web browser is that URL (uniform 
resource locator) addresses begin with https:// rather than http://. A normal HTTP 
connection uses port 80. If HTTPS is specified, port 443 is used, which invokes SSL.
When HTTPS is used, the following elements of the communication are 
encrypted:
‚ñ†
‚ñ† URL of the requested document
‚ñ†
‚ñ† Contents of the document
‚ñ†
‚ñ† Contents of browser forms (filled in by browser user)
‚ñ†
‚ñ† Cookies sent from browser to server and from server to browser
‚ñ†
‚ñ† Contents of HTTP header
HTTPS is documented in RFC 2818, HTTP Over TLS. There is no fundamen-
tal change in using HTTP over either SSL or TLS, and both implementations are 
referred to as HTTPS.
Connection Initiation
For HTTPS, the agent acting as the HTTP client also acts as the TLS client. The 
client initiates a connection to the server on the appropriate port and then sends 
the TLS ClientHello to begin the TLS handshake. When the TLS handshake has

finished, the client may then initiate the first HTTP request. All HTTP data is to be 
sent as TLS application data. Normal HTTP behavior, including retained connec-
tions, should be followed.
There are three levels of awareness of a connection in HTTPS. At the HTTP 
level, an HTTP client requests a connection to an HTTP server by sending a con-
nection request to the next lowest layer. Typically, the next lowest layer is TCP, 
but it also may be TLS/SSL. At the level of TLS, a session is established between a 
TLS client and a TLS server. This session can support one or more connections at 
any time. As we have seen, a TLS request to establish a connection begins with the 
establishment of a TCP connection between the TCP entity on the client side and 
the TCP entity on the server side.
Connection Closure
An HTTP client or server can indicate the closing of a connection by including the 
following line in an HTTP record: Connection: close. This indicates that the 
connection will be closed after this record is delivered.
The closure of an HTTPS connection requires that TLS close the connec-
tion with the peer TLS entity on the remote side, which will involve closing the 
underlying TCP connection. At the TLS level, the proper way to close a connec-
tion is for each side to use the TLS alert protocol to send a close_notify alert. 
TLS implementations must initiate an exchange of closure alerts before closing a 
connection. A TLS implementation may, after sending a closure alert, close the 
connection without waiting for the peer to send its closure alert, generating an 
‚Äúincomplete close‚Äù. Note that an implementation that does this may choose to 
reuse the session. This should only be done when the application knows (typically 
through detecting HTTP message boundaries) that it has received all the message 
data that it cares about.
HTTP clients also must be able to cope with a situation in which the underlying 
TCP connection is terminated without a prior close_notify alert and without a 
Connection: close indicator. Such a situation could be due to a programming 
error on the server or a communication error that causes the TCP connection to drop. 
However, the unannounced TCP closure could be evidence of some sort of attack. So 
the HTTPS client should issue some sort of security warning when this occurs.
 6.4 Secure SHeLL (SSH)
Secure Shell (SSH) is a protocol for secure network communications designed to 
be relatively simple and inexpensive to implement. The initial version, SSH1 was 
focused on providing a secure remote logon facility to replace TELNET and other 
 remote logon schemes that provided no security. SSH also provides a more general 
client/server capability and can be used for such network functions as file transfer and 
e-mail. A new version, SSH2, fixes a number of security flaws in the original scheme. 
SSH2 is documented as a proposed standard in IETF RFCs 4250 through 4256.
SSH client and server applications are widely available for most operating 
 systems. It has become the method of choice for remote login and X tunneling and

is rapidly becoming one of the most pervasive applications for encryption technol-
ogy outside of embedded systems.
SSH is organized as three protocols that typically run on top of TCP 
(Figure¬†6.8):
‚ñ†
‚ñ† Transport Layer Protocol: Provides server authentication, data confidentiality, 
and data integrity with forward secrecy (i.e., if a key is compromised during 
one session, the knowledge does not affect the security of earlier sessions). The 
transport layer may optionally provide compression.
‚ñ†
‚ñ† User Authentication Protocol: Authenticates the user to the server.
‚ñ†
‚ñ† Connection Protocol: Multiplexes multiple logical communications channels 
over a single, underlying SSH connection.
Transport Layer Protocol
host Keys Server authentication occurs at the transport layer, based on the server 
possessing a public/private key pair. A server may have multiple host keys using 
multiple different asymmetric encryption algorithms. Multiple hosts may share 
the same host key. In any case, the server host key is used during key exchange to 
authenticate the identity of the host. For this to be possible, the client must have a 
priori knowledge of the server‚Äôs public host key. RFC 4251 dictates two alternative 
trust models that can be used:
1. The client has a local database that associates each host name (as typed by the 
user) with the corresponding public host key. This method requires no centrally 
administered infrastructure and no third-party coordination. The downside is that 
the database of name-to-key associations may become burdensome to maintain.
Figure 6.8 SSH Protocol Stack
SSH User
Authentication Protocol
SSH Transport Layer Protocol
TCP
IP
Internet protocol provides datagram delivery across
multiple networks.
Transmission control protocol provides reliable, connection-
oriented end-to-end delivery.
Provides server authentication, confdentiality, and integrity.
It may optionally also provide compression.
Authenticates the client-side
user to the server.
SSH
Connection Protocol
Multiplexes the encrypted
tunnel into several logical
channels.

2. The host name-to-key association is certified by a trusted certification author-
ity (CA). The client only knows the CA root key and can verify the validity of 
all host keys certified by accepted CAs. This alternative eases the maintenance 
problem, since ideally, only a single CA key needs to be securely stored on the 
client. On the other hand, each host key must be appropriately certified by a 
central authority before authorization is possible.
PacKet exchange Figure 6.9 illustrates the sequence of events in the SSH 
Transport Layer Protocol. First, the client establishes a TCP connection to the 
server. This is done via the TCP protocol and is not part of the Transport Layer 
Protocol. Once the connection is established, the client and server exchange data, 
referred to as packets, in the data field of a TCP segment. Each packet is in the 
 following format (Figure 6.10).
‚ñ†
‚ñ† Packet length: Length of the packet in bytes, not including the packet length 
and MAC fields.
‚ñ†
‚ñ† Padding length: Length of the random padding field.
‚ñ†
‚ñ† Payload: Useful contents of the packet. Prior to algorithm negotiation, this 
field is uncompressed. If compression is negotiated, then in subsequent 
 packets, this field is compressed.
Figure 6.9 SSH Transport Layer Protocol Packet Exchanges
Client
Server
SSH-protoversion-softwareversion
Identifcation string
exchange
Algorithm
negotiation
End of
key exchange
Service
request
SSH-protoversion-softwareversion
SSH_MSG_KEXINIT
SSH_MSG_KEXINIT
SSH_MSG_NEWKEYS
SSH_MSG_NEWKEYS
SSH_MSG_SERVICE_REQUEST
Establish TCP Connection
Key Exchange

‚ñ†
‚ñ† Random padding: Once an encryption algorithm has been negotiated, this 
field is added. It contains random bytes of padding so that the total length of 
the packet (excluding the MAC field) is a multiple of the cipher block size, or 
8 bytes for a stream cipher.
‚ñ†
‚ñ† Message authentication code (MAC): If message authentication has been 
negotiated, this field contains the MAC value. The MAC value is computed 
over the entire packet plus a sequence number, excluding the MAC field. The 
sequence number is an implicit 32-bit packet sequence that is initialized to 
zero for the first packet and incremented for every packet. The sequence num-
ber is not included in the packet sent over the TCP connection.
Once an encryption algorithm has been negotiated, the entire packet 
 (excluding the MAC field) is encrypted after the MAC value is calculated.
The SSH Transport Layer packet exchange consists of a sequence of steps 
(Figure 6.9). The first step, the identification string exchange, begins with the  client 
sending a packet with an identification string of the form:
SSH-protoversion-softwareversion SP comments CR LF
Figure 6.10 SSH Transport Layer Protocol Packet Formation
pdl
pktl
pktl = packet length
pdl = padding length
Padd ni g
eq #
s
Payload
SSH Packet
Compressed payload
Ciphertext
COMPRESS
ENCRYPT
MAC

where SP, CR, and LF are space character, carriage return, and line feed, respec-
tively. An example of a valid string is SSH-2.0-billsSSH_3.6.3q3<CR><LF>. 
The server responds with its own identification string. These strings are used in the 
Diffie‚ÄìHellman key exchange.
Next comes algorithm negotiation. Each side sends an SSH_MSG_KEXINIT 
containing lists of supported algorithms in the order of preference to the sender. 
There is one list for each type of cryptographic algorithm. The algorithms include 
key exchange, encryption, MAC algorithm, and compression algorithm. Table 6.3 
shows the allowable options for encryption, MAC, and compression. For each cat-
egory, the algorithm chosen is the first algorithm on the client‚Äôs list that is also sup-
ported by the server.
The next step is key exchange. The specification allows for alternative meth-
ods of key exchange, but at present, only two versions of Diffie‚ÄìHellman key 
exchange are specified. Both versions are defined in RFC 2409 and require only one 
packet in each direction. The following steps are involved in the exchange. In this, 
C is the  client; S is the server; p is a large safe prime; g is a generator for a subgroup 
of GF(p); q is the order of the subgroup; V_S is S‚Äôs identification string; V_C is 
Table 6.3 SSH Transport Layer Cryptographic Algorithms
MAC algorithm
hmac-sha1*
HMAC-SHA1; digest 
length = key length = 20
hmac-sha1-96**
First 96 bits of HMAC-
SHA1; digest length = 12; 
key length = 20
hmac-md5
HMAC-MD5; digest 
length = key length = 16
hmac-md5-96
First 96 bits of 
HMAC-MD5;  
digest length = 12;  
key length = 16
Compression algorithm
none*
No compression
zlib
Defined in RFC 1950 and 
RFC 1951
Cipher
3des-cbc*
Three-key 3DES in CBC 
mode
blowfish-cbc
Blowfish in CBC mode
twofish256-cbc
Twofish in CBC mode with 
a 256-bit key
twofish192-cbc
Twofish with a 192-bit key
twofish128-cbc
Twofish with a 128-bit key
aes256-cbc
AES in CBC mode with a 
256-bit key
aes192-cbc
AES with a 192-bit key
aes128-cbc**
AES with a 128-bit key
Serpent256-cbc
Serpent in CBC mode with 
a 256-bit key
Serpent192-cbc
Serpent with a 192-bit key
Serpent128-cbc
Serpent with a 128-bit key
arcfour
RC4 with a 128-bit key
cast128-cbc
CAST-128 in CBC mode
* = Required
** = Recommended

C‚Äôs identification string; K_S is S‚Äôs public host key; I_C is C‚Äôs SSH_MSG_KEXINIT 
 message and I_S is S‚Äôs SSH_MSG_KEXINIT message that have been exchanged 
before this part begins. The values of p, g, and q are known to both client and server 
as a result of the algorithm selection negotiation. The hash function hash() is also 
decided during algorithm negotiation.
1. C generates a random number x(1 6 x 6 q) and computes e = gx mod p. C 
sends e to S.
2. S generates a random number y(0 6 y 6 q) and computes f = gy mod p. 
S receives e. It computes K = ey mod p, H = hash(V_C ‚Äò V_S ‚Äò I_C ‚Äò I_S ‚Äò K_S ‚Äò
e ‚Äò f ‚Äò K), and signature s on H with its private host key. S sends (K_S ‚Äò f ‚Äò s)  
to C. The signing operation may involve a second hashing operation.
3. C verifies that K_S really is the host key for S (e.g., using certificates or a local 
database). C is also allowed to accept the key without verification; however, 
doing so will render the protocol insecure against active attacks (but may be 
desirable for practical reasons in the short term in many environments). C then 
computes K = f x mod p, H = hash(V_C ‚Äò V_S ‚Äò I_C ‚Äò I_S ‚Äò K_S ‚Äò e ‚Äò f ‚Äò K), and 
verifies the signature s on H.
As a result of these steps, the two sides now share a master key K. In addition, 
the server has been authenticated to the client, because the server has used its pri-
vate key to sign its half of the Diffie‚ÄìHellman exchange. Finally, the hash value H 
serves as a session identifier for this connection. Once computed, the session identi-
fier is not changed, even if the key exchange is performed again for this connection 
to obtain fresh keys.
The end of key exchange is signaled by the exchange of SSH_MSG_NEWKEYS 
packets. At this point, both sides may start using the keys generated from K, as dis-
cussed subsequently.
The final step is service request. The client sends an SSH_MSG_SERVICE_
REQUEST packet to request either the User Authentication or the Connection 
Protocol. Subsequent to this, all data is exchanged as the payload of an SSH 
Transport Layer packet, protected by encryption and MAC.
Key generation The keys used for encryption and MAC (and any needed IVs) 
are generated from the shared secret key K, the hash value from the key exchange 
H, and the session identifier, which is equal to H unless there has been a subsequent 
key exchange after the initial key exchange. The values are computed as follows.
‚ñ†
‚ñ† Initial IV client to server: HASH(K ‚Äò H ‚Äò ;A< ‚Äò session_id)
‚ñ†
‚ñ† Initial IV server to client: HASH(K ‚Äò H ‚Äò ;B< ‚Äò session_id)
‚ñ†
‚ñ† Encryption key client to server: HASH(K ‚Äò H ‚Äò ;C< ‚Äò session_id)
‚ñ†
‚ñ† Encryption key server to client: HASH(K ‚Äò H ‚Äò ;D< ‚Äò session_id)
‚ñ†
‚ñ† Integrity key client to server: HASH(K ‚Äò H ‚Äò ;E< ‚Äò session_id)
‚ñ†
‚ñ† Integrity key server to client: HASH(K ‚Äò H ‚Äò ;F< ‚Äò session_id)
where HASH() is the hash function determined during algorithm negotiation.

User Authentication Protocol
The User Authentication Protocol provides the means by which the client is 
 authenticated to the server.
Message tyPes and ForMats Three types of messages are always used in the User 
Authentication Protocol. Authentication requests from the client have the format:
byte
SSH_MSG_USERAUTH_REQUEST (50)
string
user name
string
service name
string
method name
¬†.¬†.¬†.¬†
method specific fields
where user name is the authorization identity the client is claiming, service 
name is the facility to which the client is requesting access (typically the SSH 
Connection Protocol), and method name is the authentication method being 
used in this request. The first byte has decimal value 50, which is interpreted as 
SSH_MSG_USERAUTH_REQUEST.
If the server either (1) rejects the authentication request or (2) accepts the 
 request but requires one or more additional authentication methods, the server 
sends a message with the format:
byte
SSH_MSG_USERAUTH_FAILURE (51)
name-list
authentications that can continue
boolean
partial success
where the name-list is a list of methods that may productively continue the  dialog. 
If the server accepts authentication, it sends a single byte message: SSH_MSG_ 
USERAUTH_SUCCESS (52).
Message exchange The message exchange involves the following steps.
1. The client sends a SSH_MSG_USERAUTH_REQUEST with a requested method 
of none.
2. The server checks to determine if the user name is valid. If not, the server 
 returns SSH_MSG_USERAUTH_FAILURE with the partial success value of 
false. If the user name is valid, the server proceeds to step 3.
3. The server returns SSH_MSG_USERAUTH_FAILURE with a list of one or more 
authentication methods to be used.
4. The client selects one of the acceptable authentication methods and sends a 
SSH_MSG_USERAUTH_REQUEST with that method name and the required 
method-specific fields. At this point, there may be a sequence of exchanges to 
perform the method.

5. If the authentication succeeds and more authentication methods are required, 
the server proceeds to step 3, using a partial success value of true. If the 
authentication fails, the server proceeds to step 3, using a partial success value 
of false.
6. When all required authentication methods succeed, the server sends a  
SSH_MSG_USERAUTH_SUCCESS message, and the Authentication Protocol  
is over.
authentication Methods The server may require one or more of the following 
authentication methods.
‚ñ†
‚ñ† publickey: The details of this method depend on the public-key algorithm 
chosen. In essence, the client sends a message to the server that contains 
the client‚Äôs public key, with the message signed by the client‚Äôs private key. 
When the server receives this message, it checks whether the supplied key 
is acceptable for authentication and, if so, it checks whether the signature is 
correct.
‚ñ†
‚ñ† password: The client sends a message containing a plaintext password, which 
is protected by encryption by the Transport Layer Protocol.
‚ñ†
‚ñ† hostbased: Authentication is performed on the client‚Äôs host rather than the 
client itself. Thus, a host that supports multiple clients would provide authen-
tication for all its clients. This method works by having the client send a signa-
ture created with the private key of the client host. Thus, rather than directly 
verifying the user‚Äôs identity, the SSH server verifies the identity of the client 
host‚Äîand then believes the host when it says the user has already authenti-
cated on the client side.
Connection Protocol
The SSH Connection Protocol runs on top of the SSH Transport Layer Protocol 
and assumes that a secure authentication connection is in use.2 That secure authen-
tication connection, referred to as a tunnel, is used by the Connection Protocol to 
multiplex a number of logical channels.
channel MechanisM All types of communication using SSH, such as a terminal 
session, are supported using separate channels. Either side may open a channel. 
For each channel, each side associates a unique channel number, which need not be 
the same on both ends. Channels are flow controlled using a window mechanism. 
No¬†data may be sent to a channel until a message is received to indicate that window 
space is available.
2RFC 4254, The Secure Shell (SSH) Connection Protocol, states that the Connection Protocol runs on 
top of the Transport Layer Protocol and the User Authentication Protocol. RFC 4251, SSH Protocol 
Architecture, states that the Connection Protocol runs over the User Authentication Protocol. In fact, the 
Connection Protocol runs over the Transport Layer Protocol, but assumes that the User Authentication 
Protocol has been previously invoked.

The life of a channel progresses through three stages: opening a channel, data 
transfer, and closing a channel.
When either side wishes to open a new channel, it allocates a local number for 
the channel and then sends a message of the form:
byte
SSH_MSG_CHANNEL_OPEN
string
channel type
uint32
sender channel
uint32
initial window size
uint32
maximum packet size
....
channel type specific data follows
where uint32 means unsigned 32-bit integer. The channel type identifies the appli-
cation for this channel, as described subsequently. The sender channel is the local 
channel number. The initial window size specifies how many bytes of channel data 
can be sent to the sender of this message without adjusting the window. The maxi-
mum packet size specifies the maximum size of an individual data packet that can 
be sent to the sender. For example, one might want to use smaller packets for inter-
active connections to get better interactive response on slow links.
If the remote side is able to open the channel, it returns a SSH_MSG_CHANNEL_
OPEN_CONFIRMATION message, which includes the sender channel number, the 
recipient channel number, and window and packet size values for  incoming  traffic. 
Otherwise, the remote side returns a SSH_MSG_CHANNEL_OPEN_FAILURE 
 message with a reason code indicating the reason for failure.
Once a channel is open, data transfer is performed using a SSH_MSG_
CHANNEL_DATA message, which includes the recipient channel number and a block 
of data. These messages, in both directions, may continue as long as the channel 
is¬†open.
When either side wishes to close a channel, it sends a SSH_MSG_CHANNEL_
CLOSE message, which includes the recipient channel number.
Figure 6.11 provides an example of Connection Protocol Message Exchange.
channel tyPes Four channel types are recognized in the SSH Connection Protocol 
specification.
‚ñ†
‚ñ† session: The remote execution of a program. The program may be a shell, an 
application such as file transfer or e-mail, a system command, or some built-in 
subsystem. Once a session channel is opened, subsequent requests are used to 
start the remote program.
‚ñ†
‚ñ† x11: This refers to the X Window System, a computer software system and 
 network protocol that provides a graphical user interface (GUI) for net-
worked computers. X allows applications to run on a network server but to be 
displayed on a desktop machine.

‚ñ†
‚ñ† forwarded-tcpip: This is remote port forwarding, as explained in the next 
subsection.
‚ñ†
‚ñ† direct-tcpip: This is local port forwarding, as explained in the next subsection.
Port Forwarding One of the most useful features of SSH is port forwarding. In 
essence, port forwarding provides the ability to convert any insecure TCP connec-
tion into a secure SSH connection. This is also referred to as SSH tunneling. We 
need to know what a port is in this context. A port is an identifier of a user of 
TCP. So, any application that runs on top of TCP has a port number. Incoming TCP 
traffic is delivered to the appropriate application on the basis of the port number. 
An application may employ multiple port numbers. For example, for the Simple 
Mail Transfer Protocol (SMTP), the server side generally listens on port 25, so an 
Figure 6.11 Example of SSH Connection Protocol Message Exchange
Client
Server
SSH_MSG_CHANNEL_OPEN
Open a
channel
Data
transfer
Close a
channel
SSH_MSG_CHANNEL_OPEN_CONFIRMATION
SSH_MSG_CHANNEL_DATA
SSH_MSG_CHANNEL_DATA
SSH_MSG_CHANNEL_DATA
SSH_MSG_CHANNEL_DATA
SSH_MSG_CHANNEL_CLOSE
Establish Authenticated Transport Layer Connection

incoming SMTP request uses TCP and addresses the data to destination port 25.  
TCP recognizes that this is the SMTP server address and routes the data to the 
SMTP server application.
Figure 6.12 illustrates the basic concept behind port forwarding. We have 
a  client application that is identified by port number x and a server application 
 identified by port number y. At some point, the client application invokes the local 
TCP entity and requests a connection to the remote server on port y. The local 
TCP entity negotiates a TCP connection with the remote TCP entity, such that the 
 connection links local port x to remote port y.
To secure this connection, SSH is configured so that the SSH Transport Layer 
Protocol establishes a TCP connection between the SSH client and server entities, 
with TCP port numbers a and b, respectively. A secure SSH tunnel is established 
Figure 6.12 SSH Transport Layer Packet Exchanges
Client
Server
Client
application
Unsecure TCP connection
(a) Connection via TCP
TCP
entity
x
y
Server
application
TCP
entity
Client
application
Secure SSH tunnel
(b) Connection via SSH tunnel
SSH
entity
x
y
Server
application
SSH
entity
Unsecure TCP connection
TCP
entity
a 
b
TCP
entity

over this TCP connection. Traffic from the client at port x is redirected to the local 
SSH entity and travels through the tunnel where the remote SSH entity delivers the 
data to the server application on port y. Traffic in the other direction is similarly 
redirected.
SSH supports two types of port forwarding: local forwarding and remote for-
warding. Local forwarding allows the client to set up a ‚Äúhijacker‚Äù process. This will 
intercept selected application-level traffic and redirect it from an unsecured TCP 
connection to a secure SSH tunnel. SSH is configured to listen on selected ports. 
SSH grabs all traffic using a selected port and sends it through an SSH tunnel. On 
the other end, the SSH server sends the incoming traffic to the destination port dic-
tated by the client application.
The following example should help clarify local forwarding. Suppose you have 
an e-mail client on your desktop and use it to get e-mail from your mail server via 
the Post Office Protocol (POP). The assigned port number for POP3 is port 110. We 
can secure this traffic in the following way:
1. The SSH client sets up a connection to the remote server.
2. Select an unused local port number, say 9999, and configure SSH to accept 
traffic from this port destined for port 110 on the server.
3. The SSH client informs the SSH server to create a connection to the destina-
tion, in this case mailserver port 110.
4. The client takes any bits sent to local port 9999 and sends them to the server 
inside the encrypted SSH session. The SSH server decrypts the incoming bits 
and sends the plaintext to port 110.
5. In the other direction, the SSH server takes any bits received on port 110 and 
sends them inside the SSH session back to the client, who decrypts and sends 
them to the process connected to port 9999.
With remote forwarding, the user‚Äôs SSH client acts on the server‚Äôs behalf. 
The client receives traffic with a given destination port number, places the traf-
fic on the correct port and sends it to the destination the user chooses. A typical 
example of remote forwarding is the following. You wish to access a server at 
work from your home computer. Because the work server is behind a firewall, it 
will not accept an SSH request from your home computer. However, from work 
you can set up an SSH tunnel using remote forwarding. This involves the follow-
ing steps.
1. From the work computer, set up an SSH connection to your home computer. 
The firewall will allow this, because it is a protected outgoing connection.
2. Configure the SSH server to listen on a local port, say 22, and to deliver data 
across the SSH connection addressed to remote port, say 2222.
3. You can now go to your home computer, and configure SSH to accept traffic 
on port 2222.
4. You now have an SSH tunnel that can be used for remote logon to the work 
server.

6.5 Key termS, revieW QueStionS, and probLemS
Key Terms 
Alert protocol
Change Cipher Spec protocol
Handshake protocol
HTTPS (HTTP over SSL)
Master Secret
Secure Shell (SSH)
Secure Socket Layer (SSL)
Transport Layer Security 
(TLS)
Review Questions 
 
6.1 
What are the advantages of each of the three approaches shown in Figure 6.1?
 
6.2 
What protocols comprise TLS?
 
6.3 
What is the difference between a TLS connection and a TLS session?
 
6.4 
List and briefly define the parameters that define a TLS session state.
 
6.5 
List and briefly define the parameters that define a TLS session connection.
 
6.6 
What services are provided by the TLS Record Protocol?
 
6.7 
What steps are involved in the TLS Record Protocol transmission?
 
6.8 
Give brief details about different levels of awareness of a connection in HTTPS.
 
6.9 
Which protocol was replaced by SSH and why? Which version is currently in the 
 process of being standardized?
 
6.10 
List and briefly define the SSH protocols.
Problems 
 
6.1 
In SSL and TLS, why is there a separate Change Cipher Spec Protocol rather than 
including a change_cipher_spec message in the Handshake Protocol?
 
6.2 
What purpose does the MAC serve during the change cipher spec TLS exchange?
 
6.3 
Consider the following threats to Web security and describe how each is countered by 
a particular feature of TLS.
a. Brute-Force Cryptanalytic Attack: An exhaustive search of the key space for a 
conventional encryption algorithm.
b. Known Plaintext Dictionary Attack: Many messages will contain predictable 
plaintext, such as the HTTP GET command. An attacker constructs a diction-
ary containing every possible encryption of the known-plaintext message. When 
an encrypted message is intercepted, the attacker takes the portion containing 
the encrypted known plaintext and looks up the ciphertext in the dictionary. The 
ciphertext should match against an entry that was encrypted with the same secret 
key. If there are several matches, each of these can be tried against the full cipher-
text to determine the right one. This attack is especially effective against small key 
sizes (e.g., 40-bit keys).
c. Replay Attack: Earlier TLS handshake messages are replayed.
d. Man-in-the-Middle Attack: An attacker interposes during key exchange, acting as 
the client to the server and as the server to the client.
e. Password Sniffing: Passwords in HTTP or other application traffic are eaves-
dropped.
f. IP Spoofing: Uses forged IP addresses to fool a host into accepting bogus data.

g. IP Hijacking: An active, authenticated connection between two hosts is disrupted 
and the attacker takes the place of one of the hosts.
h. SYN Flooding: An attacker sends TCP SYN messages to request a connection 
but does not respond to the final message to establish the connection fully. The 
 attacked TCP module typically leaves the ‚Äúhalf-open connection‚Äù around for a few 
minutes. Repeated SYN messages can clog the TCP module.
 
6.4 
Based on what you have learned in this chapter, is it possible in TLS for the receiver 
to reorder TLS record blocks that arrive out of order? If so, explain how it can be 
done. If not, why not?
 
6.5 
For SSH packets, what is the advantage, if any, of not including the MAC in the scope 
of the packet encryption?

222
7.1 
Wireless Security
Wireless Network Threats
Wireless Security Measures
7.2 
Mobile Device Security
Security Threats
Mobile Device Security Strategy
7.3 
IEEE 802.11 Wireless LAN Overview
The Wi-Fi Alliance
IEEE 802 Protocol Architecture
IEEE 802.11 Network Components and Architectural Model
IEEE 802.11 Services
7.4 
IEEE 802.11i Wireless LAN Security
IEEE 802.11i Services
IEEE 802.11i Phases of Operation
Discovery Phase
Authentication Phase
Key Management Phase
Protected Data Transfer Phase
The IEEE 802.11i Pseudorandom Function
7.5 
Key Terms, Review Questions, and Problems
Chapter
Wireless Network Security

This chapter begins with a general overview of wireless security issues. We then focus 
on the relatively new area of mobile device security, examining threats and counter-
measures for mobile devices used in the enterprise. Then, we look at the IEEE 802.11i 
standard for wireless LAN security. This standard is part of IEEE 802.11, also referred 
to as Wi-Fi. We begin the discussion with an overview of IEEE 802.11, and then we 
look in some detail at IEEE 802.11i.
 7.1 Wireless security
Wireless networks, and the wireless devices that use them, introduce a host of secu-
rity problems over and above those found in wired networks. Some of the key fac-
tors contributing to the higher security risk of wireless networks compared to wired 
networks include the following [MA10]:
‚ñ†
‚ñ† Channel: Wireless networking typically involves broadcast communications, 
which is far more susceptible to eavesdropping and jamming than wired 
networks. Wireless networks are also more vulnerable to active attacks that 
exploit vulnerabilities in communications protocols.
‚ñ†
‚ñ† Mobility: Wireless devices are, in principal and usually in practice, far more 
portable and mobile than wired devices. This mobility results in a number of 
risks, described subsequently.
‚ñ†
‚ñ† Resources: Some wireless devices, such as smartphones and tablets, have 
sophisticated operating systems but limited memory and processing resources 
with which to counter threats, including denial of service and malware.
‚ñ†
‚ñ† Accessibility: Some wireless devices, such as sensors and robots, may be left 
unattended in remote and/or hostile locations. This greatly increases their 
 vulnerability to physical attacks.
learning Objectives
After studying this chapter, you should be able to:
‚óÜ‚ñ†
Present an overview of security threats and countermeasures for wireless 
networks.
‚óÜ‚ñ†
Understand the unique security threats posed by the use of mobile devices 
with enterprise networks.
‚óÜ‚ñ†
Describe the principal elements in a mobile device security strategy.
‚óÜ‚ñ†
Understand the essential elements of the IEEE 802.11 wireless LAN 
 standard.
‚óÜ‚ñ†
Summarize the various components of the IEEE 802.11i wireless LAN 
 security architecture.

In simple terms, the wireless environment consists of three components that 
provide point of attack (Figure 7.1). The wireless client can be a cell phone, a 
 Wi-Fi‚Äìenabled laptop or tablet, a wireless sensor, a Bluetooth device, and so on. 
The  wireless access point provides a connection to the network or service. Examples 
of access points are cell towers, Wi-Fi hotspots, and wireless access points to wired 
local or wide area networks. The transmission medium, which carries the radio 
waves for data transfer, is also a source of vulnerability.
Wireless Network Threats
[CHOI08] lists the following security threats to wireless networks:
‚ñ†
‚ñ† Accidental association: Company wireless LANs or wireless access points to 
wired LANs in close proximity (e.g., in the same or neighboring buildings) 
may create overlapping transmission ranges. A user intending to connect to 
one LAN may unintentionally lock on to a wireless access point from a neigh-
boring network. Although the security breach is accidental, it nevertheless 
 exposes resources of one LAN to the accidental user.
‚ñ†
‚ñ† Malicious association: In this situation, a wireless device is configured to 
 appear to be a legitimate access point, enabling the operator to steal pass-
words from legitimate users and then penetrate a wired network through a 
legitimate wireless access point.
‚ñ†
‚ñ† Ad hoc networks: These are peer-to-peer networks between wireless comput-
ers with no access point between them. Such networks can pose a security 
threat due to a lack of a central point of control.
‚ñ†
‚ñ† Nontraditional networks: Nontraditional networks and links, such as personal 
network Bluetooth devices, barcode readers, and handheld PDAs, pose a secu-
rity risk in terms of both eavesdropping and spoofing.
‚ñ†
‚ñ† Identity theft (MAC spoofing): This occurs when an attacker is able to eaves-
drop on network traffic and identify the MAC address of a computer with 
network privileges.
‚ñ†
‚ñ† Man-in-the middle attacks: This type of attack is described in Chapter 3 in 
the context of the Diffie‚ÄìHellman key exchange protocol. In a broader sense, 
this attack involves persuading a user and an access point to believe that they 
are talking to each other when in fact the communication is going through an 
intermediate attacking device. Wireless networks are particularly vulnerable 
to such attacks.
Figure 7.1 Wireless Networking Components
Endpoint
Wireless medium
Access point

‚ñ†
‚ñ† Denial of service (DoS): This type of attack is discussed in detail in Chapter 10.  
In the context of a wireless network, a DoS attack occurs when an attacker 
continually bombards a wireless access point or some other accessible wireless 
port with various protocol messages designed to consume system resources. 
The wireless environment lends itself to this type of attack, because it is so 
easy for the attacker to direct multiple wireless messages at the target.
‚ñ†
‚ñ† Network injection: A network injection attack targets wireless access points 
that are exposed to nonfiltered network traffic, such as routing protocol mes-
sages or network management messages. An example of such an attack is 
one in which bogus reconfiguration commands are used to affect routers and 
switches to degrade network performance.
Wireless Security Measures
Following [CHOI08], we can group wireless security measures into those dealing 
with wireless transmissions, wireless access points, and wireless networks (consist-
ing of wireless routers and endpoints).
Securing WireleSS TranSmiSSionS The principal threats to wireless transmission 
are eavesdropping, altering or inserting messages, and disruption. To deal with 
eavesdropping, two types of countermeasures are appropriate:
‚ñ†
‚ñ† Signal-hiding techniques: Organizations can take a number of measures to 
make it more difficult for an attacker to locate their wireless access points, 
including turning off service set identifier (SSID) broadcasting by wireless 
 access points; assigning cryptic names to SSIDs; reducing signal strength to the 
lowest level that still provides requisite coverage; and locating wireless access 
points in the interior of the building, away from windows and exterior walls. 
Greater security can be achieved by the use of directional antennas and of 
signal-shielding techniques.
‚ñ†
‚ñ† Encryption: Encryption of all wireless transmission is effective against eaves-
dropping to the extent that the encryption keys are secured.
The use of encryption and authentication protocols is the standard method of 
countering attempts to alter or insert transmissions.
The methods discussed in Chapter 10 for dealing with DoS apply to wireless 
transmissions. Organizations can also reduce the risk of unintentional DoS attacks. 
Site surveys can detect the existence of other devices using the same frequency 
range, to help determine where to locate wireless access points. Signal strengths can 
be adjusted and shielding used in an attempt to isolate a wireless environment from 
competing nearby transmissions.
Securing WireleSS acceSS PoinTS The main threat involving wireless access 
points is unauthorized access to the network. The principal approach for preventing 
such access is the IEEE 802.1X standard for port-based network access control. The 
standard provides an authentication mechanism for devices wishing to attach to a 
LAN or wireless network. The use of 802.1X can prevent rogue access points and 
other unauthorized devices from becoming insecure backdoors.
Section 5.3 provides an introduction to 802.1X.

Securing WireleSS neTWorkS [CHOI08] recommends the following techniques 
for wireless network security:
1. Use encryption. Wireless routers are typically equipped with built-in encryp-
tion mechanisms for router-to-router traffic.
2. Use antivirus and antispyware software, and a firewall. These facilities should 
be enabled on all wireless network endpoints.
3. Turn off identifier broadcasting. Wireless routers are typically configured to 
broadcast an identifying signal so that any device within range can learn of 
the router‚Äôs existence. If a network is configured so that authorized devices 
know the identity of routers, this capability can be disabled, so as to thwart 
attackers.
4. Change the identifier on your router from the default. Again, this measure 
thwarts attackers who will attempt to gain access to a wireless network using 
default router identifiers.
5. Change your router‚Äôs pre-set password for administration. This is another 
 prudent step.
6. Allow only specific computers to access your wireless network. A router can 
be configured to only communicate with approved MAC addresses. Of course, 
MAC addresses can be spoofed, so this is just one element of a security strategy.
 7.2 MObile Device security
Prior to the widespread use of smartphones, the dominant paradigm for computer 
and network security in organizations was as follows. Corporate IT was tightly con-
trolled. User devices were typically limited to Windows PCs. Business applications 
were controlled by IT and either run locally on endpoints or on physical servers 
in data centers. Network security was based upon clearly defined perimeters that 
separated trusted internal networks from the untrusted Internet. Today, there have 
been massive changes in each of these assumptions. An organization‚Äôs networks 
must accommodate the following:
‚ñ†
‚ñ† Growing use of new devices: Organizations are experiencing significant growth 
in employee use of mobile devices. In many cases, employees are allowed to 
use a combination of endpoint devices as part of their day-to-day activities.
‚ñ†
‚ñ† Cloud-based applications: Applications no longer run solely on physical 
servers in corporate data centers. Quite the opposite, applications can run 
 anywhere‚Äîon traditional physical servers, on mobile virtual servers, or in the 
cloud. Additionally, end users can now take advantage of a wide variety of 
cloud-based applications and IT services for personal and professional use. 
Facebook can be used for an employee‚Äôs personal profiles or as a component 
of a corporate marketing campaign. Employees depend upon Skype to speak 
with friends abroad or for legitimate business video conferencing. Dropbox 
and Box can be used to distribute documents between corporate and personal 
devices for mobility and user productivity.

‚ñ†
‚ñ† De-perimeterization: Given new device proliferation, application mobility, 
and cloud-based consumer and corporate services, the notion of a static net-
work perimeter is all but gone. Now there are a multitude of network perim-
eters around devices, applications, users, and data. These perimeters have also 
become quite dynamic as they must adapt to various environmental conditions 
such as user role, device type, server virtualization mobility, network location, 
and time-of-day.
‚ñ†
‚ñ† External business requirements: The enterprise must also provide guests, 
third-party contractors, and business partners network access using various 
devices from a multitude of locations.
The central element in all of these changes is the mobile computing device. 
Mobile devices have become an essential element for organizations as part of the 
overall network infrastructure. Mobile devices such as smartphones, tablets, and 
memory sticks provide increased convenience for individuals as well as the poten-
tial for increased productivity in the workplace. Because of their widespread use 
and unique characteristics, security for mobile devices is a pressing and complex 
issue. In essence, an organization needs to implement a security policy through a 
combination of security features built into the mobile devices and additional secu-
rity controls provided by network components that regulate the use of the mobile 
devices.
Security Threats
Mobile devices need additional, specialized protection measures beyond those 
 implemented for other client devices, such as desktop and laptop devices that are 
used only within the organization‚Äôs facilities and on the organization‚Äôs networks. 
SP 800-14 (Guidelines for Managing and Securing Mobile Devices in the Enterprise, 
July 2012) lists seven major security concerns for mobile devices. We examine each 
of these in turn.
lack of PhySical SecuriTy conTrolS Mobile devices are typically under the com-
plete control of the user, and are used and kept in a variety of locations outside the 
organization‚Äôs control, including off premises. Even if a device is required to remain 
on premises, the user may move the device within the organization between secure 
and nonsecured locations. Thus, theft and tampering are realistic threats.
The security policy for mobile devices must be based on the assumption that 
any mobile device may be stolen or at least accessed by a malicious party. The threat 
is twofold: A malicious party may attempt to recover sensitive data from the device 
itself, or may use the device to gain access to the organization‚Äôs resources.
uSe of unTruSTed mobile deviceS In addition to company-issued and company-
controlled mobile devices, virtually all employees will have personal smartphones 
and/or tablets. The organization must assume that these devices are not  trustworthy. 
That is, the devices may not employ encryption and either the user or a third party 
may have installed a bypass to the built-in restrictions on security, operating system 
use, and so on.

uSe of unTruSTed neTWorkS If a mobile device is used on premises, it can  connect 
to organization resources over the organization‚Äôs own in-house wireless networks. 
However, for off-premises use, the user will typically access organizational resources 
via Wi-Fi or cellular access to the Internet and from the Internet to the organiza-
tion. Thus, traffic that includes an off-premises segment is potentially susceptible to 
eavesdropping or man-in-the-middle types of attacks. Thus, the security policy must 
be based on the assumption that the networks between the mobile  device and the 
organization are not trustworthy.
uSe of aPPlicaTionS creaTed by unknoWn ParTieS By design, it is easy to find 
and install third-party applications on mobile devices. This poses the obvious risk of 
installing malicious software. An organization has several options for dealing with 
this threat, as described subsequently.
inTeracTion WiTh oTher SySTemS A common feature found on smartphones and 
tablets is the ability to automatically synchronize data, apps, contacts, photos, and 
so on with other computing devices and with cloud-based storage. Unless an orga-
nization has control of all the devices involved in synchronization, there is consider-
able risk of the organization‚Äôs data being stored in an unsecured location, plus the 
risk of the introduction of malware.
uSe of unTruSTed conTenT Mobile devices may access and use content that other 
computing devices do not encounter. An example is the Quick Response (QR) 
code, which is a two-dimensional barcode. QR codes are designed to be captured 
by a mobile device camera and used by the mobile device. The QR code translates 
to a URL, so that a malicious QR code could direct the mobile device to malicious 
Web sites.
uSe of locaTion ServiceS The GPS capability on mobile devices can be used to 
maintain a knowledge of the physical location of the device. While this feature 
might be useful to an organization as part of a presence service, it creates security 
risks. An attacker can use the location information to determine where the device 
and user are located, which may be of use to the attacker.
Mobile Device Security Strategy
With the threats listed in the preceding discussion in mind, we outline the principal 
elements of a mobile device security strategy. They fall into three categories: device 
security, client/server traffic security, and barrier security (Figure 7.2).
device SecuriTy A number of organizations will supply mobile devices for 
 employee use and preconfigure those devices to conform to the enterprise secu-
rity policy. However, many organizations will find it convenient or even necessary 
to adopt a bring-your-own-device (BYOD) policy that allows the personal mobile 
devices of employees to have access to corporate resources. IT managers should be 
able to inspect each device before allowing network access. IT will want to estab-
lish configuration guidelines for operating systems and applications. For example, 
‚Äúrooted‚Äù or ‚Äújail-broken‚Äù devices are not permitted on the network, and mobile

devices cannot store corporate contacts on local storage. Whether a device is owned 
by the organization or BYOD, the organization should configure the device with 
security controls, including the following:
‚ñ†
‚ñ† Enable auto-lock, which causes the device to lock if it has not been used for a 
given amount of time, requiring the user to re-enter a four-digit PIN or a pass-
word to re-activate the device.
‚ñ†
‚ñ† Enable password or PIN protection. The PIN or password is needed to unlock 
the device. In addition, it can be configured so that e-mail and other data on 
the device are encrypted using the PIN or password and can only be retrieved 
with the PIN or password.
‚ñ†
‚ñ† Avoid using auto-complete features that remember user names or passwords.
‚ñ†
‚ñ† Enable remote wipe.
‚ñ†
‚ñ† Ensure that SSL protection is enabled, if available.
‚ñ†
‚ñ† Make sure that software, including operating systems and applications, is up 
to date.
‚ñ†
‚ñ† Install antivirus software as it becomes available.
Figure 7.2 Mobile Device Security Elements
Firewall
Firewall limits 
scope of data
and application
access
Authentication
and access control
protocols used to
verify device and user
and establish limits
on access
Mobile device is
confgured with
security mechanisms and
parameters to conform to
organization security policy
Trafc is encrypted;
uses SSL or IPsec
VPN tunnel 
Authentication/
access control
server
Mobile device
confguration
server
Application/
database
server

‚ñ†
‚ñ† Either sensitive data should be prohibited from storage on the mobile device 
or it should be encrypted.
‚ñ†
‚ñ† IT staff should also have the ability to remotely access devices, wipe the device 
of all data, and then disable the device in the event of loss or theft.
‚ñ†
‚ñ† The organization may prohibit all installation of third-party applications, 
 implement whitelisting to prohibit installation of all unapproved applications, 
or implement a secure sandbox that isolates the organization‚Äôs data and appli-
cations from all other data and applications on the mobile device. Any applica-
tion that is on an approved list should be accompanied by a digital signature 
and a public-key certificate from an approved authority.
‚ñ†
‚ñ† The organization can implement and enforce restrictions on what devices can 
synchronize and on the use of cloud-based storage.
‚ñ†
‚ñ† To deal with the threat of untrusted content, security responses can include 
training of personnel on the risks inherent in untrusted content and disabling 
camera use on corporate mobile devices.
‚ñ†
‚ñ† To counter the threat of malicious use of location services, the security policy 
can dictate that such service is disabled on all mobile devices.
Traffic SecuriTy Traffic security is based on the usual mechanisms for encryption 
and authentication. All traffic should be encrypted and travel by secure means, such 
as SSL or IPv6. Virtual private networks (VPNs) can be configured so that all traffic 
between the mobile device and the organization‚Äôs network is via a VPN.
A strong authentication protocol should be used to limit the access from the 
device to the resources of the organization. Often, a mobile device has a single 
 device-specific authenticator, because it is assumed that the device has only one 
user. A preferable strategy is to have a two-layer authentication mechanism, which 
involves authenticating the device and then authenticating the user of the device.
barrier SecuriTy The organization should have security mechanisms to protect 
the network from unauthorized access. The security strategy can also include fire-
wall policies specific to mobile device traffic. Firewall policies can limit the scope 
of data and application access for all mobile devices. Similarly, intrusion detection 
and intrusion prevention systems can be configured to have tighter rules for mobile 
device traffic.
 7.3 ieee 802.11 Wireless lan OvervieW
IEEE 802 is a committee that has developed standards for a wide range of local area 
networks (LANs). In 1990, the IEEE 802 Committee formed a new working group, 
IEEE 802.11, with a charter to develop a protocol and transmission specifications 
for wireless LANs (WLANs). Since that time, the demand for WLANs at different 
frequencies and data rates has exploded. Keeping pace with this demand, the IEEE 
802.11 working group has issued an ever-expanding list of standards. Table¬†7.1 
briefly defines key terms used in the IEEE 802.11 standard.

The Wi-Fi Alliance
The first 802.11 standard to gain broad industry acceptance was 802.11b. Although 
802.11b products are all based on the same standard, there is always a concern 
whether products from different vendors will successfully interoperate. To meet 
this concern, the Wireless Ethernet Compatibility Alliance (WECA), an indus-
try consortium, was formed in 1999. This organization, subsequently renamed the 
Wi-Fi (Wireless Fidelity) Alliance, created a test suite to certify interoperability for 
802.11b products. The term used for certified 802.11b products is Wi-Fi. Wi-Fi certi-
fication has been extended to 802.11g products. The Wi-Fi Alliance has also devel-
oped a certification process for 802.11a products, called Wi-Fi5. The Wi-Fi Alliance 
is concerned with a range of market areas for WLANs, including enterprise, home, 
and hot spots.
More recently, the Wi-Fi Alliance has developed certification procedures for 
IEEE 802.11 security standards, referred to as Wi-Fi Protected Access (WPA). The 
most recent version of WPA, known as WPA2, incorporates all of the features of 
the IEEE 802.11i WLAN security specification.
IEEE 802 Protocol Architecture
Before proceeding, we need to briefly preview the IEEE 802 protocol architecture. 
IEEE 802.11 standards are defined within the structure of a layered set of protocols. 
This structure, used for all IEEE 802 standards, is illustrated in Figure 7.3.
PhySical layer The lowest layer of the IEEE 802 reference model is the physical 
layer, which includes such functions as encoding/decoding of signals and bit trans-
mission/reception. In addition, the physical layer includes a specification of the 
transmission medium. In the case of IEEE 802.11, the physical layer also defines 
frequency bands and antenna characteristics.
Access point (AP)
Any entity that has station functionality and provides access to the 
distribution system via the wireless medium for associated stations.
Basic service set (BSS)
A set of stations controlled by a single coordination function.
Coordination function
The logical function that determines when a station operating within a BSS 
is permitted to transmit and may be able to receive PDUs.
Distribution system (DS)
A system used to interconnect a set of BSSs and integrated LANs to create 
an ESS.
Extended service set (ESS)
A set of one or more interconnected BSSs and integrated LANs that 
appear as a single BSS to the LLC layer at any station associated with one 
of these BSSs.
MAC protocol data unit 
(MPDU)
The unit of data exchanged between two peer MAC entities using the 
services of the physical layer.
MAC service data unit 
(MSDU)
Information that is delivered as a unit between MAC users.
Station
Any device that contains an IEEE 802.11 conformant MAC and physical 
layer.
Table 7.1 IEEE 802.11 Terminology

media acceSS conTrol All LANs consist of collections of devices that share the 
network‚Äôs transmission capacity. Some means of controlling access to the transmis-
sion medium is needed to provide an orderly and efficient use of that capacity. This 
is the function of a media access control (MAC) layer. The MAC layer receives data 
from a higher-layer protocol, typically the Logical Link Control (LLC) layer, in the 
form of a block of data known as the MAC service data unit (MSDU). In general, 
the MAC layer performs the following functions:
‚ñ†
‚ñ† On transmission, assemble data into a frame, known as a MAC protocol data 
unit (MPDU) with address and error-detection fields.
‚ñ†
‚ñ† On reception, disassemble frame, and perform address recognition and error 
detection.
‚ñ†
‚ñ† Govern access to the LAN transmission medium.
The exact format of the MPDU differs somewhat for the various MAC proto-
cols in use. In general, all of the MPDUs have a format similar to that of Figure 7.4. 
The fields of this frame are as follows.
‚ñ†
‚ñ† MAC Control: This field contains any protocol control information needed for 
the functioning of the MAC protocol. For example, a priority level could be 
indicated here.
‚ñ†
‚ñ† Destination MAC Address: The destination physical address on the LAN for 
this MPDU.
‚ñ†
‚ñ† Source MAC Address: The source physical address on the LAN for this MPDU.
Figure 7.3 IEEE 802.11 Protocol Stack
Logical Link
Control
Medium Access
Control
Physical
Encoding/decoding of signals
Bit transmission/reception
Transmission medium
Assemble data into frame
Addressing
Error detection
Medium access
Flow control
Error control
General IEEE 802
functions
Specifc IEEE 802.11
functions
Frequency band defnition
Wireless signal encoding
Reliable data delivery
Wireless access control protocols

‚ñ†
‚ñ† MAC Service Data Unit: The data from the next higher layer.
‚ñ†
‚ñ† CRC: The cyclic redundancy check field; also known as the Frame Check 
Sequence (FCS) field. This is an error-detecting code, such as that which is 
used in other data-link control protocols. The CRC is calculated based on the 
bits in the entire MPDU. The sender calculates the CRC and adds it to the 
frame. The receiver performs the same calculation on the incoming MPDU 
and compares that calculation to the CRC field in that incoming MPDU. If 
the two values don‚Äôt match, then one or more bits have been altered in transit.
The fields preceding the MSDU field are referred to as the MAC header, and 
the field following the MSDU field is referred to as the MAC trailer. The header 
and trailer contain control information that accompany the data field and that are 
used by the MAC protocol.
logical link conTrol In most data-link control protocols, the data-link protocol 
entity is responsible not only for detecting errors using the CRC, but for recovering 
from those errors by retransmitting damaged frames. In the LAN protocol archi-
tecture, these two functions are split between the MAC and LLC layers. The MAC 
layer is responsible for detecting errors and discarding any frames that contain er-
rors. The LLC layer optionally keeps track of which frames have been successfully 
received and retransmits unsuccessful frames.
IEEE 802.11 Network Components and Architectural Model
Figure 7.5 illustrates the model developed by the 802.11 working group. The small-
est building block of a wireless LAN is a basic service set (BSS), which consists of 
wireless stations executing the same MAC protocol and competing for access to the 
same shared wireless medium. A BSS may be isolated, or it may connect to a back-
bone distribution system (DS) through an access point (AP). The AP functions as a 
bridge and a relay point. In a BSS, client stations do not communicate directly with 
one another. Rather, if one station in the BSS wants to communicate with another 
station in the same BSS, the MAC frame is first sent from the originating station to 
the AP and then from the AP to the destination station. Similarly, a MAC frame 
from a station in the BSS to a remote station is sent from the local station to the AP 
and then relayed by the AP over the DS on its way to the destination station. The 
BSS generally corresponds to what is referred to as a cell in the literature. The DS 
can be a switch, a wired network, or a wireless network.
When all the stations in the BSS are mobile stations that communicate directly 
with one another (not using an AP), the BSS is called an independent BSS (IBSS). 
An IBSS is typically an ad hoc network. In an IBSS, the stations all communicate 
directly, and no AP is involved.
Figure 7.4 General IEEE 802 MPDU Format
MAC
Control
r
elia
MAC rt 
AC h eader
M
Destination
MAC Address
Source
MAC Address
MAC Service Data Unit (MSDU)
CRC

A simple configuration is shown in Figure 7.5, in which each station belongs 
to a single BSS; that is, each station is within wireless range only of other stations 
within the same BSS. It is also possible for two BSSs to overlap geographically, so 
that a single station could participate in more than one BSS. Furthermore, the asso-
ciation between a station and a BSS is dynamic. Stations may turn off, come within 
range, and go out of range.
An extended service set (ESS) consists of two or more basic service sets 
 interconnected by a distribution system. The extended service set appears as a sin-
gle logical LAN to the logical link control (LLC) level.
IEEE 802.11 Services
IEEE 802.11 defines nine services that need to be provided by the wireless LAN to 
achieve functionality equivalent to that which is inherent to wired LANs. Table 7.2 
lists the services and indicates two ways of categorizing them.
1. The service provider can be either the station or the DS. Station services are 
implemented in every 802.11 station, including AP stations. Distribution ser-
vices are provided between BSSs; these services may be implemented in an AP 
or in another special-purpose device attached to the distribution system.
2. Three of the services are used to control IEEE 802.11 LAN access and confi-
dentiality. Six of the services are used to support delivery of MSDUs between 
stations. If the MSDU is too large to be transmitted in a single MPDU, it may 
be fragmented and transmitted in a series of MPDUs.
Figure 7.5 IEEE 802.11 Extended Service Set
STA 2
STA 3
STA4 
STA 1
STA 6
STA 7
STA 8
AP 2
AP 1
Basic Service
Set (BSS)
Basic Service
Set (BSS)
Distribution System

Following the IEEE 802.11 document, we next discuss the services in an order 
designed to clarify the operation of an IEEE 802.11 ESS network. MSDU delivery, 
which is the basic service, already has been mentioned. Services related to security 
are introduced in Section 7.4.
diSTribuTion of meSSageS WiThin a dS The two services involved with the dis-
tribution of messages within a DS are distribution and integration. Distribution is 
the primary service used by stations to exchange MPDUs when the MPDUs must 
traverse the DS to get from a station in one BSS to a station in another BSS. For 
example, suppose a frame is to be sent from station 2 (STA 2) to station 7 (STA 7) 
in Figure 7.5. The frame is sent from STA 2 to AP 1, which is the AP for this BSS. 
The AP gives the frame to the DS, which has the job of directing the frame to the 
AP associated with STA 7 in the target BSS. AP 2 receives the frame and forwards 
it to STA 7. How the message is transported through the DS is beyond the scope of 
the IEEE 802.11 standard.
If the two stations that are communicating are within the same BSS, then the 
distribution service logically goes through the single AP of that BSS.
The integration service enables transfer of data between a station on an IEEE 
802.11 LAN and a station on an integrated IEEE 802.x LAN. The term integrated 
refers to a wired LAN that is physically connected to the DS and whose stations 
may be logically connected to an IEEE 802.11 LAN via the integration service. The 
integration service takes care of any address translation and media conversion logic 
required for the exchange of data.
aSSociaTion-relaTed ServiceS The primary purpose of the MAC layer is to 
transfer MSDUs between MAC entities; this purpose is fulfilled by the distribu-
tion service. For that service to function, it requires information about stations 
within the ESS that is provided by the association-related services. Before the 
 distribution  service can deliver data to or accept data from a station, that sta-
tion must be  associated. Before looking at the concept of association, we need 
Service
Provider
Used to support
Association
Distribution system
MSDU delivery
Authentication
Station
LAN access and security
Deauthentication
Station
LAN access and security
Disassociation
Distribution system
MSDU delivery
Distribution
Distribution system
MSDU delivery
Integration
Distribution system
MSDU delivery
MSDU delivery
Station
MSDU delivery
Privacy
Station
LAN access and security
Reassociation
Distribution system
MSDU delivery
Table 7.2 IEEE 802.11 Services

to describe the concept of mobility. The standard defines three transition types, 
based on mobility:
‚ñ†
‚ñ† No transition: A station of this type is either stationary or moves only within 
the direct communication range of the communicating stations of a single BSS.
‚ñ†
‚ñ† BSS transition: This is defined as a station movement from one BSS to another 
BSS within the same ESS. In this case, delivery of data to the station requires that 
the addressing capability be able to recognize the new location of the station.
‚ñ†
‚ñ† ESS transition: This is defined as a station movement from a BSS in one ESS 
to a BSS within another ESS. This case is supported only in the sense that 
the station can move. Maintenance of upper-layer connections supported by 
802.11 cannot be guaranteed. In fact, disruption of service is likely to occur.
To deliver a message within a DS, the distribution service needs to know where 
the destination station is located. Specifically, the DS needs to know the identity of 
the AP to which the message should be delivered in order for that message to reach 
the destination station. To meet this requirement, a station must maintain an asso-
ciation with the AP within its current BSS. Three services relate to this requirement:
‚ñ†
‚ñ† Association: Establishes an initial association between a station and an AP. 
Before a station can transmit or receive frames on a wireless LAN, its iden-
tity and address must be known. For this purpose, a station must establish an 
 association with an AP within a particular BSS. The AP can then communicate 
this information to other APs within the ESS to facilitate routing and delivery 
of addressed frames.
‚ñ†
‚ñ† Reassociation: Enables an established association to be transferred from one 
AP to another, allowing a mobile station to move from one BSS to another.
‚ñ†
‚ñ† Disassociation: A notification from either a station or an AP that an existing 
association is terminated. A station should give this notification before leaving 
an ESS or shutting down. However, the MAC management facility protects 
itself against stations that disappear without notification.
 7.4 ieee 802.11i Wireless lan security
There are two characteristics of a wired LAN that are not inherent in a wireless¬†LAN.
1. In order to transmit over a wired LAN, a station must be physically connected 
to the LAN. On the other hand, with a wireless LAN, any station within radio 
range of the other devices on the LAN can transmit. In a sense, there is a form 
of authentication with a wired LAN in that it requires some positive and pre-
sumably observable action to connect a station to a wired LAN.
2. Similarly, in order to receive a transmission from a station that is part of a 
wired LAN, the receiving station also must be attached to the wired LAN. 
On the other hand, with a wireless LAN, any station within radio range can 
 receive. Thus, a wired LAN provides a degree of privacy, limiting reception of 
data to stations connected to the LAN.

These differences between wired and wireless LANs suggest the increased 
need for robust security services and mechanisms for wireless LANs. The original 
802.11 specification included a set of security features for privacy and authenti-
cation that were quite weak. For privacy, 802.11 defined the Wired Equivalent 
Privacy (WEP) algorithm. The privacy portion of the 802.11 standard contained 
major weaknesses. Subsequent to the development of WEP, the 802.11i task 
group has developed a set of capabilities to address the WLAN security issues. 
In order to accelerate the introduction of strong security into WLANs, the Wi-Fi 
Alliance promulgated Wi-Fi Protected Access (WPA) as a Wi-Fi standard. WPA 
is a set of security mechanisms that eliminates most 802.11 security issues and 
was based on the current state of the 802.11i standard. The final form of the 
802.11i standard is referred to as Robust Security Network (RSN). The Wi-Fi 
Alliance certifies vendors in compliance with the full 802.11i specification under 
the WPA2 program.
The RSN specification is quite complex, and occupies 145 pages of the 2012 
IEEE 802.11 standard. In this section, we provide an overview.
IEEE 802.11i Services
The 802.11i RSN security specification defines the following services.
‚ñ†
‚ñ† Authentication: A protocol is used to define an exchange between a user and 
an AS that provides mutual authentication and generates temporary keys to 
be used between the client and the AP over the wireless link.
‚ñ†
‚ñ† Access control:1 This function enforces the use of the authentication function, 
routes the messages properly, and facilitates key exchange. It can work with a 
variety of authentication protocols.
‚ñ†
‚ñ† Privacy with message integrity: MAC-level data (e.g., an LLC PDU) are 
 encrypted along with a message integrity code that ensures that the data have 
not been altered.
Figure 7.6a indicates the security protocols used to support these services, 
while Figure 7.6b lists the cryptographic algorithms used for these services.
IEEE 802.11i Phases of Operation
The operation of an IEEE 802.11i RSN can be broken down into five distinct phases 
of operation. The exact nature of the phases will depend on the configuration and 
the end points of the communication. Possibilities include (see Figure 7.5):
1. Two wireless stations in the same BSS communicating via the access point 
(AP) for that BSS.
2. Two wireless stations (STAs) in the same ad hoc IBSS communicating directly 
with each other.
1In this context, we are discussing access control as a security function. This is a different function than 
media access control (MAC) as described in Section 7.3. Unfortunately, the literature and the standards 
use the term access control in both contexts.

3. Two wireless stations in different BSSs communicating via their respective 
APs across a distribution system.
4. A wireless station communicating with an end station on a wired network via 
its AP and the distribution system.
IEEE 802.11i security is concerned only with secure communication between 
the STA and its AP. In case 1 in the preceding list, secure communication is assured 
if each STA establishes secure communications with the AP. Case 2 is similar, with 
the AP functionality residing in the STA. For case 3, security is not provided across 
the distribution system at the level of IEEE 802.11, but only within each BSS. End-
to-end security (if required) must be provided at a higher layer. Similarly, in case 4, 
security is only provided between the STA and its AP.
Figure 7.6 Elements of IEEE 802.11i
Access Control
Services
Protocols
Services
Algorithms
IEEE 802.1
Port-based
Access Control
Extensible
Authentication
Protocol (EAP)
Authentication
and Key
Generation
(a) Services and protocols
Confdentiality, Data
Origin Authentication
and Integrity and
Replay Protection
TKIP
CCMP
Robust Security Network (RSN)
Confdentiality
TKIP
(Michael
MIC)
CCM
(AES-
CBC-
MAC)
CCM
(AES-
CTR)
NIST
Key
Wrap
HMAC-
MD5
HMAC-
SHA-1
Integrity and
Data Origin
Authentication
(b) Cryptographic algorithms
Key
Generation
TKIP
(RC4)
Robust Security Network (RSN)
HMAC-
SHA-1
RFC
1750
CBC-MAC = Cipher Block Chaining Message Authentication Code (MAC)
CCM 
= Counter Mode with Cipher Block Chaining Message Authentication Code 
CCMP 
= Counter Mode with Cipher Block Chaining MAC Protocol
TKIP 
= Temporal Key Integrity Protocol

With these considerations in mind, Figure 7.7 depicts the five phases of op-
eration for an RSN and maps them to the network components involved. One new 
component is the authentication server (AS). The rectangles indicate the exchange 
of sequences of MPDUs. The five phases are defined as follows.
‚ñ†
‚ñ† Discovery: An AP uses messages called Beacons and Probe Responses to ad-
vertise its IEEE 802.11i security policy. The STA uses these to identify an AP 
for a WLAN with which it wishes to communicate. The STA associates with 
the AP, which it uses to select the cipher suite and authentication mechanism 
when the Beacons and Probe Responses present a choice.
‚ñ†
‚ñ† Authentication: During this phase, the STA and AS prove their identities to 
each other. The AP blocks non-authentication traffic between the STA and AS 
until the authentication transaction is successful. The AP does not participate 
in the authentication transaction other than forwarding traffic between the 
STA and AS.
‚ñ†
‚ñ† Key generation and distribution: The AP and the STA perform several opera-
tions that cause cryptographic keys to be generated and placed on the AP and 
the STA. Frames are exchanged between the AP and STA only.
‚ñ†
‚ñ† Protected data transfer: Frames are exchanged between the STA and the end 
station through the AP. As denoted by the shading and the encryption module 
icon, secure data transfer occurs between the STA and the AP only; security is 
not provided end-to-end.
Figure 7.7 IEEE 802.11i Phases of Operation
Phase 1 - Discovery
STA
AP
AS
End Station
Phase 5 - Connection Termination
Phase 3 - Key Management
Phase 4 - Protected Data Transfer
Phase 2 - Authentication

‚ñ†
‚ñ† Connection termination: The AP and STA exchange frames. During this phase, 
the secure connection is torn down and the connection is restored to the origi-
nal state.
Discovery Phase
We now look in more detail at the RSN phases of operation, beginning with the 
discovery phase, which is illustrated in the upper portion of Figure 7.8. The purpose 
of this phase is for an STA and an AP to recognize each other, agree on a set of 
security capabilities, and establish an association for future communication using 
those security capabilities.
Figure 7.8  IEEE 802.11i Phases of Operation: Capability Discovery, 
Authentication, and Association
STA
AP
AS
Probe request
Station sends a request
to join network
AP sends possible
security parameter
(security capabilities set
per the security policy)
AP performs
null authentication
AP sends the associated
security parameters
Station sends a
request to perform
 null authentication
Station sends a request to
associate with AP with
security parameters
Station sets selected
security parameters
Open system
authentication request
Probe response
802.1X EAP request
Access request
(EAP request)
802.1X EAP response
Accept/EAP-success
key material
802.1X EAP success
Association request
Association response
 Open system
authentication  response
802.1X-controlled port blocked
802.1X-controlled port blocked
Extensible Authentication Protocol Exchange

SecuriTy caPabiliTieS During this phase, the STA and AP decide on specific tech-
niques in the following areas:
‚ñ†
‚ñ† Confidentiality and MPDU integrity protocols for protecting unicast traffic 
(traffic only between this STA and AP)
‚ñ†
‚ñ† Authentication method
‚ñ†
‚ñ† Cryptography key management approach
Confidentiality and integrity protocols for protecting multicast/broadcast traf-
fic are dictated by the AP, since all STAs in a multicast group must use the same 
protocols and ciphers. The specification of a protocol, along with the chosen key 
length (if variable) is known as a cipher suite. The options for the confidentiality and 
integrity cipher suite are
‚ñ†
‚ñ† WEP, with either a 40-bit or 104-bit key, which allows backward compatibility 
with older IEEE 802.11 implementations
‚ñ†
‚ñ† TKIP
‚ñ†
‚ñ† CCMP
‚ñ†
‚ñ† Vendor-specific methods
The other negotiable suite is the authentication and key management (AKM) 
suite, which defines (1) the means by which the AP and STA perform mutual au-
thentication and (2) the means for deriving a root key from which other keys may 
be generated. The possible AKM suites are
‚ñ†
‚ñ† IEEE 802.1X
‚ñ†
‚ñ† Pre-shared key (no explicit authentication takes place and mutual authentica-
tion is implied if the STA and AP share a unique secret key)
‚ñ†
‚ñ† Vendor-specific methods
mPdu exchange The discovery phase consists of three exchanges.
‚ñ†
‚ñ† Network and security capability discovery: During this exchange, STAs dis-
cover the existence of a network with which to communicate. The AP either 
periodically broadcasts its security capabilities (not shown in figure), indicated 
by RSN IE (Robust Security Network Information Element), in a specific 
channel through the Beacon frame; or responds to a station‚Äôs Probe Request 
through a Probe Response frame. A wireless station may discover available 
access points and corresponding security capabilities by either passively moni-
toring the Beacon frames or actively probing every channel.
‚ñ†
‚ñ† Open system authentication: The purpose of this frame sequence, which pro-
vides no security, is simply to maintain backward compatibility with the IEEE 
802.11 state machine, as implemented in existing IEEE 802.11 hardware. In 
essence, the two devices (STA and AP) simply exchange identifiers.
‚ñ†
‚ñ† Association: The purpose of this stage is to agree on a set of security capa-
bilities to be used. The STA then sends an Association Request frame to 
the AP. In this frame, the STA specifies one set of matching capabilities

(one¬†authentication and key management suite, one pairwise cipher suite, 
and one group-key cipher suite) from among those advertised by the AP. 
If there is no match in capabilities between the AP and the STA, the AP 
refuses the Association Request. The STA blocks it too, in case it has associ-
ated with a rogue AP or someone is inserting frames illicitly on its channel. 
As shown in Figure 7.8, the IEEE 802.1X controlled ports are blocked, and 
no user traffic goes beyond the AP. The concept of blocked ports is  explained 
subsequently.
Authentication Phase
As was mentioned, the authentication phase enables mutual authentication between 
an STA and an authentication server (AS) located in the DS. Authentication is 
designed to allow only authorized stations to use the network and to provide the 
STA with assurance that it is communicating with a legitimate network.
ieee 802.1x acceSS conTrol aPProach IEEE 802.11i makes use of another stan-
dard that was designed to provide access control functions for LANs. The standard 
is IEEE 802.1X, Port-Based Network Access Control. The authentication proto-
col that is used, the Extensible Authentication Protocol (EAP), is defined in the 
IEEE 802.1X standard. IEEE 802.1X uses the terms supplicant, authenticator, and 
authentication server (AS). In the context of an 802.11 WLAN, the first two terms 
correspond to the wireless station and the AP. The AS is typically a separate device 
on the wired side of the network (i.e., accessible over the DS) but could also reside 
directly on the authenticator.
Before a supplicant is authenticated by the AS using an authentication proto-
col, the authenticator only passes control or authentication messages between the 
supplicant and the AS; the 802.1X control channel is unblocked, but the 802.11 data 
channel is blocked. Once a supplicant is authenticated and keys are provided, the 
authenticator can forward data from the supplicant, subject to predefined access 
control limitations for the supplicant to the network. Under these circumstances, 
the data channel is unblocked.
As indicated in Figure 5.5, 802.1X uses the concepts of controlled and uncon-
trolled ports. Ports are logical entities defined within the authenticator and refer to 
physical network connections. For a WLAN, the authenticator (the AP) may have 
only two physical ports: one connecting to the DS and one for wireless communica-
tion within its BSS. Each logical port is mapped to one of these two physical ports. 
An uncontrolled port allows the exchange of PDUs between the supplicant and the 
other AS, regardless of the authentication state of the supplicant. A controlled port 
allows the exchange of PDUs between a supplicant and other systems on the LAN 
only if the current state of the supplicant authorizes such an exchange. IEEE 802.1X 
is covered in more detail in Chapter 5.
The 802.1X framework, with an upper-layer authentication protocol, fits 
nicely with a BSS architecture that includes a number of wireless stations and an 
AP. However, for an IBSS, there is no AP. For an IBSS, 802.11i provides a more 
complex solution that, in essence, involves pairwise authentication between stations 
on the IBSS.

mPdu exchange The lower part of Figure 7.8 shows the MPDU exchange dic-
tated by IEEE 802.11 for the authentication phase. We can think of authentication 
phase as consisting of the following three phases.
‚ñ†
‚ñ† Connect to AS: The STA sends a request to its AP (the one with which it has 
an association) for connection to the AS. The AP acknowledges this request 
and sends an access request to the AS.
‚ñ†
‚ñ† EAP exchange: This exchange authenticates the STA and AS to each other. 
A¬†number of alternative exchanges are possible, as explained subsequently.
‚ñ†
‚ñ† Secure key delivery: Once authentication is established, the AS generates a 
master session key (MSK), also known as the Authentication, Authorization, 
and Accounting (AAA) key and sends it to the STA. As explained subse-
quently, all the cryptographic keys needed by the STA for secure communica-
tion with its AP are generated from this MSK. IEEE 802.11i does not  prescribe 
a method for secure delivery of the MSK but relies on EAP for this. Whatever 
method is used, it involves the transmission of an MPDU containing an en-
crypted MSK from the AS, via the AP, to the AS.
eaP exchange As mentioned, there are a number of possible EAP exchanges that 
can be used during the authentication phase. Typically, the message flow between 
STA and AP employs the EAP over LAN (EAPOL) protocol, and the message 
flow between the AP and AS uses the Remote Authentication Dial In User Service 
(RADIUS) protocol, although other options are available for both STA-to-AP and 
AP-to-AS exchanges. [FRAN07] provides the following summary of the authenti-
cation exchange using EAPOL and RADIUS.
1. The EAP exchange begins with the AP issuing an EAP-Request/Identity 
frame to the STA.
2. The STA replies with an EAP-Response/Identity frame, which the AP receives 
over the uncontrolled port. The packet is then encapsulated in RADIUS over 
EAP and passed on to the RADIUS server as a RADIUS-Access-Request 
packet.
3. The AAA server replies with a RADIUS-Access-Challenge packet, which is 
passed on to the STA as an EAP-Request. This request is of the appropriate 
authentication type and contains relevant challenge information.
4. The STA formulates an EAP-Response message and sends it to the AS. The 
response is translated by the AP into a Radius-Access-Request with the re-
sponse to the challenge as a data field. Steps 3 and 4 may be repeated multiple 
times, depending on the EAP method in use. For TLS tunneling methods, it is 
common for authentication to require 10 to 20 round trips.
5. The AAA server grants access with a Radius-Access-Accept packet. The AP 
issues an EAP-Success frame. (Some protocols require confirmation of the 
EAP success inside the TLS tunnel for authenticity validation.) The controlled 
port is authorized, and the user may begin to access the network.
Note from Figure 7.8 that the AP controlled port is still blocked to general 
user traffic. Although the authentication is successful, the ports remain blocked

until the temporal keys are installed in the STA and AP, which occurs during the 
4-Way Handshake.
Key Management Phase
During the key management phase, a variety of cryptographic keys are generated 
and distributed to STAs. There are two types of keys: pairwise keys used for com-
munication between an STA and an AP and group keys used for multicast com-
munication. Figure 7.9, based on [FRAN07], shows the two key hierarchies, and 
Table¬†7.3 defines the individual keys.
Figure 7.9 IEEE 802.11i Key Hierarchies
Out-of-band path
EAP method path
Pre-shared key
EAPOL key confrmation key
EAPOL key encryption key
Temporal key
PSK
256 bits
384 bits (CCMP)
512 bits (TKIP)
128 bits (CCMP)
256 bits (TKIP)
40 bits, 104 bits (WEP)
128 bits (CCMP)
256 bits (TKIP)
256 bits
128 bits
No modifcation
Legend
Possible truncation
PRF (pseudo random
function) using
HMAC-SHA-1
128 bits
User-defned
cryptoid
EAP
authentication
Following EAP authentication
or PSK
During 4-way handshake
These keys are
components of the PTK
‚â• 256 bits
PMK
KCK
PTK
TK
EK
K
AAAK or MSK
Pairwise master key
(b) Group key hierarchy
(a) Pairwise key hierarchy
AAA key
Pairwise transient key
256 bits
Changes periodically
or if compromised
Changes based on
policy (dissociation,
deauthentication)
GMK (generated by AS)
GTK 
Group master key
Group temporal key

Abbreviation
Name
Description / Purpose
Size (bits)
Type
AAA Key
Authentication, 
Accounting, and 
Authorization Key
Used to derive the PMK. 
Used with the IEEE 
802.1X authentication 
and key management 
approach. Same as 
MMSK.
√ö 256
Key generation key, 
root key
PSK
Pre-shared Key
Becomes the PMK 
in pre-shared key 
environments.
256
Key generation key, 
root key
PMK
Pairwise Master Key
Used with other inputs to 
derive the PTK.
256
Key generation key
GMK
Group Master Key
Used with other inputs to 
derive the GTK.
128
Key generation key
PTK
Pair-wise Transient 
Key
Derived from the PMK. 
Comprises the EAPOL-
KCK, EAPOL-KEK, and 
TK and (for TKIP) the 
MIC key.
512 (TKIP)
384 (CCMP)
Composite key
TK
Temporal Key
Used with TKIP or 
CCMP to provide 
confidentiality and 
integrity protection for 
unicast user traffic.
256 (TKIP)
128 (CCMP)
Traffic key
GTK
Group Temporal Key
Derived from the 
GMK. Used to provide 
confidentiality and 
integrity protection for 
multicast/broadcast user 
traffic.
256 (TKIP)
128 (CCMP)
40,104 (WEP)
Traffic key
MIC Key
Message Integrity 
Code Key
Used by TKIP‚Äôs Michael 
MIC to provide integrity 
protection of messages.
64
Message integrity key
EAPOL-KCK
EAPOL-Key 
Confirmation Key
Used to provide integrity 
protection for key 
material distributed 
during the 4-Way 
Handshake.
128
Message integrity key
EAPOL-KEK
EAPOL-Key 
Encryption Key
Used to ensure the 
confidentiality of the 
GTK and other key 
material in the 4-Way 
Handshake.
128
Traffic key / key 
encryption key
WEP Key
Wired Equivalent 
Privacy Key
Used with WEP.
40,104
Traffic key
Table 7.3 IEEE 802.11i Keys for Data Confidentiality and Integrity Protocols

PairWiSe keyS Pairwise keys are used for communication between a pair of  devices, 
typically between an STA and an AP. These keys form a hierarchy beginning with 
a master key from which other keys are derived dynamically and used for a limited 
period of time.
At the top level of the hierarchy are two possibilities. A pre-shared key (PSK) 
is a secret key shared by the AP and a STA and installed in some fashion outside 
the scope of IEEE 802.11i. The other alternative is the master session key (MSK), 
also known as the AAAK, which is generated using the IEEE 802.1X protocol dur-
ing the authentication phase, as described previously. The actual method of key 
generation depends on the details of the authentication protocol used. In either case 
(PSK or MSK), there is a unique key shared by the AP with each STA with which 
it communicates. All the other keys derived from this master key are also unique 
between an AP and an STA. Thus, each STA, at any time, has one set of keys, as 
depicted in the hierarchy of Figure 7.9a, while the AP has one set of such keys for 
each of its STAs.
The pairwise master key (PMK) is derived from the master key. If a PSK is 
used, then the PSK is used as the PMK; if a MSK is used, then the PMK is derived 
from the MSK by truncation (if necessary). By the end of the authentication phase, 
marked by the 802.1X EAP Success message (Figure 7.8), both the AP and the STA 
have a copy of their shared PMK.
The PMK is used to generate the pairwise transient key (PTK), which in fact 
consists of three keys to be used for communication between an STA and AP after 
they have been mutually authenticated. To derive the PTK, the HMAC-SHA-1 
function is applied to the PMK, the MAC addresses of the STA and AP, and nonces 
generated when needed. Using the STA and AP addresses in the generation of the 
PTK provides protection against session hijacking and impersonation; using nonces 
provides additional random keying material.
The three parts of the PTK are as follows.
‚ñ†
‚ñ† EAP Over LAN (EAPOL) Key Confirmation Key (EAPOL-KCK): Supports 
the integrity and data origin authenticity of STA-to-AP control frames  during 
operational setup of an RSN. It also performs an access control function: 
proof-of-possession of the PMK. An entity that possesses the PMK is autho-
rized to use the link.
‚ñ†
‚ñ† EAPOL Key Encryption Key (EAPOL-KEK): Protects the confidentiality of 
keys and other data during some RSN association procedures.
‚ñ†
‚ñ† Temporal Key (TK): Provides the actual protection for user traffic.
grouP keyS Group keys are used for multicast communication in which one STA 
sends MPDU‚Äôs to multiple STAs. At the top level of the group key hierarchy is 
the group master key (GMK). The GMK is a key-generating key used with other 
inputs to derive the group temporal key (GTK). Unlike the PTK, which is gener-
ated using material from both AP and STA, the GTK is generated by the AP and 
transmitted to its associated STAs. Exactly how this GTK is generated is unde-
fined. IEEE 802.11i, however, requires that its value is computationally indistin-
guishable from random. The GTK is distributed securely using the pairwise keys

that are  already established. The GTK is changed every time a device leaves the 
network.
PairWiSe key diSTribuTion The upper part of Figure 7.10 shows the MPDU 
 exchange for distributing pairwise keys. This exchange is known as the 4-way 
 handshake. The STA and AP use this handshake to confirm the existence of the 
Figure 7.10 IEEE 802.11i Phases of Operation: 4-Way Handshake and Group Key Handshake
STA
AP
Message 1 delivers a nonce to
the STA so that it can generate
the PTK.
Message 1 delivers a new GTK to
the STA. The GTK is encrypted
before it is sent and the entire
message is integrity protected.
The AP installs the GTK.
Message 3 demonstrates to
the STA that the authenticator
is alive, ensures that the PTK is
fresh (new) and that there is no
man-in-the-middle.
Message 2 delivers another nonce to the
AP so that it can also generate the
PTK. It demonstrates to the AP that
the STA is alive, ensures that the
PTK is fresh (new) and that there is no
man-in-the-middle.
The STA decrypts the GTK
and installs it for use.
Message 2 is delivered to the 
AP. This frame serves only as
an acknowledgment to the AP.
Message 4 serves as an acknowledgment to
Message 3. It serves no cryptographic
function. This message also ensures the
reliable start of the group key handshake.
Message 2
EAPOL-key (Snonce,
Unicast, MIC)
Message 1
EAPOL-key (Anonce, Unicast) 
Message 1
EAPOL-key (GTK, MIC) 
Message 4
EAPOL-key (Unicast, MIC)
Message 2
EAPOL-key (MIC)
Message 3
EAPOL-key (Install PTK,
Unicast, MIC) 
AP‚Äôs 802.1X-controlled port blocked
AP‚Äôs 802.1X-controlled port
 unblocked for unicast trafc

PMK, verify the selection of the cipher suite, and derive a fresh PTK for the follow-
ing data session. The four parts of the exchange are as follows.
‚ñ†‚ñ†
AP S STA: Message includes the MAC address of the AP and a nonce 
(Anonce).
‚ñ†‚ñ†
STA S AP: The STA generates its own nonce (Snonce) and uses both nonces 
and both MAC addresses, plus the PMK, to generate a PTK. The STA then 
sends a message containing its MAC address and Snonce, enabling the AP to 
generate the same PTK. This message includes a message integrity code 
(MIC)2 using HMAC-MD5 or HMAC-SHA-1-128. The key used with the MIC 
is KCK.
‚ñ†‚ñ†
AP S STA: The AP is now able to generate the PTK. The AP then sends a 
message to the STA, containing the same information as in the first message, 
but this time including a MIC.
‚ñ†‚ñ†
STA S AP: This is merely an acknowledgment message, again protected by 
a MIC.
grouP key diSTribuTion For group key distribution, the AP generates a GTK and 
distributes it to each STA in a multicast group. The two-message exchange with 
each STA consists of the following:
‚ñ†‚ñ†
AP S STA: This message includes the GTK, encrypted either with RC4 or 
with AES. The key used for encryption is KEK. A MIC value is appended.
‚ñ†‚ñ†
STA S AP: The STA acknowledges receipt of the GTK. This message  includes 
a MIC value.
Protected Data Transfer Phase
IEEE 802.11i defines two schemes for protecting data transmitted in 802.11 MPDUs: 
the Temporal Key Integrity Protocol (TKIP), and the Counter Mode-CBC MAC 
Protocol (CCMP).
TkiP TKIP is designed to require only software changes to devices that are imple-
mented with the older wireless LAN security approach called Wired Equivalent 
Privacy (WEP). TKIP provides two services:
‚ñ†
‚ñ† Message integrity: TKIP adds a message integrity code (MIC) to the 802.11 
MAC frame after the data field. The MIC is generated by an algorithm, called 
Michael, that computes a 64-bit value using as input the source and destination 
MAC address values and the Data field, plus key material.
‚ñ†
‚ñ† Data confidentiality: Data confidentiality is provided by encrypting the 
MPDU plus MIC value using RC4.
2 While MAC is commonly used in cryptography to refer to a Message Authentication Code, the term 
MIC is used instead in connection with 802.11i because MAC has another standard meaning, Media 
Access Control, in networking.

The 256-bit TK (Figure 7.9) is employed as follows. Two 64-bit keys are used 
with the Michael message digest algorithm to produce a message integrity code. 
One key is used to protect STA-to-AP messages, and the other key is used to pro-
tect AP-to-STA messages. The remaining 128 bits are truncated to generate the 
RC4 key used to encrypt the transmitted data.
For additional protection, a monotonically increasing TKIP sequence coun-
ter (TSC) is assigned to each frame. The TSC serves two purposes. First, the 
TSC is included with each MPDU and is protected by the MIC to protect against 
 replay attacks. Second, the TSC is combined with the session TK to produce a 
 dynamic  encryption key that changes with each transmitted MPDU, thus making 
 cryptanalysis more difficult.
ccmP CCMP is intended for newer IEEE 802.11 devices that are equipped with 
the hardware to support this scheme. As with TKIP, CCMP provides two services:
‚ñ†
‚ñ† Message integrity: CCMP uses the cipher block chaining message authentica-
tion code (CBC-MAC), described in Chapter 3.
‚ñ†
‚ñ† Data confidentiality: CCMP uses the CTR block cipher mode of operation 
with AES for encryption. CTR is described in Chapter 2.
The same 128-bit AES key is used for both integrity and confidentiality. The 
scheme uses a 48-bit packet number to construct a nonce to prevent replay attacks.
The IEEE 802.11i Pseudorandom Function
At a number of places in the IEEE 802.11i scheme, a pseudorandom function (PRF) is 
used. For example, it is used to generate nonces, to expand pairwise keys, and to gen-
erate the GTK. Best security practice dictates that different pseudorandom number 
streams be used for these different purposes. However, for implementation  efficiency, 
we would like to rely on a single pseudorandom number generator function.
The PRF is built on the use of HMAC-SHA-1 to generate a pseudorandom 
bit stream. Recall that HMAC-SHA-1 takes a message (block of data) and a key of 
length at least 160 bits and produces a 160-bit hash value. SHA-1 has the property 
that the change of a single bit of the input produces a new hash value with no appar-
ent connection to the preceding hash value. This property is the basis for pseudo-
random number generation.
The IEEE 802.11i PRF takes four parameters as input and produces the  desired 
number of random bits. The function is of the form PRF(K, A, B, Len), where
K = a secret key
A = a text string specific to the application (e.g., nonce generation or pairwise
key expansion)
B = some data specific to each case
Len = desired number of pseudorandom bits
For example, for the pairwise transient key for CCMP:
PTK = PRF (PMK, ‚ÄúPairwise key expansion‚Äù, min (AP-
Addr, STA-Addr) || max (AP-Addr, STA-Addr) || min
(Anonce, Snonce) || max (Anonce, Snonce), 384)

So, in this case, the parameters are
K = PMK
A = the text string ‚ÄúPairwise key expansion‚Äù
B = a sequence of bytes formed by concatenating the two MAC addresses  
and the two nonces
Len = 384 bits
Similarly, a nonce is generated by
Nonce = PRF (Random Number, ‚ÄúInitCounter‚Äù, MAC || Time, 256)
where Time is a measure of the network time known to the nonce generator. 
The group temporal key is generated by
GTK = PRF (GMK, ‚ÄúGroup key expansion‚Äù, MAC || Gnonce, 256)
Figure 7.11 illustrates the function PRF(K, A, B, Len). The parameter K 
serves as the key input to HMAC. The message input consists of four items concat-
enated together: the parameter A, a byte with value 0, the parameter B, and a coun-
ter i. The counter is initialized to 0. The HMAC algorithm is run once, producing 
a 160-bit hash value. If more bits are required, HMAC is run again with the same 
inputs, except that i is incremented each time until the necessary number of bits is 
generated. We can express the logic as
PRF (K, A, B, Len)
 R 
S
 null string
 for i 
S
 0 to ((Len + 159)/160 ‚àí 1) do
 R 
S
 R || HMAC-SHA-1 (K, A || 0 || B || i)
 Return Truncate-to-Len (R, Len)
Figure 7.11 IEEE 802.11i Pseudorandom Function
HMAC-SHA-1
| |
K
A
0
B
i
R = HMAC-SHA-1(K, A || 0 || B || i)
+ 1

7.5 Key terMs, revieW QuestiOns, anD PrObleMs
Key Terms 
4-way handshake
access point (AP)
basic service set (BSS)
Counter Mode-CBC MAC 
Protocol (CCMP)
distribution system (DS)
extended service set (ESS)
group keys
IEEE 802.1X
IEEE 802.11
IEEE 802.11i
independent BSS (IBSS)
logical link control (LLC)
media access control (MAC)
MAC protocol data unit 
(MPDU)
MAC service data unit 
(MSDU)
message integrity code  
(MIC)
Michael
pairwise keys
pseudorandom function
Robust Security Network  
(RSN)
Temporal Key Integrity 
Protocol (TKIP)
Wi-Fi
Wi-Fi Protected Access 
(WPA)
Wired Equivalent Privacy 
(WEP)
Wireless LAN (WLAN)
Review Questions 
 
7.1 
What is the basic building block of an 802.11 WLAN?
 
7.2 
List and briefly define threats to a wireless network.
 
7.3 
List and briefly define IEEE 802.11 services.
 
7.4 
List some security threats related to mobile devices.
 
7.5 
How is the concept of an association related to that of mobility?
 
7.6 
What security areas are addressed by IEEE 802.11i?
 
7.7 
Briefly describe the five IEEE 802.11i phases of operation.
 
7.8 
What is the difference between TKIP and CCMP?
Problems 
 
7.1 
In IEEE 802.11, open system authentication simply consists of two communications. 
An authentication is requested by the client, which contains the station ID (typically 
the MAC address). This is followed by an authentication response from the AP/router 
containing a success or failure message. An example of when a failure may occur is if 
the client‚Äôs MAC address is explicitly excluded in the AP/router configuration.
a. What are the benefits of this authentication scheme?
b. What are the security vulnerabilities of this authentication scheme?
 
7.2 
Prior to the introduction of IEEE 802.11i, the security scheme for IEEE 802.11 was 
Wired Equivalent Privacy (WEP). WEP assumed all devices in the network share a 
secret key. The purpose of the authentication scenario is for the STA to prove that 
it possesses the secret key. Authentication proceeds as shown in Figure 7.12. The 
STA sends a message to the AP requesting authentication. The AP issues a chal-
lenge, which is a sequence of 128 random bytes sent as plaintext. The STA encrypts 
the challenge with the shared key and returns it to the AP. The AP decrypts the 
 incoming value and compares it to the challenge that it sent. If there is a match, the 
AP confirms that authentication has succeeded.
a. What are the benefits of this authentication scheme?
b. This authentication scheme is incomplete. What is missing and why is this impor-
tant? Hint: The addition of one or two messages would fix the problem.
c. What is a cryptographic weakness of this scheme?

7.3 
For WEP, data integrity and data confidentiality are achieved using the RC4 stream 
encryption algorithm. The transmitter of an MPDU performs the following steps, 
referred to as encapsulation:
1. The transmitter selects an initial vector (IV) value.
2. The IV value is concatenated with the WEP key shared by transmitter and receiver 
to form the seed, or key input, to RC4.
3. A 32-bit cyclic redundancy check (CRC) is computed over all the bits of the MAC 
data field and appended to the data field. The CRC is a common error-detection 
code used in data link control protocols. In this case, the CRC serves as a integrity 
check value (ICV).
4. The result of step 3 is encrypted using RC4 to form the ciphertext block.
5. The plaintext IV is prepended to the ciphertext block to form the encapsulated 
MPDU for transmission.
a. Draw a block diagram that illustrates the encapsulation process.
b. Describe the steps at the receiver end to recover the plaintext and perform the 
integrity check.
c. Draw a block diagram that illustrates part b.
 
7.4 
A potential weakness of the CRC as an integrity check is that it is a linear function. 
This means that you can predict which bits of the CRC are changed if a single bit of 
the message is changed. Furthermore, it is possible to determine which combination 
of bits could be flipped in the message so that the net result is no change in the CRC. 
Thus, there are a number of combinations of bit flippings of the plaintext message 
that leave the CRC unchanged, so message integrity is defeated. However, in WEP, 
if an attacker does not know the encryption key, the attacker does not have access to 
the plaintext, only to the ciphertext block. Does this mean that the ICV is protected 
from the bit flipping attack? Explain.
Figure 7.12 WEP Authentication; refer to Problem 7.2
STA
AP
Request
Station sends a request
for authentication
AP sends challenge message
containing 128-bit random
number
AP decrypts challenge response.
If match, send authentication
success message
Station responds
with encrypted version
of challenge number
Response
Challenge
 Success

8.1 
Internet Mail Architecture
8.2 
E-mail Formats
8.3 
E-mail Threats and Comprehensive E-mail Security
8.4 
S/MIME
8.5 
Pretty Good Privacy
8.6 
DNSSEC
8.7 
DNS-Based Authentication of Named Entities
8.8 
Sender Policy Framework
8.9 
DomainKeys Identified Mail
8.10 Domain-Based Message Authentication, Reporting, and Conformance
8.11 Key Terms, Review Questions, and Problems
Chapter
Electronic Mail Security

8.1 Internet MaIl archItecture
For an understanding of the topics in this chapter, it is useful to have a basic grasp of 
the Internet mail architecture, which is currently defined in RFC 5598 (Internet Mail 
Architecture, July 2009). This section provides an overview of the basic concepts.
E-mail Components
At its most fundamental level, the Internet mail architecture consists of a user world 
in the form of Message User Agents (MUA), and the transfer world, in the form 
of the Message Handling Service (MHS), which is composed of Message Transfer 
Agents (MTA). The MHS accepts a message from one user and delivers it to one 
or more other users, creating a virtual MUA-to-MUA exchange environment. This 
architecture involves three types of interoperability. One is directly between users: 
messages must be formatted by the MUA on behalf of the message author so that 
learnIng ObjectIves
After studying this chapter, you should be able to:
‚óÜ‚óÜ
Summarize the key functional components of the Internet mail  architecture.
‚óÜ‚óÜ
Explain the basic functionality of SMTP, POP3, and IMAP.
‚óÜ‚óÜ
Explain the need for MIME as an enhancement to ordinary e-mail.
‚óÜ‚óÜ
Describe the key elements of MIME.
‚óÜ‚óÜ
Understand the functionality of S/MIME and the security threats it  addresses.
‚óÜ‚óÜ
Understand the basic mechanisms of STARTTLS and its role in e-mail 
 security.
‚óÜ‚óÜ
Understand the basic mechanisms of DANE and its role in e-mail security.
‚óÜ‚óÜ
Understand the basic mechanisms of SPF and its role in e-mail security.
‚óÜ‚óÜ
Understand the basic mechanisms of DKIM and its role in e-mail security.
‚óÜ‚óÜ
Understand the basic mechanisms of DMARC and its role in e-mail  security.
In virtually all distributed environments, electronic mail is the most heavily used 
 network-based application. Users expect to be able to, and do, send e-mail to oth-
ers who are connected directly or indirectly to the Internet, regardless of host operat-
ing system or communications suite. With the explosively growing reliance on e-mail, 
there grows a demand for authentication and confidentiality services. Two schemes 
stand out as approaches that enjoy widespread use: Pretty Good Privacy (PGP) and  
S/MIME. Both are examined in this chapter. This chapter concludes with a discussion 
of DomainKeys Identified Mail.

the message can be displayed to the message recipient by the destination MUA. 
There are also interoperability requirements between the MUA and the MHS‚Äî
first when a message is posted from an MUA to the MHS and later when it is deliv-
ered from the MHS to the destination MUA. Interoperability is required among the 
MTA components along the transfer path through the MHS.
Figure 8.1 illustrates the key components of the Internet mail architecture, 
which include the following.
‚óÜ
‚ñ† Message User Agent (MUA): Operates on behalf of user actors and user 
 applications. It is their representative within the e-mail service. Typically, this 
function is housed in the user‚Äôs computer and is referred to as a client e-mail 
program or a local network e-mail server. The author MUA formats a message 
and performs initial submission into the MHS via a MSA. The recipient MUA 
processes received mail for storage and/or display to the recipient user.
‚óÜ
‚ñ† Mail Submission Agent (MSA): Accepts the message submitted by an MUA 
and enforces the policies of the hosting domain and the requirements of 
Internet standards. This function may be located together with the MUA or 
Figure 8.1  Function Modules and Standardized Protocols Used between them 
in the Internet Mail Architecture
Message user
agent (MUA)
Message
author
Message
recipient
ESMTP
(Submission)
SMTP
SMTP
SMTP
ESMTP
(Submission)
(SMTP,
local)
(IMAP, POP,
local)
Mail submission
agent (MSA)
Message transfer
agent (MTA)
Message transfer
agent (MTA)
MESSAGE HANDLING
SYSTEM (MHS)
Message transfer
agent (MTA)
Mail delivery
agent (MDA)
Message store
(MS)
Message user
agent (MUA)

as a separate functional model. In the latter case, the Simple Mail Transfer 
Protocol (SMTP) is used between the MUA and the MSA.
‚óÜ
‚ñ† Message Transfer Agent (MTA): Relays mail for one application-level hop. It 
is like a packet switch or IP router in that its job is to make routing assessments 
and to move the message closer to the recipients. Relaying is performed by a 
sequence of MTAs until the message reaches a destination MDA. An MTA 
also adds trace information to the message header. SMTP is used  between 
MTAs and between an MTA and an MSA or MDA.
‚óÜ
‚ñ† Mail Delivery Agent (MDA): Responsible for transferring the message from 
the MHS to the MS.
‚óÜ
‚ñ† Message Store (MS): An MUA can employ a long-term MS. An MS can be 
located on a remote server or on the same machine as the MUA. Typically, 
an MUA retrieves messages from a remote server using POP (Post Office 
Protocol) or IMAP (Internet Message Access Protocol).
Two other concepts need to be defined. An administrative management 
 domain (ADMD) is an Internet e-mail provider. Examples include a  department 
that operates a local mail relay (MTA), an IT department that operates an  enterprise 
mail relay, and an ISP that operates a public shared e-mail service. Each ADMD 
can have different operating policies and trust-based decision making. One obvi-
ous  example is the distinction between mail that is exchanged within an organiza-
tion and mail that is exchanged between independent organizations. The rules for 
 handling the two types of traffic tend to be quite different.
The Domain Name System (DNS) is a directory lookup service that provides 
a mapping between the name of a host on the Internet and its numerical address. 
DNS is discussed subsequently in this chapter.
E-mail Protocols
Two types of protocols are used for transferring e-mail. The first type is used to 
move messages through the Internet from source to destination. The protocol used 
for this purpose is SMTP, with various extensions and in some cases restrictions. The 
second type consists of protocols used to transfer messages between mail  servers, of 
which IMAP and POP are the most commonly used.
Simple mail TranSfer proTocol SMTP encapsulates an e-mail message in an 
 envelope and is used to relay the encapsulated messages from source to  destination 
through multiple MTAs. SMTP was originally specified in 1982 as RFC 821 and 
has undergone several revisions, the most current being RFC 5321 (October 2008). 
These revisions have added additional commands and introduced extensions. The 
term Extended SMTP (ESMTP) is often used to refer to these later versions of 
SMTP.
SMTP is a text-based client-server protocol where the client (e-mail sender) 
contacts the server (next-hop recipient) and issues a set of commands to tell the 
server about the message to be sent, then sending the message itself. The majority 
of these commands are ASCII text messages sent by the client and a resulting return 
code (and additional ASCII text) returned by the server.

The transfer of a message from a source to its ultimate destination can occur 
over a single SMTP client/server conversation over a single TCP connection. 
Alternatively, an SMTP server may be an intermediate relay that assumes the role 
of an SMTP client after receiving a message and then forwards that message to an 
SMTP server along a route to the ultimate destination.
The operation of SMTP consists of a series of commands and responses 
 exchanged between the SMTP sender and receiver. The initiative is with the SMTP 
sender, who establishes the TCP connection. Once the connection is established, 
the SMTP sender sends commands over the connection to the receiver. Each com-
mand consists of a single line of text, beginning with a four-letter command code 
followed in some cases by an argument field. Each command generates exactly one 
reply from the SMTP receiver. Most replies are a single-line, although multiple-line 
replies are possible. Each reply begins with a three-digit code and may be followed 
by additional information.
Figure 8.2 illustrates the SMTP exchange between a client (C) and server¬†(S). 
The interchange begins with the client establishing a TCP connection to TCP port 
25 on the server (not shown in figure). This causes the server to activate SMTP 
and send a 220 reply to the client. The HELO command identifies the sending 
domain, which the server acknowledges and accepts with a 250 reply. The SMTP 
sender is transmitting mail that originates with the user Smith@bar.com. The MAIL 
 command identifies the originator of the message. The message is addressed to 
three users on machine foo.com, namely, Jones, Green, and Brown. The client 
S: 220 foo.com Simple Mail Transfer Service Ready
C: HELO bar.com
S: 250 OK
C: MAIL FROM:<Smith@bar.com>
S: 250 OK
C: RCPT TO:<Jones@foo.com>
S: 250 OK
C: RCPT TO:<Green@foo.com>
S: 550 No such user here
C: RCPT TO:<Brown@foo.com>
S: 250 OK
C: DATA
S: 354 Start mail input; end with <crlf>.<crlf>
C: Blah blah blah¬†.¬†.¬†.¬†
C:¬†.¬†.¬†.¬†etc. etc. etc.
C: <crlf><crlf>
S: 250 OK
C: QUIT
S: 221 foo.com Service closing transmission channel
Figure 8.2 Example SMTP Transaction Scenario

identifies each of these in a separate RCPT command. The SMTP receiver indicates 
that it has mailboxes for Jones and Brown but does not have information on Green. 
Because at least one of the intended recipients has been verified, the client proceeds 
to send the text message, by first sending a DATA command to ensure the server 
is ready for the data. After the server acknowledges receipt of all the data, it issues 
a 250 OK message. Then the client issues a QUIT command and the server closes 
the connection.
A significant security-related extension for SMTP, called STARTTLS, is 
 defined in RFC 3207 (SMTP Service Extension for Secure SMTP over Transport 
Layer Security, February 2002). STARTTLS enables the addition of confidentiality 
and authentication in the exchange between SMTP agents. This gives SMTP agents 
the ability to protect some or all of their communications from eavesdroppers 
and attackers. If the client does initiate the connection over a TLS-enabled port 
(e.g.,¬†port 465 was previously used for SMTP over SSL), the server may prompt with 
a message indicating that the STARTTLS option is available. The client can then 
issue the STARTTLS command in the SMTP command stream, and the two parties 
proceed to establish a secure TLS connection. An advantage of using STARTTLS 
is that the server can offer SMTP service on a single port, rather than requiring 
separate port numbers for secure and cleartext operations. Similar mechanisms are 
available for running TLS over IMAP and POP protocols.
Historically, MUA/MSA message transfers have used SMTP. The standard 
currently preferred is SUBMISSION, defined in RFC 6409 (Message Submission 
for Mail, November 2011). Although SUBMISSION derives from SMTP, it uses a 
separate TCP port and imposes distinct requirements, such as access authorization.
mail acceSS proTocolS (pop3, imap) Post Office Protocol (POP3) allows an 
 e-mail client (user agent) to download an e-mail from an e-mail server (MTA). 
POP3 user agents connect via TCP to the server (typically port 110). The user 
agent enters a username and password (either stored internally for convenience or 
 entered each time by the user for stronger security). After authorization, the UA 
can issue POP3 commands to retrieve and delete mail.
As with POP3, Internet Mail Access Protocol (IMAP) also enables an e-mail 
client to access mail on an e-mail server. IMAP also uses TCP, with server TCP port 
143. IMAP is more complex than POP3. IMAP provides stronger authentication 
than POP3 and provides other functions not supported by POP3.
 8.2 e-MaIl FOrMats
To understand S/MIME, we need first to have a general understanding of the 
 underlying e-mail format that it uses, namely, MIME. But to understand the sig-
nificance of MIME, we need to go back to the traditional e-mail format standard, 
RFC¬†822, which is still in common use. The most recent version of this format speci-
fication is RFC 5322 (Internet Message Format, October 2008). Accordingly, this 
section first provides an introduction to these two earlier standards and then moves 
on to a discussion of S/MIME.

RFC 5322
RFC 5322 defines a format for text messages that are sent using electronic mail. It 
has been the standard for Internet-based text mail messages and remains in com-
mon use. In the RFC 5322 context, messages are viewed as having an envelope and 
contents. The envelope contains whatever information is needed to accomplish 
transmission and delivery. The contents compose the object to be delivered to the 
recipient. The RFC 5322 standard applies only to the contents. However, the con-
tent standard includes a set of header fields that may be used by the mail system to 
create the envelope, and the standard is intended to facilitate the acquisition of such 
information by programs.
The overall structure of a message that conforms to RFC 5322 is very 
 simple. A message consists of some number of header lines (the header) fol-
lowed by  unrestricted text (the body). The header is separated from the body 
by a blank line. Put differently, a message is ASCII text, and all lines up to the 
first blank line are  assumed to be header lines used by the user agent part of the 
mail system.
A header line usually consists of a keyword, followed by a colon, followed by 
the keyword‚Äôs arguments; the format allows a long line to be broken up into several 
lines. The most frequently used keywords are From, To, Subject, and Date. Here is 
an example message:
Date: October 8, 2009 2:15:49 PM EDT
From: ‚ÄúWilliam Stallings‚Äù <ws@shore.net>
Subject: The Syntax in RFC 5322
To: Smith@Other-host.com
Cc: Jones@Yet-Another-Host.com
Hello. This section begins the actual 
message body, which is delimited from the 
message heading by a blank line.
Another field that is commonly found in RFC 5322 headers is Message-ID. 
This field contains a unique identifier associated with this message.
Multipurpose Internet Mail Extensions
Multipurpose Internet Mail Extension (MIME) is an extension to the RFC 5322 
framework that is intended to address some of the problems and limitations of the 
use of Simple Mail Transfer Protocol (SMTP) or some other mail transfer protocol 
and RFC 5322 for electronic mail. RFCs 2045 through 2049 define MIME, and there 
have been a number of updating documents since then.
As justification for the use of MIME, [PARZ06] lists the following limitations 
of the SMTP/5322 scheme.

1. SMTP cannot transmit executable files or other binary objects. A number 
of schemes are in use for converting binary files into a text form that can 
be used by SMTP mail systems, including the popular UNIX UUencode/
UUdecode scheme. However, none of these is a standard or even a de facto 
standard.
2. SMTP cannot transmit text data that includes national language characters, 
because these are represented by 8-bit codes with values of 128 decimal or 
higher, and SMTP is limited to 7-bit ASCII.
3. SMTP servers may reject mail message over a certain size.
4. SMTP gateways that translate between ASCII and the character code EBCDIC 
do not use a consistent set of mappings, resulting in translation problems.
5. SMTP gateways to X.400 electronic mail networks cannot handle nontextual 
data included in X.400 messages.
6. Some SMTP implementations do not adhere completely to the SMTP 
 standards defined in RFC 821. Common problems include:
‚ÄîDeletion, addition, or reordering of carriage return and linefeed
‚ÄîTruncating or wrapping lines longer than 76 characters
‚ÄîRemoval of trailing white space (tab and space characters)
‚ÄîPadding of lines in a message to the same length
‚ÄîConversion of tab characters into multiple space characters
MIME is intended to resolve these problems in a manner that is compatible 
with existing RFC 5322 implementations.
overview The MIME specification includes the following elements.
1. Five new message header fields are defined, which may be included in an 
RFC¬†5322 header. These fields provide information about the body of the 
message.
2. A number of content formats are defined, thus standardizing representations 
that support multimedia electronic mail.
3. Transfer encodings are defined that enable the conversion of any content 
 format into a form that is protected from alteration by the mail system.
In this subsection, we introduce the five message header fields. The next two 
subsections deal with content formats and transfer encodings.
The five header fields defined in MIME are as follows:
‚óÜ
‚ñ† MIME-Version: Must have the parameter value 1.0. This field indicates that 
the message conforms to RFCs 2045 and 2046.
‚óÜ
‚ñ† Content-Type: Describes the data contained in the body with sufficient detail 
that the receiving user agent can pick an appropriate agent or mechanism to 
represent the data to the user or otherwise deal with the data in an  appropriate 
manner.

‚óÜ
‚ñ† Content-Transfer-Encoding: Indicates the type of transformation that has 
been used to represent the body of the message in a way that is acceptable for 
mail transport.
‚óÜ
‚ñ† Content-ID: Used to identify MIME entities uniquely in multiple contexts.
‚óÜ
‚ñ† Content-Description: A text description of the object with the body; this is 
useful when the object is not readable (e.g., audio data).
Any or all of these fields may appear in a normal RFC 5322 header. A compli-
ant implementation must support the MIME-Version, Content-Type, and Content-
Transfer-Encoding fields; the Content-ID and Content-Description fields are 
 optional and may be ignored by the recipient implementation.
mime conTenT TypeS The bulk of the MIME specification is concerned with 
the¬†definition of a variety of content types. This reflects the need to provide stan-
dardized ways of dealing with a wide variety of information representations in a 
multimedia environment.
Table 8.1 lists the content types specified in RFC 2046. There are seven different 
major types of content and a total of 15 subtypes. In general, a content type declares the 
general type of data, and the subtype specifies a particular format for that type of data.
Type
Subtype
Description
Text
Plain
Unformatted text; may be ASCII or ISO 8859.
Enriched
Provides greater format flexibility.
Multipart
Mixed
The different parts are independent but are to be transmitted 
together. They should be presented to the receiver in the order 
that they appear in the mail message.
Parallel
Differs from Mixed only in that no order is defined for delivering 
the parts to the receiver.
Alternative
The different parts are alternative versions of the same 
information. They are ordered in increasing faithfulness to the 
original, and the recipient‚Äôs mail system should display the ‚Äúbest‚Äù 
version to the user.
Digest
Similar to Mixed, but the default type/subtype of each part is 
message/rfc822.
Message
rfc822
The body is itself an encapsulated message that conforms to RFC 822.
Partial
Used to allow fragmentation of large mail items, in a way that is 
transparent to the recipient.
External-body
Contains a pointer to an object that exists elsewhere.
Image
jpeg
The image is in JPEG format, JFIF encoding.
gif
The image is in GIF format.
Video
mpeg
MPEG format.
Audio
Basic
Single-channel 8-bit ISDN m-law encoding at a sample rate of  
8 kHz.
Application
PostScript
Adobe Postscript format.
octet-stream
General binary data consisting of 8-bit bytes.
Table 8.1 MIME Content Types

For the text type of body, no special software is required to get the full  meaning 
of the text aside from support of the indicated character set. The primary subtype is 
plain text, which is simply a string of ASCII characters or ISO 8859 characters. The 
enriched subtype allows greater formatting flexibility.
The multipart type indicates that the body contains multiple, independent 
parts. The Content-Type header field includes a parameter (called boundary) that 
defines the delimiter between body parts. This boundary should not appear in 
any parts of the message. Each boundary starts on a new line and consists of two 
 hyphens followed by the boundary value. The final boundary, which indicates the 
end of the last part, also has a suffix of two hyphens. Within each part, there may be 
an optional ordinary MIME header.
Here is a simple example of a multipart message containing two parts‚Äîboth 
consisting of simple text (taken from RFC 2046):
From: Nathaniel Borenstein <nsb@bellcore.com>
To: Ned Freed <ned@innosoft.com>
Subject: Sample message
MIME-Version: 1.0
Content-type: multipart/mixed; boundary=‚Äúsimple boundary‚Äù
This is the preamble. It is to be ignored, though it is a 
handy place for mail composers to include an explanatory 
note to non-MIME conformant readers.
‚Äîsimple boundary
This is implicitly typed plain ASCII text. It does NOT end 
with a linebreak.
‚Äîsimple boundary
Content-type: text/plain; charset=us-ascii
This is explicitly typed plain ASCII text. It DOES end 
with a linebreak.
‚Äîsimple boundary‚Äî
This is the epilogue. It is also to be ignored.
There are four subtypes of the multipart type, all of which have the same 
 overall syntax. The multipart/mixed subtype is used when there are multiple inde-
pendent body parts that need to be bundled in a particular order. For the multipart/
parallel subtype, the order of the parts is not significant. If the recipient‚Äôs system is 
appropriate, the multiple parts can be presented in parallel. For example, a picture 
or text part could be accompanied by a voice commentary that is played while the 
picture or text is displayed.
For the multipart/alternative subtype, the various parts are different represen-
tations of the same information. The following is an example:
From: Nathaniel Borenstein <nsb@bellcore.com>
To: Ned Freed <ned@innosoft.com>
Subject: Formatted text mail

MIME-Version: 1.0
Content-Type: multipart/alternative; 
boundary=boundary42
‚Äîboundary42
Content-Type: text/plain; charset=us-ascii
. . . plain text version of message goes here. . . .
‚Äîboundary42
Content-Type: text/enriched
. . . RFC 1896 text/enriched version of same message 
goes here¬†. . . 
‚Äîboundary42‚Äî
In this subtype, the body parts are ordered in terms of increasing preference. 
For this example, if the recipient system is capable of displaying the message in the 
text/enriched format, this is done; otherwise, the plain text format is used.
The multipart/digest subtype is used when each of the body parts is inter-
preted as an RFC 5322 message with headers. This subtype enables the construction 
of a message whose parts are individual messages. For example, the moderator of a 
group might collect e-mail messages from participants, bundle these messages, and 
send them out in one encapsulating MIME message.
The message type provides a number of important capabilities in MIME. 
The message/rfc822 subtype indicates that the body is an entire message, including 
header and body. Despite the name of this subtype, the encapsulated message may 
be not only a simple RFC 5322 message, but also any MIME message.
The message/partial subtype enables fragmentation of a large message into a 
number of parts, which must be reassembled at the destination. For this subtype, 
three parameters are specified in the Content-Type: Message/Partial field: an id 
common to all fragments of the same message, a sequence number unique to each 
fragment, and the total number of fragments.
The message/external-body subtype indicates that the actual data to be con-
veyed in this message are not contained in the body. Instead, the body contains the 
information needed to access the data. As with the other message types, the mes-
sage/external-body subtype has an outer header and an encapsulated message with 
its own header. The only necessary field in the outer header is the Content-Type 
field, which identifies this as a message/external-body subtype. The inner header is 
the message header for the encapsulated message. The Content-Type field in the 
outer header must include an access-type parameter, which indicates the method of 
access, such as FTP (file transfer protocol).
The application type refers to other kinds of data, typically either uninter-
preted binary data or information to be processed by a mail-based application.
mime TranSfer encodingS The other major component of the MIME specifica-
tion, in addition to content type specification, is a definition of transfer encodings 
for message bodies. The objective is to provide reliable delivery across the largest 
range of environments.

The MIME standard defines two methods of encoding data. The Content-
Transfer-Encoding field can actually take on six values, as listed in Table 8.2. 
However, three of these values (7-bit, 8-bit, and binary) indicate that no encod-
ing has been done but provide some information about the nature of the data. For 
SMTP transfer, it is safe to use the 7-bit form. The 8-bit and binary forms may be 
 usable in other mail transport contexts. Another Content-Transfer-Encoding value 
is x-token, which indicates that some other encoding scheme is used for which 
a name is to be supplied. This could be a vendor-specific or application-specific 
scheme. The two actual encoding schemes defined are quoted-printable and base64. 
Two schemes are defined to provide a choice between a transfer technique that is 
essentially human readable and one that is safe for all types of data in a way that is 
reasonably compact.
The quoted-printable transfer encoding is useful when the data consists largely 
of octets that correspond to printable ASCII characters. In essence, it represents 
nonsafe characters by the hexadecimal representation of their code and introduces 
reversible (soft) line breaks to limit message lines to 76 characters.
The base64 transfer encoding, also known as radix-64 encoding, is a common 
one for encoding arbitrary binary data in such a way as to be invulnerable to the 
processing by mail-transport programs. It is also used in PGP and is described in 
Appendix H.
a mulTiparT example Figure 8.3, taken from RFC 2045, is the outline of a com-
plex multipart message. The message has five parts to be displayed serially: two 
introductory plain text parts, an embedded multipart message, a richtext part, and 
a closing encapsulated text message in a non-ASCII character set. The embedded 
multipart message has two parts to be displayed in parallel: a picture and an audio 
fragment.
canonical form An important concept in MIME and S/MIME is that of canonical 
form. Canonical form is a format, appropriate to the content type, that is standard-
ized for use between systems. This is in contrast to native form, which is a format that 
may be peculiar to a particular system. RFC 2049 defines these two forms as follows:
‚óÜ
‚ñ† Native form: The body to be transmitted is created in the system‚Äôs native for-
mat. The native character set is used and, where appropriate, local end-of-line 
conventions are used as well. The body may be any format that corresponds to 
7 bit
The data are all represented by short lines of ASCII characters.
8 bit
The lines are short, but there may be non-ASCII characters (octets with the 
high-order bit set).
binary
Not only may non-ASCII characters be present but the lines are not necessarily 
short enough for SMTP transport.
quoted-printable
Encodes the data in such a way that if the data being encoded are mostly ASCII 
text, the encoded form of the data remains largely recognizable by humans.
base64
Encodes data by mapping 6-bit blocks of input to 8-bit blocks of output, all of 
which are printable ASCII characters.
x-token
A named nonstandard encoding.
Table 8.2 MIME Transfer Encodings

MIME-Version: 1.0
From: Nathaniel Borenstein <nsb@bellcore.com>
To: Ned Freed <ned@innosoft.com>
Subject: A multipart example
Content-Type: multipart/mixed;
boundary=unique-boundary-1
This is the preamble area of a multipart message. Mail readers that 
 understand multipart format should ignore this preamble. If you are  reading 
this text, you might want to consider changing to a mail reader that 
 understands how to properly display multipart messages.
‚Äîunique-boundary-1
¬†.¬†.¬†.¬†Some text appears here¬†.¬†.¬†.¬†
[Note that the preceding blank line means no header fields were given and 
this is text, with charset US ASCII. It could have been done with explicit 
typing as in the next part.]
‚Äîunique-boundary-1
Content-type: text/plain; charset=US-ASCII
This could have been part of the previous part, but illustrates explicit 
versus implicit typing of body parts.
‚Äîunique-boundary-1
Content-Type: multipart/parallel; boundary=unique-boundary-2
‚Äîunique-boundary-2
Content-Type: audio/basic
Content-Transfer-Encoding: base64
¬†.¬†.¬†.¬†base64-encoded 8000 Hz single-channel mu-law-format audio data goes 
here¬†.¬†.¬†.¬†.
‚Äîunique-boundary-2
Content-Type: image/jpeg
Content-Transfer-Encoding: base64
¬†.¬†.¬†.¬†base64-encoded image data goes here¬†.¬†.¬†.¬†.
‚Äîunique-boundary-2‚Äî
‚Äîunique-boundary-1
Content-type: text/enriched
This is richtext. as defined in RFC 1896
Isn‚Äôt it cool?
‚Äîunique-boundary-1
Content-Type: message/rfc822
From: (mailbox in US-ASCII)
To: (address in US-ASCII)
Subject: (subject in US-ASCII)
Content-Type: Text/plain; charset=ISO-8859-1
Content-Transfer-Encoding: Quoted-printable
¬†.¬†.¬†.¬†Additional text in ISO-8859-1 goes here¬†.¬†.¬†.¬†
‚Äîunique-boundary-1‚Äî
Figure 8.3 Example MIME Message Structure

the local model for the representation of some form of information. Examples 
include a UNIX-style text file, or a Sun raster image, or a VMS indexed file, and 
audio data in a system-dependent format stored only in memory. In  essence, 
the data are created in the native form that corresponds to the type specified 
by the media type.
‚óÜ
‚ñ† Canonical form: The entire body, including out-of-band information such as 
record lengths and possibly file attribute information, is converted to a univer-
sal canonical form. The specific media type of the body as well as its associated 
attributes dictates the nature of the canonical form that is used. Conversion to 
the proper canonical form may involve character set conversion, transforma-
tion of audio data, compression, or various other operations specific to the 
various media types.
 8.3 e-MaIl threats and cOMprehensIve e-MaIl securIty
For both organizations and individuals, e-mail is both pervasive and especially vul-
nerable to a wide range of security threats. In general terms, e-mail security threats 
can be classified as follows:
‚óÜ
‚ñ† Authenticity-related threats: Could result in unauthorized access to an enter-
prise‚Äôs e-mail system.
‚óÜ
‚ñ† Integrity-related threats: Could result in unauthorized modification of e-mail 
content.
‚óÜ
‚ñ† Confidentiality-related threats: Could result in unauthorized disclosure of 
 sensitive information.
‚óÜ
‚ñ† Availability-related threats: Could prevent end users from being able to send 
or receive e-mail.
A useful list of specific e-mail threats, together with approaches to mitigation, 
is provided in SP 800-177 (Trustworthy E-mail, September 2015) and is shown in 
Table 8.3.
SP 800-177 recommends use of a variety of standardized protocols as a means 
for countering these threats. These include:
‚óÜ
‚ñ† STARTTLS: An SMTP security extension that provides authentication, integ-
rity, non-repudiation (via digital signatures) and confidentiality (via encryp-
tion) for the entire SMTP message by running SMTP over TLS.
‚óÜ
‚ñ† S/MIME: Provides authentication, integrity, non-repudiation (via digital 
 signatures) and confidentiality (via encryption) of the message body carried 
in SMTP messages.
‚óÜ
‚ñ† DNS Security Extensions (DNSSEC): Provides authentication and integ-
rity protection of DNS data, and is an underlying tool used by various e-mail 
 security protocols.
‚óÜ
‚ñ† DNS-based Authentication of Named Entities (DANE): Is designed to over-
come problems in the certificate authority (CA) system by providing an 
 alternative channel for authenticating public keys based on DNSSEC, with the

Threat
Impact on Purported 
Sender
Impact on Receiver
Mitigation
E-mail sent by 
unauthorized MTA in 
enterprise (e.g., malware 
botnet)
Loss of reputation, valid 
e-mail from enterprise 
may be blocked as 
possible spam/phishing 
attack.
UBE and/or e-mail 
containing malicious 
links may be delivered 
into user inboxes.
Deployment of domain-
based authentication 
techniques. Use of 
digital signatures over 
e-mail.
E-mail message sent 
using spoofed or 
unregistered sending 
domain
Loss of reputation, valid 
e-mail from enterprise 
may be blocked as 
possible spam/phishing 
attack.
UBE and/or e-mail 
containing malicious 
links may be delivered 
into user inboxes.
Deployment of domain-
based authentication 
techniques. Use of 
digital signatures over 
e-mail.
E-mail message sent 
using forged sending 
address or e-mail 
address (i.e., phishing, 
spear phishing)
Loss of reputation, valid 
e-mail from enterprise 
may be blocked as 
possible spam/phishing 
attack.
UBE and/or e-mail 
containing malicious 
links may be delivered. 
Users may inadvertently 
divulge sensitive 
information or PII.
Deployment of domain-
based authentication 
techniques. Use of 
digital signatures over 
e-mail.
E-mail modified in 
transit
Leak of sensitive 
information or PII.
Leak of sensitive 
information, altered 
message may contain 
malicious information.
Use of TLS to encrypt 
e-mail transfer between 
servers. Use of end-to-
end e-mail encryption.
Disclosure of sensitive 
information (e.g., PII) via 
monitoring and capturing 
of e-mail traffic
Leak of sensitive 
information or PII.
Leak of sensitive 
information, altered 
message may contain 
malicious information.
Use of TLS to encrypt 
e-mail transfer between 
servers. Use of end-to-
end e-mail encryption.
Unsolicited Bulk E-mail 
(UBE) (i.e., spam)
None, unless purported 
sender is spoofed.
UBE and/or e-mail 
containing malicious 
links may be delivered 
into user inboxes.
Techniques to address 
UBE.
DoS/DDoS attack 
against an enterprises‚Äô 
e-mail servers
Inability to send e-mail.
Inability to receive 
e-mail.
Multiple mail servers, 
use of cloud-based 
e-mail providers.
Table 8.3 E-mail Threats and Mitigations
result that the same trust relationships used to certify IP addresses are used to 
certify servers operating on those addresses.
‚óÜ
‚ñ† Sender Policy Framework (SPF): Uses the Domain Name System (DNS) to 
allow domain owners to create records that associate the domain name with a 
specific IP address range of authorized message senders. It is a simple matter 
for receivers to check the SPF TXT record in the DNS to confirm that the pur-
ported sender of a message is permitted to use that source address and reject 
mail that does not come from an authorized IP address.
‚óÜ
‚ñ† DomainKeys Identified Mail (DKIM): Enables an MTA to sign selected 
 headers and the body of a message. This validates the source domain of the 
mail and provides message body integrity.
‚óÜ
‚ñ† Domain-based Message Authentication, Reporting, and Conformance 
(DMARC): Lets senders know the proportionate effectiveness of their SPF 
and DKIM policies, and signals to receivers what action should be taken in 
various individual and bulk attack scenarios.

Figure 8.4 shows how these components interact to provide message authen-
ticity and integrity. Not shown, for simplicity, is that S/MIME also provides message 
confidentiality by encrypting messages.
 8.4 s/MIMe
Secure/Multipurpose Internet Mail Extension (S/MIME) is a security enhancement 
to the MIME Internet e-mail format standard based on technology from RSA Data 
Security. S/MIME is a complex capability that is defined in a number of documents. 
The most important documents relevant to S/MIME include the following:
‚óÜ
‚ñ† RFC 5750, S/MIME Version 3.2 Certificate Handling: Specifies conventions 
for X.509 certificate usage by (S/MIME) v3.2.
Figure 8.4  The Interrelationship of DNSSEC, SPF, DKIM, DMARC, DANE, and  
S/MIME for Assuring Message Authenticity and Integrity
msg
msg
sig
msg
sig
msg
sig
Sender
MUA
Sender‚Äôs S/MIME
signing key
(private key)
DKIM
signature
DKIM TXT RR provides
sending MTA‚Äôs public key
to receiving MTA
DMARC TXT tells receiving
MTA that sender uses
DKIM and SPF
DANE TLSA RR
specifies SMTP
TLS certificate
Receiver MUA
verifies S/MIME
signature
DNSSEC secured
DNSSEC secured
MTA‚Äôs DKIM
signing key
DANE = DNS-based Authentication of Named Entities
DKIM = DomainKeys Identified Mail
DMARC = Domain-based Message Authentication, Reporting, and Conformance
DNSSEC = Domain Name System Security Extensions
SPF = Sender Policy Framework
S/MIME = Secure Multi-Purpose Internet Mail Extensions
TLSA RR = Transport Layer Security Authentication Resource Record
SPF TXT specfies
sender‚Äôs IP address
Sender
DNS
Receiver
DNS
Receiver
MUA
Sending
MTA
Receiving
MTA

‚óÜ
‚ñ† RFC 5751, S/MIME) Version 3.2 Message Specification: The principal defining 
document for S/MIME message creation and processing.
‚óÜ
‚ñ† RFC 4134, Examples of S/MIME Messages: Gives examples of message  bodies 
formatted using S/MIME.
‚óÜ
‚ñ† RFC 2634, Enhanced Security Services for S/MIME: Describes four optional 
security service extensions for S/MIME.
‚óÜ
‚ñ† RFC 5652, Cryptographic Message Syntax (CMS): Describes the Crypto-
graphic Message Syntax (CMS). This syntax is used to digitally sign, digest, 
authenticate, or encrypt arbitrary message content.
‚óÜ
‚ñ† RFC 3370, CMS Algorithms: Describes the conventions for using several 
 cryptographic algorithms with the CMS.
‚óÜ
‚ñ† RFC 5752, Multiple Signatures in CMS: Describes the use of multiple, parallel 
signatures for a message.
‚óÜ
‚ñ† RFC 1847, Security Multiparts for MIME‚ÄîMultipart/Signed and Multipart/
Encrypted: Defines a framework within which security services may be applied 
to MIME body parts. The use of a digital signature is relevant to S/MIME, as 
explained subsequently.
Operational Description
S/MIME provides for four message-related services: authentication, confidential-
ity, compression, and e-mail compatibility (Table 8.4). This subsection provides 
an overview. We then look in more detail at this capability by examining message 
 formats and message preparation.
auThenTicaTion Authentication is provided by means of a digital  signature, using 
the general scheme discussed in Chapter 3 and illustrated in Figure¬†3.15. Most 
commonly RSA with SHA-256 is used. The sequence is as follows:
1. The sender creates a message.
2. SHA-256 is used to generate a 256-bit message digest of the message.
Function
Typical Algorithm
Typical Action
Digital signature
RSA/SHA-256
A hash code of a message is created using SHA-256. 
This message digest is encrypted using SHA-256 
with the sender‚Äôs private key and included with 
the¬†message.
Message encryption
AES-128 with CBC
A message is encrypted using AES-128 with CBC 
with a one-time session key generated by the 
sender. The session key is encrypted using RSA 
with the recipient‚Äôs public key and included with 
the¬†message.
Compression
unspecified
A message may be compressed for storage or 
transmission.
E-mail compatibility
Radix-64 conversion
To provide transparency for e-mail applications, an 
encrypted message may be converted to an ASCII 
string using radix-64 conversion.
Table 8.4 Summary of S/MIME Services

3. The message digest is encrypted with RSA using the sender‚Äôs private key, and 
the result is appended to the message. Also appended is identifying information 
for the signer, which will enable the receiver to retrieve the  signer‚Äôs public key.
4. The receiver uses RSA with the sender‚Äôs public key to decrypt and recover the 
message digest.
5. The receiver generates a new message digest for the message and compares 
it with the decrypted hash code. If the two match, the message is accepted as 
authentic.
The combination of SHA-256 and RSA provides an effective digital signature 
scheme. Because of the strength of RSA, the recipient is assured that only the pos-
sessor of the matching private key can generate the signature. Because of the strength 
of SHA-256, the recipient is assured that no one else could generate a new message 
that matches the hash code and, hence, the signature of the original message.
Although signatures normally are found attached to the message or file that 
they sign, this is not always the case: Detached signatures are supported. A¬† detached 
signature may be stored and transmitted separately from the message it signs. This 
is useful in several contexts. A user may wish to maintain a separate signature log 
of all messages sent or received. A detached signature of an executable program 
can detect subsequent virus infection. Finally, detached signatures can be used 
when more than one party must sign a document, such as a legal contract. Each 
person‚Äôs signature is independent and therefore is applied only to the document. 
Otherwise, signatures would have to be nested, with the second signer signing both 
the  document and the first signature, and so on.
confidenTialiTy S/MIME provides confidentiality by encrypting messages. Most 
commonly AES with a 128-bit key is used, with the cipher block chaining (CBC) 
mode. The key itself is also encrypted, typically with RSA, as explained below.
As always, one must address the problem of key distribution. In S/MIME, 
each symmetric key, referred to as a content-encryption key, is used only once. That 
is, a new key is generated as a random number for each message. Because it is to be 
used only once, the content-encryption key is bound to the message and transmit-
ted with it. To protect the key, it is encrypted with the receiver‚Äôs public key. The 
sequence can be described as follows:
1. The sender generates a message and a random 128-bit number to be used as a 
content-encryption key for this message only.
2. The message is encrypted using the content-encryption key.
3. The content-encryption key is encrypted with RSA using the recipient‚Äôs public 
key and is attached to the message.
4. The receiver uses RSA with its private key to decrypt and recover the 
 content-encryption key.
5. The content-encryption key is used to decrypt the message.
Several observations may be made. First, to reduce encryption time, the com-
bination of symmetric and public-key encryption is used in preference to simply 
using public-key encryption to encrypt the message directly: Symmetric algorithms

are substantially faster than asymmetric ones for a large block of content. Second, 
the use of the public-key algorithm solves the session-key distribution problem, 
because only the recipient is able to recover the session key that is bound to the 
message. Note that we do not need a session-key exchange protocol of the type 
discussed in Chapter 4,  because we are not beginning an ongoing session. Rather, 
each message is a one-time independent event with its own key. Furthermore, given 
the store-and-forward nature of electronic mail, the use of handshaking to assure 
that both sides have the same session key is not practical. Finally, the use of one-
time symmetric keys strengthens what is already a strong symmetric encryption 
 approach. Only a small amount of plaintext is encrypted with each key, and there is 
no relationship among the keys. Thus, to the extent that the public-key algorithm is 
secure, the entire scheme is secure.
confidenTialiTy and auThenTicaTion As Figure 8.5 illustrates, both confidential-
ity and encryption may be used for the same message. The figure shows a  sequence 
in which a signature is generated for the plaintext message and appended to the 
message. Then the plaintext message and signature are encrypted as a single block 
using symmetric encryption and the symmetric encryption key is encrypted using 
public-key encryption.
S/MIME allows the signing and message encryption operations to be per-
formed in either order. If signing is done first, the identity of the signer is hidden 
by the encryption. Plus, it is generally more convenient to store a signature with a 
plaintext version of a message. Furthermore, for purposes of third-party verifica-
tion, if the signature is performed first, a third party need not be concerned with the 
symmetric key when verifying the signature.
If encryption is done first, it is possible to verify a signature without exposing 
the message content. This can be useful in a context in which automatic signature 
verification is desired, as no private key material is required to verify a signature. 
However, in this case the recipient cannot determine any relationship between the 
signer and the unencrypted content of the message.
e-mail compaTibiliTy When S/MIME is used, at least part of the block to be trans-
mitted is encrypted. If only the signature service is used, then the message digest is 
encrypted (with the sender‚Äôs private key). If the confidentiality service is used, the 
message plus signature (if present) are encrypted (with a one-time symmetric key). 
Thus, part or all of the resulting block consists of a stream of arbitrary 8-bit octets. 
However, many electronic mail systems only permit the use of blocks consisting 
of ASCII text. To accommodate this restriction, S/MIME provides the service of 
converting the raw 8-bit binary stream to a stream of printable ASCII characters, 
a¬†process referred to as 7-bit encoding.
The scheme typically used for this purpose is Base64 conversion. Each group of 
three octets of binary data is mapped into four ASCII characters. See Appendix K for 
a description.
One noteworthy aspect of the Base64 algorithm is that it blindly converts the 
input stream to Base64 format regardless of content, even if the input happens to 
be ASCII text. Thus, if a message is signed but not encrypted and the conversion 
is  applied to the entire block, the output will be unreadable to the casual observer, 
which provides a certain level of confidentiality.

RFC 5751 also recommends that even if outer 7-bit encoding is not used, the 
original MIME content should be 7-bit encoded. The reason for this is that it allows 
the MIME entity to be handled in any environment without changing it. For exam-
ple, a trusted gateway might remove the encryption, but not the signature, of a mes-
sage, and then forward the signed message on to the end recipient so that they can 
verify the signatures directly. If the transport internal to the site is not 8-bit clean, 
such as on a wide area network with a single mail gateway, verifying the signature 
will not be possible unless the original MIME entity was only 7-bit data.
compreSSion S/MIME also offers the ability to compress a message. This has 
the benefit of saving space both for e-mail transmission and for file storage. 
Figure 8.5 Simplified S/MIME Functional Flow
Sign
(e.g., RSA/
SHA-256)
Sender‚Äôs
private key
(a) Sender signs, then encrypts message
(b) Receiver decrypts message, then verifes sender‚Äôs signature
One-time
secret key
Encrypt
(e.g,
AES-128/
CBC 
Encrypt
(e.g., RSA)
msg
msg
sig
sig
sig
msg
sig
Receiver‚Äôs
public key
Sender‚Äôs
public key
Decrypt
(e.g., RSA)
Receiver‚Äôs
private key
Secret key
generated by
sender
Decrypt
(e.g,
AES-128/
CBC
Verify
signature
(e.g., RSA/
SHA-256)
msg
msg

Compression can be applied in any order with respect to the signing and message 
encryption  operations. RFC 5751 provides the following guidelines:
‚óÜ
‚ñ† Compression of binary encoded encrypted data is discouraged, since it will not 
yield significant compression. Base64 encrypted data could very well benefit, 
however.
‚óÜ
‚ñ† If a lossy compression algorithm is used with signing, you will need to  compress 
first, then sign.
S/MIME Message Content Types
S/MIME uses the following message content types, which are defined in RFC 5652, 
Cryptographic Message Syntax:
‚óÜ
‚ñ† Data: Refers to the inner MIME-encoded message content, which may then 
be encapsulated in a SignedData, EnvelopedData, or CompressedData con-
tent type.
‚óÜ
‚ñ† SignedData: Used to apply a digital signature to a message.
‚óÜ
‚ñ† EnvelopedData: This consists of encrypted content of any type and encrypted-
content encryption keys for one or more recipients.
‚óÜ
‚ñ† CompressedData: Used to apply data compression to a message.
The Data content type is also used for a procedure known as clear signing. 
For clear signing, a digital signature is calculated for a MIME-encoded message and 
the two parts, the message and signature, form a multipart MIME message. Unlike 
SignedData, which involves encapsulating the message and signature in a special 
format, clear-signed messages can be read and their signatures verified by e-mail 
entities that do not implement S/MIME.
Approved Cryptographic Algorithms
Table 8.5 summarizes the cryptographic algorithms used in S/MIME. S/MIME uses 
the following terminology taken from RFC 2119 (Key Words for use in RFCs to 
Indicate Requirement Levels, March 1997) to specify the requirement level:
‚óÜ
‚ñ† MUST: The definition is an absolute requirement of the specification. An 
 implementation must include this feature or function to be in conformance 
with the specification.
‚óÜ
‚ñ† SHOULD: There may exist valid reasons in particular circumstances to ignore 
this feature or function, but it is recommended that an implementation include 
the feature or function.
The S/MIME specification includes a discussion of the procedure for deciding 
which content encryption algorithm to use. In essence, a sending agent has two deci-
sions to make. First, the sending agent must determine if the receiving agent is  capable 
of decrypting using a given encryption algorithm. Second, if the receiving agent is only 
capable of accepting weakly encrypted content, the sending agent must decide if it is 
acceptable to send using weak encryption. To support this decision process, a sending 
agent may announce its decrypting capabilities in order of preference for any message 
that it sends out. A receiving agent may store that information for future use.

The following rules, in the following order, should be followed by a sending agent.
1. If the sending agent has a list of preferred decrypting capabilities from an 
 intended recipient, it SHOULD choose the first (highest preference) capabil-
ity on the list that it is capable of using.
2. If the sending agent has no such list of capabilities from an intended recipient 
but has received one or more messages from the recipient, then the outgoing 
message SHOULD use the same encryption algorithm as was used on the last 
signed and encrypted message received from that intended recipient.
3. If the sending agent has no knowledge about the decryption capabilities of the 
intended recipient and is willing to risk that the recipient may not be able to 
decrypt the message, then the sending agent SHOULD use triple DES.
4. If the sending agent has no knowledge about the decryption capabilities of the 
intended recipient and is not willing to risk that the recipient may not be able 
to decrypt the message, then the sending agent MUST use RC2/40.
If a message is to be sent to multiple recipients and a common encryption 
 algorithm cannot be selected for all, then the sending agent will need to send two 
messages. However, in that case, it is important to note that the security of the 
 message is made vulnerable by the transmission of one copy with lower security.
S/MIME Messages
S/MIME makes use of a number of new MIME content types. All of the new applica-
tion types use the designation PKCS. This refers to a set of public-key cryptography 
specifications issued by RSA Laboratories and made available for the S/MIME effort.
Function
Requirement
Create a message digest to be used in 
forming a digital signature.
MUST support SHA-256
SHOULD support SHA-1
Receiver SHOULD support MD5 for backward compatibility
Use message digest to form a digital 
signature.
MUST support RSA with SHA-256
SHOULD support
‚ÄîDSA with SHA-256
‚ÄîRSASSA-PSS with SHA-256
‚ÄîRSA with SHA-1
‚ÄîDSA with SHA-1
‚ÄîRSA with MD5
Encrypt session key for transmission with 
a message.
MUST support RSA encryption
SHOULD support
‚ÄîRSAES-OAEP
‚ÄîDiffie‚ÄìHellman ephemeral-static mode
Encrypt message for transmission with a 
one-time session key.
MUST support AES-128 with CBC
SHOULD support
‚ÄîAES-192 CBC and AES-256 CBC
‚ÄîTriple DES CBC
Table 8.5 Cryptographic Algorithms Used in S/MIME

We examine each of these in turn after first looking at the general procedures 
for S/MIME message preparation.
Securing a mime enTiTy S/MIME secures a MIME entity with a signature, 
 encryption, or both. A MIME entity may be an entire message (except for the RFC 
5322 headers), or if the MIME content type is multipart, then a MIME entity is one 
or more of the subparts of the message. The MIME entity is prepared according 
to the normal rules for MIME message preparation. Then the MIME entity plus 
some security-related data, such as algorithm identifiers and certificates, are pro-
cessed by S/MIME to produce what is known as a PKCS object. A PKCS object is 
then treated as message content and wrapped in MIME (provided with appropriate 
MIME headers). This process should become clear as we look at specific objects 
and provide examples.
In all cases, the message to be sent is converted to canonical form. In par-
ticular, for a given type and subtype, the appropriate canonical form is used for the 
message content. For a multipart message, the appropriate canonical form is used 
for each subpart.
The use of transfer encoding requires special attention. For most cases, the 
result of applying the security algorithm will be to produce an object that is partially 
or totally represented in arbitrary binary data. This will then be wrapped in an outer 
MIME message and transfer encoding can be applied at that point, typically base64. 
However, in the case of a multipart signed message (described in more detail later), 
the message content in one of the subparts is unchanged by the security process. 
Unless that content is 7 bit, it should be transfer encoded using base64 or quoted-
printable so that there is no danger of altering the content to which the signature 
was applied.
We now look at each of the S/MIME content types.
envelopeddaTa An application/pkcs7-mime subtype is used for one of four cat-
egories of S/MIME processing, each with a unique smime-type parameter. In all 
cases, the resulting entity, (referred to as an object) is represented in a form known 
as Basic Encoding Rules (BER), which is defined in ITU-T Recommendation 
X.209. The BER format consists of arbitrary octet strings and is therefore binary 
data. Such an object should be transfer encoded with base64 in the outer MIME 
message. We first look at envelopedData.
The steps for preparing an envelopedData MIME entity are:
1. Generate a pseudorandom session key for a particular symmetric encryption 
algorithm (RC2/40 or triple DES).
2. For each recipient, encrypt the session key with the recipient‚Äôs public RSA key.
3. For each recipient, prepare a block known as RecipientInfo that contains 
an identifier of the recipient‚Äôs public-key certificate,1 an identifier of the 
 algorithm used to encrypt the session key, and the encrypted session key.
4. Encrypt the message content with the session key.
1This is an X.509 certificate, discussed later in this section.

The RecipientInfo blocks followed by the encrypted content constitute the 
envelopedData. This information is then encoded into base64. A sample message 
(excluding the RFC 5322 headers) is given below.
Content-Type: application/pkcs7-mime; smime-type=enveloped-
data; name=smime.p7m
Content-Transfer-Encoding: base64
Content-Disposition: attachment; filename=smime.p7m
rfvbnj756tbBghyHhHUujhJhjH77n8HHGT9HG4VQpfyF467GhIGfHfYT6
7n8HHGghyHhHUujhJh4VQpfyF467GhIGfHfYGTrfvbnjT6jH7756tbB9H
f8HHGTrfvhJhjH776tbB9HG4VQbnj7567GhIGfHfYT6ghyHhHUujpfyF4
0GhIGfHfQbnj756YT64V
To recover the encrypted message, the recipient first strips off the base64 
 encoding. Then the recipient‚Äôs private key is used to recover the session key. Finally, 
the message content is decrypted with the session key.
SigneddaTa The signedData smime-type can be used with one or more signers. 
For clarity, we confine our description to the case of a single digital signature. The 
steps for preparing a signedData MIME entity are as follows.
1. Select a message digest algorithm (SHA or MD5).
2. Compute the message digest (hash function) of the content to be signed.
3. Encrypt the message digest with the signer‚Äôs private key.
4. Prepare a block known as SignerInfo that contains the signer‚Äôs public-key 
certificate, an identifier of the message digest algorithm, an identifier of the 
 algorithm used to encrypt the message digest, and the encrypted message 
digest.
The signedData entity consists of a series of blocks, including a message 
digest algorithm identifier, the message being signed, and SignerInfo. The 
signedData entity may also include a set of public-key certificates sufficient to 
constitute a chain from a recognized root or top-level certification authority to the 
signer. This information is then encoded into base64. A sample message (excluding 
the RFC 5322 headers) is the following.
Content-Type: application/pkcs7-mime; smime-type=signed-
data; name=smime.p7m
Content-Transfer-Encoding: base64
Content-Disposition: attachment; filename=smime.p7m
567GhIGfHfYT6ghyHhHUujpfyF4f8HHGTrfvhJhjH776tbB9HG4VQbnj7
77n8HHGT9HG4VQpfyF467GhIGfHfYT6rfvbnj756tbBghyHhHUujhJhjH
HUujhJh4VQpfyF467GhIGfHfYGTrfvbnjT6jH7756tbB9H7n8HHGghyHh
6YT64V0GhIGfHfQbnj75

To recover the signed message and verify the signature, the recipient first strips 
off the base64 encoding. Then the signer‚Äôs public key is used to decrypt the message 
digest. The recipient independently computes the message digest and  compares it to 
the decrypted message digest to verify the signature.
clear Signing Clear signing is achieved using the multipart content type with 
a signed subtype. As was mentioned, this signing process does not involve trans-
forming the message to be signed, so that the message is sent ‚Äúin the clear.‚Äù Thus, 
recipients with MIME capability but not S/MIME capability are able to read the 
 incoming message.
A multipart/signed message has two parts. The first part can be any MIME 
type but must be prepared so that it will not be altered during transfer from source 
to destination. This means that if the first part is not 7 bit, then it needs to be  encoded 
using base64 or quoted-printable. Then this part is processed in the same manner 
as signedData, but in this case an object with signedData format is created that 
has an empty message content field. This object is a detached signature. It is then 
transfer encoded using base64 to become the second part of the multipart/signed 
message. This second part has a MIME content type of application and a subtype of 
pkcs7-signature. Here is a sample message:
Content-Type: multipart/signed;
protocol=‚Äùapplication/pkcs7-signature‚Äù;
micalg=sha1; boundary=boundary42
‚Äîboundary42
Content-Type: text/plain
This is a clear-signed message.
‚Äîboundary42
Content-Type: application/pkcs7-signature; name=smime.p7s
Content-Transfer-Encoding: base64
Content-Disposition: attachment; filename=smime.p7s
ghyHhHUujhJhjH77n8HHGTrfvbnj756tbB9HG4VQpfyF467GhIGfHfYT6
4VQpfyF467GhIGfHfYT6jH77n8HHGghyHhHUujhJh756tbB9HGTrfvbnj
n8HHGTrfvhJhjH776tbB9HG4VQbnj7567GhIGfHfYT6ghyHhHUujpfyF4
7GhIGfHfYT64VQbnj756
‚Äîboundary42‚Äî
The protocol parameter indicates that this is a two-part clear-signed entity. 
The micalg parameter indicates the type of message digest used. The receiver can 
verify the signature by taking the message digest of the first part and comparing this 
to the message digest recovered from the signature in the second part.
regiSTraTion requeST Typically, an application or user will apply to a certi-
fication authority for a public-key certificate. The application/pkcs10 S/MIME

entity is used to transfer a certification request. The certification  request 
 includes  certificationRequestInfo block, followed by an  identifier 
of the public-key  encryption algorithm, followed by the signature of the 
 certificationRequestInfo block, made using the sender‚Äôs private key. The 
certificationRequestInfo block includes a name of the certificate subject 
(the entity whose public key is to be certified) and a bit-string representation of the 
user‚Äôs public key.
cerTificaTeS-only meSSage A message containing only certificates or a certificate 
revocation list (CRL) can be sent in response to a registration request. The message 
is an application/pkcs7-mime type/subtype with an smime-type parameter of degen-
erate. The steps involved are the same as those for creating a signedData  message, 
except that there is no message content and the signerInfo field is empty.
S/MIME Certificate Processing
S/MIME uses public-key certificates that conform to version 3 of X.509 (see Chapter¬†4). 
S/MIME managers and/or users must configure each client with a list of trusted keys 
and with certificate revocation lists. That is, the responsibility is local for maintaining 
the certificates needed to verify incoming signatures and to encrypt outgoing messages. 
On the other hand, the certificates are signed by certification authorities.
uSer agenT role An S/MIME user has several key management functions to 
perform.
‚óÜ
‚ñ† Key generation: The user of some related administrative utility (e.g., one 
 associated with LAN management) MUST be capable of generating separate 
Diffie‚ÄìHellman and DSS key pairs and SHOULD be capable of generating 
RSA key pairs. Each key pair MUST be generated from a good source of 
nondeterministic random input and be protected in a secure fashion. A user 
agent SHOULD generate RSA key pairs with a length in the range of 768 to 
1024¬†bits and MUST NOT generate a length of less than 512 bits.
‚óÜ
‚ñ† Registration: A user‚Äôs public key must be registered with a certification 
 authority in order to receive an X.509 public-key certificate.
‚óÜ
‚ñ† Certificate storage and retrieval: A user requires access to a local list of certifi-
cates in order to verify incoming signatures and to encrypt outgoing messages. 
Such a list could be maintained by the user or by some local administrative 
entity on behalf of a number of users.
Enhanced Security Services
RFC 2634 defines four enhanced security services for S/MIME:
‚óÜ
‚ñ† Signed receipts: A signed receipt may be requested in a SignedData  object. 
Returning a signed receipt provides proof of delivery to the originator of a 
message and allows the originator to demonstrate to a third party that the 
 recipient received the message. In essence, the recipient signs the entire 
 original message plus the original (sender‚Äôs) signature and appends the new 
signature to form a new S/MIME message.

‚óÜ
‚ñ† Security labels: A security label may be included in the authenticated  attributes 
of a SignedData object. A security label is a set of security information 
 regarding the sensitivity of the content that is protected by S/MIME encapsu-
lation. The labels may be used for access control, by indicating which users are 
permitted access to an object. Other uses include priority (secret, confidential, 
restricted, and so on) or role based, describing which kind of people can see 
the information (e.g., patient‚Äôs health-care team, medical billing agents).
‚óÜ
‚ñ† Secure mailing lists: When a user sends a message to multiple recipients, a 
certain amount of per-recipient processing is required, including the use of 
each recipient‚Äôs public key. The user can be relieved of this work by employ-
ing the services of an S/MIME Mail List Agent (MLA). An MLA can take a 
single incoming message, perform the recipient-specific encryption for each 
recipient, and forward the message. The originator of a message need only 
send the message to the MLA with encryption performed using the MLA‚Äôs 
public key.
‚óÜ
‚ñ† Signing certificates: This service is used to securely bind a sender‚Äôs certificate 
to their signature through a signing certificate attribute.
 8.5 pretty gOOd prIvacy
An alternative e-mail security protocol is Pretty Good Privacy (PGP), which has 
 essentially the same functionality as S/MIME. PGP was created by Phil Zimmerman 
and implemented as a product first released in 1991. It was made available free of 
charge and became quite popular for personal use. The initial PGP protocol was 
proprietary and used some encryption algorithms with intellectual property restric-
tions. In 1996, version 5.x of PGP was defined in IETF RFC 1991, PGP Message 
Exchange Formats. Subsequently, OpenPGP was developed as a new standard 
protocol based on PGP version 5.x. OpenPGP is defined in RFC 4880 (OpenPGP 
Message Format, November 2007) and RFC 3156 (MIME Security with OpenPGP, 
August 2001).
There are two significant differences between S/MIME and OpenPGP:
‚óÜ
‚ñ† Key Certification: S/MIME uses X.509 certificates that are issued by Certificate 
Authorities (or local agencies that have been delegated authority by a CA to 
issue certificates). In OpenPGP, users generate their own OpenPGP public 
and private keys and then solicit signatures for their public keys from individu-
als or organizations to which they are known. Whereas X.509 certificates are 
trusted if there is a valid PKIX chain to a trusted root, an OpenPGP public key 
is trusted if it is signed by another OpenPGP public key that is trusted by the 
recipient. This is called the Web-of-Trust.
‚óÜ
‚ñ† Key Distribution: OpenPGP does not include the sender‚Äôs public key with 
each message, so it is necessary for recipients of OpenPGP messages to sepa-
rately obtain the sender‚Äôs public key in order to verify the message. Many 
 organizations post OpenPGP keys on TLS-protected websites: People who 
wish to verify digital signatures or send these organizations encrypted mail

need to manually download these keys and add them to their OpenPGP 
 clients. Keys may also be registered with the OpenPGP public key servers, 
which are servers that maintain a database of PGP public keys organized by 
e-mail  address. Anyone may post a public key to the OpenPGP key servers, 
and that public key may contain any e-mail address. There is no vetting of 
OpenPGP keys, so users must use the Web-of-Trust to decide whether to trust 
a given public key.
SP 800-177 recommends the use of S/MIME rather than PGP because of the 
greater confidence in the CA system of verifying public keys.
Appendix H provides an overview of PGP.
 8.6 dnssec
DNS Security Extensions (DNSSEC) are used by several protocols that provide 
 e-mail security. This section provides a brief overview of the Domain Name System 
(DNS) and then looks at DNSSEC.
Domain Name System
DNS is a directory lookup service that provides a mapping between the name of a 
host on the Internet and its numeric IP address. DNS is essential to the functioning 
of the Internet. The DNS is used by MUAs and MTAs to find the address of the 
next hop server for mail delivery. Sending MTAs query DNS for the Mail Exchange 
Resource Record (MX RR) of the recipient‚Äôs domain (the right hand side of the 
‚Äú@‚Äù symbol) in order to find the receiving MTA to contact.
Four elements comprise the DNS:
‚óÜ
‚ñ† Domain name space: DNS uses a tree-structured name space to identify 
 resources on the Internet.
‚óÜ
‚ñ† DNS database: Conceptually, each node and leaf in the name space tree struc-
ture names a set of information (e.g., IP address, name server for this domain 
name) that is contained in resource record. The collection of all RRs is orga-
nized into a distributed database.
‚óÜ
‚ñ† Name servers: These are server programs that hold information about a por-
tion of the domain name tree structure and the associated RRs.
‚óÜ
‚ñ† Resolvers: These are programs that extract information from name servers in 
response to client requests. A typical client request is for an IP address corre-
sponding to a given domain name.
The dnS daTabaSe DNS is based on a hierarchical database containing resource 
records (RRs) that include the name, IP address, and other information about hosts. 
The key features of the database are as follows:
‚óÜ
‚ñ† Variable-depth hierarchy for names: DNS allows essentially unlimited levels 
and uses the period (.) as the level delimiter in printed names, as described 
earlier.

‚óÜ
‚ñ† Distributed database: The database resides in DNS servers scattered through-
out the Internet.
‚óÜ
‚ñ† Distribution controlled by the database: The DNS database is divided into 
thousands of separately managed zones, which are managed by  separate 
 administrators. Distribution and update of records is controlled by the  database 
software.
Using this database, DNS servers provide a name-to-address directory service 
for network applications that need to locate specific servers. For example, every 
time an e-mail message is sent or a Web page is accessed, there must be a DNS 
name lookup to determine the IP address of the e-mail server or Web server.
Table 8.6 lists the various types of resource records.
dnS operaTion DNS operation typically includes the following steps (Figure¬†8.6):
1. A user program requests an IP address for a domain name.
2. A resolver module in the local host or local ISP queries a local name server in 
the same domain as the resolver.
3. The local name server checks to see if the name is in its local database or cache, 
and, if so, returns the IP address to the requestor. Otherwise, the name server 
queries other available name servers, if necessary going to the root server, as 
explained subsequently.
4. When a response is received at the local name server, it stores the name/ 
address mapping in its local cache and may maintain this entry for the amount 
of time specified in the time-to-live field of the retrieved RR.
5. The user program is given the IP address or an error message.
Type
Description
A
A host address. This RR type maps the name of a system to its IPv4 address. Some 
systems (e.g., routers) have multiple addresses, and there is a separate RR for each.
AAAA
Similar to A type, but for IPv6 addresses.
CNAME
Canonical name. Specifies an alias name for a host and maps this to the canonical 
(true) name.
HINFO
Host information. Designates the processor and operating system used by the host.
MINFO
Mailbox or mail list information. Maps a mailbox or mail list name to a host name.
MX
Mail exchange. Identifies the system(s) via which mail to the queried domain name 
should be relayed.
NS
Authoritative name server for this domain.
PTR
Domain name pointer. Points to another part of the domain name space.
SOA
Start of a zone of authority (which part of naming hierarchy is implemented). Includes 
parameters related to this zone.
SRV
For a given service provides name of server or servers in domain that provide that service.
TXT
Arbitrary text. Provides a way to add text comments to the database.
WKS
Well-known services. May list the application services available at this host.
Table 8.6 Resource Record Types

The distributed DNS database that supports the DNS functionality must be 
updated frequently because of the rapid and continued growth of the Internet. 
Further, the DNS must cope with dynamic assignment of IP addresses, such as is 
done for home DSL users by their ISP. Accordingly, dynamic updating functions 
for DNS have been defined. In essence, DNS name servers automatically send out 
updates to other relevant name servers as conditions warrant.
DNS Security Extensions
DNSSEC provides end-to-end protection through the use of digital signatures that 
are created by responding zone administrators and verified by a recipient‚Äôs resolver 
software. In particular, DNSSEC avoids the need to trust intermediate name  servers 
and resolvers that cache or route the DNS records originating from the responding 
zone administrator before they reach the source of the query. DNSSEC consists of 
a set of new resource record types and modifications to the existing DNS protocol, 
and is defined in the following documents:
‚óÜ
‚ñ† RFC 4033, DNS Security Introduction and Requirements: Introduces the 
DNS security extensions and describes their capabilities and limitations. The 
document also discusses the services that the DNS security extensions do and 
do not provide.
‚óÜ
‚ñ† RFC 4034, Resource Records for the DNS Security Extensions: Defines four 
new resource records that provide security for DNS.
Figure 8.6 DNS Name Resolution
User
program
User
system
Internet
user
query
query
query
user
response
response
response
Name
resolver
Cache
Name
server
Cache
Database
Database
Foreign
name
server
Cache

‚óÜ
‚ñ† RFC 4035, Protocol Modifications for the DNS Security Extensions: Defines 
the concept of a signed zone, along with the requirements for serving and 
 resolving by using DNSSEC. These techniques allow a security-aware resolver 
to authenticate both DNS resource records and authoritative DNS error 
indications.
dnSSec operaTion In essence, DNSSEC is designed to protect DNS clients 
from accepting forged or altered DNS resource records. It does this by using digital 
 signatures to provide:
‚óÜ
‚ñ† Data origin authentication: Ensures that data has originated from the correct 
source.
‚óÜ
‚ñ† Data integrity verification: Ensures that the content of a RR has not been 
modified.
The DNS zone administrator digitally signs every Resource Record set 
(RRset) in the zone, and publishes this collection of digital signatures, along with 
the zone administrator‚Äôs public key, in the DNS itself. In DNSSEC, trust in the pub-
lic key (for signature verification) of the source is established not by going to a third 
party or a chain of third parties (as in public key infrastructure [PKI] chaining), but 
by starting from a trusted zone (such as the root zone) and establishing the chain of 
trust down to the current source of response through successive verifications of sig-
nature of the public key of a child by its parent. The public key of the trusted zone 
is called the trust anchor.
reSource recordS for dnSSec RFC 4034 defines four new DNS resource 
records:
‚óÜ
‚ñ† DNSKEY: Contains a public key.
‚óÜ
‚ñ† RRSIG: A resource record digital signature.
‚óÜ
‚ñ† NSEC: Authenticated denial of existence record.
‚óÜ
‚ñ† DS: Delegation signer.
An RRSIG is associated with each RRset, where an RRset is the set of 
 resource records that have the same label, class, and type. When a client requests 
data, an RRset is returned, together with the associated digital signature in an 
RRSIG record. The client obtains the relevant DNSKEY public key and verifies 
the signature for this RRset.
DNSSEC depends on establishing the authenticity of the DNS hierarchy lead-
ing to the domain name in question, and thus its operation depends on beginning 
the use of cryptographic digital signatures in the root zone. The DS resource record 
facilitates key signing and authentication between DNS zones to create an authen-
tication chain, or trusted sequence of signed data, from the root of the DNS tree 
down to a specific domain name. To secure all DNS lookups, including those for 
non-existent domain names and record types, DNSSEC uses the NSEC resource 
record to authenticate negative responses to queries. NSEC is used to identify the

range of DNS names or resource record types that do not exist among the sequence 
of domain names in a zone.
 8.7 dns-based authentIcatIOn OF naMed entItIes
DANE is a protocol to allow X.509 certificates, commonly used for Transport Layer 
Security (TLS), to be bound to DNS names using DNSSEC. It is proposed in RFC 
6698 as a way to authenticate TLS client and server entities without a certificate 
authority (CA).
The rationale for DANE is the vulnerability of the use of CAs in a global PKI 
system. Every browser developer and operating system supplier maintains a list of 
CA root certificates as trust anchors. These are called the software‚Äôs root certifi-
cates and are stored in its root certificate store. The PKIX procedure allows a cer-
tificate recipient to trace a certificate back to the root. So long as the root certificate 
remains trustworthy, and the authentication concludes successfully, the client can 
proceed with the connection.
However, if any of the hundreds of CAs operating on the Internet is compro-
mised, the effects can be widespread. The attacker can obtain the CA‚Äôs private key, 
get issued certificates under a false name, or introduce new bogus root certificates 
into a root certificate store. There is no limitation of scope for the global PKI and 
a compromise of a single CA damages the integrity of the entire PKI system. In 
 addition, some CAs have engaged in poor security practices. For example, some 
CAs have issued wildcard certificates that allow the holder to issue sub-certificates 
for any domain or entity, anywhere in the world.
The purpose of DANE is to replace reliance on the security of the CA system 
with reliance on the security provided by DNSSEC. Given that the DNS adminis-
trator for a domain name is authorized to give identifying information about the 
zone, it makes sense to allow that administrator to also make an authoritative bind-
ing between the domain name and a certificate that might be used by a host at that 
domain name.
TLSA Record
DANE defines a new DNS record type, TLSA, that can be used for a secure method 
of authenticating SSL/TLS certificates. The TLSA provides for:
‚óÜ
‚ñ† Specifying constraints on which CA can vouch for a certificate, or which 
 specific PKIX end-entity certificate is valid.
‚óÜ
‚ñ† Specifying that a service certificate or a CA can be directly authenticated in 
the DNS itself.
The TLSA RR enables certificate issue and delivery to be tied to a given 
 domain. A server domain owner creates a TLSA resource record that identifies the 
certificate and its public key. When a client receives an X.509 certificate in the TLS 
negotiation, it looks up the TLSA RR for that domain and matches the TLSA data 
against the certificate as part of the client‚Äôs certificate validation procedure.

Figure 8.7 shows the format of a TLSA RR as it is transmitted to a request-
ing entity. It contains four fields. The Certificate Usage field defines four different 
usage models, to accommodate users who require different forms of authentication. 
The usage models are:
‚óÜ
‚ñ† PKIX-TA (CA constraint): Specifies which CA should be trusted to authen-
ticate the certificate for the service. This usage model limits which CA can be 
used to issue certificates for a given service on a host. The server certificate 
chain must pass PKIX validation that terminates with a trusted root certificate 
stored in the client.
‚óÜ
‚ñ† PKIX-EE (service certificate constraint): Defines which specific end entity 
service certificate should be trusted for the service. This usage model limits 
which end entity certificate can be used by a given service on a host. The server 
certificate chain must pass PKIX validation that terminates with a trusted root 
certificate stored in the client.
‚óÜ
‚ñ† DANE-TA (trust anchor assertion): Specifies a domain-operated CA to be 
used as a trust anchor. This usage model allows a domain name administrator 
to specify a new trust anchor‚Äîfor example, if the domain issues its own certifi-
cates under its own CA that is not expected to be in the end users‚Äô collection 
of trust anchors. The server certificate chain is self-issued and does not need to 
verify against a trusted root stored in the client.
‚óÜ
‚ñ† DANE-EE (domain-issued certificate): Specifies a domain-operated CA to 
be used as a trust anchor. This certificate usage allows a domain name admin-
istrator to issue certificates for a domain without involving a third-party CA. 
The server certificate chain is self-issued and does not need to verify against a 
trusted root stored in the client.
The first two usage models are designed to co-exist with and strengthen 
the¬†public CA system. The final two usage models operate without the use of 
public¬†CAs.
The Selector field indicates whether the full certificate will be matched or just 
the value of the public key. The match is made between the certificate presented 
in TLS negotiation and the certificate in the TLSA RR. The Matching Type field 
indicates how the match of the certificate is made. The options are exact match, 
SHA-256 hash match, or SHA-512 hash match. The Certificate Association Data is 
the raw certificate data in hex format.
Figure 8.7 TLSA RR Transmission Format
Certifcate usage
Selector
Matching type
Certifcate association data
0
Bit:
31
8
16
24

Use of DANE for SMTP
DANE can be used in conjunction with SMTP over TLS, as provided by STARTTLS, 
to more fully secure e-mail delivery. DANE can authenticate the certificate of the 
SMTP submission server that the user‚Äôs mail client (MUA) communicates with. It 
can also authenticate the TLS connections between SMTP servers (MTAs). The 
use of DANE with SMTP is documented in an Internet Draft (SMTP Security via 
Opportunistic DANE TLS, draft-ietf-dane-smtp-with-dane-19, May 29, 2015).
As discussed in Section 8.1, SMTP can use the STARTTLS extension to 
run SMTP over TLS, so that the entire e-mail message plus SMTP envelope are 
 encrypted. This is done opportunistically, that is, if both sides support STARTTLS. 
Even when TLS is used to provide confidentiality, it is vulnerable to attack in the 
following ways:
‚óÜ
‚ñ† Attackers can strip away the TLS capability advertisement and downgrade the 
connection to not use TLS.
‚óÜ
‚ñ† TLS connections are often unauthenticated (e.g., the use of self-signed certifi-
cates as well as mismatched certificates is common).
DANE can address both these vulnerabilities. A domain can use the presence 
of the TLSA RR as an indicator that encryption must be performed, thus prevent-
ing malicious downgrade. A domain can authenticate the certificate used in the TLS 
connection setup using a DNSSEC-signed TLSA RR.
Use of DNSSEC for S/MIME
DNSSEC can be used in conjunction with S/MIME to more fully secure e-mail 
 delivery, in a manner similar to the DANE functionality. This use is documented in 
an Internet Draft (Using Secure DNS to Associate Certificates with Domain Names 
for S/MIME, draft-ietf-dane-smime-09, August 27, 2015), which proposes a new 
SMIMEA DNS RR. The purpose of the SMIMEA RR is to associate certificates 
with DNS domain names.
As discussed in Section 8.4, S/MIME messages often contain certificates 
that can assist in authenticating the message sender and can be used in encrypt-
ing messages sent in reply. This feature requires that the receiving MUA validate 
the  certificate associated with the purported sender. SMIMEA RRs can provide a 
 secure means of doing this validation.
In essence, the SMIMEA RR will have the same format and content as¬†the 
TLSA RR, with the same functionality. The difference is that it is geared to 
the¬†needs of MUAs in dealing with domain names as specified in e-mail addresses in 
the message body, rather than domain names specified in the outer SMTP envelope.
 8.8 sender pOlIcy FraMewOrk
SPF is the standardized way for a sending domain to identify and assert the mail 
senders for a given domain. The problem that SPF addresses is the following: With 
the current e-mail infrastructure, any host can use any domain name for each of the

various identifiers in the mail header, not just the domain name where the host is 
located. Two major drawbacks of this freedom are:
‚óÜ
‚ñ† It is a major obstacle to reducing unsolicited bulk e-mail (UBE), also known 
as spam. It makes it difficult for mail handlers to filter out e-mails on the basis 
of known UBE sources.
‚óÜ
‚ñ† ADMDs (see Section 8.1) are understandably concerned about the ease with 
which other entities can make use of their domain names, often with malicious 
intent.
RFC 7208 defines the SPF. It provides a protocol by which ADMDs can 
 authorize hosts to use their domain names in the ‚ÄúMAIL FROM‚Äù or ‚ÄúHELO‚Äù 
identities. Compliant ADMDs publish Sender Policy Framework (SPF) records in 
the DNS specifying which hosts are permitted to use their names, and compliant 
mail receivers use the published SPF records to test the authorization of sending 
Mail Transfer Agents (MTAs) using a given ‚ÄúHELO‚Äù or ‚ÄúMAIL FROM‚Äù identity 
 during a mail transaction.
SPF works by checking a sender‚Äôs IP address against the policy encoded in any 
SPF record found at the sending domain. The sending domain is the domain used 
in the SMTP connection, not the domain indicated in the message header as dis-
played in the MUA. This means that SPF checks can be applied before the message 
 content is received from the sender.
Figure 8.8 is an example in which SPF would come into play. Assume that the 
sender‚Äôs IP address is 192.168.0.1. The message arrives from the MTA with  domain 
mta.example.net. The sender uses the MAIL FROM tag of alice@example.org, 
 indicating that the message originates in the example.org domain. But the message 
header specifies alice.sender@example.net. The receiver uses SPF to query for the 
SPF RR that corresponds to example.com to check if the IP address 192.168.0.1 is 
S: 220 foo.com Simple Mail Transfer Service Ready
C: HELO mta.example.net
S: 250 OK
C: MAIL FROM:<alice@example.org>
S: 250 OK
C: RCPT TO:<Jones@foo.com>
S: 250 OK
C: DATA
S: 354 Start mail input; end with <crlf>.<crlf>
C: To: bob@foo.com
C: From: alice.sender@example.net
C: Date: Today
C: Subject: Meeting Today
¬†.¬†.¬†.¬†
Figure 8.8  Example in which SMTP Envelope Header Does 
Not Match Message Header

Tag
Description
ip4
Specifies an IPv4 address or range of addresses that are authorized senders for 
a¬†domain.
ip6
Specifies an IPv6 address or range of addresses that are authorized senders for 
a¬†domain.
mx
Asserts that the listed hosts for the Mail Exchange RRs are also valid senders for 
the domain.
include
Lists another domain where the receiver should look for an SPF RR for further 
senders. This can be useful for large organizations with many domains or  
sub-domains that have a single set of shared senders. The include mechanism is 
recursive, in that the SPF check in the record found is tested in its entirety before 
proceeding. It is not simply a concatenation of the checks.
all
Matches every IP address that has not otherwise been matched.
(a) SPF Mechanisms
Modifier
Description
+
The given mechanism check must pass. This is the default mechanism and does not 
need to be explicitly listed.
-
The given mechanism is not allowed to send e-mail on behalf of the domain.
‚àº
The given mechanism is in transition and if an e-mail is seen from the listed host/IP 
address, then it should be accepted but marked for closer inspection.
?
The SPF RR explicitly states nothing about the mechanism. In this case, the default 
behavior is to accept the e-mail. (This makes it equivalent to = + > unless some sort of 
discrete or aggregate message review is conducted.)
(b) SPF Mechanism Modifiers
Table 8.7 Common SPF Mechanisms and Modifiers
listed as a valid sender, and then takes appropriate action based on the results of 
checking the RR.
SPF on the Sender Side
A sending domain needs to identify all the senders for a given domain and add 
that information into the DNS as a separate resource record. Next, the sending 
domain encodes the appropriate policy for each sender using the SPF syntax. The 
encoding is done in a TXT DNS resource record as a list of mechanisms and mod-
ifiers. Mechanisms are used to define an IP address or range of addresses to be 
matched, and modifiers indicate the policy for a given match. Table 8.7 lists the 
most  important mechanisms and modifiers used in SPF.
The SPF syntax is fairly complex and can express complex relationships 
 between senders. For more detail, see RFC 7208.
SPF on the Receiver Side
If SPF is implemented at a receiver, the SPF entity uses the SMTP envelope MAIL 
FROM: address domain and the IP address of the sender to query an SPF TXT RR. 
The SPF checks can be started before the body of the e-mail message is received,

which may result in blocking the transmission of the e-mail content. Alternatively, 
the entire message can be absorbed and buffered until all the checks are finished. 
In either case, checks must be completed before the mail message is sent to the end 
user‚Äôs inbox.
The checking involves the following rules:
1. If no SPF TXT RR is returned, the default behavior is to accept the message.
2. If the SPF TXT RR has formatting errors, the default behavior is to accept the 
message.
3. Otherwise the mechanisms and modifiers in the RR are used to determine 
disposition of the e-mail message.
Figure 8.9 illustrates SPF operation.
 8.9 dOMaInkeys IdentIFIed MaIl
DomainKeys Identified Mail (DKIM) is a specification for cryptographically 
signing e-mail messages, permitting a signing domain to claim responsibility for a 
message in the mail stream. Message recipients (or agents acting in their  behalf) 
can verify the signature by querying the signer‚Äôs domain directly to  retrieve the 
appropriate public key and thereby can confirm that the message was attested to 
by a party in possession of the private key for the signing domain. DKIM is an 
Internet Standard (RFC 6376: DomainKeys Identified Mail (DKIM) Signatures). 
DKIM has been widely adopted by a range of e-mail providers, including 
 corporations, government agencies, gmail, Yahoo!, and many Internet Service 
Providers (ISPs).
Figure 8.9 Sender Policy Framework Operation
Sender
Inbound
mail server
SPF record
lookup
Authorization
pass/fail
Further
policy
checks
Inbox
Junk e-mail
Quarantine
Block/delete
DNS
Internet

E-mail Threats
RFC 4686 (Analysis of Threats Motivating DomainKeys Identified Mail) describes 
the threats being addressed by DKIM in terms of the characteristics, capabilities, 
and location of potential attackers.
characTeriSTicS RFC 4686 characterizes the range of attackers on a spectrum of 
three levels of threat.
1. At the low end are attackers who simply want to send e-mail that a  recipient 
does not want to receive. The attacker can use one of a number of  commercially 
available tools that allow the sender to falsify the origin address of messages. 
This makes it difficult for the receiver to filter spam on the basis of originating 
address or domain.
2. At the next level are professional senders of bulk spam mail. These attackers 
often operate as commercial enterprises and send messages on behalf of third 
parties. They employ more comprehensive tools for attack, including Mail 
Transfer Agents (MTAs) and registered domains and networks of compro-
mised computers (zombies), to send messages and (in some cases) to harvest 
addresses to which to send.
3. The most sophisticated and financially motivated senders of messages are 
those who stand to receive substantial financial benefit, such as from an 
 e-mail-based fraud scheme. These attackers can be expected to employ all of 
the above mechanisms and additionally may attack the Internet infrastructure 
 itself, including DNS cache-poisoning attacks and IP routing attacks.
capabiliTieS RFC 4686 lists the following as capabilities that an attacker might 
have.
1. Submit messages to MTAs and Message Submission Agents (MSAs) at 
 multiple locations in the Internet.
2. Construct arbitrary Message Header fields, including those claiming to be 
mailing lists, resenders, and other mail agents.
3. Sign messages on behalf of domains under their control.
4. Generate substantial numbers of either unsigned or apparently signed 
 messages that might be used to attempt a denial-of-service attack.
5. Resend messages that may have been previously signed by the domain.
6. Transmit messages using any envelope information desired.
7. Act as an authorized submitter for messages from a compromised computer.
8. Manipulation of IP routing. This could be used to submit messages from 
 specific IP addresses or difficult-to-trace addresses, or to cause diversion of 
messages to a specific domain.
9. Limited influence over portions of DNS using mechanisms such as cache 
 poisoning. This might be used to influence message routing or to falsify adver-
tisements of DNS-based keys or signing practices.

10. Access to significant computing resources, for example, through the conscrip-
tion of worm-infected ‚Äúzombie‚Äù computers. This could allow the ‚Äúbad actor‚Äù to 
perform various types of brute-force attacks.
11. Ability to eavesdrop on existing traffic, perhaps from a wireless network.
locaTion DKIM focuses primarily on attackers located outside of the administra-
tive units of the claimed originator and the recipient. These administrative units 
frequently correspond to the protected portions of the network adjacent to the orig-
inator and recipient. It is in this area that the trust relationships required for authen-
ticated message submission do not exist and do not scale adequately to be practical. 
Conversely, within these administrative units, there are other mechanisms (such as 
authenticated message submission) that are easier to deploy and more likely to be 
used than DKIM. External bad actors are usually attempting to exploit the ‚Äúany-to-
any‚Äù nature of e-mail that motivates most recipient MTAs to accept messages from 
anywhere for delivery to their local domain. They may generate messages without 
signatures, with incorrect signatures, or with correct signatures from domains with 
little traceability. They may also pose as mailing lists, greeting cards, or other agents 
that legitimately send or resend messages on behalf of others.
DKIM Strategy
DKIM is designed to provide an e-mail authentication technique that is transparent 
to the end user. In essence, a user‚Äôs e-mail message is signed by a private key of the 
administrative domain from which the e-mail originates. The signature covers all of 
the content of the message and some of the RFC 5322 message headers. At the 
 receiving end, the MDA can access the corresponding public key via a DNS and 
verify the signature, thus authenticating that the message comes from the claimed 
administrative domain. Thus, mail that originates from somewhere else but claims 
to come from a given domain will not pass the authentication test and can be 
 rejected. This approach differs from that of S/MIME and PGP, which use the origi-
nator‚Äôs private key to sign the content of the message. The motivation for DKIM is 
based on the following reasoning:2
1. S/MIME depends on both the sending and receiving users employing S/MIME. 
For almost all users, the bulk of incoming mail does not use S/MIME, and the 
bulk of the mail the user wants to send is to recipients not using S/MIME.
2. S/MIME signs only the message content. Thus, RFC 5322 header information 
concerning origin can be compromised.
3. DKIM is not implemented in client programs (MUAs) and is therefore trans-
parent to the user; the user need not take any action.
4. DKIM applies to all mail from cooperating domains.
5. DKIM allows good senders to prove that they did send a particular message 
and to prevent forgers from masquerading as good senders.
2 The reasoning is expressed in terms of the use of S/MIME. The same argument applies to PGP.

Figure 8.10 Simple Example of DKIM Deployment
Mail origination
network
Mail delivery
network
DNS Public key query/response
DNS = Domain Name System
MDA = Mail Delivery Agent
MSA = Mail Submission Agent
MTA = Message Transfer Agent
MUA = Message User Agent
SMTP
MUA
MUA
SMTP
SMTP
Signer
Verifer
SMTP
POP, IMAP
MTA
MSA
MTA
MDA
DNS
Figure 8.10 is a simple example of the operation of DKIM. We begin with a 
message generated by a user and transmitted into the MHS to an MSA that is within 
the user‚Äôs administrative domain. An e-mail message is generated by an e-mail cli-
ent program. The content of the message, plus selected RFC 5322 headers, is signed 
by the e-mail provider using the provider‚Äôs private key. The signer is associated 
with a domain, which could be a corporate local network, an ISP, or a public e-mail 
facility such as gmail. The signed message then passes through the Internet via a 
sequence of MTAs. At the destination, the MDA retrieves the public key for the 
incoming signature and verifies the signature before passing the message on to the 
destination e-mail client. The default signing algorithm is RSA with SHA-256. RSA 
with SHA-1 also may be used.
DKIM Functional Flow
Figure 8.11 provides a more detailed look at the elements of DKIM operation. 
Basic message processing is divided between a signing Administrative Management 
Domain (ADMD) and a verifying ADMD. At its simplest, this is between the origi-
nating ADMD and the delivering ADMD, but it can involve other ADMDs in the 
handling path.
Signing is performed by an authorized module within the signing ADMD 
and uses private information from a Key Store. Within the originating ADMD,

this might be performed by the MUA, MSA, or an MTA. Verifying is  performed 
by an authorized module within the verifying ADMD. Within a delivering 
ADMD, verifying might be performed by an MTA, MDA or MUA. The mod-
ule verifies the signature or determines whether a particular signature was 
 required. Verifying the signature uses public information from the Key Store. 
If the signature passes, reputation information is used to assess the signer and 
that information is passed to the message filtering system. If the signature fails 
or there is no signature using the author‚Äôs domain, information about signing 
practices related to the author can be retrieved remotely and/or locally, and that 
information is passed to the message filtering system. For example, if the sender 
(e.g.,¬†gmail) uses DKIM but no DKIM signature is present, then the message 
may be  considered fraudulent.
Figure 8.11 DKIM Functional Flow
Originating or relaying ADMD:
Sign message with SDID
RFC 5322 message
yes
pass
fail
no
Relaying or delivering ADMD:
Message signed?
Verify
signature
Private
key
store
(paired)
Public
key
store
Remote
sender
practices
Local info
on sender
practices
Reputation/
accreditation
information
Assessments
Message
fltering
engine
Check
signing
practices
Internet

The signature is inserted into the RFC 5322 message as an additional header 
entry, starting with the keyword Dkim-Signature. You can view examples from your 
own incoming mail by using the View Long Headers (or similar wording) option for 
an incoming message. Here is an example:
Dkim-Signature: 
v=1; a=rsa-sha256; c=relaxed/relaxed;  
 
 
d=gmail.com; s=gamma; h=domainkey- 
 
 
signature:mime-version:received:date: 
 
 
message-id:subject :from:to:content-type: 
 
 
content-transfer-encoding; 
 
 
bh=5mZvQDyCRuyLb1Y28K4zgS2MPOemFToDBgvbJ 
 
 
7GO90s=; 
 
 
 b=PcUvPSDygb4ya5Dyj1rbZGp/VyRiScuaz7TTG 
J5qW5slM+klzv6kcfYdGDHzEVJW+Z 
 
 
FetuPfF1ETOVhELtwH0zjSccOyPkEiblOf6gILO
 
 
bm3DDRm3Ys1/FVrbhVOlA+/jH9Aei 
 
 
uIIw/5iFnRbSH6qPDVv/beDQqAWQfA/wF7O5k=
Before a message is signed, a process known as canonicalization is performed 
on both the header and body of the RFC 5322 message. Canonicalization is necessary 
to deal with the possibility of minor changes in the message made en route, includ-
ing character encoding, treatment of trailing white space in message lines, and the 
‚Äúfolding‚Äù and ‚Äúunfolding‚Äù of header lines. The intent of canonicalization is to make a 
minimal transformation of the message (for the purpose of signing; the message itself 
is not changed, so the canonicalization must be performed again by the verifier) that 
will give it its best chance of producing the same canonical value at the receiving end. 
DKIM defines two header canonicalization algorithms (‚Äúsimple‚Äù and ‚Äúrelaxed‚Äù) and 
two for the body (with the same names). The simple algorithm tolerates almost no 
modification, while the relaxed algorithm tolerates common modifications.
The signature includes a number of fields. Each field begins with a tag consist-
ing of a tag code followed by an equals sign and ends with a semicolon. The fields 
include the following:
‚óÜ
‚ñ† v= DKIM version/
‚óÜ
‚ñ† a= Algorithm used to generate the signature; must be either rsa-sha1 or 
rsa-sha256
‚óÜ
‚ñ† c= Canonicalization method used on the header and the body.
‚óÜ
‚ñ† d= A domain name used as an identifier to refer to the identity of a responsible 
person or organization. In DKIM, this identifier is called the Signing Domain 
IDentifier (SDID). In our example, this field indicates that the sender is using 
a gmail address.
‚óÜ
‚ñ† s= In order that different keys may be used in different circumstances for the 
same signing domain (allowing expiration of old keys, separate departmen-
tal signing, or the like), DKIM defines a selector (a name associated with a 
key) that is used by the verifier to retrieve the proper key during signature 
verification.

‚óÜ
‚ñ† h= Signed Header fields. A colon-separated list of header field names that 
identify the header fields presented to the signing algorithm. Note that in our 
example above, the signature covers the domainkey-signature field. This refers 
to an older algorithm (since replaced by DKIM) that is still in use.
‚óÜ
‚ñ† bh= The hash of the canonicalized body part of the message. This provides 
 additional information for diagnosing signature verification failures.
‚óÜ
‚ñ† b= The signature data in base64 format; this is the encrypted hash code.
 8.10  dOMaIn-based Message authentIcatIOn, 
repOrtIng, and cOnFOrMance
Domain-Based Message Authentication, Reporting, and Conformance (DMARC) 
 allows e-mail senders to specify policy on how their mail should be handled, the 
types of reports that receivers can send back, and the frequency those reports 
should be sent. It is defined in RFC 7489 (Domain-based Message Authentication, 
Reporting, and Conformance, March 2015).
DMARC works with SPF and DKIM. SPF and DKM enable senders to advise 
receivers, via DNS, whether mail purporting to come from the sender is valid, and 
whether it should be delivered, flagged, or discarded. However, neither SPF nor 
DKIM include a mechanism to tell receivers if SPF or DKIM are in use, nor do they 
have feedback mechanism to inform senders of the effectiveness of the anti-spam 
techniques. For example, if a message arrives at a receiver without a DKIM signa-
ture, DKIM provides no mechanism to allow the receiver to learn if the message is 
authentic but was sent from a sender that did not implement DKIM, or if the mes-
sage is a spoof. DMARC addresses these issues essentially by standardizing how 
e-mail receivers perform e-mail authentication using SPF and DKIM mechanisms.
Identifier Alignment
DKIM, SPF, and DMARC authenticate various aspects of an individual mes-
sage. DKIM authenticates the domain that affixed a signature to the message. SPF 
 focuses on the SMTP envelope, defined in RFC 5321. It can authenticate either the 
domain that appears in the MAIL FROM portion of the SMTP envelope or the 
HELO domain, or both. These may be different domains, and they are typically not 
visible to the end user.
DMARC authentication deals with the From domain in the message header, 
as defined in RFC 5322. This field is used as the central identity of the DMARC 
mechanism because it is a required message header field and therefore guaranteed 
to be present in compliant messages, and most MUAs represent the RFC 5322 
From field as the originator of the message and render some or all of this header 
field‚Äôs content to end users. The e-mail address in this field is the one used by end 
users to identify the source of the message and therefore is a prime target for abuse.
DMARC requires that From address match (be aligned with) an Authenticated 
Identifier from DKIM or SPF. In the case of DKIM, the match is made between 
the DKIM signing domain and the From domain. In the case of SPF, the match is 
 between the SPF-authenticated domain and the From domain.

DMARC on the Sender Side
A mail sender that uses DMARC must also use SPF or DKIM, or both. The sender 
posts a DMARC policy in the DNS that advises receivers on how to treat messages 
that purport to originate from the sender‚Äôs domain. The policy is in the form of 
a DNS TXT resource record. The sender also needs to establish e-mail addresses 
to receive aggregate and forensic reports. As these e-mail addresses are published 
unencrypted in the DNS TXT RR, they are easily discovered, leaving the poster 
subject to unsolicited bulk e-mail. Thus, the poster of the DNS TXT RR needs to 
employ some kind of abuse countermeasures.
Similar to SPF and DKIM, the DMARC policy in the TXT RR is encoded 
in a series of tag=value pairs separated by semicolons. Table 8.8 describes the 
 common tags.
Once the DMARC RR is posted, messages from the sender are typically 
 processed as follows:
1. The domain owner constructs an SPF policy and publishes it in its DNS 
 database. The domain owner also configures its system for DKIM  signing. 
Finally, the domain owner publishes via the DNS a DMARC message- handling 
policy.
2. The author generates a message and hands the message to the domain owner‚Äôs 
designated mail submission service.
3. The submission service passes relevant details to the DKIM signing module in 
order to generate a DKIM signature to be applied to the message.
4. The submission service relays the now-signed message to its designated trans-
port service for routing to its intended recipient(s).
DMARC on the Receiver Side
A message generated on the sender side may pass through other relays but even-
tually arrives at a receiver‚Äôs transport service. The typical processing order for 
DMARC on the receiving side is the following:
1. The receiver performs standard validation tests, such as checking against IP 
blocklists and domain reputation lists, as well as enforcing rate limits from a 
particular source.
2. The receiver extracts the RFC 5322 From address from the message. This must 
contain a single, valid address or else the mail is refused as an error.
3. The receiver queries for the DMARC DNS record based on the sending 
 domain. If none exists, terminate DMARC processing.
4. The receiver performs DKIM signature checks. If more than one DKIM signa-
ture exists in the message, one must verify.
5. The receiver queries for the sending domain‚Äôs SPF record and performs SPF 
validation checks.
6. The receiver conducts Identifier Alignment checks between the RFC 5321 
From and the results of the SPF and DKIM records (if present).

Tag (Name)
Description
v= (Version)
Version field that must be present as the first element. By default the value is 
always DMARC1.
p= (Policy)
Mandatory policy field. May take values none or quarantine or reject. This 
allows for a gradually tightening policy where the sender domain recommends 
no specific action on mail that fails DMARC checks (p= none), through treating 
failed mail as suspicious (p= quarantine),  to rejecting all failed mail  
(p= reject),  preferably at the SMTP transaction stage.
aspf= (SPF Policy)
Values are r (default) for relaxed and s for strict SPF domain enforcement. Strict 
alignment requires an exact match between the From address domain and the 
(passing) SPF check must exactly match the MailFrom address (HELO address). 
Relaxed requires that only the From and MailFrom address domains be in 
alignment. For example, the MailFrom address domain smtp.example.org and the 
From address announce@example.org are in alignment, but not a strict match.
adkim= (DKIM 
Policy)
Optional. Values are r (default) for relaxed and s for strict DKIM domain 
enforcement. Strict alignment requires an exact match between the From 
domain in the message header and the DKIM domain presented in the  
(d= DKIM), tag. Relaxed requires only that the domain part is in alignment 
(as¬†in aspf).
fo= (Failure reporting 
options)
Optional. Ignore if a ruf argument is not also present. Value 0 indicates the 
receiver should generate a DMARC failure report if all underlying mechanisms 
fail to produce an aligned pass result. Value 1 means generate a DMARC failure 
report if any underlying mechanism produces something other than an aligned 
pass result. Other possible values are d (generate a DKIM failure report if a 
signature failed evaluation), and s (generate an SPF failure report if the message 
failed SPF evaluation). These values are not exclusive and may be combined.
ruf=
Optional, but requires the fo argument to be present. Lists a series of URIs 
(currently just mailto:<emailaddress>) that list where to send forensic feedback 
reports. This is for reports on message-specific failures.
rua=
Optional list of URIs (like in ruf= , using the mailto: URI) listing where to 
send aggregate feedback back to the sender. These reports are sent based on the 
interval requested using the ri= option with a default of 86400 seconds if not 
listed.
ri= (Reporting interval)
Optional with the default value of 86400 seconds. The value listed is the 
reporting interval desired by the sender.
pct= (Percent)
Optional with the default value of 100. Expresses the percentage of a sender‚Äôs 
mail that should be subject to the given DMARC policy. This allows senders to 
ramp up their policy enforcement gradually and prevent having to commit to a 
rigorous policy before getting feedback on their existing policy.
sp= (Receiver Policy)
Optional with a default value of none. Other values include the same range 
of values as the p= argument. This is the policy to be applied to mail from all 
identified subdomains of the given DMARC RR.
Table 8.8 DMARC Tag and Value Descriptions
7. The results of these steps are passed to the DMARC module along with the Author‚Äôs 
domain. The DMARC module attempts to retrieve a policy from¬†the¬†DNS for 
that domain. If none is found, the DMARC module determines the  organizational 
 domain and repeats the attempt to retrieve a policy from the¬†DNS.
8. If a policy is found, it is combined with the Author‚Äôs domain and the SPF and 
DKIM results to produce a DMARC policy result (a ‚Äúpass‚Äù or ‚Äúfail‚Äù) and can 
optionally cause one of two kinds of reports to be generated.

Figure 8.12 DMARC Functional Flow
DKIM
DKIM
SPF
SPF
Failure
report
Block
Pass
Sender
Receiver
Fail
Quaran-
tine
Author composes
and sends e-mail
Standard processing
(including antispam)
Sending mail server
attaches DKIM signature
Standard validation
tests at receiver
(including IP
blocklists,
reputation, rate
limits, etc)
Retrieve verifed
DKIM domains
Retrieve 
‚Äúenvelope from‚Äù
via SPF
Update periodic
aggregate report
to be sent to sender
Apply
DMARC
policy
9. Recipient transport service either delivers the message to the recipient inbox 
or takes other local policy action based on the DMARC result.
10. When requested, Recipient transport service collects data from the message 
delivery session to be used in providing feedback.
Figure 8.12, based on one at DMARC.org, summarizes the sending and 
 receiving functional flow.

DMARC Reports
DMARC reporting provides the sender‚Äôs feedback on their SPF, DKIM, Identifier 
Alignment, and message disposition policies, which enable the sender to make 
these policies more effective. Two types of reports are sent: aggregate reports and 
forensic reports.
Aggregate reports are sent by receivers periodically and include aggregate 
 figures for successful and unsuccessful message authentications, including:
‚óÜ
‚ñ† The sender‚Äôs DMARC policy for that interval.
‚óÜ
‚ñ† The message disposition by the receiver (i.e., delivered, quarantined, rejected).
‚óÜ
‚ñ† SPF result for a given SPF identifier.
‚óÜ
‚ñ† DKIM result for a given DKIM identifier.
‚óÜ
‚ñ† Whether identifiers are in alignment or not.
‚óÜ
‚ñ† Results classified by sender subdomain.
‚óÜ
‚ñ† The sending and receiving domain pair.
‚óÜ
‚ñ† The policy applied, and whether this is different from the policy requested.
‚óÜ
‚ñ† The number of successful authentications.
‚óÜ
‚ñ† Totals for all messages received.
This information enables the sender to identify gaps in e-mail infrastruc-
ture and¬†policy. SP 800-177 recommends that a sending domain begin by setting 
a¬†DMARC policy of p= none, so that the ultimate disposition of a message that 
fails some check is determined by the receiver‚Äôs local policy. As DMARC aggregate 
reports are collected, the sender will have a quantitatively better assessment of the 
extent to which the sender‚Äôs e-mail is authenticated by outside receivers, and will 
be able to set a policy of p  =  reject, indicating that any message that fails the 
SPF, DKIM, and alignment checks really should be rejected. From their own traffic 
analysis, receivers can develop a determination of whether a sender‚Äôs p  =   reject 
policy is sufficiently trustworthy to act on.
A forensic report helps the sender refine the component SPF and DKIM 
mechanisms as well as alerting the sender that their domain is being used as part 
of a phishing/spam campaign. Forensic reports are similar in format to aggregation 
reports, with these changes:
‚óÜ
‚ñ† Receivers include as much of the message and message header as is reason-
able to allow the domain to investigate the failure. Add an Identity-Alignment 
field, with DKIM and SPF DMARC-method fields as appropriate.
‚óÜ
‚ñ† Optionally add a Delivery-Result field.
‚óÜ
‚ñ† Add DKIM Domain, DKIM Identity, and DKIM selector fields, if the message 
was DKIM signed. Optionally also add DKIM Canonical header and body 
fields.
‚óÜ
‚ñ† Add an additional DMARC authentication failure type, for use when some 
authentication mechanisms fail to produce aligned identifiers.

8.11  key terMs, revIew QuestIOns, and prObleMs
Key Terms 
administrative management 
domain (ADMD)
base64
Cryptographic Message 
Syntax (CMS)
detached signature
DNS-based Authentication of 
Named Entities (DANE)
DNS Security Extensions 
(DNSSEC)
Domain-based Message 
Authentication, Reporting, 
and Conformance 
(DMARC)
Domain Name System (DNS)
DomainKeys Identified Mail 
(DKIM)
electronic mail
Internet Mail Access Protocol 
(IMAP)
Mail Delivery Agent (MDA)
Mail Submission Agent 
(MSA)
Message Handling Service 
(MHS)
Message Store
Message Transfer Agents 
(MTA)
Message User Agent (MUA)
Multipurpose Internet Mail 
Extensions (MIME)
Post Office Protocol (POP3)
Pretty Good Privacy (PGP)
Sender Policy Framework 
(SPF)
session key
Simple Mail Transfer Protocol 
(SMTP)
STARTTLS
SUBMISSION
S/MIME
trust
Review Questions 
 
8.1 
What types of interoperability issues are involved in internet mail architecture and 
how are they handled?
 
8.2 
What are the SMTP and MIME standards?
 
8.3 
What is the difference between a MIME content type and a MIME transfer encoding?
 
8.4 
Briefly explain base64 encoding.
 
8.5 
Why is base64 conversion useful for an e-mail application?
 
8.6 
What is S/MIME?
 
8.7 
What are the four principal services provided by S/MIME?
 
8.8 
What is the utility of a detached signature?
 
8.9 
What is DKIM?
Problems 
 
8.1 
The character sequence ‚Äú<CR><LF>.<CR><LF>‚Äù indicates the end of mail data to a 
SMTP-server. What happens if the mail data itself contains that character sequence?
 
8.2 
What are POP3 and IMAP?
 
8.3 
If a lossless compression algorithm, such as ZIP, is used with S/MIME, why is it pref-
erable to generate a signature before applying compression?
 
8.4 
Before the deployment of the Domain Name System, a simple text file (HOSTS.
TXT) centrally maintained at the SRI Network Information Center was used to 
 enable mapping between host names and addresses. Each host connected to the 
Internet had to have an updated local copy of it to be able to use host names instead 
of having to cope directly with their IP addresses. Discuss the main advantages of the 
DNS over the old centralized HOSTS.TXT system.
 
8.5 
For this problem and the next few, consult Appendix H. In Figure H.2, each entry in 
the public-key ring contains an Owner Trust field that indicates the degree of trust 
 associated with this public-key owner. Why is that not enough? That is, if this owner 
is trusted and this is supposed to be the owner‚Äôs public key, why is that trust not 
enough to permit PGP to use this public key?

8.6 
What is the basic difference between X.509 and PGP in terms of key hierarchies and 
key trust?
 
8.7 
In PGP, what is the expected number of session keys generated before a previously 
created key is produced?
 
8.8 
A PGP user may have multiple public keys. So that a recipient knows which public 
key is being used by a sender, a key ID, consisting of the least significant 64 bits of the 
public key, is sent with the message. What is the probability that a user with N public 
keys will have at least one duplicate key ID?
 
8.9 
The first 16 bits of the message digest in a PGP signature are translated in the clear. 
This enables the recipient to determine if the correct public key was used to decrypt 
the message digest by comparing this plaintext copy of the first two octets with the 
first two octets of the decrypted digest.
a. To what extent does this compromise the security of the hash algorithm?
b. To what extent does it in fact perform its intended function, namely, to help deter-
mine if the correct RSA key was used to decrypt the digest?
 
8.10 
Consider base64 conversion as a form of encryption. In this case, there is no key. But 
suppose that an opponent knew only that some form of substitution algorithm was 
being used to encrypt English text and did not guess that it was base64. How effective 
would this algorithm be against cryptanalysis?
 
8.11 
Encode the text ‚Äúciphertext‚Äù using the following techniques. Assume characters are 
stored in 8-bit ASCII with zero parity.
a. base64
b. Quoted-printable
 
8.12 
Use a 2 * 2 matrix to categorize the properties of the four certificate usage models in 
DANE.

302
IP Security
Chapter
9.1 
IP Security Overview
Applications of IPsec
Benefits of IPsec
Routing Applications
IPsec Documents
IPsec Services
Transport and Tunnel Modes
9.2 
IP Security Policy
Security Associations
Security Association Database
Security Policy Database
IP Traffic Processing
9.3 
Encapsulating Security Payload
ESP Format
Encryption and Authentication Algorithms
Padding
Anti-Replay Service
Transport and Tunnel Modes
9.4 
Combining Security Associations
Authentication Plus Confidentiality
Basic Combinations of Security Associations
9.5 
Internet Key Exchange
Key Determination Protocol
Header and Payload Formats
9.6 
Cryptographic Suites
9.7 
Key Terms, Review Questions, and Problems

There are application-specific security mechanisms for a number of application 
areas, including electronic mail (S/MIME, PGP), client/server (Kerberos), Web 
 access (Secure Sockets Layer), and others. However, users have security concerns 
that cut across protocol layers. For example, an enterprise can run a secure, private 
IP network by disallowing links to untrusted sites, encrypting packets that leave 
the premises, and authenticating packets that enter the premises. By implementing 
security at the IP level, an organization can ensure secure networking not only for 
applications that have security mechanisms but also for the many security-ignorant 
applications.
IP-level security encompasses three functional areas: authentication, confiden-
tiality, and key management. The authentication mechanism assures that a received 
packet was, in fact, transmitted by the party identified as the source in the packet 
header. In addition, this mechanism assures that the packet has not been altered in 
transit. The confidentiality facility enables communicating nodes to encrypt messages 
to prevent eavesdropping by third parties. The key management facility is concerned 
with the secure exchange of keys.
We begin this chapter with an overview of IP security (IPsec) and an introduc-
tion to the IPsec architecture. We then look at each of the three functional areas in 
detail. Appendix D reviews Internet protocols.
 9.1 Ip SecurIty OvervIew
In 1994, the Internet Architecture Board (IAB) issued a report titled ‚ÄúSecurity in 
the Internet Architecture‚Äù (RFC 1636). The report identified key areas for security 
mechanisms. Among these were the need to secure the network infrastructure from 
LearnIng ObjectIveS
After studying this chapter, you should be able to:
‚óÜ‚óÜ
Present an overview of IP security (IPsec).
‚óÜ‚óÜ
Explain the difference between transport mode and tunnel mode.
‚óÜ‚óÜ
Understand the concept of security association.
‚óÜ‚óÜ
Explain the difference between the security association database and the 
security policy database.
‚óÜ‚óÜ
Summarize the traffic processing functions performed by IPsec for out-
bound packets and for inbound packets.
‚óÜ‚óÜ
Present an overview of Encapsulating Security Payload.
‚óÜ‚óÜ
Discuss the alternatives for combining security associations.
‚óÜ‚óÜ
Present an overview of Internet Key Exchange.
‚óÜ‚óÜ
Summarize the alternative cryptographic suites approved for use with IPsec.

unauthorized monitoring and control of network traffic and the need to secure end-
user-to-end-user traffic using authentication and encryption mechanisms.
To provide security, the IAB included authentication and encryption as nec-
essary security features in the next-generation IP, which has been issued as IPv6. 
Fortunately, these security capabilities were designed to be usable both with the 
current IPv4 and the future IPv6. This means that vendors can begin offering these 
features now, and many vendors now do have some IPsec capability in their prod-
ucts. The IPsec specification now exists as a set of Internet standards.
Applications of IPsec
IPsec provides the capability to secure communications across a LAN, across pri-
vate and public WANs, and across the Internet. Examples of its use include:
‚óÜ
‚ñ† Secure branch office connectivity over the Internet: A company can build a 
secure virtual private network over the Internet or over a public WAN. This 
enables a business to rely heavily on the Internet and reduce its need for pri-
vate networks, saving costs and network management overhead.
‚óÜ
‚ñ† Secure remote access over the Internet: An end user whose system is equipped 
with IP security protocols can make a local call to an Internet Service Provider 
(ISP) and gain secure access to a company network. This reduces the cost of 
toll charges for traveling employees and telecommuters.
‚óÜ
‚ñ† Establishing extranet and intranet connectivity with partners: IPsec can be 
used to secure communication with other organizations, ensuring authentica-
tion and confidentiality and providing a key exchange mechanism.
‚óÜ
‚ñ† Enhancing electronic commerce security: Even though some Web and elec-
tronic commerce applications have built-in security protocols, the use of IPsec 
enhances that security. IPsec guarantees that all traffic designated by the net-
work administrator is both encrypted and authenticated, adding an additional 
layer of security to whatever is provided at the application layer.
The principal feature of IPsec that enables it to support these varied applica-
tions is that it can encrypt and/or authenticate all traffic at the IP level. Thus, all dis-
tributed applications (including remote logon, client/server, e-mail, file transfer, Web 
access, and so on) can be secured. Figure 9.1a shows a simplified packet format for 
an IPsec option known as tunnel mode, described subsequently. Tunnel mode makes 
use of an IPsec function, a combined authentication/encryption  function called 
Encapsulating Security Payload (ESP), and a key exchange function. For VPNs, 
both authentication and encryption are generally desired, because it is  important 
both to (1) assure that unauthorized users do not penetrate the VPN, and (2)¬†assure 
that eavesdroppers on the Internet cannot read messages sent over the VPN.
Figure 9.1b is a typical scenario of IPsec usage. An organization maintains 
LANs at dispersed locations. Nonsecure IP traffic is conducted on each LAN. For 
traffic offsite, through some sort of private or public WAN, IPsec protocols are used. 
These protocols operate in networking devices, such as a router or firewall, that 
connect each LAN to the outside world. The IPsec networking device will  typically 
encrypt all traffic going into the WAN and decrypt traffic coming from the WAN; 
these operations are transparent to workstations and servers on the LAN. Secure

transmission is also possible with individual users who dial into the WAN. Such user 
workstations must implement the IPsec protocols to provide security.
Benefits of IPsec
Some of the benefits of IPsec:
‚óÜ
‚ñ† When IPsec is implemented in a firewall or router, it provides strong security 
that can be applied to all traffic crossing the perimeter. Traffic within a com-
pany or workgroup does not incur the overhead of security-related processing.
Figure 9.1 An IPSec VPN Scenario
Networking device
with IPSec
Ethernet
switch
Unprotected
IP trafc
Legend:
User system
with IPSec
(a) Tunnel-mode format
(b) Example confguration
Public (Internet)
or private
network
Authenticated
Encrypted
ESP
auth
orig IP
hdr
IP payload
ESP
trlr
ESP
hdr
IP trafc
protected
by IPSec
Virtual tunnel:
protected
by IPSec
New IP
hdr

‚óÜ
‚ñ† IPsec in a firewall is resistant to bypass if all traffic from the outside must use 
IP and the firewall is the only means of entrance from the Internet into the 
organization.
‚óÜ
‚ñ† IPsec is below the transport layer (TCP, UDP) and so is transparent to appli-
cations. There is no need to change software on a user or server system when 
IPsec is implemented in the firewall or router. Even if IPsec is implemented in 
end systems, upper-layer software, including applications, is not affected.
‚óÜ
‚ñ† IPsec can be transparent to end users. There is no need to train users on secu-
rity mechanisms, issue keying material on a per-user basis, or revoke keying 
material when users leave the organization.
‚óÜ
‚ñ† IPsec can provide security for individual users if needed. This is useful for off-
site workers and for setting up a secure virtual subnetwork within an organiza-
tion for sensitive applications.
Routing Applications
In addition to supporting end users and protecting premises systems and networks, 
IPsec can play a vital role in the routing architecture required for internetworking. 
[HUIT98] lists the following examples of the use of IPsec. IPsec can assure that
‚óÜ
‚ñ† A router advertisement (a new router advertises its presence) comes from an 
authorized router.
‚óÜ
‚ñ† A neighbor advertisement (a router seeks to establish or maintain a neighbor 
relationship with a router in another routing domain) comes from an autho-
rized router.
‚óÜ
‚ñ† A redirect message comes from the router to which the initial IP packet was sent.
‚óÜ
‚ñ† A routing update is not forged.
Without such security measures, an opponent can disrupt communications 
or divert some traffic. Routing protocols such as Open Shortest Path First (OSPF) 
should be run on top of security associations between routers that are defined by 
IPsec.
IPsec Documents
IPsec encompasses three functional areas: authentication, confidentiality, and key 
management. The totality of the IPsec specification is scattered across dozens of 
RFCs and draft IETF documents, making this the most complex and difficult to 
grasp of all IETF specifications. The best way to grasp the scope of IPsec is to 
consult the latest version of the IPsec document roadmap, which as of this writ-
ing is RFC 6071 (IP Security (IPsec) and Internet Key Exchange (IKE) Document 
Roadmap, February 2011). The documents can be categorized into the following 
groups.
‚óÜ
‚ñ† Architecture: Covers the general concepts, security requirements, definitions, 
and mechanisms defining IPsec technology. The current specification is RFC 
4301, Security Architecture for the Internet Protocol.

‚óÜ
‚ñ† Authentication Header (AH): AH is an extension header to provide mes-
sage authentication. The current specification is RFC 4302, IP Authentication 
Header. Because message authentication is provided by ESP, the use of 
AH is deprecated. It is included in IPsecv3 for backward compatibility 
but should not be used in new applications. We do not discuss AH in this 
chapter.
‚óÜ
‚ñ† Encapsulating Security Payload (ESP): ESP consists of an encapsulat-
ing header and trailer used to provide encryption or combined encryption/ 
authentication. The current specification is RFC 4303, IP Encapsulating 
Security Payload (ESP).
‚óÜ
‚ñ† Internet Key Exchange (IKE): This is a collection of documents describing 
the key management schemes for use with IPsec. The main specification is 
RFC 7296, Internet Key Exchange (IKEv2) Protocol, but there are a number 
of  related RFCs.
‚óÜ
‚ñ† Cryptographic algorithms: This category encompasses a large set of docu-
ments that define and describe cryptographic algorithms for encryption, mes-
sage authentication, pseudorandom functions (PRFs), and cryptographic key 
exchange.
‚óÜ
‚ñ† Other: There are a variety of other IPsec-related RFCs, including those deal-
ing with security policy and management information base (MIB) content.
IPsec Services
IPsec provides security services at the IP layer by enabling a system to select 
 required security protocols, determine the algorithm(s) to use for the service(s), 
and put in place any cryptographic keys required to provide the requested  services. 
Two protocols are used to provide security: an authentication protocol designated 
by the header of the protocol, Authentication Header (AH); and a combined 
 encryption/authentication protocol designated by the format of the packet for 
that protocol, Encapsulating Security Payload (ESP). RFC 4301 lists the following 
services:
‚óÜ
‚ñ† Access control
‚óÜ
‚ñ† Connectionless integrity
‚óÜ
‚ñ† Data origin authentication
‚óÜ
‚ñ† Rejection of replayed packets (a form of partial sequence integrity)
‚óÜ
‚ñ† Confidentiality (encryption)
‚óÜ
‚ñ† Limited traffic flow confidentiality
Transport and Tunnel Modes
Both AH and ESP support two modes of use: transport and tunnel mode. The oper-
ation of these two modes is best understood in the context of a description of ESP, 
which is covered in Section 9.3. Here we provide a brief overview.

TransporT Mode Transport mode provides protection primarily for upper-layer 
protocols. That is, transport mode protection extends to the payload of an IP 
packet.1 Examples include a TCP or UDP segment or an ICMP packet, all of which 
operate directly above IP in a host protocol stack. Typically, transport mode is used 
for end-to-end communication between two hosts (e.g., a client and a server, or two 
workstations). When a host runs AH or ESP over IPv4, the payload is the data that 
normally follow the IP header. For IPv6, the payload is the data that normally fol-
low both the IP header and any IPv6 extensions headers that are present, with the 
possible exception of the destination options header, which may be included in the 
protection.
ESP in transport mode encrypts and optionally authenticates the IP payload 
but not the IP header. AH in transport mode authenticates the IP payload and 
 selected portions of the IP header.
Tunnel Mode Tunnel mode provides protection to the entire IP packet. To achieve 
this, after the AH or ESP fields are added to the IP packet, the entire packet plus 
security fields is treated as the payload of new outer IP packet with a new outer 
IP header. The entire original, inner, packet travels through a tunnel from one 
point of an IP network to another; no routers along the way are able to examine 
the inner IP header. Because the original packet is encapsulated, the new, larger 
packet may have totally different source and destination addresses, adding to the 
security. Tunnel mode is used when one or both ends of a security association (SA) 
are a security gateway, such as a firewall or router that implements IPsec. With tun-
nel mode, a number of hosts on networks behind firewalls may engage in secure 
communications without implementing IPsec. The unprotected packets generated 
by such hosts are tunneled through external networks by tunnel mode SAs set up 
by the IPsec software in the firewall or secure router at the boundary of the local 
network.
Here is an example of how tunnel mode IPsec operates. Host A on a network 
generates an IP packet with the destination address of host B on another network. 
This packet is routed from the originating host to a firewall or secure router at the 
boundary of A‚Äôs network. The firewall filters all outgoing packets to determine the 
need for IPsec processing. If this packet from A to B requires IPsec, the firewall 
performs IPsec processing and encapsulates the packet with an outer IP header. 
The source IP address of this outer IP packet is this firewall, and the destination 
address may be a firewall that forms the boundary to B‚Äôs local network. This packet 
is now routed to B‚Äôs firewall, with intermediate routers examining only the outer IP 
header. At B‚Äôs firewall, the outer IP header is stripped off, and the inner packet is 
delivered to B.
ESP in tunnel mode encrypts and optionally authenticates the entire inner IP 
packet, including the inner IP header. AH in tunnel mode authenticates the entire 
inner IP packet and selected portions of the outer IP header.
Table 9.1 summarizes transport and tunnel mode functionality.
1In this chapter, the term IP packet refers to either an IPv4 datagram or an IPv6 packet.

9.2 Ip SecurIty pOLIcy
Fundamental to the operation of IPsec is the concept of a security policy  applied 
to each IP packet that transits from a source to a destination. IPsec policy is 
 determined primarily by the interaction of two databases, the security association 
 database (SAD) and the security policy database (SPD). This section provides an 
overview of these two databases and then summarizes their use during IPsec opera-
tion. Figure 9.2 illustrates the relevant relationships.
Security Associations
A key concept that appears in both the authentication and confidentiality mecha-
nisms for IP is the security association (SA). An association is a one-way logical 
connection between a sender and a receiver that affords security services to the traf-
fic carried on it. If a peer relationship is needed for two-way secure exchange, then 
two security associations are required.
A security association is uniquely identified by three parameters.
‚óÜ
‚ñ† Security Parameters Index (SPI): A 32-bit unsigned integer assigned to this 
SA and having local significance only. The SPI is carried in AH and ESP head-
ers to enable the receiving system to select the SA under which a received 
packet will be processed.
‚óÜ
‚ñ† IP Destination Address: This is the address of the destination endpoint of the 
SA, which may be an end-user system or a network system such as a firewall 
or router.
‚óÜ
‚ñ† Security Protocol Identifier: This field from the outer IP header indicates 
whether the association is an AH or ESP security association.
Hence, in any IP packet, the security association is uniquely identified by the 
Destination Address in the IPv4 or IPv6 header and the SPI in the enclosed exten-
sion header (AH or ESP).
Transport Mode SA
Tunnel Mode SA
AH
Authenticates IP payload and selected 
portions of IP header and IPv6 
extension headers.
Authenticates entire inner IP packet (inner 
header plus IP payload) plus selected portions 
of outer IP header and outer IPv6 extension 
headers.
ESP
Encrypts IP payload and any IPv6 
extension headers following the ESP 
header.
Encrypts entire inner IP packet.
ESP with 
Authentication
Encrypts IP payload and any IPv6 
extension headers following the ESP 
header. Authenticates IP payload but 
not IP header.
Encrypts entire inner IP packet. Authenticates 
inner IP packet.
Table 9.1 Tunnel Mode and Transport Mode Functionality

Security Association Database
In each IPsec implementation, there is a nominal2 Security Association Database 
that defines the parameters associated with each SA. A security association is nor-
mally defined by the following parameters in an SAD entry.
‚óÜ
‚ñ† Security Parameter Index: A 32-bit value selected by the receiving end of an 
SA to uniquely identify the SA. In an SAD entry for an outbound SA, the SPI 
is used to construct the packet‚Äôs AH or ESP header. In an SAD entry for an 
inbound SA, the SPI is used to map traffic to the appropriate SA.
‚óÜ
‚ñ† Sequence Number Counter: A 32-bit value used to generate the Sequence 
Number field in AH or ESP headers, described in Section 9.3 (required for all 
implementations).
‚óÜ
‚ñ† Sequence Counter Overflow: A flag indicating whether overflow of the 
Sequence Number Counter should generate an auditable event and prevent 
further transmission of packets on this SA (required for all implementations).
‚óÜ
‚ñ† Anti-Replay Window: Used to determine whether an inbound AH or ESP 
packet is a replay, described in Section 9.3 (required for all implementations).
‚óÜ
‚ñ† AH Information: Authentication algorithm, keys, key lifetimes, and related 
parameters being used with AH (required for AH implementations).
‚óÜ
‚ñ† ESP Information: Encryption and authentication algorithm, keys, initialization 
values, key lifetimes, and related parameters being used with ESP  (required 
for ESP implementations).
‚óÜ
‚ñ† Lifetime of this Security Association: A time interval or byte count after 
which an SA must be replaced with a new SA (and new SPI) or terminated, 
plus an indication of which of these actions should occur (required for all 
implementations).
2Nominal in the sense that the functionality provided by a Security Association Database must be present 
in any IPsec implementation, but the way in which that functionality is provided is up to the implementer.
Figure 9.2 IPsec Architecture
SPD
SPD
SAD
IKEv2
IKEv2
IPsecv3
IPsecv3
Security
association
database
Key exchange
IKE SA
IPsec SA Pair
ESP protects data
Security
association
database
Security
policy
database
Security
policy
database
SAD

‚óÜ
‚ñ† IPsec Protocol Mode: Tunnel, transport, or wildcard.
‚óÜ
‚ñ† Path MTU: Any observed path maximum transmission unit (maximum size of 
a packet that can be transmitted without fragmentation) and aging variables 
(required for all implementations).
The key management mechanism that is used to distribute keys is coupled to 
the authentication and privacy mechanisms only by way of the Security Parameters 
Index (SPI). Hence, authentication and privacy have been specified independent of 
any specific key management mechanism.
IPsec provides the user with considerable flexibility in the way in which IPsec 
services are applied to IP traffic. As we will see later, SAs can be combined in a 
number of ways to yield the desired user configuration. Furthermore, IPsec pro-
vides a high degree of granularity in discriminating between traffic that is afforded 
IPsec protection and traffic that is allowed to bypass IPsec, as in the former case 
relating IP traffic to specific SAs.
Security Policy Database
The means by which IP traffic is related to specific SAs (or no SA in the case of traffic 
allowed to bypass IPsec) is the nominal Security Policy Database (SPD). In its simplest 
form, an SPD contains entries, each of which defines a subset of IP traffic and points 
to an SA for that traffic. In more complex environments, there may be multiple entries 
that potentially relate to a single SA or multiple SAs associated with a single SPD 
entry. The reader is referred to the relevant IPsec documents for a full discussion.
Each SPD entry is defined by a set of IP and upper-layer protocol field values, 
called selectors. In effect, these selectors are used to filter outgoing traffic in order 
to map it into a particular SA. Outbound processing obeys the following general 
sequence for each IP packet.
1. Compare the values of the appropriate fields in the packet (the selector fields) 
against the SPD to find a matching SPD entry, which will point to zero or more SAs.
2. Determine the SA if any for this packet and its associated SPI.
3. Do the required IPsec processing (i.e., AH or ESP processing).
The following selectors determine an SPD entry:
‚óÜ
‚ñ† Remote IP Address: This may be a single IP address, an enumerated list or 
range of addresses, or a wildcard (mask) address. The latter two are required to 
support more than one destination system sharing the same SA (e.g., behind 
a firewall).
‚óÜ
‚ñ† Local IP Address: This may be a single IP address, an enumerated list or range 
of addresses, or a wildcard (mask) address. The latter two are required to sup-
port more than one source system sharing the same SA (e.g., behind a firewall).
‚óÜ
‚ñ† Next Layer Protocol: The IP protocol header (IPv4, IPv6, or IPv6 Extension) 
includes a field (Protocol for IPv4, Next Header for IPv6 or IPv6 Extension) 
that designates the protocol operating over IP. This is an individual protocol 
number, ANY, or for IPv6 only, OPAQUE. If AH or ESP is used, then this IP 
protocol header immediately precedes the AH or ESP header in the packet.

‚óÜ
‚ñ† Name: A user identifier from the operating system. This is not a field in the IP 
or upper-layer headers but is available if IPsec is running on the same operat-
ing system as the user.
‚óÜ
‚ñ† Local and Remote Ports: These may be individual TCP or UDP port values, an 
enumerated list of ports, or a wildcard port.
Table 9.2 provides an example of an SPD on a host system (as opposed to 
a network system such as a firewall or router). This table reflects the following 
 configuration: A local network configuration consists of two networks. The basic 
corporate network configuration has the IP network number 1.2.3.0/24. The local 
configuration also includes a secure LAN, often known as a DMZ, that is identified 
as 1.2.4.0/24. The DMZ is protected from both the outside world and the rest of the 
corporate LAN by firewalls. The host in this example has the IP address 1.2.3.10, 
and it is authorized to connect to the server 1.2.4.10 in the DMZ.
The entries in the SPD should be self-explanatory. For example, UDP port 
500 is the designated port for IKE. Any traffic from the local host to a remote host 
for purposes of an IKE exchange bypasses the IPsec processing.
IP Traffic Processing
IPsec is executed on a packet-by-packet basis. When IPsec is implemented, each 
outbound IP packet is processed by the IPsec logic before transmission, and each 
inbound packet is processed by the IPsec logic after reception and before passing 
the packet contents on to the next higher layer (e.g., TCP or UDP). We look at the 
logic of these two situations in turn.
ouTbound packeTs Figure 9.3 highlights the main elements of IPsec processing for 
outbound traffic. A¬†block of data from a higher layer, such as TCP, is passed down 
to the IP layer and an IP packet is formed, consisting of an IP header and an IP 
body. Then the  following steps occur:
1. IPsec searches the SPD for a match to this packet.
2. If no match is found, then the packet is discarded and an error message is generated.
Protocol
Local IP
Port
Remote IP
Port
Action
Comment
UDP
1.2.3.101
500
*
500
BYPASS
IKE
ICMP
1.2.3.101
*
*
*
BYPASS
Error messages
*
1.2.3.101
*
1.2.3.0/24
*
PROTECT: ESP 
intransport-mode
Encrypt intranet traffic
TCP
1.2.3.101
*
1.2.4.10
80
PROTECT: ESP 
intransport-mode
Encrypt to server
TCP
1.2.3.101
*
1.2.4.10
443
BYPASS
TLS: avoid double encryption
*
1.2.3.101
*
1.2.4.0/24
*
DISCARD
Others in DMZ
*
1.2.3.101
*
*
*
BYPASS
Internet
Table 9.2 Host SPD Example

3. If a match is found, further processing is determined by the first matching 
entry in the SPD. If the policy for this packet is DISCARD, then the packet is 
discarded. If the policy is BYPASS, then there is no further IPsec processing; 
the packet is forwarded to the network for transmission.
4. If the policy is PROTECT, then a search is made of the SAD for a match-
ing entry. If no entry is found, then IKE is invoked to create an SA with the 
 appropriate keys and an entry is made in the SA.
5. The matching entry in the SAD determines the processing for this packet. 
Either encryption, authentication, or both can be performed, and either trans-
port or tunnel mode can be used. The packet is then forwarded to the network 
for transmission.
Inbound packeTs Figure 9.4 highlights the main elements of IPsec processing for 
inbound traffic. An incoming IP packet triggers the IPsec processing. The following 
steps occur:
1. IPsec determines whether this is an unsecured IP packet or one that has ESP 
or AH headers/trailers, by examining the IP Protocol field (IPv4) or Next 
Header field (IPv6).
Figure 9.3 Processing Model for Outbound Packets
Search
security policy
database
Search
security association
database
Determine
policy
Outbound IP packet
(e.g., from TCP or UDP)
Discard
packet
No match
found
No match
found
Match found
Match
found
DISCARD
PROTECT
BYPASS
Forward
packet via
IP
Internet
key
exchange
Process
(AH/ESP)

2. If the packet is unsecured, IPsec searches the SPD for a match to this packet. 
If the first matching entry has a policy of BYPASS, the IP header is processed 
and stripped off and the packet body is delivered to the next higher layer, such 
as TCP. If the first matching entry has a policy of PROTECT or DISCARD, or 
if there is no matching entry, the packet is discarded.
3. For a secured packet, IPsec searches the SAD. If no match is found, the packet 
is discarded. Otherwise, IPsec applies the appropriate ESP or AH processing. 
Then, the IP header is processed and stripped off and the packet body is deliv-
ered to the next higher layer, such as TCP.
 9.3 encapSuLatIng SecurIty payLOad
ESP can be used to provide confidentiality, data origin authentication, connection-
less integrity, an anti-replay service (a form of partial sequence integrity), and (lim-
ited) traffic flow confidentiality. The set of services provided depends on options 
selected at the time of Security Association (SA) establishment and on the location 
of the implementation in a network topology.
ESP can work with a variety of encryption and authentication algorithms, 
 including authenticated encryption algorithms such as GCM.
ESP Format
Figure 9.5a shows the top-level format of an ESP packet. It contains the following fields.
Figure 9.4 Processing Model for Inbound Packets
Search
security policy
database
Search
security association
database
Packet
type
Inbound IP packet
(from Internet)
Discard
packet
No match
found
IPsec
P
I
Not
BYPASS
Match
found
BYPASS
Deliver packet
to higher layer
(e.g., TCP, UDP)
Process
(AH/ESP)

‚óÜ
‚ñ† Security Parameters Index (32 bits): Identifies a security association.
‚óÜ
‚ñ† Sequence Number (32 bits): A monotonically increasing counter value; this 
provides an anti-replay function, as discussed for AH.
‚óÜ
‚ñ† Payload Data (variable): This is a transport-level segment (transport mode) or 
IP packet (tunnel mode) that is protected by encryption.
‚óÜ
‚ñ† Padding (0‚Äì255 bytes): The purpose of this field is discussed later.
‚óÜ
‚ñ† Pad Length (8 bits): Indicates the number of pad bytes immediately preceding 
this field.
‚óÜ
‚ñ† Next Header (8 bits): Identifies the type of data contained in the payload data 
field by identifying the first header in that payload (e.g., an extension header 
in IPv6, or an upper-layer protocol such as TCP).
‚óÜ
‚ñ† Integrity Check Value (variable): A variable-length field (must be an integral 
number of 32-bit words) that contains the Integrity Check Value computed 
over the ESP packet minus the Authentication Data field.
Figure 9.5 ESP Packet Format
Security parameters index (SPI)
32 bits
Sequence number 
Padding (0‚Äì255 bytes)
Pad length
Next header
Payload data (variable)
Integrity check value - ICV (variable)
ICV coverage
Encrypted
Encrypted
(a)  Top-level format of an ESP Packet
(b)  Substructure of payload data
Security parameters index (SPI)
Sequence number 
Initialization value - IV (optional)
Padding (0‚Äì255 bytes)
TFC padding (optional, variable)
Pad length
Next header
Rest of payload data (variable)
Integrity check value - ICV (variable)
ICV coverage
Payload

When any combined mode algorithm is employed, the algorithm itself is 
 expected to return both decrypted plaintext and a pass/fail indication for the integ-
rity check. For combined mode algorithms, the ICV that would normally appear 
at the end of the ESP packet (when integrity is selected) may be omitted. When 
the ICV is omitted and integrity is selected, it is the responsibility of the combined 
mode algorithm to encode within the Payload Data an ICV-equivalent means of 
verifying the integrity of the packet.
Two additional fields may be present in the payload (Figure 9.5b). An 
 initialization value (IV), or nonce, is present if this is required by the encryption 
or authenticated encryption algorithm used for ESP. If tunnel mode is being used, 
then the IPsec implementation may add traffic flow confidentiality (TFC) padding 
after the Payload Data and before the Padding field, as explained subsequently.
Encryption and Authentication Algorithms
The Payload Data, Padding, Pad Length, and Next Header fields are encrypted by 
the ESP service. If the algorithm used to encrypt the payload requires cryptographic 
synchronization data, such as an initialization vector (IV), then these data may be 
carried explicitly at the beginning of the Payload Data field. If included, an IV is 
usually not encrypted, although it is often referred to as being part of the ciphertext.
The ICV field is optional. It is present only if the integrity service is selected 
and is provided by either a separate integrity algorithm or a combined mode algo-
rithm that uses an ICV. The ICV is computed after the encryption is performed. 
This order of processing facilitates rapid detection and rejection of replayed or 
bogus packets by the receiver prior to decrypting the packet, hence potentially re-
ducing the impact of denial of service (DoS) attacks. It also allows for the possibility 
of parallel processing of packets at the receiver that is decryption can take place 
in parallel with integrity checking. Note that because the ICV is not protected by 
 encryption, a keyed integrity algorithm must be employed to compute the ICV.
Padding
The Padding field serves several purposes:
‚óÜ
‚ñ† If an encryption algorithm requires the plaintext to be a multiple of some 
number of bytes (e.g., the multiple of a single block for a block cipher), the 
Padding field is used to expand the plaintext (consisting of the Payload Data, 
Padding, Pad Length, and Next Header fields) to the required length.
‚óÜ
‚ñ† The ESP format requires that the Pad Length and Next Header fields be right 
aligned within a 32-bit word. Equivalently, the ciphertext must be an integer 
multiple of 32 bits. The Padding field is used to assure this alignment.
‚óÜ
‚ñ† Additional padding may be added to provide partial traffic-flow confidential-
ity by concealing the actual length of the payload.
Anti-Replay Service
A replay attack is one in which an attacker obtains a copy of an authenticated 
packet and later transmits it to the intended destination. The receipt of duplicate, 
authenticated IP packets may disrupt service in some way or may have some other 
 undesired consequence. The Sequence Number field is designed to thwart such

attacks. First, we discuss sequence number generation by the sender, and then we 
look at how it is processed by the recipient.
When a new SA is established, the sender initializes a sequence number 
 counter to 0. Each time that a packet is sent on this SA, the sender increments the 
counter and places the value in the Sequence Number field. Thus, the first value to 
be used is 1. If anti-replay is enabled (the default), the sender must not allow the 
sequence number to cycle past 232 - 1 back to zero. Otherwise, there would be mul-
tiple valid packets with the same sequence number. If the limit of 232 - 1 is reached, 
the sender should terminate this SA and negotiate a new SA with a new key.
Because IP is a connectionless, unreliable service, the protocol does not guar-
antee that packets will be delivered in order and does not guarantee that all packets 
will be delivered. Therefore, the IPsec authentication document dictates that the 
receiver should implement a window of size W, with a default of W = 64. The right 
edge of the window represents the highest sequence number, N, so far received for a 
valid packet. For any packet with a sequence number in the range from N - W + 1 
to N that has been correctly received (i.e., properly authenticated), the correspond-
ing slot in the window is marked (Figure 9.6). Inbound processing proceeds as fol-
lows when a packet is received:
1. If the received packet falls within the window and is new, the MAC is checked. 
If the packet is authenticated, the corresponding slot in the window is marked.
2. If the received packet is to the right of the window and is new, the MAC is 
checked. If the packet is authenticated, the window is advanced so that this 
sequence number is the right edge of the window, and the corresponding slot 
in the window is marked.
3. If the received packet is to the left of the window or if authentication fails, the 
packet is discarded; this is an auditable event.
Transport and Tunnel Modes
Figure 9.7 shows two ways in which the IPsec ESP service can be used. In the upper 
part of the figure, encryption (and optionally authentication) is provided directly 
 between two hosts. Figure 9.7b shows how tunnel mode operation can be used to set up 
a virtual private network. In this example, an organization has four private networks  
Figure 9.6 Anti-replay Mechanism
Fixed window size W
N
N + 1
N ‚Äì W
Marked if valid
packet received
Unmarked if valid
packet not yet received
‚Ä¢ ‚Ä¢ ‚Ä¢
Advance window if
valid packet to the
right is received

interconnected across the Internet. Hosts on the internal networks use the Internet 
for transport of data but do not interact with other Internet-based hosts. By termi-
nating the tunnels at the security gateway to each internal network, the configuration 
 allows the hosts to avoid implementing the security capability. The former technique is 
 supported by a transport mode SA, while the latter technique uses a tunnel mode SA.
In this section, we look at the scope of ESP for the two modes. The consid-
erations are somewhat different for IPv4 and IPv6. We use the packet formats of 
Figure 9.8a as a starting point.
TransporT Mode esp Transport mode ESP is used to encrypt and optionally 
 authenticate the data carried by IP (e.g., a TCP segment), as shown in Figure 9.8b. 
For this mode using IPv4, the ESP header is inserted into the IP packet immedi-
ately prior to the transport-layer header (e.g., TCP, UDP, ICMP), and an ESP 
trailer (Padding, Pad Length, and Next Header fields) is placed after the IP packet. 
If authentication is selected, the ESP Authentication Data field is added after the 
ESP trailer. The entire transport-level segment plus the ESP trailer are encrypted. 
Authentication covers all of the ciphertext plus the ESP header.
In the context of IPv6, ESP is viewed as an end-to-end payload; that is, it is not 
examined or processed by intermediate routers. Therefore, the ESP header  appears 
after the IPv6 base header and the hop-by-hop, routing, and fragment extension 
headers. The destination options extension header could appear before or after 
the ESP header, depending on the semantics desired. For IPv6, encryption covers 
Figure 9.7 Transport-Mode versus Tunnel-Mode Encryptionx
Internal
Network
External
Network
Encrypted
TCP Session
(a) Transport-level security
Internet
Corporate
network
Corporate
network
Corporate
network
Corporate
network
(b) A virtual private network via tunnel mode
Encrypted tunnels
carrying IP trafc

the entire transport-level segment plus the ESP trailer plus the destination options 
 extension header if it occurs after the ESP header. Again, authentication covers the 
ciphertext plus the ESP header.
Transport mode operation may be summarized as follows.
1. At the source, the block of data consisting of the ESP trailer plus the entire 
transport-layer segment is encrypted and the plaintext of this block is replaced 
Figure 9.8 Scope of ESP Encryption and Authentication
Orig IP
hdr
Hop-by-hop, dest,
routing, fragment
IPv6
Orig IP
hdr
IPv4
New IP
hdr
IPv4
(b) Transport Mode
New IP
hdr
Ext
headers
IPv6
Authenticated
Encrypted
Authenticated
Encrypted
Authenticated
Encrypted
Authenticated
Encrypted
(c) Tunnel Mode
Orig IP
hdr
Ext
headers
TCP
Data
ESP
trlr
ESP
auth
ESP
hdr
ESP
auth
Orig IP
hdr
TCP
Data
ESP
trlr
ESP
auth
ESP
hdr
Dest
TCP
Data
TCP
Data
ESP
trlr
ESP
auth
ESP
trlr
ESP
hdr
ESP
hdr
Orig IP
hdr
Extension headers
(if present)
TCP
Data
IPv6
Orig IP
hdr
TCP
Data
IPv4
(a) Before Applying ESP

with its ciphertext to form the IP packet for transmission. Authentication is 
added if this option is selected.
2. The packet is then routed to the destination. Each intermediate router needs 
to examine and process the IP header plus any plaintext IP extension headers 
but does not need to examine the ciphertext.
3. The destination node examines and processes the IP header plus any plaintext 
IP extension headers. Then, on the basis of the SPI in the ESP header, the 
destination node decrypts the remainder of the packet to recover the plaintext 
transport-layer segment.
Transport mode operation provides confidentiality for any application that 
uses it, thus avoiding the need to implement confidentiality in every individual 
 application. One drawback to this mode is that it is possible to do traffic analysis on 
the transmitted packets.
Tunnel Mode esp Tunnel mode ESP is used to encrypt an entire IP packet 
(Figure¬†9.8c). For this mode, the ESP header is prefixed to the packet and then the 
packet plus the ESP trailer is encrypted. This method can be used to counter traffic 
analysis.
Because the IP header contains the destination address and possibly source rout-
ing directives and hop-by-hop option information, it is not possible simply to transmit 
the encrypted IP packet prefixed by the ESP header. Intermediate routers would be 
unable to process such a packet. Therefore, it is necessary to encapsulate the entire 
block (ESP header plus ciphertext plus Authentication Data, if present) with a new IP 
header that will contain sufficient information for routing but not for traffic analysis.
Whereas the transport mode is suitable for protecting connections between 
hosts that support the ESP feature, the tunnel mode is useful in a configuration that 
includes a firewall or other sort of security gateway that protects a trusted network 
from external networks. In this latter case, encryption occurs only between an exter-
nal host and the security gateway or between two security gateways. This relieves 
hosts on the internal network of the processing burden of encryption and simplifies 
the key distribution task by reducing the number of needed keys. Further, it thwarts 
traffic analysis based on ultimate destination.
Consider a case in which an external host wishes to communicate with a host 
on an internal network protected by a firewall, and in which ESP is implemented 
in the external host and the firewalls. The following steps occur for transfer of a 
transport-layer segment from the external host to the internal host.
1. The source prepares an inner IP packet with a destination address of the target 
internal host. This packet is prefixed by an ESP header; then the packet and 
ESP trailer are encrypted and Authentication Data may be added. The result-
ing block is encapsulated with a new IP header (base header plus  optional 
 extensions such as routing and hop-by-hop options for IPv6) whose  destination 
address is the firewall; this forms the outer IP packet.
2. The outer packet is routed to the destination firewall. Each intermediate 
router needs to examine and process the outer IP header plus any outer IP 
extension headers but does not need to examine the ciphertext.

3. The destination firewall examines and processes the outer IP header plus any 
outer IP extension headers. Then, on the basis of the SPI in the ESP header, the 
destination node decrypts the remainder of the packet to recover the plaintext 
inner IP packet. This packet is then transmitted in the internal network.
4. The inner packet is routed through zero or more routers in the internal net-
work to the destination host.
Figure 9.9 shows the protocol architecture for the two modes.
Figure 9.9 Protocol Operation for ESP
Data
Data
TCP
hdr
Data
TCP
hdr
Data
Orig IP
hdr
TCP
hdr
Data
ESP
trlr
ESP
hdr
Orig IP
hdr
ESP
auth
New IP
hdr
TCP
hdr
Data
ESP
trlr
ESP
hdr
Orig IP
hdr
ESP
auth
TCP
hdr
Data
Orig IP
hdr
TCP
hdr
Data
Orig IP
hdr
TCP
hdr
Data
(a) Transport mode
(b) Tunnel mode
ESP
trlr
ESP
hdr
ESP
auth
Application
TCP
IP
IPsec
Application
TCP
IP
IPsec
IP

9.4 cOmbInIng SecurIty aSSOcIatIOnS
An individual SA can implement either the AH or ESP protocol but not both. 
Sometimes a particular traffic flow will call for the services provided by both AH 
and ESP. Further, a particular traffic flow may require IPsec services between hosts 
and, for that same flow, separate services between security gateways, such as fire-
walls. In all of these cases, multiple SAs must be employed for the same traffic flow 
to achieve the desired IPsec services. The term security association bundle refers to 
a sequence of SAs through which traffic must be processed to provide a desired set 
of IPsec services. The SAs in a bundle may terminate at different endpoints or at 
the same endpoints.
Security associations may be combined into bundles in two ways:
‚óÜ
‚ñ† Transport adjacency: Refers to applying more than one security protocol to 
the same IP packet without invoking tunneling. This approach to combining 
AH and ESP allows for only one level of combination; further nesting yields 
no added benefit since the processing is performed at one IPsec instance: the 
(ultimate) destination.
‚óÜ
‚ñ† Iterated tunneling: Refers to the application of multiple layers of security pro-
tocols effected through IP tunneling. This approach allows for multiple levels 
of nesting, since each tunnel can originate or terminate at a different IPsec site 
along the path.
The two approaches can be combined, for example, by having a transport SA 
 between hosts travel part of the way through a tunnel SA between security gateways.
One interesting issue that arises when considering SA bundles is the order 
in which authentication and encryption may be applied between a given pair of 
endpoints and the ways of doing so. We examine that issue next. Then we look at 
 combinations of SAs that involve at least one tunnel.
Authentication Plus Confidentiality
Encryption and authentication can be combined in order to transmit an IP packet 
that has both confidentiality and authentication between hosts. We look at several 
approaches.
esp wITh auThenTIcaTIon opTIon This approach is illustrated in Figure 9.8. 
In this approach, the user first applies ESP to the data to be protected and then 
 appends the authentication data field. There are actually two subcases:
‚óÜ
‚ñ† Transport mode ESP: Authentication and encryption apply to the IP payload 
delivered to the host, but the IP header is not protected.
‚óÜ
‚ñ† Tunnel mode ESP: Authentication applies to the entire IP packet delivered 
to the outer IP destination address (e.g., a firewall), and authentication is 
 performed at that destination. The entire inner IP packet is protected by the 
privacy mechanism for delivery to the inner IP destination.
For both cases, authentication applies to the ciphertext rather than the plaintext.

TransporT adjacency Another way to apply authentication after encryption is to 
use two bundled transport SAs, with the inner being an ESP SA and the outer being 
an AH SA. In this case, ESP is used without its authentication option. Because the 
inner SA is a transport SA, encryption is applied to the IP payload. The resulting 
packet consists of an IP header (and possibly IPv6 header extensions) followed by 
an ESP. AH is then applied in transport mode, so that authentication covers the 
ESP plus the original IP header (and extensions) except for mutable fields. The 
advantage of this approach over simply using a single ESP SA with the ESP authen-
tication option is that the authentication covers more fields, including the source 
and destination IP addresses. The disadvantage is the overhead of two SAs versus 
one SA.
TransporT-Tunnel bundle The use of authentication prior to encryption might 
be preferable for several reasons. First, because the authentication data are pro-
tected by encryption, it is impossible for anyone to intercept the message and alter 
the authentication data without detection. Second, it may be desirable to store the 
authentication information with the message at the destination for later reference. 
It is more convenient to do this if the authentication information applies to the un-
encrypted message; otherwise the message would have to be reencrypted to verify 
the authentication information.
One approach to applying authentication before encryption between two hosts 
is to use a bundle consisting of an inner AH transport SA and an outer ESP tunnel 
SA. In this case, authentication is applied to the IP payload plus the IP header (and 
extensions) except for mutable fields. The resulting IP packet is then processed 
in tunnel mode by ESP; the result is that the entire, authenticated inner packet is 
 encrypted and a new outer IP header (and extensions) is added.
Basic Combinations of Security Associations
The IPsec Architecture document lists four examples of combinations of SAs that 
must be supported by compliant IPsec hosts (e.g., workstation, server) or security 
gateways (e.g., firewall, router). These are illustrated in Figure 9.10. The lower part 
of each case in the figure represents the physical connectivity of the elements; the 
upper part represents logical connectivity via one or more nested SAs. Each SA can 
be either AH or ESP. For host-to-host SAs, the mode may be either transport or 
tunnel; otherwise it must be tunnel mode.
Case 1. All security is provided between end systems that implement IPsec. 
For any two end systems to communicate via an SA, they must share the appropri-
ate secret keys. Among the possible combinations are
a. AH in transport mode
b. ESP in transport mode
c. ESP followed by AH in transport mode (an ESP SA inside an AH SA)
d. Any one of a, b, or c inside an AH or ESP in tunnel mode
We have already discussed how these various combinations can be used to 
support authentication, encryption, authentication before encryption, and authenti-
cation after encryption.

Figure 9.10 Basic Combinations of Security Associations
Internet
Tunnel SA
One or Two SAs
Local
Intranet
Local
Intranet
Host*
Host*
Security
Gateway*
Security
Gateway*
(c) Case 3
Internet
Tunnel SA
Local
Intranet
Local
Intranet
Host
Host
Security
Gateway*
Security
Gateway*
(b) Case 2 
* = implements IPsec
Internet
One or More SAs
Local
Intranet
Local
Intranet
Host*
Host*
Router
Router
(a) Case 1
Internet
Local
Intranet
Host*
Host*
Security
Gateway*
(d) Case 4
Tunnel SA
One or Two SAs

Case 2. Security is provided only between gateways (routers, firewalls, etc.) 
and no hosts implement IPsec. This case illustrates simple virtual private network 
support. The security architecture document specifies that only a single tunnel SA is 
needed for this case. The tunnel could support AH, ESP, or ESP with the authenti-
cation option. Nested tunnels are not required, because the IPsec services apply to 
the entire inner packet.
Case 3. This builds on case 2 by adding end-to-end security. The same combi-
nations discussed for cases 1 and 2 are allowed here. The gateway-to-gateway tun-
nel provides either authentication, confidentiality, or both for all traffic between 
end systems. When the gateway-to-gateway tunnel is ESP, it also provides a limited 
form of traffic confidentiality. Individual hosts can implement any additional IPsec 
services required for given applications or given users by means of end-to-end SAs.
Case 4. This provides support for a remote host that uses the Internet to reach 
an organization‚Äôs firewall and then to gain access to some server or workstation 
behind the firewall. Only tunnel mode is required between the remote host and the 
firewall. As in case 1, one or two SAs may be used between the remote host and the 
local host.
 9.5 Internet Key exchange
The key management portion of IPsec involves the determination and distribution 
of secret keys. A typical requirement is four keys for communication between two 
applications: transmit and receive pairs for both integrity and confidentiality. The 
IPsec Architecture document mandates support for two types of key management:
‚óÜ
‚ñ† Manual: A system administrator manually configures each system with its own 
keys and with the keys of other communicating systems. This is practical for 
small, relatively static environments.
‚óÜ
‚ñ† Automated: An automated system enables the on-demand creation of keys for 
SAs and facilitates the use of keys in a large distributed system with an evolv-
ing configuration.
The default automated key management protocol for IPsec is referred to as 
ISAKMP/Oakley and consists of the following elements:
‚óÜ
‚ñ† Oakley Key Determination Protocol: Oakley is a key exchange protocol based 
on the Diffie‚ÄìHellman algorithm but providing added security. Oakley is 
 generic in that it does not dictate specific formats.
‚óÜ
‚ñ† Internet Security Association and Key Management Protocol (ISAKMP): 
ISAKMP provides a framework for Internet key management and provides 
the specific protocol support, including formats, for negotiation of security 
attributes.
ISAKMP by itself does not dictate a specific key exchange algorithm; rather, 
ISAKMP consists of a set of message types that enable the use of a variety of key 
exchange algorithms. Oakley is the specific key exchange algorithm mandated for 
use with the initial version of ISAKMP.

In IKEv2, the terms Oakley and ISAKMP are no longer used, and there 
are significant differences from the use of Oakley and ISAKMP in IKEv1. 
Nevertheless, the basic functionality is the same. In this section, we describe the 
IKEv2 specification.
Key Determination Protocol
IKE key determination is a refinement of the Diffie‚ÄìHellman key exchange algo-
rithm. Recall that Diffie‚ÄìHellman involves the following interaction between users 
A and B. There is prior agreement on two global parameters: q, a large prime num-
ber; and a, a primitive root of q. A selects a random integer XA as its private key and 
transmits to B its public key Œ•A = aXA mod q. Similarly, B selects a random integer 
XB as its private key and transmits to A its public key Œ•B = aXB mod q. Each side 
can now compute the secret session key:
 
K = (Œ•B)XA mod q = (Œ•A)XB mod q = aXAXB mod q 
The Diffie‚ÄìHellman algorithm has two attractive features:
‚óÜ
‚ñ† Secret keys are created only when needed. There is no need to store secret 
keys for a long period of time, exposing them to increased vulnerability.
‚óÜ
‚ñ† The exchange requires no pre-existing infrastructure other than an agreement 
on the global parameters.
However, there are a number of weaknesses to Diffie‚ÄìHellman, as pointed out in 
[HUIT98].
‚óÜ
‚ñ† It does not provide any information about the identities of the parties.
‚óÜ
‚ñ† It is subject to a man-in-the-middle attack, in which a third party C imperson-
ates B while communicating with A and impersonates A while communicating 
with B. Both A and B end up negotiating a key with C, which can then listen to 
and pass on traffic. The man-in-the-middle attack proceeds as
1. B sends his public key YB in a message addressed to A (see Figure 3.14).
2. The enemy (E) intercepts this message. E saves B‚Äôs public key and sends a 
message to A that has B‚Äôs User ID but E‚Äôs public key YE. This message is 
sent in such a way that it appears as though it was sent from B‚Äôs host system. 
A receives E‚Äôs message and stores E‚Äôs public key with B‚Äôs User ID. Similarly, 
E sends a message to B with E‚Äôs public key, purporting to come from A.
3. B computes a secret key K1 based on B‚Äôs private key and YE. A computes 
a secret key K2 based on A‚Äôs private key and YE. E computes K1 using E‚Äôs 
secret key XE and YB and computers K2 using XE and YA.
4. From now on, E is able to relay messages from A to B and from B to A, 
appropriately changing their encipherment en route in such a way that nei-
ther A nor B will know that they share their communication with E.
‚óÜ
‚ñ† It is computationally intensive. As a result, it is vulnerable to a clogging attack, 
in which an opponent requests a high number of keys. The victim spends con-
siderable computing resources doing useless modular exponentiation rather 
than real work.

IKE key determination is designed to retain the advantages of Diffie‚ÄìHellman, 
while countering its weaknesses.
FeaTures oF Ike key deTerMInaTIon The IKE key determination algorithm is 
characterized by five important features:
1. It employs a mechanism known as cookies to thwart clogging attacks.
2. It enables the two parties to negotiate a group; this, in essence, specifies the 
global parameters of the Diffie‚ÄìHellman key exchange.
3. It uses nonces to ensure against replay attacks.
4. It enables the exchange of Diffie‚ÄìHellman public key values.
5. It authenticates the Diffie‚ÄìHellman exchange to thwart man-in-the-middle 
attacks.
We have already discussed Diffie‚ÄìHellman. Let us look at the remainder 
of these elements in turn. First, consider the problem of clogging attacks. In this 
 attack, an opponent forges the source address of a legitimate user and sends a  public 
Diffie‚ÄìHellman key to the victim. The victim then performs a modular exponen-
tiation to compute the secret key. Repeated messages of this type can clog the 
 victim‚Äôs system with useless work. The cookie exchange requires that each side send 
a pseudorandom number, the cookie, in the initial message, which the other side 
acknowledges. This acknowledgment must be repeated in the first message of the 
Diffie‚ÄìHellman key exchange. If the source address was forged, the opponent gets 
no answer. Thus, an opponent can only force a user to generate acknowledgments 
and not to perform the Diffie‚ÄìHellman calculation.
IKE mandates that cookie generation satisfy three basic requirements:
1. The cookie must depend on the specific parties. This prevents an attacker from 
obtaining a cookie using a real IP address and UDP port and then using it to 
swamp the victim with requests from randomly chosen IP addresses or ports.
2. It must not be possible for anyone other than the issuing entity to generate 
cookies that will be accepted by that entity. This implies that the issuing entity 
will use local secret information in the generation and subsequent verification 
of a cookie. It must not be possible to deduce this secret information from any 
particular cookie. The point of this requirement is that the issuing entity need 
not save copies of its cookies, which are then more vulnerable to discovery, but 
can verify an incoming cookie acknowledgment when it needs to.
3. The cookie generation and verification methods must be fast to thwart attacks 
intended to sabotage processor resources.
The recommended method for creating the cookie is to perform a fast hash 
(e.g., MD5) over the IP Source and Destination addresses, the UDP Source and 
Destination ports, and a locally generated secret value.
IKE key determination supports the use of different groups for the Diffie‚Äì
Hellman key exchange. Each group includes the definition of the two global 
 parameters and the identity of the algorithm. The current specification includes the 
following groups.

‚óÜ
‚ñ† Modular exponentiation with a 768-bit modulus
q = 2768 - 2704 - 1 + 264 * (:2638 * p; + 149686)
a = 2
‚óÜ
‚ñ† Modular exponentiation with a 1024-bit modulus
q = 21024 - 2960 - 1 + 264 * (:2894 * p; + 129093)
a = 2
‚óÜ
‚ñ† Modular exponentiation with a 1536-bit modulus
‚óÜ
‚Äì‚óÜ Parameters to be determined
‚óÜ
‚ñ† Elliptic curve group over 2155
‚óÜ
‚Äì‚óÜ Generator (hexadecimal): X = 7B, Y = 1C8
‚óÜ
‚Äì‚óÜ Elliptic curve parameters (hexadecimal): A = 0, Y = 7338F
‚óÜ
‚ñ† Elliptic curve group over 2185
‚óÜ
‚Äì‚óÜ Generator (hexadecimal): X = 18, Y = D
‚óÜ
‚Äì‚óÜ Elliptic curve parameters (hexadecimal): A = 0, Y = 1EE9
The first three groups are the classic Diffie‚ÄìHellman algorithm using modular 
exponentiation. The last two groups use the elliptic curve analog to Diffie‚ÄìHellman.
IKE key determination employs nonces to ensure against replay attacks. 
Each nonce is a locally generated pseudorandom number. Nonces appear in 
 responses and are encrypted during certain portions of the exchange to secure 
their use.
Three different authentication methods can be used with IKE key determination:
‚óÜ
‚ñ† Digital signatures: The exchange is authenticated by signing a mutually 
 obtainable hash; each party encrypts the hash with its private key. The hash is 
generated over important parameters, such as user IDs and nonces.
‚óÜ
‚ñ† Public-key encryption: The exchange is authenticated by encrypting param-
eters such as IDs and nonces with the sender‚Äôs private key.
‚óÜ
‚ñ† Symmetric-key encryption: A key derived by some out-of-band mechanism 
can be used to authenticate the exchange by symmetric encryption of  exchange 
parameters.
Ikev2 exchanges The IKEv2 protocol involves the exchange of messages 
in pairs. The first two pairs of exchanges are referred to as the initial exchanges 
(Figure ¬†9.11a). In the first exchange, the two peers exchange information concern-
ing cryptographic algorithms and other security parameters they are willing to use 
along with nonces and Diffie‚ÄìHellman (DH) values. The result of this exchange is to 
set up a special SA called the IKE SA (see Figure 9.2). This SA defines parameters 
for a secure channel between the peers over which subsequent message exchanges 
take place. Thus, all subsequent IKE message exchanges are protected by encryp-
tion and message authentication. In the second exchange, the two parties authenti-
cate one another and set up a first IPsec SA to be placed in the SADB and used for

Figure 9.11 IKEv2 Exchanges
HDR, SAi1, KEi, Ni
Responder
Initiator
(a) Initial exchanges
HDR, SAr1, KEr, Nr, [CERTREQ]
HDR, SK {IDi, [CERT,] [CERTREQ,] [IDr,] AUTH, SAi2, TSi, TSr}
HDR, SK {IDr, [CERT,] AUTH, SAr2, TSi, TSr}
HDR, SK {[N], SA, Ni, [KEi], [TSi, TSr]}
(b) CREATE_CHILD_SA exchange
HDR, SK {SA, Nr, [KEr], [TSi, TSr]}
HDR, SK {[N,] [D,] [CP,] ...}
(c) Informational exchange
HDR, SK {[N,] [D,] [CP], ...}
HDR = IKE header
SAx1 = ofered and chosen algorithms, DH group
KEx = Dife‚ÄìHellman public key
Nx = nonces
CERTREQ = Certifcate request
IDx = identity
CERT = certifcate
SK {...} = MAC and encrypt
AUTH = Authentication
SAx2 = algorithms, parameters for IPsec SA
TSx = trafc selectors for IPsec SA
N = Notify
D = Delete
CP = Confguration
protecting ordinary (i.e. non-IKE) communications between the peers. Thus, four 
messages are needed to establish the first SA for general use.
The CREATE_CHILD_SA exchange can be used to establish further SAs 
for protecting traffic. The informational exchange is used to exchange management 
information, IKEv2 error messages, and other notifications.
Header and Payload Formats
IKE defines procedures and packet formats to establish, negotiate, modify, and 
 delete security associations. As part of SA establishment, IKE defines payloads for 
exchanging key generation and authentication data. These payload formats provide 
a consistent framework independent of the specific key exchange protocol, encryp-
tion algorithm, and authentication mechanism.
Ike header ForMaT An IKE message consists of an IKE header followed by one 
or more payloads. All of this is carried in a transport protocol. The specification dic-
tates that implementations must support the use of UDP for the transport protocol.

Figure 9.12a shows the header format for an IKE message. It consists of the 
following fields.
‚óÜ
‚ñ† Initiator SPI (64 bits): A value chosen by the initiator to identify a unique IKE 
security association (SA).
‚óÜ
‚ñ† Responder SPI (64 bits): A value chosen by the responder to identify a unique 
IKE SA.
‚óÜ
‚ñ† Next Payload (8 bits): Indicates the type of the first payload in the message; 
payloads are discussed in the next subsection.
‚óÜ
‚ñ† Major Version (4 bits): Indicates major version of IKE in use.
‚óÜ
‚ñ† Minor Version (4 bits): Indicates minor version in use.
‚óÜ
‚ñ† Exchange Type (8 bits): Indicates the type of exchange; these are discussed 
later in this section.
‚óÜ
‚ñ† Flags (8 bits): Indicates specific options set for this IKE exchange. Three bits 
are defined so far. The initiator bit indicates whether this packet is sent by 
the SA initiator. The version bit indicates whether the transmitter is capable 
of using a higher major version number than the one currently indicated. The 
response bit indicates whether this is a response to a message containing the 
same message ID.
‚óÜ
‚ñ† Message ID (32 bits): Used to control retransmission of lost packets and 
matching of requests and responses.
‚óÜ
‚ñ† Length (32 bits): Length of total message (header plus all payloads) in octets.
Ike payload Types All IKE payloads begin with the same generic payload header 
shown in Figure 9.12b. The Next Payload field has a value of 0 if this is the last 
Figure 9.12 IKE Formats
MjVer
MnVer
Exchange Type
Flags
Next Payload
Message ID
Length
(a) IKE header
(b) Generic Payload header
Initiator‚Äôs Security Parameter Index (SPI)
Responder‚Äôs Security Parameter Index (SPI)
0
Bit:
8
16
24
31
RESERVED
Payload Length
Next Payload
C
0
Bit:
8
16
31

Type
Parameters
Security Association
Proposals
Key Exchange
DH Group #, Key Exchange Data
Identification
ID Type, ID Data
Certificate
Cert Encoding, Certificate Data
Certificate Request
Cert Encoding, Certification Authority
Authentication
Auth Method, Authentication Data
Nonce
Nonce Data
Notify
Protocol-ID, SPI Size, Notify Message Type, SPI, Notification Data
Delete
Protocol-ID, SPI Size, # of SPIs, SPI (one or more)
Vendor ID
Vendor ID
Traffic Selector
Number of TSs, Traffic Selectors
Encrypted
IV, Encrypted IKE payloads, Padding, Pad Length, ICV
Configuration
CFG Type, Configuration Attributes
Extensible Authentication 
Protocol
EAP Message
Table 9.3 IKE Payload Types
payload in the message; otherwise its value is the type of the next payload. The 
Payload Length field indicates the length in octets of this payload, including the 
generic payload header.
The critical bit is 0 if the sender wants the recipient to skip this payload if it 
does not understand the payload type code in the Next Payload field of the previous 
payload. It is set to 1 if the sender wants the recipient to reject this entire message if 
it does not understand the payload type.
Table 9.3 summarizes the payload types defined for IKE and lists the fields, 
or parameters, that are part of each payload. The SA payload is used to begin the 
establishment of an SA. The payload has a complex, hierarchical structure. The 
payload may contain multiple proposals. Each proposal may contain multiple pro-
tocols. Each protocol may contain multiple transforms. And each transform may 
contain multiple attributes. These elements are formatted as substructures within 
the payload as follows.
‚óÜ
‚ñ† Proposal: This substructure includes a proposal number, a protocol ID (AH, 
ESP, or IKE), an indicator of the number of transforms, and then a transform 
substructure. If more than one protocol is to be included in a proposal, then 
there is a subsequent proposal substructure with the same proposal number.
‚óÜ
‚ñ† Transform: Different protocols support different transform types. The trans-
forms are used primarily to define cryptographic algorithms to be used with a 
particular protocol.
‚óÜ
‚ñ† Attribute: Each transform may include attributes that modify or complete the 
specification of the transform. An example is key length.

The Key Exchange payload can be used for a variety of key exchange tech-
niques, including Oakley, Diffie‚ÄìHellman, and the RSA-based key exchange used 
by PGP. The Key Exchange data field contains the data required to generate a ses-
sion key and is dependent on the key exchange algorithm used.
The Identification payload is used to determine the identity of communicating 
peers and may be used for determining authenticity of information. Typically the 
ID Data field will contain an IPv4 or IPv6 address.
The Certificate payload transfers a public-key certificate. The Certificate 
Encoding field indicates the type of certificate or certificate-related information, 
which may include the following:
‚óÜ
‚ñ† PKCS #7 wrapped X.509 certificate
‚óÜ
‚ñ† PGP certificate
‚óÜ
‚ñ† DNS signed key
‚óÜ
‚ñ† X.509 certificate‚Äîsignature
‚óÜ
‚ñ† X.509 certificate‚Äîkey exchange
‚óÜ
‚ñ† Kerberos tokens
‚óÜ
‚ñ† Certificate Revocation List (CRL)
‚óÜ
‚ñ† Authority Revocation List (ARL)
‚óÜ
‚ñ† SPKI certificate
At any point in an IKE exchange, the sender may include a Certificate Request 
payload to request the certificate of the other communicating entity. The payload 
may list more than one certificate type that is acceptable and more than one certifi-
cate authority that is acceptable.
The Authentication payload contains data used for message authentication 
purposes. The authentication method types so far defined are RSA digital signa-
ture, shared-key message integrity code, and DSS digital signature.
The Nonce payload contains random data used to guarantee liveness during 
an exchange and to protect against replay attacks.
The Notify payload contains either error or status information associated with 
this SA or this SA negotiation. The following table lists the IKE notify messages.
Error Messages
Status Messages
Unsupported Critical
Initial Contact
Payload
Set Window Size
Invalid IKE SPI
Additional TS Possible
Invalid Major Version
IPCOMP Supported
Invalid Syntax
NAT Detection Source IP
Invalid Payload Type
NAT Detection Destination IP
Invalid Message ID
Cookie
Invalid SPI
Use Transport Mode

Error Messages
Status Messages
No Proposal Chosen
HTTP Cert Lookup Supported
Invalid KE Payload
Rekey SA
Authentication Failed
ESP TFC Padding Not Supported
Single Pair Required
Non First Fragments Also
No Additional SAS
Internal Address Failure
Failed CP Required
TS Unacceptable
Invalid Selectors
The Delete payload indicates one or more SAs that the sender has deleted 
from its database and that therefore are no longer valid.
The Vendor ID payload contains a vendor-defined constant. The constant 
is used by vendors to identify and recognize remote instances of their implemen-
tations. This mechanism allows a vendor to experiment with new features while 
 maintaining backward compatibility.
The Traffic Selector payload allows peers to identify packet flows for process-
ing by IPsec services.
The Encrypted payload contains other payloads in encrypted form. The 
 encrypted payload format is similar to that of ESP. It may include an IV if the 
 encryption algorithm requires it and an ICV if authentication is selected.
The Configuration payload is used to exchange configuration information 
 between IKE peers.
The Extensible Authentication Protocol (EAP) payload allows IKE SAs to 
be authenticated using EAP, which was discussed in Chapter 5.
 9.6 cryptOgraphIc SuIteS
The IPsecv3 and IKEv3 protocols rely on a variety of types of cryptographic algo-
rithms. As we have seen in this book, there are many cryptographic algorithms of 
each type, each with a variety of parameters, such as key size. To promote interop-
erability, two RFCs define recommended suites of cryptographic algorithms and 
parameters for various applications.
RFC 4308 defines two cryptographic suites for establishing virtual private net-
works. Suite VPN-A matches the commonly used corporate VPN security used in 
older IKEv1 implementations at the time of the issuance of IKEv2 in 2005. Suite 
VPN-B provides stronger security and is recommended for new VPNs that imple-
ment IPsecv3 and IKEv2.
Table 9.4a lists the algorithms and parameters for the two suites. There are 
several points to note about these two suites. Note that for symmetric cryptography,

VPN-A relies on 3DES and HMAC, while VPN-B relies exclusively on AES. Three 
types of secret-key algorithms are used:
‚óÜ
‚ñ† Encryption: For encryption, the cipher block chaining (CBC) mode is used.
‚óÜ
‚ñ† Message authentication: For message authentication, VPN-A relies on HMAC 
with SHA-1 with the output truncated to 96 bits. VPN-B relies on a variant of 
CMAC with the output truncated to 96 bits.
‚óÜ
‚ñ† Pseudorandom function: IKEv2 generates pseudorandom bits by repeated use 
of the MAC used for message authentication.
RFC 6379 defines four optional cryptographic suites that are compatible with 
the United States National Security Agency‚Äôs Suite B specifications. In 2005, the 
NSA issued Suite B, which defined the algorithms and strengths needed to pro-
tect both sensitive but unclassified (SBU) and classified information for use in 
its Cryptographic Modernization program [LATT09]. The four suites defined in 
RFC 6379 provide choices for ESP and IKE. The four suites are differentiated by 
the choice of cryptographic algorithm strengths and a choice of whether ESP is to 
provide both confidentiality and integrity or integrity only. All of the suites offer 
greater protection than the two VPN suites defined in RFC 4308.
VPN-A
VPN-B
ESP encryption
3DES-CBC
AES-CBC (128-bit key)
ESP integrity
HMAC-SHA1-96
AES-XCBC-MAC-96
IKE encryption
3DES-CBC
AES-CBC (128-bit key)
IKE PRF
HMAC-SHA1
AES-XCBC-PRF-128
IKE Integrity
HMAC-SHA1-96
AES-XCBC-MAC-96
IKE DH group
1024-bit MODP
2048-bit MODP
(a) Virtual private networks (RFC 4308)
GCM-128
GCM-256
GMAC-128
GMAC-256
ESP encryption/ 
Integrity
AES-GCM  
(128-bit key)
AES-GCM  
(256-bit key)
Null
Null
ESP integrity
Null
Null
AES-GMAC (128-
bit key)
AES-GMAC  
(256-bit key)
IKE encryption
AES-CBC  
(128-bit key)
AES-CBC  
(256-bit key)
AES-CBC  
(128-bit key)
AES-CBC  
(256-bit key)
IKE PRF
HMAC-SHA-256
HMAC-SHA-384
HMAC-SHA-256
HMAC-SHA-384
IKE Integrity
HMAC-SHA-  
256-128
HMAC-SHA-  
384-192
HMAC-SHA-  
256-128
HMAC-SHA-  
384-192
IKE DH group
256-bit random 
ECP
384-bit random 
ECP
256-bit random 
ECP
384-bit random 
ECP
(b) NSA Suite B (RFC 6379)
Table 9.4 Cryptographic Suites for IPsec

Key Terms 
Table 9.4b lists the algorithms and parameters for the two suites. As with 
RFC¬†4308, three categories of secret key algorithms are listed:
‚óÜ
‚ñ† Encryption: For ESP, authenticated encryption is provided using the GCM 
mode with either 128-bit or 256-bit AES keys. For IKE encryption, CBC is 
used, as it was for the VPN suites.
‚óÜ
‚ñ† Message authentication: For ESP, if only authentication is required, then a 
message authentication algorithm known as GMAC is used. For IKE, mes-
sage authentication is provided using HMAC with one of the SHA-3 hash 
functions.
‚óÜ
‚ñ† Pseudorandom function: As with the VPN suites, IKEv2 in these suites gen-
erates pseudorandom bits by repeated use of the MAC used for message 
authentication.
For the Diffie‚ÄìHellman algorithm, the use of elliptic curve groups modulo 
a prime is specified. For authentication, elliptic curve digital signatures are listed. 
The original IKEv2 documents used RSA-based digital signatures. Equivalent or 
greater strength can be achieved using ECC with fewer key bits.
 9.7 Key termS, revIew QueStIOnS, and prObLemS
anti-replay service
Authentication Header (AH)
Encapsulating Security 
Payload (ESP)
Internet Key Exchange  
(IKE)
Internet Security Association 
and Key Management 
Protocol (ISAKMP)
IP Security (IPsec)
IPv4
IPv6
Oakley key determination 
protocol
replay attack
security association (SA)
transport mode
tunnel mode
Review Questions 
 
9.1 
List and briefly describe some benefits of IPsec.
 
9.2 
List and briefly define different categories of IPsec documents.
 
9.3 
What parameters identify an SA and what parameters characterize the nature of a 
particular SA?
 
9.4 
What is the difference between transport mode and tunnel mode?
 
9.5 
What are the types of secret key algorithm used in IPsec?
 
9.6 
Why does ESP include a padding field?
 
9.7 
What are the basic approaches to bundling SAs?
 
9.8 
What are the roles of the Oakley key determination protocol and ISAKMP in IPsec?

Problems 
 
9.1 
Describe and explain each of the entries in Table 9.2.
 
9.2 
Draw a figure similar to Figure 9.8 for AH.
 
9.3 
List the major security services provided by AH and ESP, respectively.
 
9.4 
In discussing AH processing, it was mentioned that not all of the fields in an IP header 
are included in MAC calculation.
a. For each of the fields in the IPv4 header, indicate whether the field is immutable, 
mutable but predictable, or mutable (zeroed prior to ICV calculation).
b. Do the same for the IPv6 header.
c. Do the same for the IPv6 extension headers.
In each case, justify your decision for each field.
 
9.5 
Suppose that the current replay window spans from 120 to 530.
a. If the next incoming authenticated packet has sequence number 340, what will the 
receiver do with the packet, and what will be the parameters of the window after 
that?
b. If instead the next incoming authenticated packet has sequence number 598, 
what will the receiver do with the packet, and what will be the parameters of the 
 window after that?
c. If instead the next incoming authenticated packet has sequence number 110, 
what will the receiver do with the packet, and what will be the parameters of the 
 window after that?
 
9.6 
When tunnel mode is used, a new outer IP header is constructed. For both IPv4 
and IPv6, indicate the relationship of each outer IP header field and each extension 
header in the outer packet to the corresponding field or extension header of the inner 
IP packet. That is, indicate which outer values are derived from inner values and 
which are constructed independently of the inner values.
 
9.7 
End-to-end authentication and encryption are desired between two hosts. Draw 
 figures similar to Figure 9.8 that show each of the following.
a. Transport adjacency with encryption applied before authentication.
b. A transport SA bundled inside a tunnel SA with encryption applied before 
 authentication.
c. A transport SA bundled inside a tunnel SA with authentication applied before 
encryption.
 
9.8 
The IPsec architecture document states that when two transport mode SAs are 
bundled to allow both AH and ESP protocols on the same end-to-end flow, only 
one ordering of security protocols seems appropriate: performing the ESP protocol 
 before performing the AH protocol. Why is this approach recommended rather than 
authentication before encryption?
 
9.9 
For the IKE key exchange, indicate which parameters in each message go in which 
ISAKMP payload types.
 
9.10 
Where does IPsec reside in a protocol stack?

10.1 Types of Malicious Software (Malware)
10.2 Advanced Persistent Threats
10.3 Propagation‚ÄîInfected Content‚ÄîViruses
10.4 Propagation‚ÄîVulnerability Exploit‚ÄîWorms
10.5 Propagation‚ÄîSocial Engineering‚ÄîSpam E-mail, Trojans
10.6 Payload‚ÄîSystem Corruption
10.7 Payload‚ÄîAttack Agent‚ÄîZombie, Bots
10.8 Payload‚ÄîInformation Theft‚ÄîKeyloggers, Phishing, Spyware
10.9 Payload‚ÄîStealthing‚ÄîBackdoors, Rootkits
10.10 Countermeasures
10.11 Distributed Denial of Service Attacks
10.12 Key Terms, Review Questions, and Problems
Malicious Software
Part 3: SyStem Security
Chapter

Malicious software, or malware, arguably constitutes one of the most significant 
categories of threats to computer systems. SP 800-83 (Guide to Malware Incident 
Prevention and Handling for Desktops and Laptops, July 2013) defines malware 
as ‚Äúa program that is covertly inserted into another program with the intent to de-
stroy data, run destructive or intrusive programs, or otherwise compromise the con-
fidentiality, integrity, or availability of the victim‚Äôs data, applications, or operating 
system.‚Äù Hence, we are concerned with the threat malware poses to application 
programs, to utility programs, such as editors and compilers, and to kernel-level 
programs. We are also concerned with its use on compromised or malicious Web 
sites and servers, or in especially crafted spam e-mails or other messages, which aim 
to trick users into revealing sensitive personal information.
This chapter1 examines the wide spectrum of malware threats and counter-
measures. We begin with a survey of various types of malware and offer a broad 
classification based first on the means malware uses to spread or propagate, and 
then on the variety of actions or payloads used once the malware has reached a 
target. Propagation mechanisms include those used by viruses, worms, and trojans. 
Payloads include system corruption, bots, phishing, spyware, and rootkits. The dis-
cussion then includes a review of countermeasure approaches. Finally, distributed 
denial-of-service (DDoS) attacks are reviewed.
 10.1 Types of Malicious sofTware (Malware)
The terminology in this area presents problems because of a lack of universal agree-
ment on all of the terms and because some of the categories overlap. Table 10.1 is a 
useful guide to some of the terms in use.
1I am indebted to Lawrie Brown of the Australian Defence Force Academy, who contributed substan-
tially to this chapter.
learning objecTives
After studying this chapter, you should be able to:
‚ñ†‚ñ†
Describe three broad mechanisms malware uses to propagate.
‚ñ†‚ñ†
Understand the basic operation of viruses, worms, and trojans.
‚ñ†‚ñ†
Describe four broad categories of malware payloads.
‚ñ†‚ñ†
Understand the different threats posed by bots, spyware, and rootkits.
‚ñ†‚ñ†
Describe some malware countermeasure elements.
‚ñ†‚ñ†
Describe three locations for malware detection mechanisms.

A Broad Classification of Malware
Although a range of schemes can be used, one useful approach classifies malware into 
two broad categories, based first on how it spreads or propagates to reach the  desired 
targets and then on the actions or payloads it performs once a target is reached.
Propagation mechanisms include infection of existing executable or interpreted 
content by viruses that is subsequently spread to other systems; exploit of software 
vulnerabilities either locally or over a network by worms or drive-by-downloads to 
allow the malware to replicate; and social engineering attacks that convince users to 
bypass security mechanisms to install trojans or to respond to phishing attacks.
Name
Description
Virus
Malware that, when executed, tries to replicate itself into other executable code; when it 
succeeds the code is said to be infected. When the infected code is executed, the virus also 
executes.
Worm
A computer program that can run independently and can propagate a complete working 
version of itself onto other hosts on a network.
Logic bomb
A program inserted into software by an intruder. A logic bomb lies dormant until a 
 predefined condition is met; the program then triggers an unauthorized act.
Trojan horse
A computer program that appears to have a useful function, but also has a hidden and 
potentially malicious function that evades security mechanisms, sometimes by exploiting 
legitimate authorizations of a system entity that invokes the Trojan horse program.
Backdoor  
(trapdoor)
Any mechanism that bypasses a normal security check; it may allow unauthorized access 
to functionality.
Mobile code
Software (e.g., script, macro, or other portable instruction) that can be shipped unchanged 
to a heterogeneous collection of platforms and execute with identical semantics.
Exploits
Code specific to a single vulnerability or set of vulnerabilities.
Downloaders
Program that installs other items on a machine that is under attack. Usually, a downloader 
is sent in an e-mail.
Auto-rooter
Malicious hacker tools used to break into new machines remotely.
Kit (virus  
generator)
Set of tools for generating new viruses automatically.
Spammer  
programs
Used to send large volumes of unwanted e-mail.
Flooders
Used to attack networked computer systems with a large volume of traffic to carry out a 
denial-of-service (DoS) attack.
Keyloggers
Captures keystrokes on a compromised system.
Rootkit
Set of hacker tools used after attacker has broken into a computer system and gained 
root-level access.
Zombie, bot
Program activated on an infected machine that is activated to launch attacks on other 
machines.
Spyware
Software that collects information from a computer and transmits it to another system.
Adware
Advertising that is integrated into software. It can result in pop-up ads or redirection of a 
browser to a commercial site.
Table 10.1 Terminology for Malicious Software

Earlier approaches to malware classification distinguished between those that 
need a host program, being parasitic code such as viruses, and those that are inde-
pendent, self-contained programs run on the system such as worms, trojans, and 
bots. Another distinction used was between malware that does not replicate, such as 
trojans and spam e-mail, and malware that does, including viruses and worms.
Payload actions performed by malware once it reaches a target system can in-
clude corruption of system or data files; theft of service in order to make the system 
a zombie agent of attack as part of a botnet; theft of information from the system, 
especially of logins, passwords, or other personal details by keylogging or spyware 
programs; and stealthing where the malware hides its presence on the system from 
attempts to detect and block it.
While early malware tended to use a single means of propagation to deliver a 
single payload, as it evolved we see a growth of blended malware that incorporates a 
range of both propagation mechanisms and payloads that increase its ability to spread, 
hide, and perform a range of actions on targets. A blended attack uses multiple meth-
ods of infection or propagation, to maximize the speed of contagion and the sever-
ity of the attack. Some malware even support an update mechanism that allows it to 
change the range of propagation and payload mechanisms utilized once it is deployed.
In the following sections, we survey these various categories of malware, and 
then follow with a discussion of appropriate countermeasures.
Attack Kits
Initially, the development and deployment of malware required considerable tech-
nical skill by software authors. This changed with the development of virus-creation 
toolkits in the early 1990s, and then later of more general attack kits in the 2000s, that 
greatly assisted in the development and deployment of malware [FOSS10]. These 
toolkits, often known as crimeware, now include a variety of propagation mecha-
nisms and payload modules that even novices can combine, select, and deploy.
They can also easily be customized with the latest discovered vulnerabilities in 
order to exploit the window of opportunity between the publication of a weakness 
and the widespread deployment of patches to close it. These kits greatly enlarged 
the population of attackers able to deploy malware. Although the malware created 
with such toolkits tends to be less sophisticated than that designed from scratch, the 
sheer number of new variants that can be generated by attackers using these tool-
kits creates a significant problem for those defending systems against them.
The Zeus crimeware toolkit is a prominent, recent example of such an attack 
kit, which was used to generate a wide range of very effective, stealthed malware 
that facilitates a range of criminal activities, in particular capturing and exploit-
ing banking credentials [BINS10]. Other widely used toolkits include Blackhole, 
Sakura, and Phoenix [SYMA13].
Attack Sources
Another significant malware development over the last couple of decades is the 
change from attackers being individuals, often motivated to demonstrate their 
technical competence to their peers, to more organized and dangerous attack 
sources. These include politically motivated attackers, criminals, and organized

crime; organizations that sell their services to companies and nations; and national 
government agencies. This has significantly changed the resources available and 
motivation behind the rise of malware, and indeed has led to development of a 
large underground economy involving the sale of attack kits, access to compro-
mised hosts, and to stolen information.
 10.2 advanced persisTenT ThreaT
Advanced Persistent Threats (APTs) have risen to prominence in recent years. 
These are not a new type of malware, but rather the well-resourced, persistent 
 application of a wide variety of intrusion technologies and malware to selected tar-
gets, usually business or political. APTs are typically attributed to state-sponsored 
organizations, with some attacks likely from criminal enterprises as well. We discuss 
these categories of intruders further in Chapter 11.
APTs differ from other types of attack by their careful target selection, and 
persistent, often stealthy, intrusion efforts over extended periods. A number of 
high profile attacks, including Aurora, RSA, APT1, and Stuxnet, are often cited as 
 examples. They are named as a result of these characteristics:
‚ñ†
‚ñ† Advanced: Used by the attackers of a wide variety of intrusion technologies 
and malware, including the development of custom malware if required. The 
individual components may not necessarily be technically advanced, but are 
carefully selected to suit the chosen target.
‚ñ†
‚ñ† Persistent: Determined application of the attacks over an extended period 
against the chosen target in order to maximize the chance of success. A variety 
of attacks may be progressively, and often stealthily, applied until the target is 
compromised.
‚ñ†
‚ñ† Threats: Threats to the selected targets as a result of the organized, capable, 
and well-funded attackers intent to compromise the specifically chosen tar-
gets. The active involvement of people in the process greatly raises the threat 
level from that due to automated attacks tools and the likelihood of successful 
attack.
The aim of these attacks varies from theft of intellectual property or secu-
rity and infrastructure related data, to the physical disruption of infrastructure. 
Techniques used include social engineering, spear-phishing e-mails, drive-by- 
downloads from selected compromised Web sites likely to be visited by personnel 
in the target organization, to infect the target with sophisticated malware with mul-
tiple propagation mechanisms and payloads. Once they have gained initial access 
to systems in the target organization, a further range of attack tools are used to 
maintain and extend their access.
As a result, these attacks are much harder to defend against due to this spe-
cific targeting and persistence. It requires a combination of technical countermea-
sures, such as we discuss later in this chapter, as well as awareness training to assist 
personnel to resist such attacks. Even with current best-practice countermeasures, 
the use of zero-day exploits and new attack approaches means that some of these

attacks are likely to succeed [SYMA13, MAND13]. Thus multiple layers of defense 
are needed, with mechanisms to detect, respond and mitigate such attacks. These 
may include monitoring for malware command and control traffic, and detection of 
exfiltration traffic.
 10.3 propagaTion‚ÄîinfecTed conTenT‚Äîviruses
The first category of malware propagation concerns parasitic software fragments 
that attach themselves to some existing executable content. The fragment may be 
machine code that infects some existing application, utility, or system program, or 
even the code used to boot a computer system. More recently, the fragment has 
been some form of scripting code, typically used to support active content within 
data files such as Microsoft Word documents, Excel spreadsheets, or Adobe PDF 
documents.
The Nature of Viruses
A computer virus is a piece of software that can ‚Äúinfect‚Äù other programs, or indeed 
any type of executable content, by modifying them. The modification includes in-
jecting the original code with a routine to make copies of the virus code, which can 
then go on to infect other content.
A computer virus carries in its instructional code the recipe for making perfect 
copies of itself. The typical virus becomes embedded in a program, or carrier of 
executable content, on a computer. Then, whenever the infected computer comes 
into contact with an uninfected piece of code, a fresh copy of the virus passes into 
the new location. Thus, the infection can spread from computer to computer, aided 
by unsuspecting users, who exchange these programs or carrier files on disk or USB 
stick, or who send them to one another over a network. In a network environment, 
the ability to access documents, applications, and system services on other comput-
ers provides a perfect culture for the spread of such viral code.
A virus that attaches to an executable program can do anything that the pro-
gram is permitted to do. It executes secretly when the host program is run. Once 
the virus code is executing, it can perform any function, such as erasing files and 
programs, that is allowed by the privileges of the current user. One reason viruses 
dominated the malware scene in earlier years was the lack of user authentication 
and access controls on personal computer systems at that time. This enabled a virus 
to infect any executable content on the system. The significant quantity of programs 
shared on floppy disk also enabled its easy, if somewhat slow, spread. The inclusion 
of tighter access controls on modern operating systems significantly hinders the ease 
of infection of such traditional, machine-executable code, viruses. This resulted in 
the development of macro viruses that exploit the active content supported by some 
document types, such as Microsoft Word or Excel files, or Adobe PDF documents. 
Such documents are easily modified and shared by users as part of their normal sys-
tem use and are not protected by the same access controls as programs. Currently, 
a viral mode of infection is typically one of several propagation mechanisms used 
by contemporary malware, which may also include worm and Trojan capabilities.

A computer virus, and more generally many contemporary types of malware, 
includes one or more variants of each of these components:
‚ñ†
‚ñ† Infection mechanism: The means by which a virus spreads or propagates, en-
abling it to replicate. The mechanism is also referred to as the infection vector.
‚ñ†
‚ñ† Trigger: The event or condition that determines when the payload is activated 
or delivered, sometimes known as a logic bomb.
‚ñ†
‚ñ† Payload: What the virus does, besides spreading. The payload may involve 
damage or benign but noticeable activity.
During its lifetime, a typical virus goes through the following four phases:
‚ñ†
‚ñ† Dormant phase: The virus is idle. The virus will eventually be activated by 
some event, such as a date, the presence of another program or file, or the ca-
pacity of the disk exceeding some limit. Not all viruses have this stage.
‚ñ†
‚ñ† Propagation phase: The virus places a copy of itself into other programs or 
into certain system areas on the disk. The copy may not be identical to the 
propagating version; viruses often morph to evade detection. Each infected 
program will now contain a clone of the virus, which will itself enter a propaga-
tion phase.
‚ñ†
‚ñ† Triggering phase: The virus is activated to perform the function for which it 
was intended. As with the dormant phase, the triggering phase can be caused 
by a variety of system events, including a count of the number of times that this 
copy of the virus has made copies of itself.
‚ñ†
‚ñ† Execution phase: The function is performed. The function may be harmless, 
such as a message on the screen, or damaging, such as the destruction of pro-
grams and data files.
Most viruses that infect executable program files carry out their work in a 
manner that is specific to a particular operating system and, in some cases, specific 
to a particular hardware platform. Thus, they are designed to take advantage of the 
details and weaknesses of particular systems. Macro viruses, though, target specific 
document types, which are often supported on a variety of systems.
ExEcutablE Virus structurE Traditional machine-executable virus code can be 
prepended or postpended to some executable program, or it can be embedded into 
the program in some other fashion. The key to its operation is that the infected pro-
gram, when invoked, will first execute the virus code and then execute the original 
code of the program.
A very general depiction of virus structure is shown in Figure 10.1a. In this 
case, the virus code, V, is prepended to infected programs, and it is assumed that the 
entry point to the program, when invoked, is the first line of the program.
The infected program begins with the virus code and works as follows. The first 
line of code labels the program, which then begins execution with the main  action 
block of the virus. The second line is a special marker that is used by the virus to 
determine whether or not a potential victim program has already been infected with 
this virus. When the program is invoked, control is immediately transferred to the 
main virus program. The virus program may first seek out uninfected executable 
files and infect them. Next, the virus may execute its payload if the required trigger

conditions, if any, are met. Finally, the virus transfers control to the original program. 
If the infection phase of the program is reasonably rapid, a user is unlikely to notice 
any difference between the execution of an infected and an uninfected program.
A virus such as the one just described is easily detected because an infected 
version of a program is longer than the corresponding uninfected one. A way to 
thwart such a simple means of detecting a virus is to compress the executable file so 
that both the infected and uninfected versions are of identical length. Figure 10.1b 
shows in general terms the logic required. The key lines in this virus are labeled with 
times, and Figure 10.2 illustrates the operation. We begin at time t0, with program 
P1
=, which is program P1 infected with virus CV, and a clean program P2, which is not 
infected with CV. When P1 is invoked, control passes to its virus, which performs 
the following steps:
t1: For each uninfected file P2 that is found, the virus first compresses that file to 
produce P2
=, which is shorter than the original program by the size of the virus CV.
t2: A copy of CV is prepended to the compressed program.
t3: The compressed version of the original infected program, P1
= is uncompressed.
t4: The uncompressed original program P1 is executed.
In this example, the virus does nothing other than propagate. As previously 
mentioned, the virus may also include one or more payloads.
Once a virus has gained entry to a system by infecting a single program, it is 
in a position to potentially infect some or all other executable files on that system 
program V
1234567;
procedure attach-to-program;
begin
repeat
file := get-random-program;
until first-program-line ‚â† 1234567;
prepend V to file;
end;
procedure execute-payload;
begin
(* perform payload actions *)
end;
procedure trigger-condition;
begin
(* return true if trigger condition is true *)
end;
begin (* main action block *)
attach-to-program;
if trigger-condition then execute-payload;
goto main;
end;
program CV
1234567;
procedure attach-to-program;
begin
repeat
file := get-random-program;
until first-program-line ‚â† 1234567;
compress file; (* t1 *)
prepend CV to file; (* t2 *)
end;
procedure (* main action block *)
if ask-permission then attach-to-program;
uncompress rest of this file into tempfile; (* t3 *)
execute tempfile; (* t4 *)
end;
(a) A simple virus
(b) A compression virus
Figure 10.1 Example Virus Logic

when the infected program executes, depending on the access permissions the 
 infected program has. Thus, viral infection can be completely prevented by  blocking 
the virus from gaining entry in the first place. Unfortunately, prevention is extraor-
dinarily difficult because a virus can be part of any program outside a system. Thus, 
unless one is content to take an absolutely bare piece of iron and write all one‚Äôs own 
system and application programs, one is vulnerable. Many forms of infection can 
also be blocked by denying normal users the right to modify programs on the¬†system.
Viruses Classification
There has been a continuous arms race between virus writers and writers of antivi-
rus software since viruses first appeared. As effective countermeasures are devel-
oped for existing types of viruses, newer types are developed. There is no simple or 
universally agreed-upon classification scheme for viruses. In this section, we follow 
[AYCO06] and classify viruses along two orthogonal axes: the type of target the 
virus tries to infect and the method the virus uses to conceal itself from detection by 
users and antivirus software.
Figure 10.2 A Compression Virus
P2
t0: P1¬ø is infected version of P1;
P2 is clean
CV
P2
t1: P2 is compressed into P2¬ø
t2: CV attaches itself to P2¬ø
CV
CV
t3: P1¬ø is decompressed into the
original program P1
CV
P1≈ì
P1
P1≈ì
P1≈ì
P2≈ì
P2≈ì

A virus classification by target includes the following categories:
‚ñ†
‚ñ† Boot sector infector: Infects a master boot record or boot record and spreads 
when a system is booted from the disk containing the virus.
‚ñ†
‚ñ† File infector: Infects files that the operating system or shell consider to be 
executable.
‚ñ†
‚ñ† Macro virus: Infects files with macro or scripting code that is interpreted by an 
application.
‚ñ†
‚ñ† Multipartite virus: Infects files in multiple ways. Typically, the multipartite 
virus is capable of infecting multiple types of files, so that virus eradication 
must deal with all of the possible sites of infection.
A virus classification by concealment strategy includes the following categories:
‚ñ†
‚ñ† Encrypted virus: A typical approach is as follows. A portion of the virus cre-
ates a random encryption key and encrypts the remainder of the virus. The key 
is stored with the virus. When an infected program is invoked, the virus uses 
the stored random key to decrypt the virus. When the virus replicates, a differ-
ent random key is selected. Because the bulk of the virus is encrypted with a 
different key for each instance, there is no constant bit pattern to observe.
‚ñ†
‚ñ† Stealth virus: A form of virus explicitly designed to hide itself from detection 
by antivirus software. Thus, the entire virus, not just a payload, is hidden. It 
may use both code mutation, for example, compression, and rootkit techniques 
to achieve this.
‚ñ†
‚ñ† Polymorphic virus: A form of virus that creates copies during replication that 
are functionally equivalent but have distinctly different bit patterns, in order 
to defeat programs that scan for viruses. In this case, the ‚Äúsignature‚Äù of the 
virus will vary with each copy. To achieve this variation, the virus may ran-
domly insert superfluous instructions or interchange the order of independent 
instructions. A more effective approach is to use encryption. The strategy of 
the encryption virus is followed. The portion of the virus that is responsible 
for generating keys and performing encryption/decryption is referred to as the 
mutation engine. The mutation engine itself is altered with each use.
‚ñ†
‚ñ† Metamorphic virus: As with a polymorphic virus, a metamorphic virus  mutates 
with every infection. The difference is that a metamorphic virus  rewrites 
 itself completely at each iteration, increasing the difficulty of detection. 
Metamorphic viruses may change their behavior as well as their appearance.
Macro and Scripting Viruses
Macro viruses infect scripting code used to support active content in a variety of 
user document types. Macro viruses are particularly threatening for a number of 
reasons:
1. A macro virus is platform independent. Many macro viruses infect active con-
tent in commonly used applications, such as macros in Microsoft Word docu-
ments or other Microsoft Office documents, or scripting code in Adobe PDF

documents. Any hardware platform and operating system that supports these 
applications can be infected.
2. Macro viruses infect documents, not executable portions of code. Most of the 
information introduced onto a computer system is in the form of documents 
rather than programs.
3. Macro viruses are easily spread, as the documents they exploit are shared in 
normal use. A very common method is by electronic mail.
4. Because macro viruses infect user documents rather than system programs, 
traditional file system access controls are of limited use in preventing their 
spread, since users are expected to modify them.
Macro viruses take advantage of support for active content using a scripting 
or macro language, embedded in a word processing document or other type of file. 
Typically, users employ macros to automate repetitive tasks and thereby save key-
strokes. They are also used to support dynamic content, form validation, and other 
useful tasks associated with these documents.
Successive releases of MS Office products provide increased protection against 
macro viruses. For example, Microsoft offers an optional Macro Virus Protection 
tool that detects suspicious Word files and alerts the customer to the potential risk 
of opening a file with macros. Various antivirus product vendors have also devel-
oped tools to detect and remove macro viruses. As in other types of viruses, the 
arms race continues in the field of macro viruses, but they no longer are the pre-
dominant virus threat.
Another possible host for macro virus‚Äìstyle malware is in Adobe‚Äôs PDF 
documents. These can support a range of embedded components, including 
Javascript and other types of scripting code. Although recent PDF viewers include 
measures to warn users when such code is run, the message the user is shown can 
be manipulated to trick them into permitting its execution. If this occurs, the code 
could potentially act as a virus to infect other PDF documents the user can access 
on his or her system. Alternatively, it can install a Trojan, or act as a worm, as we 
discuss later.
 10.4 propagaTion‚ÄîvulnerabiliTy exploiT‚ÄîworMs
A worm is a program that actively seeks out more machines to infect, and then 
each infected machine serves as an automated launching pad for attacks on other 
machines. Worm programs exploit software vulnerabilities in client or server 
 programs to gain access to each new system. They can use network connections to 
spread from system to system. They can also spread through shared media, such 
as USB drives or optical data disks. E-mail worms spread in macro or script code 
included in documents attached to e-mail or to instant messenger file  transfers. 
Upon activation, the worm may replicate and propagate again. In addition to 
 propagation, the worm usually carries some form of payload, such as those we 
discuss later.

To replicate itself, a worm uses some means to access remote systems. These 
include the following, most of which are still seen in active use [SYMA13]:
‚ñ†
‚ñ† Electronic mail or instant messenger facility: A worm e-mails a copy of  itself 
to other systems or sends itself as an attachment via an instant message 
 service, so that its code is run when the e-mail or attachment is received or 
viewed.
‚ñ†
‚ñ† File sharing: A worm either creates a copy of itself or infects other suitable 
files as a virus on removable media such as a USB drive; it then executes when 
the drive is connected to another system using the autorun mechanism by ex-
ploiting some software vulnerability or when a user opens the infected file on 
the target system.
‚ñ†
‚ñ† Remote execution capability: A worm executes a copy of itself on another 
system, either by using an explicit remote execution facility or by exploiting a 
program flaw in a network service to subvert its operations.
‚ñ†
‚ñ† Remote file access or transfer capability: A worm uses a remote file access or 
transfer service to another system to copy itself from one system to the other, 
where users on that system may then execute it.
‚ñ†
‚ñ† Remote login capability: A worm logs onto a remote system as a user and 
then uses commands to copy itself from one system to the other, where it then 
executes.
The new copy of the worm program is then run on the remote system where, 
in addition to any payload functions that it performs on that system, it continues to 
propagate.
A worm typically uses the same phases as a computer virus: dormant, propa-
gation, triggering, and execution. The propagation phase generally performs the 
following functions:
‚ñ†
‚ñ† Search for appropriate access mechanisms to other systems to infect by exam-
ining host tables, address books, buddy lists, trusted peers, and other similar 
repositories of remote system access details; by scanning possible target host 
addresses; or by searching for suitable removable media devices to use.
‚ñ†
‚ñ† Use the access mechanisms found to transfer a copy of itself to the remote 
system and cause the copy to be run.
The worm may also attempt to determine whether a system has previously 
been infected before copying itself to the system. In a multiprogramming system, 
it can also disguise its presence by naming itself as a system process or using some 
other name that may not be noticed by a system operator. More recent worms can 
even inject their code into existing processes on the system and run using additional 
threads in that process, to further disguise their presence.
Target Discovery
The first function in the propagation phase for a network worm is for it to search 
for other systems to infect, a process known as scanning or fingerprinting. For 
such worms, which exploit software vulnerabilities in remotely accessible network

services, it must identify potential systems running the vulnerable service, and then 
infect them. Then, typically, the worm code now installed on the infected machines 
repeats the same scanning process, until a large distributed network of infected 
 machines is created.
[MIRK04] lists the following types of network address scanning strategies that 
such a worm can use:
‚ñ†
‚ñ† Random: Each compromised host probes random addresses in the IP  address 
space, using a different seed. This technique produces a high volume of Internet 
traffic, which may cause generalized disruption even before the actual attack 
is launched.
‚ñ†
‚ñ† Hit list: The attacker first compiles a long list of potential vulnerable machines. 
This can be a slow process done over a long period to avoid detection that an 
attack is underway. Once the list is compiled, the attacker begins infecting ma-
chines on the list. Each infected machine is provided with a portion of the list 
to scan. This strategy results in a very short scanning period, which may make 
it difficult to detect that infection is taking place.
‚ñ†
‚ñ† Topological: This method uses information contained on an infected victim 
machine to find more hosts to scan.
‚ñ†
‚ñ† Local subnet: If a host is infected behind a firewall, that host then looks for 
targets in its own local network. The host uses the subnet address structure to 
find other hosts that would otherwise be protected by the firewall.
Worm Propagation Model
A well-designed worm can spread rapidly and infect massive numbers of hosts. It is 
useful to have a general model for the rate of worm propagation. Computer viruses 
and worms exhibit similar self-replication and propagation behavior to biological 
viruses. Thus we can look to classic epidemic models for understanding computer 
virus and worm propagation behavior. A simplified, classic epidemic model can be 
expressed as follows:
dI(t)
dt
= bI(t)S(t)
where
I(t) = number of individuals infected as of time t
S(t) =  number of susceptible individuals (susceptible to infection but not yet 
infected) at time t
b = infection rate
N = size of the population, N = I(t) + S(t) 
Figure 10.3 shows the dynamics of worm propagation using this model. 
Propagation proceeds through three phases. In the initial phase, the number of hosts 
increases exponentially. To see that this is so, consider a simplified case in which a 
worm is launched from a single host and infects two nearby hosts. Each of these

hosts infects two more hosts, and so on. This results in exponential growth. After 
a time, infecting hosts waste some time attacking already-infected hosts, which 
 reduces the rate of infection. During this middle phase, growth is approximately 
 linear, but the rate of infection is rapid. When most vulnerable computers have 
been infected, the attack enters a slow finish phase as the worm seeks out those 
remaining hosts that are difficult to identify.
Clearly, the objective in countering a worm is to catch the worm in its slow 
start phase, at a time when few hosts have been infected.
Zou and others [ZOU05] describe a model for worm propagation based on 
an analysis of network worm attacks at that time. The speed of propagation and 
the total number of hosts infected depend on a number of factors, including the 
mode of propagation, the vulnerability or vulnerabilities exploited, and the degree 
of similarity to preceding attacks. For the latter factor, an attack that is a variation 
of a recent previous attack may be countered more effectively than a more novel 
attack. Zou‚Äôs model agrees closely with Figure 10.3.
The Morris Worm
The earliest significant worm infection was released onto the Internet by Robert 
Morris in 1988 [ORMA03]. The Morris worm was designed to spread on UNIX 
systems and used a number of different techniques for propagation. When a copy 
began execution, its first task was to discover other hosts known to this host that 
would allow entry from this host. The worm performed this task by examining a 
variety of lists and tables, including system tables that declared which other ma-
chines were trusted by this host, users‚Äô mail forwarding files, tables by which users 
gave themselves permission for access to remote accounts, and from a program that 
Figure 10.3 Worm Propagation Model
0.2
0
Slow start phase
Fraction of
hosts infected
Fraction of
hosts not
infected
Time
0.4
0.6
0.8
1.0
Fast spread sphase
Slow fnish phase

reported the status of network connections. For each discovered host, the worm 
tried a number of methods for gaining access:
1. It attempted to log on to a remote host as a legitimate user. In this method, 
the worm first attempted to crack the local password file and then used the 
discovered passwords and corresponding user IDs. The assumption was that 
many users would use the same password on different systems. To obtain the 
passwords, the worm ran a password-cracking program that tried
a. Each user‚Äôs account name and simple permutations of it
b. A list of 432 built-in passwords that Morris thought to be likely candidates2
c. All the words in the local system dictionary
4. It exploited a bug in the UNIX finger protocol, which reports the whereabouts 
of a remote user.
5. It exploited a trapdoor in the debug option of the remote process that receives 
and sends mail.
If any of these attacks succeeded, the worm achieved communication with the 
operating system command interpreter. It then sent this interpreter a short boot-
strap program, issued a command to execute that program, and then logged off. 
The bootstrap program then called back the parent program and downloaded the 
remainder of the worm. The new worm was then executed.
State of Worm Technology
The state of the art in worm technology includes the following:
‚ñ†
‚ñ† Multiplatform: Newer worms are not limited to Windows machines but can 
 attack a variety of platforms, especially the popular varieties of UNIX, or 
 exploit macro or scripting languages supported in popular document types.
‚ñ†
‚ñ† Multiexploit: New worms penetrate systems in a variety of ways, using exploits 
against Web servers, browsers, e-mail, file sharing, and other network-based 
applications, or via shared media.
‚ñ†
‚ñ† Ultrafast spreading: Exploit various techniques to optimize the rate of spread 
of a worm to maximize its likelihood of locating as many vulnerable machines 
as possible in a short time period.
‚ñ†
‚ñ† Polymorphic: To evade detection, skip past filters, and foil real-time analysis, 
worms adopt the virus polymorphic technique. Each copy of the worm has 
new code generated on the fly using functionally equivalent instructions and 
encryption techniques.
‚ñ†
‚ñ† Metamorphic: In addition to changing their appearance, metamorphic worms 
have a repertoire of behavior patterns that are unleashed at different stages of 
propagation.
‚ñ†
‚ñ† Transport vehicles: Because worms can rapidly compromise a large number 
of systems, they are ideal for spreading a wide variety of malicious payloads, 
2The complete list is provided at this book‚Äôs Premium Content Web site.

such as distributed denial-of-service bots, rootkits, spam e-mail generators, and 
spyware.
‚ñ†
‚ñ† Zero-day exploit: To achieve maximum surprise and distribution, a worm 
should exploit an unknown vulnerability that is only discovered by the general 
network community when the worm is launched.
Mobile Code
SP 800-28 (Guidelines on Active Content and Mobile Code, March 2008) defines 
mobile code as programs (e.g., script, macro, or other portable instruction) that can 
be shipped unchanged to a heterogeneous collection of platforms and execute with 
identical semantics.
Mobile code is transmitted from a remote system to a local system and then 
executed on the local system without the user‚Äôs explicit instruction. Mobile code 
often acts as a mechanism for a virus, worm, or Trojan horse to be transmitted 
to the user‚Äôs workstation. In other cases, mobile code takes advantage of vulner-
abilities to perform its own exploits, such as unauthorized data access or root 
compromise. Popular vehicles for mobile code include Java applets, ActiveX, 
JavaScript, and VBScript. The most common ways of using mobile code for mali-
cious operations on local system are cross-site scripting, interactive and dynamic 
Web sites, e-mail attachments, and downloads from untrusted sites or of un-
trusted software.
Client-Side Vulnerabilities and Drive-by-Downloads
Another approach to exploiting software vulnerabilities involves the exploit of 
bugs in user applications to install malware. One common approach to this exploits 
browser vulnerabilities so that when the user views a Web page controlled by the 
attacker, it contains code that exploits the browser bug to download and install mal-
ware on the system without the user‚Äôs knowledge or consent. This is known as a 
drive-by-download and is a common exploit in recent attack kits. In most cases this 
malware does not actively propagate as a worm does, but rather waits for unsus-
pecting users to visit the malicious Web page in order to spread to their systems.
In general, drive-by-download attacks are aimed at anyone who visits a com-
promised site and is vulnerable to the exploits used. Watering-hole attacks are a vari-
ant of this used in highly targeted attacks. The attacker researches their intended 
victims to identify Web sites they are likely to visit and then scans these sites to iden-
tify those with vulnerabilities that allow their compromise with a drive-by-download 
attack. They then wait for one of their intended victims to visit one of the compro-
mised sites. Their attack code may even be written so that it will only infect systems 
belonging to the target organization and take no action for other visitors to the site. 
This greatly increases the likelihood of the site compromise remaining undetected.
Malvertising is another technique used to place malware on Web sites without 
actually compromising them. The attacker pays for advertisements that are highly 
likely to be placed on their intended target Web sites, and which incorporate mal-
ware in them. Using these malicious adds, attackers can infect visitors to sites dis-
playing them. Again, the malware code may be dynamically generated to either 
reduce the chance of detection or only infect specific systems.

Related variants can exploit bugs in common e-mail clients, such as the Klez 
mass-mailing worm seen in October 2001, which targeted a bug in the HTML han-
dling in Microsoft‚Äôs Outlook and Outlook Express programs to automatically run 
itself. Or, such malware may target common PDF viewers to also download and 
install malware without the user‚Äôs consent, when they view a malicious PDF docu-
ment [STEV11]. Such documents may be spread by spam e-mail or be part of a 
targeted phishing attack, as we discuss next.
Clickjacking
Clickjacking, also known as a user-interface (UI) redress attack, is a vulnerability 
used by an attacker to collect an infected user‚Äôs clicks. The attacker can force the 
user to do a variety of things from adjusting the user‚Äôs computer settings to unwit-
tingly sending the user to Web sites that might have malicious code. Also, by tak-
ing advantage of Adobe Flash or JavaScript, an attacker could even place a button 
under or over a legitimate button, making it difficult for users to detect. A typical 
attack uses multiple transparent or opaque layers to trick a user into clicking on a 
button or link on another page when they were intending to click on the top level 
page. Thus, the attacker is hijacking clicks meant for one page and routing them to 
another page, most likely owned by another application, domain, or both.
Using a similar technique, keystrokes can also be hijacked. With a carefully 
crafted combination of stylesheets, iframes, and text boxes, a user can be led to 
 believe they are typing in the password to their e-mail or bank account but are 
 instead typing into an invisible frame controlled by the attacker.
There is a wide variety of techniques for accomplishing a clickjacking attack, 
and new techniques are developed as defenses to older techniques are put in place. 
[NIEM11] and [STON10] are useful discussions.
 10.5 propagaTion‚Äîsocial engineering‚ÄîspaM 
e-Mail,¬†Trojans
The final category of malware propagation we consider involves social engineer-
ing, ‚Äútricking‚Äù users to assist in the compromise of their own systems or personal 
information. This can occur when a user views and responds to some SPAM 
 e-mail or permits the installation and execution of some Trojan horse program or 
scripting¬†code.
Spam (Unsolicited Bulk) E-Mail
Unsolicited bulk e-mail, commonly known as spam, imposes significant costs on 
both the network infrastructure needed to relay this traffic and on users who need 
to filter their legitimate e-mails out of this flood. In response to the explosive growth 
in spam, there has been the equally rapid growth of the antispam industry, which 
provides products to detect and filter spam e-mails. This has led to an arms race 
between the spammers devising techniques to sneak their content through and the 
defenders taking efforts to block them. In recent years, the volume of spam e-mail 
has started to decline. One reason is the rapid growth of attacks, including spam,

spread via social media networks. This reflects the rapid growth in use of these net-
works, which form a new arena for attackers to exploit [SYMA13].
While some spam is sent from legitimate mail servers, most recent spam is 
sent by botnets using compromised user systems, as we discuss in Section 10.6. A 
significant portion of spam e-mail content is just advertising, trying to convince the 
recipient to purchase some product online, or used in scams, such as stock scams or 
money mule job ads. But spam is also a significant carrier of malware. The e-mail 
may have an attached document, which, if opened, may exploit a software vulner-
ability to install malware on the user‚Äôs system, as we discussed in the previous sec-
tion. Or, it may have an attached Trojan horse program or scripting code that, if 
run, also installs malware on the user‚Äôs system. Some trojans avoid the need for user 
agreement by exploiting a software vulnerability in order to install themselves, as 
we discuss next. Finally the spam may be used in a phishing attack, typically direct-
ing the user either to a fake Web site that mirrors some legitimate service, such 
as an online banking site, where it attempts to capture the user‚Äôs login and pass-
word details, or to complete some form with sufficient personal details to allow the 
 attacker to impersonate the user in an identity theft. All of these uses make spam 
e-mails a significant security concern. However, in many cases it requires the user‚Äôs 
active choice to view the e-mail and any attached document or to permit the instal-
lation of some program, in order for the compromise to occur.
Trojan Horses
A Trojan horse is a useful, or apparently useful, program or utility containing hid-
den code that, when invoked, performs some unwanted or harmful function.
Trojan horse programs can be used to accomplish functions indirectly that 
the attacker could not accomplish directly. For example, to gain access to sensitive, 
personal information stored in the files of a user, an attacker could create a Trojan 
horse program that, when executed, scans the user‚Äôs files for the desired sensitive 
information and sends a copy of it to the attacker via a Web form or e-mail or text 
message. The author could then entice users to run the program by incorporating it 
into a game or useful utility program and making it available via a known software 
distribution site or app store. This approach has been used recently with utilities 
that ‚Äúclaim‚Äù to be the latest antivirus scanner, or security update, for systems, but 
which are actually malicious trojans, often carrying payloads such as spyware that 
searches for banking credentials. Hence, users need to take precautions to validate 
the source of any software they install.
Trojan horses fit into one of three models:
‚ñ†
‚ñ† Continuing to perform the function of the original program and additionally 
performing a separate malicious activity
‚ñ†
‚ñ† Continuing to perform the function of the original program but modifying the 
function to perform malicious activity (e.g., a Trojan horse version of a login 
program that collects passwords) or to disguise other malicious activity (e.g., a 
Trojan horse version of a process-listing program that does not display certain 
processes that are malicious)
‚ñ†
‚ñ† Performing a malicious function that completely replaces the function of the 
original program

Some trojans avoid the requirement for user assistance by exploiting some 
software vulnerability to enable their automatic installation and execution. In this 
they share some features of a worm, but unlike it, they do not replicate. A prominent 
example of such an attack was the Hydraq Trojan used in Operation Aurora in 2009 
and early 2010. This exploited a vulnerability in Internet Explorer to install itself 
and targeted several high-profile companies [SYMA13]. It was typically distributed 
either by spam e-mail or via a compromised Web site using a ‚Äúdrive-by-download.‚Äù
 10.6 payload‚ÄîsysTeM corrupTion
Once malware is active on the target system, the next concern is what actions it will 
take on this system, that is, what payload does it carry. Some malware has a non-
existent or nonfunctional payload. Its only purpose, either deliberate or due to ac-
cidental early release, is to spread. More commonly, it carries one or more payloads 
that perform covert actions for the attacker.
An early payload seen in a number of viruses and worms resulted in data destruc-
tion on the infected system when certain trigger conditions were met [WEAV03]. A 
related payload is one that displays unwanted messages or content on the user‚Äôs system 
when triggered. More seriously, another variant attempts to inflict real-world damage 
on the system. All of these actions target the integrity of the computer system‚Äôs soft-
ware or hardware, or of the user‚Äôs data. These changes may not occur immediately, but 
only when specific trigger conditions are met that satisfy their logic-bomb code.
As an alternative to just destroying data, some malware encrypts the user‚Äôs data 
and demands payment in order to access the key needed to recover this information. 
This is sometimes known as ransomware. The PC Cyborg Trojan seen in 1989 was an 
early example of this. However, around mid-2006 a number of worms and trojans, such 
as the Gpcode Trojan, that used public-key cryptography with increasingly larger key 
sizes to encrypt data. The user needed to pay a ransom or to make a purchase from cer-
tain sites, in order to receive the key to decrypt this data. While earlier instances used 
weaker cryptography that could be cracked without paying the ransom, the later ver-
sions using public-key cryptography with large key sizes could not be broken this way.
Real-World Damage
A further variant of system corruption payloads aims to cause damage to physi-
cal equipment. The infected system is clearly the device most easily targeted. The 
Chernobyl virus not only corrupts data, it attempts to rewrite the BIOS code used 
to initially boot the computer. If it is successful, the boot process fails, and the sys-
tem is unusable until the BIOS chip is either reprogrammed or replaced.
The Stuxnet worm targets some specific industrial control system software as 
its key payload [CHEN11]. If control systems using certain Siemens industrial con-
trol software with a specific configuration of devices are infected, then the worm 
replaces the original control code with code that deliberately drives the controlled 
equipment outside its normal operating range, resulting in the failure of the attached 
equipment. The centrifuges used in the Iranian uranium enrichment program were 
strongly suspected as the target, with reports of much higher than normal failure 
rates observed in them over the period when this worm was active. As noted in our

earlier discussion, this has raised concerns over the use of sophisticated targeted 
malware for industrial sabotage.
Logic Bomb
A key component of data-corrupting malware is the logic bomb. The logic bomb is 
code embedded in the malware that is set to ‚Äúexplode‚Äù when certain conditions are 
met. Examples of conditions that can be used as triggers for a logic bomb are the 
presence or absence of certain files or devices on the system, a particular day of the 
week or date, a particular version or configuration of some software, or a particular 
user running the application. Once triggered, a bomb may alter or delete data or 
entire files, cause a machine halt, or do some other damage. All of the examples we 
describe in this section include such code.
 10.7 payload‚ÄîaTTack agenT‚ÄîZoMbie, boTs
The next category of payload we discuss is where the malware subverts the compu-
tational and network resources of the infected system for use by the attacker. Such a 
system is known as a bot (robot), zombie, or drone, and secretly takes over another 
Internet-attached computer and then uses that computer to launch or manage attacks 
that are difficult to trace to the bot‚Äôs creator. The bot is typically planted on hundreds or 
thousands of computers belonging to unsuspecting third parties. The collection of bots 
often is capable of acting in a coordinated manner; such a collection is referred to as a 
botnet. This type of payload attacks the integrity and availability of the infected system.
Uses of Bots
[HONE05] lists the following uses of bots:
‚ñ†
‚ñ† Distributed denial-of-service (DDoS) attacks: A DDoS attack is an attack on 
a computer system or network that causes a loss of service to users. We exam-
ine DDoS attacks in Section 10.10.
‚ñ†
‚ñ† Spamming: With the help of a botnet and thousands of bots, an attacker is able 
to send massive amounts of bulk e-mail (spam).
‚ñ†
‚ñ† Sniffing traffic: Bots can also use a packet sniffer to watch for interesting clear-
text data passing by a compromised machine. The sniffers are mostly used to 
retrieve sensitive information like usernames and passwords.
‚ñ†
‚ñ† Keylogging: If the compromised machine uses encrypted communication 
channels (e.g., HTTPS or POP3S), then just sniffing the network packets on 
the victim‚Äôs computer is useless because the appropriate key to decrypt the 
packets is missing. But by using a keylogger, which captures keystrokes on the 
infected machine, an attacker can retrieve sensitive information.
‚ñ†
‚ñ† Spreading new malware: Botnets are used to spread new bots. This is very 
easy since all bots implement mechanisms to download and execute a file via 
HTTP or FTP. A botnet with 10,000 hosts that acts as the start base for a worm 
or mail virus allows very fast spreading and thus causes more harm.

‚ñ†
‚ñ† Installing advertisement add-ons and browser helper objects (BHOs): Botnets 
can also be used to gain financial advantages. This works by setting up a fake 
Web site with some advertisements: The operator of this Web site negotiates a 
deal with some hosting companies that pay for clicks on ads. With the help of 
a botnet, these clicks can be ‚Äúautomated‚Äù so that instantly a few thousand bots 
click on the pop-ups. This process can be further enhanced if the bot hijacks 
the start-page of a compromised machine so that the ‚Äúclicks‚Äù are executed 
each time the victim uses the browser.
‚ñ†
‚ñ† Attacking IRC chat networks: Botnets are also used for attacks against Internet 
Relay Chat (IRC) networks. Popular among attackers is the so-called clone  attack: 
In this kind of attack, the controller orders each bot to connect a large number of 
clones to the victim IRC network. The victim is flooded by service requests from 
thousands of bots or thousands of channel-joins by these cloned bots. In this way, 
the victim IRC network is brought down, similar to a DDoS attack.
‚ñ†
‚ñ† Manipulating online polls/games: Online polls/games are getting more and 
more attention and it is rather easy to manipulate them with botnets. Since 
every bot has a distinct IP address, every vote will have the same credibility as 
a vote cast by a real person. Online games can be manipulated in a similar way.
Remote Control Facility
The remote control facility is what distinguishes a bot from a worm. A worm propa-
gates itself and activates itself, whereas a bot is controlled from some central facil-
ity, at least initially.
A typical means of implementing the remote control facility is on an IRC 
server. All bots join a specific channel on this server and treat incoming messages 
as commands. More recent botnets tend to avoid IRC mechanisms and use covert 
communication channels via protocols such as HTTP. Distributed control mecha-
nisms, using peer-to-peer protocols, are also used, to avoid a single point of failure.
Once a communications path is established between a control module and the 
bots, the control module can activate the bots. In its simplest form, the control mod-
ule simply issues command to the bot that causes the bot to execute routines that 
are already implemented in the bot. For greater flexibility, the control module can 
issue update commands that instruct the bots to download a file from some Internet 
location and execute it. The bot in this latter case becomes a more general-purpose 
tool that can be used for multiple attacks.
 10.8 payload‚ÄîinforMaTion ThefT‚Äîkeyloggers, 
phishing, spyware
We now consider payloads where the malware gathers data stored on the infected 
system for use by the attacker. A common target is the user‚Äôs login and password 
credentials to banking, gaming, and related sites, which the attacker then uses to 
impersonate the user to access these sites for gain. Less commonly, the payload may 
target documents or system configuration details for the purpose of reconnaissance 
or espionage. These attacks target the confidentiality of this information.

Credential Theft, Keyloggers, and Spyware
Typically, users send their login and password credentials to banking, gaming, and 
related sites over encrypted communication channels (e.g., HTTPS or POP3S), 
which protect them from capture by monitoring network packets. To bypass this, an 
 attacker can install a keylogger, which captures keystrokes on the infected machine 
to allow an attacker to monitor this sensitive information. Since this would result 
in the attacker receiving a copy of all text entered on the compromised machine, 
keyloggers typically implement some form of filtering mechanism that only returns 
 information close to desired keywords (e.g., ‚Äúlogin‚Äù or ‚Äúpassword‚Äù or ‚Äúpaypal.com‚Äù).
In response to the use of keyloggers, some banking and other sites switched 
to using a graphical applet to enter critical information, such as passwords. Since 
these do not use text entered via the keyboard, traditional keyloggers do not cap-
ture this information. In response, attackers developed more general spyware pay-
loads, which subvert the compromised machine to allow monitoring of a wide range 
of activity on the system. This may include monitoring the history and content of 
browsing activity, redirecting certain Web page requests to fake sites controlled by 
the attacker, dynamically modifying data exchanged between the browser and cer-
tain Web sites of interest. All of which can result in significant compromise of the 
user‚Äôs personal information.
Phishing and Identity Theft
Another approach used to capture a user‚Äôs login and password credentials is to in-
clude a URL in a spam e-mail that links to a fake Web site controlled by the at-
tacker, but which mimics the login page of some banking, gaming, or similar site. 
This is normally included in some message suggesting that urgent action is required 
by the user to authenticate his or her account, to prevent it being locked. If the user 
is careless, and doesn‚Äôt realize that he or she is being conned, then following the link 
and supplying the requested details will certainly result in the attackers exploiting 
the user‚Äôs account using the captured credentials.
More generally, such a spam e-mail may direct a user to a fake Web site 
controlled by the attacker or to complete some enclosed form and return to an  
e-mail accessible to the attacker, which is used to gather a range of private, personal 
 information on the user. Given sufficient details, the attacker can then ‚Äúassume‚Äù 
the user‚Äôs identity for the purpose of obtaining credit or sensitive access to other 
resources. This is known as a phishing attack, which exploits social engineering to 
leverage user‚Äôs trust by masquerading as communications from a trusted source 
[GOLD10].
Such general spam e-mails are typically widely distributed to very large num-
bers of users, often via a botnet. While the content will not match appropriate 
trusted sources for a significant fraction of the recipients, the attackers rely on it 
reaching sufficient users of the named trusted source, a gullible portion of whom 
will respond, for it to be profitable.
A more dangerous variant of this is the spear-phishing attack. This again is an 
e-mail claiming to be from a trusted source. However, the recipients are carefully 
researched by the attacker, and each e-mail is carefully crafted to suit its recipi-
ent specifically, often quoting a range of information to convince him or her of its

authenticity. This greatly increases the likelihood of the recipient responding as de-
sired by the attacker.
Reconnaissance and Espionage
Credential theft and identity theft are special cases of a more general reconnais-
sance payload, which aims to obtain certain types of desired information and return 
this to the attacker. These special cases are certainly the most common; however 
other targets are known. Operation Aurora in 2009 used a Trojan to gain access 
to and potentially modify source code repositories at a range of high-tech, secu-
rity, and defense contractor companies [SYMA13]. The Stuxnet worm discovered 
in 2010 included capture of hardware and software configuration details in order to 
determine whether it had compromised the specific desired target systems. Early 
versions of this worm returned this same information, which was then used to 
 develop the attacks deployed in later versions [CHEN11].
 10.9 payload‚ÄîsTealThing‚Äîbackdoors, rooTkiTs
The final category of payload we discuss concerns techniques used by malware to 
hide its presence on the infected system and to provide covert access to that system. 
This type of payload also attacks the integrity of the infected system.
Backdoor
A backdoor, also known as a trapdoor, is a secret entry point into a program that 
allows someone who is aware of the backdoor to gain access without going through 
the usual security access procedures. The backdoor is code that recognizes some 
special sequence of input or is triggered by being run from a certain user ID or by an 
unlikely sequence of events.
A backdoor is usually implemented as a network service listening on some 
nonstandard port that the attacker can connect to and issue commands through to 
be run on the compromised system.
It is difficult to implement operating system controls for backdoors in appli-
cations. Security measures must focus on the program development and software 
update activities, and on programs that wish to offer a network service.
Rootkit
A rootkit is a set of programs installed on a system to maintain covert access to that 
system with administrator (or root)3 privileges, while hiding evidence of its pres-
ence to the greatest extent possible. This provides access to all the functions and 
services of the operating system. The rootkit alters the host‚Äôs standard functionality 
in a malicious and stealthy way. With root access, an attacker has complete control 
of the system and can add or change programs and files, monitor processes, send 
and receive network traffic, and get backdoor access on demand.
3On UNIX systems, the administrator, or superuser, account is called root; hence the term root access.

A rootkit can make many changes to a system to hide its existence, making 
it difficult for the user to determine that the rootkit is present and to identify what 
changes have been made. In essence, a rootkit hides by subverting the mechanisms 
that monitor and report on the processes, files, and registries on a computer.
A rootkit can be classified using the following characteristics:
‚ñ†
‚ñ† Persistent: Activates each time the system boots. The rootkit must store code 
in a persistent store, such as the registry or file system, and configure a method 
by which the code executes without user intervention. This means it is easier to 
detect, as the copy in persistent storage can potentially be scanned.
‚ñ†
‚ñ† Memory based: Has no persistent code and therefore cannot survive a reboot. 
However, because it is only in memory, it can be harder to detect.
‚ñ†
‚ñ† User mode: Intercepts calls to APIs (application program interfaces) and 
modifies returned results. For example, when an application performs a direc-
tory listing, the return results don‚Äôt include entries identifying the files associ-
ated with the rootkit.
‚ñ†
‚ñ† Kernel mode: Can intercept calls to native APIs in kernel mode.4 The rootkit 
can also hide the presence of a malware process by removing it from the ker-
nel‚Äôs list of active processes.
‚ñ†
‚ñ† Virtual machine based: This type of rootkit installs a lightweight virtual 
 machine monitor and then runs the operating system in a virtual machine 
above it. The rootkit can then transparently intercept and modify states and 
events occurring in the virtualized system.
‚ñ†
‚ñ† External mode: The malware is located outside the normal operation mode 
of the targeted system, in BIOS or system management mode, where it can 
directly access hardware.
This classification shows a continuing arms race between rootkit authors, who 
exploit ever more stealthy mechanisms to hide their code, and those who develop 
mechanisms to harden systems against such subversion or to detect when it has 
occurred.
 10.10 counTerMeasures
Malware Countermeasure Approaches
SP 800-83 lists four main elements of prevention: policy, awareness, vulnerability mit-
igation, and threat mitigation. Having a suitable policy to address malware preven-
tion provides a basis for implementing appropriate preventative countermeasures.
4The kernel is the portion of the OS that includes the most heavily used and most critical portions of 
software. Kernel mode is a privileged mode of execution reserved for the kernel. Typically, kernel mode 
allows access to regions of main memory that are unavailable to processes executing in a less privileged 
mode and also enables execution of certain machine instructions that are restricted to the kernel mode.

One of the first countermeasures that should be employed is to ensure all 
systems are as current as possible, with all patches applied, in order to reduce the 
number of vulnerabilities that might be exploited on the system. The next is to set 
appropriate access controls on the applications and data stored on the system, to 
reduce the number of files that any user can access, and hence potentially infect or 
corrupt, as a result of them executing some malware code. These measures directly 
target the key propagation mechanisms used by worms, viruses, and some trojans.
The third common propagation mechanism, which targets users in a social 
 engineering attack, can be countered using appropriate user awareness and training. 
This aims to equip users to be more aware of these attacks, and less likely to take 
actions that result in their compromise. SP 800-83 provides examples of  suitable 
awareness issues.
If prevention fails, then technical mechanisms can be used to support the fol-
lowing threat mitigation options:
‚ñ†
‚ñ† Detection: Once the infection has occurred, determine that it has occurred and 
locate the malware.
‚ñ†
‚ñ† Identification: Once detection has been achieved, identify the specific mal-
ware that has infected the system.
‚ñ†
‚ñ† Removal: Once the specific malware has been identified, remove all traces of 
malware virus from all infected systems so that it cannot spread further.
If detection succeeds but either identification or removal is not possible, then 
the alternative is to discard any infected or malicious files and reload a clean backup 
version. In the case of some particularly nasty infections, this may require a com-
plete wipe of all storage, and rebuild of the infected system from known clean media.
To begin, let us consider some requirements for effective malware 
countermeasures:
‚ñ†
‚ñ† Generality: The approach taken should be able to handle a wide variety of 
attacks.
‚ñ†
‚ñ† Timeliness: The approach should respond quickly so as to limit the number of 
infected programs or systems and the consequent activity.
‚ñ†
‚ñ† Resiliency: The approach should be resistant to evasion techniques employed 
by attackers to hide the presence of their malware.
‚ñ†
‚ñ† Minimal denial-of-service costs: The approach should result in minimal reduc-
tion in capacity or service due to the actions of the countermeasure software, 
and should not significantly disrupt normal operation.
‚ñ†
‚ñ† Transparency: The countermeasure software and devices should not require 
modification to existing (legacy) OSs, application software, and hardware.
‚ñ†
‚ñ† Global and local coverage: The approach should be able to deal with attack 
sources both from outside and inside the enterprise network.
Achieving all these requirements often requires the use of multiple approaches. 
Detection of the presence of malware can occur in a number of locations. It 
may occur on the infected system, where some host-based ‚Äúantivirus‚Äù program is 
running, monitoring data imported into the system, and the execution and behavior

of programs running on the system. Or, it may take place as part of the perimeter 
 security mechanisms used in an organization‚Äôs firewall and intrusion detection systems 
(IDSs). Lastly, detection may use distributed mechanisms that gather data from both 
host-based and perimeter sensors, potentially over a large number of networks and 
organizations, in order to obtain the largest scale view of the movement of malware.
Host-Based Scanners
The first location where antivirus software is used is on each end system. This gives 
the software the maximum access to information not only on the behavior of the 
malware as it interacts with the targeted system but also on the smallest overall 
view of malware activity. The use of antivirus software on personal computers is 
now widespread, in part caused by the explosive growth in malware volume and 
 activity. Advances in virus and other malware technology, and in antivirus tech-
nology and other countermeasures, go hand in hand. Early malware used rela-
tively simple and easily detected code, and hence could be identified and purged 
with relatively simple antivirus software packages. As the malware arms race has 
evolved, both the malware code and, necessarily, antivirus software have grown 
more  complex and sophisticated.
[STEP93] identifies four generations of antivirus software:
‚ñ†
‚ñ† First generation: Simple scanners
‚ñ†
‚ñ† Second generation: Heuristic scanners
‚ñ†
‚ñ† Third generation: Activity traps
‚ñ†
‚ñ† Fourth generation: Full-featured protection
A first-generation scanner requires a malware signature to identify the mal-
ware. The signature may contain ‚Äúwildcards‚Äù but matches essentially the same 
structure and bit pattern in all copies of the malware. Such signature-specific scan-
ners are limited to the detection of known malware. Another type of first-genera-
tion scanner maintains a record of the length of programs and looks for changes in 
length as a result of virus infection.
A second-generation scanner does not rely on a specific signature. Rather, the 
scanner uses heuristic rules to search for probable malware instances. One class of 
such scanners looks for fragments of code that are often associated with malware. 
For example, a scanner may look for the beginning of an encryption loop used in a 
polymorphic virus and discover the encryption key. Once the key is discovered, the 
scanner can decrypt the malware to identify it, and then remove the infection and 
return the program to service.
Another second-generation approach is integrity checking. A checksum 
can be appended to each program. If malware alters or replaces some program 
without changing the checksum, then an integrity check will catch this change. 
To counter malware that is sophisticated enough to change the checksum when 
it alters a program, an encrypted hash function can be used. The encryption key 
is stored separately from the program so that the malware cannot generate a new 
hash code and encrypt that. By using a hash function rather than a simpler check-
sum, the malware is prevented from adjusting the program to produce the same 
hash code as before. If a protected list of programs in trusted locations is kept, this

approach can also detect attempts to replace or install rogue code or programs in 
these locations.
Third-generation programs are memory-resident programs that identify mal-
ware by its actions rather than its structure in an infected program. Such programs 
the advantage that it is not necessary to develop signatures and heuristics for a wide 
array of malware. Rather, it is necessary only to identify the small set of actions that 
indicate that malicious activity is being attempted and then to intervene.
Fourth-generation products are packages consisting of a variety of antivirus 
techniques used in conjunction. These include scanning and activity trap compo-
nents. In addition, such a package includes access control capability, which limits 
the ability of malware to penetrate a system and then limits the ability of a malware 
to update files in order to propagate.
The arms race continues. With fourth-generation packages, a more compre-
hensive defense strategy is employed, broadening the scope of defense to more 
general-purpose computer security measures. These include more sophisticated 
 antivirus approaches. We now highlight two of the most important.
Host-basEd bEHaVior-blocking softwarE Unlike heuristics or fingerprint-based 
scanners, behavior-blocking software integrates with the operating system of a 
host computer and monitors program behavior in real time for malicious actions 
[CONR02, NACH02]. The behavior blocking software then blocks potentially mali-
cious actions before they have a chance to affect the system. Monitored behaviors 
can include the following:
‚ñ†
‚ñ† Attempts to open, view, delete, and/or modify files
‚ñ†
‚ñ† Attempts to format disk drives and other unrecoverable disk operations
‚ñ†
‚ñ† Modifications to the logic of executable files or macros
‚ñ†
‚ñ† Modification of critical system settings, such as start-up settings
‚ñ†
‚ñ† Scripting of e-mail and instant messaging clients to send executable content
‚ñ†
‚ñ† Initiation of network communications
Because a behavior blocker can block suspicious software in real time, it has 
an advantage over such established antivirus detection techniques as fingerprinting 
or heuristics. There are literally trillions of different ways to obfuscate and rear-
range the instructions of a virus or worm, many of which will evade detection by a 
fingerprint scanner or heuristic. But eventually, malicious code must make a well-
defined request to the operating system. Given that the behavior blocker can inter-
cept all such requests, it can identify and block malicious actions regardless of how 
obfuscated the program logic appears to be.
Behavior blocking alone has limitations. Because the malicious code must 
run on the target machine before all its behaviors can be identified, it can cause 
harm before it has been detected and blocked. For example, a new item of malware 
might shuffle a number of seemingly unimportant files around the hard drive before 
modifying a single file and being blocked. Even though the actual modification was 
blocked, the user may be unable to locate his or her files, causing a loss to produc-
tivity or possibly having worse consequences.

spywarE dEtEction and rEmoVal Although general antivirus products include 
signatures to detect spyware, the threat this type of malware poses, and its use of 
stealthing techniques, means that a range of spyware specific detection and removal 
utilities exist. These specialize in the detection and removal of spyware, and provide 
more robust capabilities. Thus they complement, and should be used along with, 
more general antivirus products.
rootkit countErmEasurEs Rootkits can be extraordinarily difficult to detect and 
neutralize, particularly so for kernel-level rootkits. Many of the administrative tools 
that could be used to detect a rootkit or its traces can be compromised by the root-
kit precisely so that it is undetectable.
Countering rootkits requires a variety of network- and computer-level secu-
rity tools. Both network- and host-based IDSs can look for the code signatures of 
known rootkit attacks in incoming traffic. Host-based antivirus software can also be 
used to recognize the known signatures.
Of course, there are always new rootkits and modified versions of existing 
rootkits that display novel signatures. For these cases, a system needs to look for 
behaviors that could indicate the presence of a rootkit, such as the interception of 
system calls or a keylogger interacting with a keyboard driver. Such behavior detec-
tion is far from straightforward. For example, antivirus software typically intercepts 
system calls.
Another approach is to do some sort of file integrity check. An example of 
this is RootkitRevealer, a freeware package from SysInternals. The package com-
pares the results of a system scan using APIs with the actual view of storage using 
instructions that do not go through an API. Because a rootkit conceals itself by 
modifying the view of storage seen by administrator calls, RootkitRevealer catches 
the discrepancy.
If a kernel-level rootkit is detected, the only secure and reliable way to  recover 
is to do an entire new OS install on the infected machine.
Perimeter Scanning Approaches
The next location where antivirus software is used is on an organization‚Äôs firewall 
and IDS. It is typically included in e-mail and Web proxy services running on these 
systems. It may also be included in the traffic analysis component of an IDS. This 
gives the antivirus software access to malware in transit over a network connection 
to any of the organization‚Äôs systems, providing a larger-scale view of malware activ-
ity. This software may also include intrusion prevention measures, blocking the flow 
of any suspicious traffic, thus preventing it reaching and compromising some target 
system, either inside or outside the organization.
However, this approach is limited to scanning the malware content, as it does 
not have access to any behavior observed when it runs on an infected system. Two 
types of monitoring software may be used:
‚ñ†
‚ñ† Ingress monitors: These are located at the border between the enterprise net-
work and the Internet. They can be part of the ingress-filtering software of a 
border router or external firewall or a separate passive monitor. A honeypot

can also capture incoming malware traffic. An example of a detection tech-
nique for an ingress monitor is to look for incoming traffic to unused local IP 
addresses.
‚ñ†
‚ñ† Egress monitors: These can be located at the egress point of individual LANs 
on the enterprise network as well as at the border between the enterprise net-
work and the Internet. In the former case, the egress monitor can be part of the 
egress-filtering software of a LAN router or switch. As with ingress monitors, 
the external firewall or a honeypot can house the monitoring software. Indeed, 
the two types of monitors can be collocated. The egress monitor is  designed to 
catch the source of a malware attack by monitoring outgoing  traffic for signs of 
scanning or other suspicious behavior.
Perimeter monitoring can also assist in detecting and responding to botnet 
activity by detecting abnormal traffic patterns associated with this activity. Once 
bots are activated and an attack is underway, such monitoring can be used to detect 
the attack. However, the primary objective is to try to detect and disable the botnet 
during its construction phase, using the various scanning techniques we have just 
discussed, identifying and blocking the malware that is used to propagate this type 
of payload.
pErimEtEr worm countErmEasurEs There is considerable overlap in techniques 
for dealing with viruses and worms. Once a worm is resident on a machine, antivirus 
software can be used to detect it, and possibly remove it. In addition, because worm 
propagation generates considerable network activity, perimeter network activity 
and usage monitoring can form the basis of a worm defense. Following [JHI07], we 
list six classes of worm defense that address the network activity it may generate:
A. Signature-based worm scan filtering: This type of approach generates a worm 
signature, which is then used to prevent worm scans from entering/leaving a 
network/host. Typically, this approach involves identifying suspicious flows and 
generating a worm signature. This approach is vulnerable to the use of poly-
morphic worms: Either the detection software misses the worm or, if it is suf-
ficiently sophisticated to deal with polymorphic worms, the scheme may take a 
long time to react. [NEWS05] is an example of this approach.
B. Filter-based worm containment: This approach is similar to class A but focuses 
on worm content rather than a scan signature. The filter checks a message 
to determine if it contains worm code. An example is Vigilante [COST05], 
which relies on collaborative worm detection at end hosts. This approach can 
be quite effective but requires efficient detection algorithms and rapid alert 
 dissemination.
C. Payload-classification-based worm containment: These network-based tech-
niques examine packets to see if they contain a worm. Various anomaly detec-
tion techniques can be used, but care is needed to avoid high levels of false 
positives or negatives. An example of this approach, which looks for exploit 
code in network flows, is reported in [CHIN05]. This approach does not gener-
ate signatures based on byte patterns but rather looks for control and data flow 
structures that suggest an exploit.

D. Threshold random walk (TRW) scan detection: TRW exploits randomness 
in picking destinations to connect to as a way of detecting if a scanner is in 
operation [JUNG04]. TRW is suitable for deployment in high-speed, low-
cost  network devices. It is effective against the common behavior seen in 
worm¬†scans.
E. Rate limiting: This class limits the rate of scanlike traffic from an infected 
host. Various strategies can be used, including limiting the number of new 
machines a host can connect to in a window of time, detecting a high con-
nection failure rate, and limiting the number of unique IP addresses a host 
can scan in a window of time. [CHEN04] is an example. This class of counter-
measures may introduce longer delays for normal traffic. This class is also not 
suited for slow, stealthy worms that spread slowly to avoid detection based on 
 activity¬†level.
F. Rate halting: This approach immediately blocks outgoing traffic when a thresh-
old is exceeded either in outgoing connection rate or in diversity of connection 
attempts [JHI07]. The approach must include measures to quickly unblock 
mistakenly blocked hosts in a transparent way. Rate halting can integrate with 
a signature- or filter-based approach so that once a signature or filter is gener-
ated, every blocked host can be unblocked. Rate halting appears to offer a very 
effective countermeasure. As with rate limiting, rate-halting techniques are not 
suitable for slow, stealthy worms.
Distributed Intelligence Gathering Approaches
The final location where antivirus software is used is in a distributed configu-
ration. It gathers data from a large number of both host-based and perimeter 
sensors, relays this intelligence to a central analysis system able to correlate and 
analyze the data, which can then return updated signatures and behavior pat-
terns to enable all of the coordinated systems to respond and defend against mal-
ware attacks. A number of such systems have been proposed. We discuss one such 
 approach in the remainder of this section.
Figure 10.4 shows an example of a distributed worm countermeasure architec-
ture (based on [SIDI05]). The system works as follows (numbers in figure refer to 
numbers in the following list):
1. Sensors deployed at various network locations detect a potential worm. The 
sensor logic can also be incorporated in IDS sensors.
2. The sensors send alerts to a central server, which correlates and analyzes the 
incoming alerts. The correlation server determines the likelihood that a worm 
attack is being observed and the key characteristics of the attack.
3. The server forwards its information to a protected environment, where the 
potential worm may be sandboxed for analysis and testing.
4. The protected system tests the suspicious software against an appropriately 
instrumented version of the targeted application to identify the vulnerability.

5. The protected system generates one or more software patches and tests these.
6. If the patch is not susceptible to the infection and does not compromise the 
application‚Äôs functionality, the system sends the patch to the application host 
to update the targeted application.
 10.11 disTribuTed denial of service aTTacks
A denial-of-service (DoS) attack is an attempt to prevent legitimate users of a ser-
vice from using that service. When this attack comes from a single host or network 
node, then it is simply referred to as a DoS attack. A more serious threat is posed 
by a DDoS attack. DDoS attacks make computer systems inaccessible by flooding 
servers, networks, or even end-user systems with useless traffic so that legitimate 
users can no longer gain access to those resources. In a typical DDoS attack, a large 
number of compromised hosts are amassed to send useless packets.
This section is concerned with DDoS attacks. First, we look at the nature and 
types of attacks. Next, we examine methods by which an attacker is able to recruit 
a network of hosts for attack launch. Finally, this section looks at countermeasures.
Figure 10.4 Placement of Worm Monitors
Internet
Remote sensor
Honeypot
Passive
sensor
Firewall
sensor
Correlation
server
Application
server
Instrumented applications
Sandboxed
environment
Enterprise network
Hypothesis testing
and analysis
Patch
generation
5. Possible fx generation
3. Forward
features
6. Application update
4. Vulnerability 
testing and 
identifcation
1. Worm scans or 
infection attempts
2. Notifcations

DDoS Attack Description
A DDoS attack attempts to consume the target‚Äôs resources so that it cannot provide 
service. One way to classify DDoS attacks is in terms of the type of resource that 
is consumed. Broadly speaking, the resource consumed is either an internal host 
resource on the target system or data transmission capacity in the local network to 
which the target is attacked.
A simple example of an internal resource attack is the SYN flood attack. 
Figure 10.5a shows the steps involved:
1. The attacker takes control of multiple hosts over the Internet, instructing them 
to contact the target Web server.
2. The slave hosts begin sending TCP/IP SYN (synchronize/initialization) pack-
ets, with erroneous return IP address information, to the target.
3. Each SYN packet is a request to open a TCP connection. For each such packet, 
the Web server responds with a SYN/ACK (synchronize/acknowledge) packet, 
trying to establish a TCP connection with a TCP entity at a spurious IP  address. 
The Web server maintains a data structure for each SYN request waiting for a 
response back and becomes bogged down as more traffic floods in. The result 
is that legitimate connections are denied while the victim machine is waiting to 
complete bogus ‚Äúhalf-open‚Äù connections.
The TCP state data structure is a popular internal resource target but by no 
means the only one. [CERT01] gives the following examples:
1. An intruder may attempt to use up available data structures that are used by 
the OS to manage processes, such as process table entries and process control 
information entries. The attack can be quite simple, such as a program that 
forks new processes repeatedly.
2. An intruder may attempt to allocate to itself large amounts of disk space by a 
variety of straightforward means. These include generating numerous e-mails, 
forcing errors that trigger audit trails, and placing files in shareable areas.
Figure 10.5b illustrates an example of an attack that consumes data transmis-
sion resources. The following steps are involved:
1. The attacker takes control of multiple hosts over the Internet, instructing them 
to send ICMP ECHO packets5 with the target‚Äôs spoofed IP address to a group 
of hosts that act as reflectors, as described subsequently.
2. Nodes at the bounce site receive multiple spoofed requests and respond by 
sending echo reply packets to the target site.
3. The target‚Äôs router is flooded with packets from the bounce site, leaving no 
data transmission capacity for legitimate traffic.
5The Internet Control Message Protocol (ICMP) is an IP-level protocol for the exchange of control pack-
ets between a router and a host or between hosts. The ECHO packet requires the recipient to respond 
with an echo reply to check that communication is possible between entities.

Another way to classify DDoS attacks is as either direct or reflector DDoS 
 attacks. In a direct DDoS attack (Figure 10.6a), the attacker is able to implant zombie 
software on a number of sites distributed throughout the Internet. Often, the DDoS 
attack involves two levels of zombie machines: master zombies and slave zombies. 
The hosts of both machines have been infected with malicious code. The attacker 
coordinates and triggers the master zombies, which in turn coordinate and trigger 
the slave zombies. The use of two levels of zombies makes it more difficult to trace 
the attack back to its source and provides for a more resilient network of attackers.
A reflector DDoS attack adds another layer of machines (Figure 10.6b). In this 
type of attack, the slave zombies construct packets requiring a response that contain 
the target‚Äôs IP address as the source IP address in the packet‚Äôs IP header. These pack-
ets are sent to uninfected machines known as reflectors. The uninfected machines 
 respond with packets directed at the target machine. A reflector DDoS attack can 
easily involve more machines and more traffic than a direct DDoS attack and hence 
be more damaging. Further, tracing back the attack or filtering out the attack packets 
is more difficult because the attack comes from widely dispersed uninfected machines.
Figure 10.5 Examples of Simple DDoS Attacks
SYN
packets
Attack
machine
Attack
machine
Refector
machines
Slave
servers
1
1
2
2
3
3
(a) Distributed SYN food attack
(b) Distributed ICMP attack
Internet
Target Web
server
Target
router
SYN
packets
SYN/ACK
packets

Constructing the Attack Network
The first step in a DDoS attack is for the attacker to infect a number of machines 
with zombie software that will ultimately be used to carry out the attack. The essen-
tial ingredients in this phase of the attack are the following:
1. Software that can carry out the DDoS attack. The software must be able to 
run on a large number of machines, must be able to conceal its existence, must 
Figure 10.6 Types of Flooding-Based DDoS Attacks
(a) Direct DDoS Attack
Attacker
Attacker
Refectors
Victim
Victim
Master
zombies
Master
zombies
Slave
zombies
Slave
zombies
(b) Refector DDoS Attack

be able to communicate with the attacker or have some sort of time-triggered 
mechanism, and must be able to launch the intended attack toward the target.
2. A vulnerability in a large number of systems. The attacker must become aware 
of a vulnerability that many system administrators and individual users have 
failed to patch and that enables the attacker to install the zombie software.
3. A strategy for locating vulnerable machines, a process known as scanning.
In the scanning process, the attacker first seeks out a number of vulnerable 
machines and infects them. Then, typically, the zombie software that is installed in 
the infected machines repeats the same scanning process, until a large distributed 
network of infected machines is created. [MIRK04] lists the following types of scan-
ning strategies:
‚ñ†
‚ñ† Random: Each compromised host probes random addresses in the IP  address 
space, using a different seed. This technique produces a high volume of Internet 
traffic, which may cause generalized disruption even before the actual attack 
is launched.
‚ñ†
‚ñ† Hit list: The attacker first compiles a long list of potential vulnerable machines. 
This can be a slow process done over a long period to avoid detection that an 
attack is underway. Once the list is compiled, the attacker begins infecting ma-
chines on the list. Each infected machine is provided with a portion of the list 
to scan. This strategy results in a very short scanning period, which may make 
it difficult to detect that infection is taking place.
‚ñ†
‚ñ† Topological: This method uses information contained on an infected victim 
machine to find more hosts to scan.
‚ñ†
‚ñ† Local subnet: If a host is infected behind a firewall, that host then looks for 
targets in its own local network. The host uses the subnet address structure to 
find other hosts that would otherwise be protected by the firewall.
DDoS Countermeasures
In general, there are three lines of defense against DDoS attacks [CHAN02]:
‚ñ†
‚ñ† Attack prevention and preemption (before the attack): These mechanisms en-
able the victim to endure attack attempts without denying service to legiti-
mate clients. Techniques include enforcing policies for resource consumption 
and providing backup resources available on demand. In addition, prevention 
mechanisms modify systems and protocols on the Internet to reduce the pos-
sibility of DDoS attacks.
‚ñ†
‚ñ† Attack detection and filtering (during the attack): These mechanisms attempt to 
detect the attack as it begins and respond immediately. This minimizes the impact 
of the attack on the target. Detection involves looking for suspicious patterns of 
behavior. Response involves filtering out packets likely to be part of the attack.
‚ñ†
‚ñ† Attack source traceback and identification (during and after the attack): This 
is an attempt to identify the source of the attack as a first step in preventing  
 future attacks. However, this method typically does not yield results fast 
enough, if at all, to mitigate an ongoing attack.

The challenge in coping with DDoS attacks is the sheer number of ways in 
which they can operate. Thus, DDoS countermeasures must evolve with the threat.
 10.12 key TerMs, review QuesTions, and probleMs
Key Terms 
adware
attack kit
backdoor
behavior-blocking software
blended attack
boot sector infector
bot
botnet
crimeware
direct DDoS attack
distributed denial of service 
(DDoS)
downloader
drive-by-download
e-mail virus
flooders
keyloggers
logic bomb
macro virus
malicious software
malware
metamorphic virus
mobile code
parasitic virus
phishing
polymorphic virus
ransomware
reflector DDoS attack
rootkit
scanning
spear-phishing
spyware
stealth virus
trapdoor
Trojan horse
virus
worm
zombie
zero-day exploit
Review Questions 
 
10.1 
What are three broad mechanisms that malware can use to propagate?
 
10.2 
What is a blended attack?
 
10.3 
What are typical phases of operation of a virus or worm?
 
10.4 
Classify viruses based on the targets they try to infect.
 
10.5 
List the features of macro viruses that enable them to infect scripting codes.
 
10.6 
What functions does a worm perform during the propagation phase?
 
10.7 
Give some examples of client side vulnerabilities that can be exploited by malware?
 
10.8 
What is an ‚Äúinfection vector‚Äù?
 
10.9 
Explain the difference between a keylogger and spyware with an example.
 
10.10 
What kind of activities can be performed by an attacker using a rootkit? What makes 
it difficult to detect a rootkit?
 
10.11 
Describe some malware countermeasure elements.
 
10.12 
List three places malware mitigation mechanisms may be located.
 
10.13 
Briefly describe the four generations of antivirus software.
 
10.14 
List the activities that can be monitored by ‚Äúbehavior-blocking software‚Äù.
 
10.15 
What is the difference between a reflector DDoS attack and a direct DDoS attack?
Problems 
 
10.1 
There is a flaw in the virus program of Figure 10.1a. What is it?
 
10.2 
The question arises as to whether it is possible to develop a program that can analyze 
a piece of software to determine if it is a virus. Consider that we have a program D

that is supposed to be able to do that. That is, for any program P, if we run D(P), the 
result returned is TRUE (P is a virus) or FALSE (P is not a virus). Now consider the 
following program:
Program CV :=
{¬†.¬†.¬†.¬†
main-program :=
{if D(CV) then goto next:
else infect-executable;
}
next:
}
In the preceding program, infect-executable is a module that scans memory for 
 executable programs and replicates itself in those programs. Determine if D can 
 correctly decide whether CV is a virus.
 
10.3 
The following code fragments show a sequence of virus instructions and a metamor-
phic version of the virus. Describe the effect produced by the metamorphic code.
Original Code
Metamorphic Code
mov eax, 5
mov eax, 5
add eax, ebx
push edx
call [ebx]
jmp 0x89AB
swap eax, ebx
call [ebx]
nop
 
10.4 
The list of passwords used by the Morris worm is provided at this book‚Äôs Premium 
Content Web site.
a. The assumption has been expressed by many people that this list represents words 
commonly used as passwords. Does this seem likely? Justify your answer.
b. If the list does not reflect commonly used passwords, suggest some approaches 
that Morris may have used to construct the list.
 
10.5 
What type of malware is the following code fragment?
legitimate code
if data is Friday the 13th;
crash_computer();
legitimate code 
 
10.6 
Consider the following situation and identify the type of software attack, if any:
You are the owner of a small business. After you login to your client server appli-
cation with your credentials, you find that the data is displayed in the form of a 
jumbled collection of alphabets, numbers, special characters, and symbols. You 
are unpleasantly surprised and wonder what happened. You get a call after some 
time, and the person at the other end tells you that your system is hacked, and you 
can recover the data once you pay him a certain amount of money.
 
10.7 
Assume that you have received an e-mail with an attachment from your friend‚Äôs  
e-mail id. You access the e-mail using your work computer, and click on the

attachment without screening it for malware. What threats might this pose to your 
work computer?
 
10.8 
Suppose you observe that your home PC is responding very slowly to information re-
quests from the net. And then you further observe that your network gateway shows 
high levels of network activity, even though you have closed your e-mail client, Web 
browser, and other programs that access the net. What types of malware could cause 
these symptoms? Discuss how the malware might have gained access to your system. 
What steps can you take to check whether this has occurred? If you do identify mal-
ware on your PC, how can you restore it to safe operation?
 
10.9 
Suppose while browsing the Internet, you get a popup window stating that you need 
to install this software in order to clean your system as it is running low on resources. 
Since the message seems to be from a genuine OS vendor like Microsoft Windows or 
Mac iOS, you click the ‚ÄòOK‚Äô button. How could your action harm your system? How 
can you fix the issue?
 
10.10 
Suppose you have a new smartphone and are excited about the range of apps avail-
able for it. You read about a really interesting new game that is available for your 
phone. You do a quick Web search for it and see that a version is available from one 
of the free marketplaces. When you download and start to install this app, you are 
asked to approve the access permissions granted to it. You see that it wants permis-
sion to ‚ÄúSend SMS messages‚Äù and to ‚ÄúAccess your address-book.‚Äù Should you be 
suspicious that a game wants these types of permissions? What threat might the app 
pose to your smartphone? Should you grant these permissions and proceed to install 
it? What types of malware might it be?
 
10.11 
Assume you receive an e-mail that appears to come from a senior manager of your 
company, with a subject indicating that it concerns a project that you are currently 
working on. When you view the e-mail, you see that it asks you to review the  attached 
revised press release, supplied as a PDF document, to check that all details are correct 
before management releases it. When you attempt to open the PDF, the viewer pops 
up a dialog labeled ‚ÄúLaunch File,‚Äù indicating that ‚Äúthe file and its viewer  application 
are set to be launched by this PDF file.‚Äù In the section of this dialog  labeled ‚ÄúFile‚Äù 
there are a number of blank lines and finally the text ‚ÄúClick the ‚ÄòOpen‚Äô button to view 
this document.‚Äù You also note that there is a vertical scroll-bar visible for this  region. 
What type of threat might this pose to your computer system should you indeed 
 select the ‚ÄúOpen‚Äù button? How could you check your suspicions without threatening 
your system? What type of attack is this type of message associated with? How many 
people are likely to have received this particular e-mail?
 
10.12 
Assume you work in a financial auditing company. An e-mail arrives in your inbox 
that appears to be from your chief auditor with the following content:
‚ÄúWe have identified a few threats which pose potential danger to our informa-
tion systems. In order to address this, our information security team has decided 
to ensure proper credentials of all the employees. Please cooperate and complete 
this process immediately by clicking the given link.‚Äù
What kind of an attack is this e-mail attempting? How should you respond to such 
e-mails?
 
10.13 
There are hundreds of unsolicited e-mails in your inbox. What kind of attack is this? 
Analyze related issues.
 
10.14 
Suggest some methods of attacking the worm countermeasure architecture, discussed 
in Section 10.9, that could be used by worm creators. Suggest some possible counter-
measures to these methods.

11.1 Intruders
Intruder Behavior Patterns
Intrusion Techniques
11.2 Intrusion Detection
Audit Records
Statistical Anomaly Detection
Rule-Based Intrusion Detection
The Base-Rate Fallacy
Distributed Intrusion Detection
Honeypots
Intrusion Detection Exchange Format
11.3 Password Management
The Vulnerability of Passwords
The Use of Hashed Passwords
User Password Choices
Password Selection Strategies
Bloom Filter
11.4 Key Terms, Review Questions, and Problems
Chapter
Intruders

A significant security problem for networked systems is hostile, or at least unwanted, 
trespass by users or software. User trespass can take the form of unauthorized logon 
to a machine or, in the case of an authorized user, acquisition of privileges or perfor-
mance of actions beyond those that have been authorized. Software trespass can take 
the form of a virus, worm, or Trojan horse.
All these attacks relate to network security because system entry can be achieved 
by means of a network. However, these attacks are not confined to network-based 
attacks. A user with access to a local terminal may attempt trespass without using an 
intermediate network. A virus or Trojan horse may be introduced into a system by 
means of an optical disc. Only the worm is a uniquely network phenomenon. Thus, 
system trespass is an area in which the concerns of network security and computer 
security overlap.
Because the focus of this book is network security, we do not attempt a compre-
hensive analysis of either the attacks or the security countermeasures related to system 
trespass. Instead, in this Part we present a broad overview of these concerns.
This chapter covers the subject of intruders. First, we examine the nature of the 
attack and then look at strategies intended for prevention and, failing that, detection. 
Next we examine the related topic of password management.
 11.1 Intruders
One of the two most publicized threats to security is the intruder (the other is viruses), 
often referred to as a hacker or cracker. In an important early study of intrusion, 
Anderson [ANDE80] identified three classes of intruders:
‚ñ†
‚ñ† Masquerader: An individual who is not authorized to use the computer and 
who penetrates a system‚Äôs access controls to exploit a legitimate user‚Äôs account
LearnIng ObjectIves
After studying this chapter, you should be able to:
‚óÜ‚ñ†
Distinguish among various types of intruder behavior patterns.
‚óÜ‚ñ†
Understand the basic principles of and requirements for intrusion detection.
‚óÜ‚ñ†
Discuss the key features of intrusion detection systems.
‚óÜ‚ñ†
Define the intrusion detection exchange format.
‚óÜ‚ñ†
Explain the purpose of honeypots.
‚óÜ‚ñ†
Explain the mechanism by which hashed passwords are used for user 
 authentication.
‚óÜ‚ñ†
Understand the use of the Bloom filter in password management.

‚ñ†
‚ñ† Misfeasor: A legitimate user who accesses data, programs, or resources for 
which such access is not authorized, or who is authorized for such access but 
misuses his or her privileges
‚ñ†
‚ñ† Clandestine user: An individual who seizes supervisory control of the  system 
and uses this control to evade auditing and access controls or to suppress audit 
collection
The masquerader is likely to be an outsider, the misfeasor generally is an insider, 
and the clandestine user can be either an outsider or an insider.
Intruder attacks range from the benign to the serious. At the benign end of the 
scale, there are many people who simply wish to explore internets and see what is 
out there. At the serious end are individuals who are attempting to read privileged 
data, perform unauthorized modifications to data, or disrupt the system.
[GRAN04] lists the following examples of intrusion:
‚ñ†
‚ñ† Performing a remote root compromise of an e-mail server
‚ñ†
‚ñ† Defacing a Web server
‚ñ†
‚ñ† Guessing and cracking passwords
‚ñ†
‚ñ† Copying a database containing credit card numbers
‚ñ†
‚ñ† Viewing sensitive data, including payroll records and medical information, 
without authorization
‚ñ†
‚ñ† Running a packet sniffer on a workstation to capture usernames and passwords
‚ñ†
‚ñ† Using a permission error on an anonymous FTP server to distribute pirated 
software and music files
‚ñ†
‚ñ† Dialing into an unsecured modem and gaining internal network access
‚ñ†
‚ñ† Posing as an executive, calling the help desk, resetting the executive‚Äôs e-mail 
password, and learning the new password
‚ñ†
‚ñ† Using an unattended, logged-in workstation without permission
Intruder Behavior Patterns
The techniques and behavior patterns of intruders are constantly shifting, to exploit 
newly discovered weaknesses and to evade detection and countermeasures. Even 
so, intruders typically follow one of a number of recognizable behavior patterns, 
and these patterns typically differ from those of ordinary users. In the following, we 
look at three broad examples of intruder behavior patterns, to give the reader some 
feel for the challenge facing the security administrator.
Hackers Traditionally, those who hack into computers do so for the thrill of it or 
for status. The hacking community is a strong meritocracy in which status is deter-
mined by level of competence. Thus, attackers often look for targets of opportunity 
and then share the information with others. A typical example is a break-in at a large 
financial institution reported in [RADC04]. The intruder took advantage of the fact 
that the corporate network was running unprotected services, some of which were 
not even needed. In this case, the key to the break-in was the pcAnywhere appli-
cation. The manufacturer, Symantec, advertises this program as a remote  control

solution that enables secure connection to remote devices. But the attacker had an 
easy time gaining access to pcAnywhere; the administrator used the same three-
letter username and password for the program. In this case, there was no intrusion 
detection system on the 700-node corporate network. The intruder was only discov-
ered when a vice-president walked into her office and saw the cursor moving files 
around on her Windows workstation.
Benign intruders might be tolerable, although they do consume resources and 
may slow performance for legitimate users. However, there is no way in advance to 
know whether an intruder will be benign or malign. Consequently, even for systems 
with no particularly sensitive resources, there is a motivation to control this problem.
Intrusion detection systems (IDSs) and intrusion prevention systems (IPSs) 
are designed to counter this type of hacker threat. In addition to using such systems, 
organizations can consider restricting remote logons to specific IP addresses and/or 
use virtual private network technology.
One of the results of the growing awareness of the intruder problem has been 
the establishment of a number of computer emergency response teams (CERTs). 
These cooperative ventures collect information about system vulnerabilities and 
 disseminate it to systems managers. Hackers also routinely read CERT reports. 
Thus, it is important for system administrators to quickly insert all software patches 
to discovered vulnerabilities. Unfortunately, given the complexity of many IT 
 systems, and the rate at which patches are released, this is increasingly difficult 
to achieve without automated updating. Even then, there are problems caused by 
 incompatibilities resulting from the updated software. Hence the need for multiple 
layers of defense in managing security threats to IT systems.
criminals Organized groups of hackers have become a widespread and common 
threat to Internet-based systems. These groups can be in the employ of a corpora-
tion or government but often are loosely affiliated gangs of hackers. Typically, these 
gangs are young, often Eastern European, Russian, or southeast Asian hackers who 
do business on the Web [ANTE06]. They meet in underground forums with names 
like DarkMarket.org and theftservices.com to trade tips and data and coordinate 
attacks. A common target is a credit card file at an e-commerce server. Attackers 
attempt to gain root access. The card numbers are used by organized crime gangs 
to purchase expensive items and are then posted to carder sites, where others can 
access and use the account numbers; this obscures usage patterns and complicates 
investigation.
Whereas traditional hackers look for targets of opportunity, criminal hack-
ers usually have specific targets, or at least classes of targets in mind. Once a site is 
penetrated, the attacker acts quickly, scooping up as much valuable information as 
possible and exiting.
IDSs and IPSs can also be used for these types of attackers, but may be less 
effective because of the quick in-and-out nature of the attack. For e-commerce 
sites, database encryption should be used for sensitive customer information, espe-
cially credit cards. For hosted e-commerce sites (provided by an outsider service), 
the  e-commerce organization should make use of a dedicated server (not used to 
 support multiple customers) and closely monitor the provider‚Äôs security services.

insider attacks Insider attacks are among the most difficult to detect and prevent. 
Employees already have access and knowledge about the structure and content of 
corporate databases. Insider attacks can be motivated by revenge or simply a feel-
ing of entitlement. An example of the former is the case of Kenneth Patterson, fired 
from his position as data communications manager for American Eagle Outfitters. 
Patterson disabled the company‚Äôs ability to process credit card purchases during five 
days of the holiday season of 2002. As for a sense of entitlement, there have always 
been many employees who felt entitled to take extra office supplies for home use, but 
this now extends to corporate data. An example is that of a vice-president of sales for 
a stock analysis firm who quit to go to a competitor. Before she left, she copied the 
customer database to take with her. The offender reported feeling no animus toward 
her former employee; she simply wanted the data because it would be useful to her.
Although IDS and IPS facilities can be useful in countering insider attacks, 
other more direct approaches are of higher priority. Examples include the following:
‚ñ†
‚ñ† Enforce least privilege, only allowing access to the resources employees need 
to do their job.
‚ñ†
‚ñ† Set logs to see what users access and what commands they are entering.
‚ñ†
‚ñ† Protect sensitive resources with strong authentication.
‚ñ†
‚ñ† Upon termination, delete employee‚Äôs computer and network access.
‚ñ†
‚ñ† Upon termination, make a mirror image of employee‚Äôs hard drive before reis-
suing it. That evidence might be needed if your company information turns up 
at a competitor.
In this section, we look at the techniques used for intrusion. Then we examine 
ways to detect intrusion.
Intrusion Techniques
The objective of the intruder is to gain access to a system or to increase the range of 
privileges accessible on a system. Most initial attacks use system or software vulner-
abilities that allow a user to execute code that opens a backdoor into the system. 
Alternatively, the intruder attempts to acquire information that should have been 
protected. In some cases, this information is in the form of a user password. With 
knowledge of some other user‚Äôs password, an intruder can log in to a system and 
exercise all the privileges accorded to the legitimate user.
Typically, a system must maintain a file that associates a password with each autho-
rized user. If such a file is stored with no protection, then it is an easy matter to gain 
access to it and learn passwords. The password file can be protected in one of two ways:
‚ñ†
‚ñ† One-way function: The system stores only the value of a function based on the 
user‚Äôs password. When the user presents a password, the system transforms 
that password and compares it with the stored value. In practice, the system 
usually performs a one-way transformation (not reversible), in which the pass-
word is used to generate a key for the one-way function and in which a fixed-
length output is produced.
‚ñ†
‚ñ† Access control: Access to the password file is limited to one or a very few 
accounts.

If one or both of these countermeasures are in place, some effort is needed 
for a potential intruder to learn passwords. On the basis of a survey of the literature 
and interviews with a number of password crackers, [ALVA90] reports the follow-
ing techniques for learning passwords:
1. Try default passwords used with standard accounts that are shipped with the 
system. Many administrators do not bother to change these defaults.
2. Exhaustively try all short passwords (those of one to three characters).
3. Try words in the system‚Äôs online dictionary or a list of likely passwords. 
Examples of the latter are readily available on hacker bulletin boards.
4. Collect information about users, such as their full names, the names of their 
spouse and children, pictures in their office, and books in their office that are 
related to hobbies.
5. Try users‚Äô phone numbers, Social Security numbers, and room numbers.
6. Try all legitimate license plate numbers for this state.
7. Use a Trojan horse (described in Chapter 10) to bypass restrictions on access.
8. Tap the line between a remote user and the host system.
The first six methods are various ways of guessing a password. If an intruder 
has to verify the guess by attempting to log in, it is a tedious and easily countered 
means of attack. For example, a system can simply reject any login after three pass-
word attempts, thus requiring the intruder to reconnect to the host to try again. 
Under these circumstances, it is not practical to try more than a handful of pass-
words. However, the intruder is unlikely to try such crude methods. For example, if 
an intruder can gain access with a low level of privileges to an encrypted password 
file, then the strategy would be to capture that file and then use the encryption 
mechanism of that particular system at leisure until a valid password that provided 
greater privileges was discovered.
Guessing attacks are feasible, and indeed highly effective, when a large num-
ber of guesses can be attempted automatically and each guess verified, without the 
guessing process being detectable. Later in this chapter, we have much to say about 
thwarting guessing attacks.
The seventh method of attack listed earlier, the Trojan horse, can be par-
ticularly difficult to counter. An example of a program that bypasses access con-
trols has been cited in [ALVA90]. A low-privilege user produced a game program 
and invited the system operator to use it in his or her spare time. The program did 
indeed play a game, but in the background it also contained code to copy the pass-
word file, which was unencrypted but access protected, into the user‚Äôs file. Because 
the game was running under the operator‚Äôs high-privilege mode, it was able to gain 
access to the password file.
The eighth attack listed, line tapping, is a matter of physical security.
Other intrusion techniques do not require learning a password. Intruders can 
get access to a system by exploiting attacks such as buffer overflows on a program 
that runs with certain privileges. Privilege escalation can be done this way as well.
We turn now to a discussion of the two principal countermeasures: detection 
and prevention. Detection is concerned with learning of an attack, either before or

after its success. Prevention is a challenging security goal and an uphill battle at all 
times. The difficulty stems from the fact that the defender must attempt to thwart 
all possible attacks, whereas the attacker is free to try to find the weakest link in the 
defense chain and attack at that point.
 11.2 IntrusIOn detectIOn
Inevitably, the best intrusion prevention system will fail. A system‚Äôs second line 
of defense is intrusion detection, and this has been the focus of much research in 
 recent years. This interest is motivated by a number of considerations, including the 
following:
1. If an intrusion is detected quickly enough, the intruder can be identified and 
ejected from the system before any damage is done or any data are compro-
mised. Even if the detection is not sufficiently timely to preempt the intruder, 
the sooner that the intrusion is detected, the less the amount of damage and 
the more quickly that recovery can be achieved.
2. An effective intrusion detection system can serve as a deterrent, so acting to 
prevent intrusions.
3. Intrusion detection enables the collection of information about intrusion tech-
niques that can be used to strengthen the intrusion prevention facility.
Intrusion detection is based on the assumption that the behavior of the intruder 
differs from that of a legitimate user in ways that can be quantified. Of course, we 
cannot expect that there will be a crisp, exact distinction between an attack by an 
intruder and the normal use of resources by an authorized user. Rather, we must 
expect that there will be some overlap.
Figure 11.1 suggests, in very abstract terms, the nature of the task confront-
ing the designer of an intrusion detection system. Although the typical behavior 
of an intruder differs from the typical behavior of an authorized user, there is an 
overlap in these behaviors. Thus, a loose interpretation of intruder behavior, which 
will catch more intruders, will also lead to a number of false positives, or autho-
rized users identified as intruders. On the other hand, an attempt to limit false posi-
tives by a tight interpretation of intruder behavior will lead to an increase in false 
 negatives, or intruders not identified as intruders. Thus, there is an element of com-
promise and art in the practice of intrusion detection.
In Anderson‚Äôs study [ANDE80], it was postulated that one could, with reason-
able confidence, distinguish between a masquerader and a legitimate user. Patterns 
of legitimate user behavior can be established by observing past history, and signifi-
cant deviation from such patterns can be detected. Anderson suggests that the task 
of detecting a misfeasor (legitimate user performing in an unauthorized fashion) is 
more difficult, in that the distinction between abnormal and normal behavior may 
be small. Anderson concluded that such violations would be undetectable solely 
through the search for anomalous behavior. However, misfeasor behavior might 
nevertheless be detectable by intelligent definition of the class of conditions that 
suggest unauthorized use. Finally, the detection of the clandestine user was felt to

be beyond the scope of purely automated techniques. These observations, which 
were made in 1980, remain true today.
[PORR92] identifies the following approaches to intrusion detection:
1. Statistical anomaly detection: Involves the collection of data relating to the 
behavior of legitimate users over a period of time. Then statistical tests are 
applied to observed behavior to determine with a high level of confidence 
whether that behavior is not legitimate user behavior.
a. Threshold detection: This approach involves defining thresholds, indepen-
dent of user, for the frequency of occurrence of various events.
b. Profile based: A profile of the activity of each user is developed and used to 
detect changes in the behavior of individual accounts.
2. Rule-based detection: Involves an attempt to define a set of rules or attack 
patterns that can be used to decide that a given behavior is that of an intruder. 
This is often referred to as signature detection.
In essence, anomaly approaches attempt to define normal, or expected, behav-
ior, whereas signature-based approaches attempt to define proper behavior.
In terms of the types of attackers listed earlier, statistical anomaly detection is 
effective against masqueraders, who are unlikely to mimic the behavior patterns of 
the accounts they appropriate. On the other hand, such techniques may be unable 
Figure 11.1 Profiles of Behavior of Intruders and Authorized Users
Overlap in observed
or expected behavior
Profle of
intruder behavior
Profle of
authorized user
behavior
Measurable behavior
parameter
Average behavior
of intruder
Average behavior
of authorized user
Probability
density function

to deal with misfeasors. For such attacks, rule-based approaches may be able to rec-
ognize events and sequences that, in context, reveal penetration. In practice, a sys-
tem may exhibit a combination of both approaches to be effective against a broad 
range of attacks.
Audit Records
A fundamental tool for intrusion detection is the audit record. Some record of ongo-
ing activity by users must be maintained as input to an intrusion detection system. 
Basically, two plans are used:
‚ñ†
‚ñ† Native audit records: Virtually all multiuser operating systems include 
accounting software that collects information on user activity. The advantage 
of using this information is that no additional collection software is needed. 
The disadvantage is that the native audit records may not contain the needed 
information or may not contain it in a convenient form.
‚ñ†
‚ñ† Detection-specific audit records: A collection facility can be implemented 
that generates audit records containing only that information required by the 
intrusion detection system. One advantage of such an approach is that it could 
be made vendor independent and ported to a variety of systems. The disadvan-
tage is the extra overhead involved in having, in effect, two accounting pack-
ages running on a machine.
A good example of detection-specific audit records is one developed by 
Dorothy Denning [DENN87]. Each audit record contains the following fields:
‚ñ†
‚ñ† Subject: A subject initiates actions. A subject could be a user or a process act-
ing on behalf of users or groups of users. Subjects may be grouped into differ-
ent access classes, and these classes may overlap.
‚ñ†
‚ñ† Action: An action initiated by a subject refers to some object; for example, 
login, read, perform I/O, execute.
‚ñ†
‚ñ† Object: Actions are performed on or with objects. Examples include files, pro-
grams, messages, records, terminals, printers, and user- or program-created struc-
tures. When a subject is the recipient of an action, such as electronic mail, then 
that subject is considered an object. Objects may be grouped by type. Object 
granularity may vary by object type and by environment. For example, database 
actions may be audited for the database as a whole or at the record level.
‚ñ†
‚ñ† Exception-Condition: If an exception condition occurs, this field contains 
identifying information.
‚ñ†
‚ñ† Resource-Usage: This is a list, in which each item gives the amount used of 
some resource (e.g., number of lines printed or displayed, number of records 
read or written, processor time, I/O units used, session elapsed time).
‚ñ†
‚ñ† Time-Stamp: The time stamp specifies the data and time of an action.
Most user operations are made up of a number of elementary actions. For 
example, a file copy involves the execution of the user command, which includes 
doing access validation and setting up the copy, plus the read from one file, plus the 
write to another file. Consider the command

COPY GAME.EXE TO <Libray>GAME.EXE
issued by Smith to copy an executable file GAME from the current directory to the 
<Library> directory. The following audit records may be generated:
Smith
execute
<Library>COPY.EXE
0
CPU = 00002
11058721678
Smith
read
<Smith>GAME.EXE
0
RECORDS = 0
11058721679
Smith
execute
<Library>COPY.EXE
write-viol
RECORDS = 0
11058721680
In this case, the copy is aborted because Smith does not have write permission to 
<Library>.
The decomposition of a user operation into elementary actions has three 
advantages:
1. Because objects are the protectable entities in a system, the use of elementary 
actions enables an audit of all behavior affecting an object. Thus, the system 
can detect attempted subversions of access controls (by noting an abnormal-
ity in the number of exception conditions returned) and can detect successful 
subversions by noting an abnormality in the set of objects accessible to the 
subject.
2. Single-object, single-action audit records simplify the model and the 
implementation.
3. Because of the simple, uniform structure of the detection-specific audit records, 
it may be relatively easy to obtain this information or at least part of it by a 
straightforward mapping from existing native audit records to the detection-
specific audit records.
Statistical Anomaly Detection
As was mentioned, statistical anomaly detection techniques fall into two broad 
 categories: threshold detection and profile-based systems. Threshold detection 
 involves counting the number of occurrences of a specific event type over an 
 interval of time. If the count surpasses what is considered a reasonable number that 
one might expect to occur, then intrusion is assumed.
Threshold analysis, by itself, is a crude and ineffective detector of even 
 moderately sophisticated attacks. Both the threshold and the time interval must 
be determined. Because of the variability across users, such thresholds are likely 
to generate either a lot of false positives or a lot of false negatives. However, 
simple threshold detectors may be useful in conjunction with more sophisticated 
techniques.
Profile-based anomaly detection focuses on characterizing the past  behavior of 
individual users or related groups of users and then detecting significant  deviations. 
A profile may consist of a set of parameters, so that deviation on just a single 
 parameter may not be sufficient in itself to signal an alert.

The foundation of this approach is an analysis of audit records. The audit 
 records provide input to the intrusion detection function in two ways. First, the 
 designer must decide on a number of quantitative metrics that can be used to mea-
sure user behavior. An analysis of audit records over a period of time can be used to 
determine the activity profile of the average user. Thus, the audit records serve to 
define typical behavior. Second, current audit records are the input used to detect 
intrusion. That is, the intrusion detection model analyzes incoming audit records to 
determine deviation from average behavior.
Examples of metrics that are useful for profile-based intrusion detection are 
the following:
‚ñ†
‚ñ† Counter: A nonnegative integer that may be incremented but not decremented 
until it is reset by management action. Typically, a count of certain event types 
is kept over a particular period of time. Examples include the number of log-
ins by a single user during an hour, the number of times a given command is 
executed during a single user session, and the number of password failures 
during a minute.
‚ñ†
‚ñ† Gauge: A nonnegative integer that may be incremented or decremented. 
Typically, a gauge is used to measure the current value of some entity. Examples 
include the number of logical connections assigned to a user application and 
the number of outgoing messages queued for a user process.
‚ñ†
‚ñ† Interval timer: The length of time between two related events. An example is 
the length of time between successive logins to an account.
‚ñ†
‚ñ† Resource utilization: Quantity of resources consumed during a specified 
 period. Examples include the number of pages printed during a user session 
and total time consumed by a program execution.
Given these general metrics, various tests can be performed to determine 
whether current activity fits within acceptable limits. [DENN87] lists the following 
approaches that may be taken:
‚ñ†
‚ñ† Mean and standard deviation
‚ñ†
‚ñ† Multivariate
‚ñ†
‚ñ† Markov process
‚ñ†
‚ñ† Time series
‚ñ†
‚ñ† Operational
The simplest statistical test is to measure the mean and standard deviation 
of a parameter over some historical period. This gives a reflection of the average 
behavior and its variability. The use of mean and standard deviation is applicable to 
a wide variety of counters, timers, and resource measures. But these measures, by 
themselves, are typically too crude for intrusion detection purposes.
The mean and standard deviation of a parameter are simple measures to 
 calculate. Taken over a given period of time, these values provide a measure aver-
age behavior and its variability. These two calculations can be applied to a  variety 
of counters, timers, and resource measures. However, these two measures are 
 inadequate, by themselves, for effective intrusion detection.

A multivariate calculation determines a correlate between two or more 
 variables. Intruder behavior may be characterized with greater confidence by con-
sidering such correlations (for example, processor time and resource usage, or login 
frequency and session elapsed time).
A Markov process estimates transition probabilities among various states. 
As an example, this model might be used to look at transitions between certain 
commands.
A time series model observes and calculates values based on a sequence of 
events over time. Such models can be used to detect a series of actions that happens 
to rapidly or too slowly. A variety of statistical tests can be applied to characterize 
abnormal timing.
An operational model can be used to characterize what is considered abnor-
mal, as opposed to performing an automated analysis of past audit records. 
Typically, fixed limits are defined and intrusion is suspected for an observation that 
is outside the limits. This type of approach works best where intruder behavior can 
be deduced from certain types of activities. For example, a large number of login 
attempts over a short period suggests an attempted intrusion.
As an example of the use of these various metrics and models, Table 11.1 
shows various measures considered or tested for the Stanford Research Institute 
(SRI) Intrusion Detection System (IDES) [ANDE95, JAVI91] and the follow-on 
program Emerald [NEUM99].
The main advantage of the use of statistical profiles is that a prior  knowledge 
of security flaws is not required. The detector program learns what is ‚Äúnormal‚Äù 
behavior and then looks for deviations. The approach is not based on system- 
dependent characteristics and vulnerabilities. Thus, it should be readily portable 
among a variety of systems.
Rule-Based Intrusion Detection
Rule-based techniques detect intrusion by observing events in the system and apply-
ing a set of rules that lead to a decision regarding whether a given pattern of activity 
is or is not suspicious. In very general terms, we can characterize all approaches as 
focusing on either anomaly detection or penetration identification, although there is 
some overlap in these approaches.
Rule-based anomaly detection is similar in terms of its approach and strengths 
to statistical anomaly detection. With the rule-based approach, historical audit 
records are analyzed to identify usage patterns and to automatically generate rules 
that describe those patterns. Rules may represent past behavior patterns of users, 
programs, privileges, time slots, terminals, and so on. Current behavior is then 
observed, and each transaction is matched against the set of rules to determine if it 
conforms to any historically observed pattern of behavior.
As with statistical anomaly detection, rule-based anomaly detection does not 
require knowledge of security vulnerabilities within the system. Rather, the scheme 
is based on observing past behavior and, in effect, assuming that the future will be 
like the past. In order for this approach to be effective, a rather large database of 
rules will be needed. For example, a scheme described in [VACC89] contains any-
where from 104 to 106 rules.

Rule-based penetration identification takes a very different approach to 
 intrusion detection. The key feature of such systems is the use of rules for identi-
fying known penetrations or penetrations that would exploit known weaknesses. 
Rules can also be defined that identify suspicious behavior, even when the behavior 
is within the bounds of established patterns of usage. Typically, the rules used in 
these systems are specific to the machine and operating system. The most fruitful 
approach to developing such rules is to analyze attack tools and scripts collected on 
the Internet. These rules can be supplemented with rules generated by knowledge-
able security personnel. In this latter case, the normal procedure is to interview 
system administrators and security analysts to collect a suite of known penetration 
scenarios and key events that threaten the security of the target system.
Measure
Model
Type of Intrusion Detected
Login and Session Activity
Login frequency by day and 
time
Mean and standard 
deviation
Intruders may be likely to log in during off-hours
Frequency of login at different 
locations
Mean and standard 
deviation
Intruders may log in from a location that a particu-
lar user rarely or never uses
Time since last login
Operational
Break in on a ‚Äúdead‚Äù account
Elapsed time per session
Mean and standard 
deviation
Significant deviations might indicate masquerader
Quantity of output to location
Mean and standard 
deviation
Excessive amounts of data transmitted to remote 
locations could signify leakage of sensitive data
Session resource utilization
Mean and standard 
deviation
Unusual processor or I/O levels could signal an 
intruder
Password failures at login
Operational
Attempted break-in by password guessing
Failures to login from specified 
terminals
Operational
Attempted break-in
Command or Program Execution Activity
Execution frequency
Mean and standard 
deviation
May detect intruders, who are likely to use differ-
ent commands, or a successful penetration by a 
legitimate user, who has gained access to privileged 
commands
Program resource utilization
Mean and standard 
deviation
An abnormal value might suggest injection of a 
virus or Trojan horse, which performs side-effects 
that increase I/O or processor utilization
Execution denials
Operational model
May detect penetration attempt by individual user 
who seeks higher privileges
File Access Activity
Read, write, create, delete 
frequency
Mean and standard 
deviation
Abnormalities for read and write access for individ-
ual users may signify masquerading or browsing
Records read, written
Mean and standard 
deviation
Abnormality could signify an attempt to obtain sen-
sitive data by inference and aggregation
Failure count for read, write, 
create, delete
Operational
May detect users who persistently attempt to access 
unauthorized files
Table 11.1 Measures That May Be Used for Intrusion Detection

A simple example of the type of rules that can be used is found in NIDX, an 
early system that used heuristic rules that can be used to assign degrees of suspicion 
to activities [BAUE88]. Example heuristics are the following:
1. Suspicious activity: A user accesses the personal directory of another user and 
attempts to read files in that directory.
2. Suspicious activity: A user accesses the personal directory of another user and 
attempts to write or create files in that directory.
3. Expected activity: A user logs in after hours and accesses the same file he or 
she accessed during business hours.
4. Suspicious activity: A user opens a disk devices directly rather than relying on 
higher-level operating system utilities.
5. Suspicious activity: A user is logged onto one system twice at the same time.
6. Suspicious activity: A user makes copies of system programs.
The penetration identification scheme used in IDES is representative of the 
strategy followed. Audit records are examined as they are generated, and they are 
matched against the rule base. If a match is found, then the user‚Äôs suspicion rating 
is increased. If enough rules are matched, then the rating will pass a threshold that 
results in the reporting of an anomaly.
The IDES approach is based on an examination of audit records. A weakness 
of this plan is its lack of flexibility. For a given penetration scenario, there may be 
a number of alternative audit record sequences that could be produced, each vary-
ing from the others slightly or in subtle ways. It may be difficult to pin down all 
these variations in explicit rules. Another method is to develop a higher-level model 
independent of specific audit records. An example of this is a state transition model 
known as USTAT [VIGN02, ILGU95]. USTAT deals in general actions rather than 
the detailed specific actions recorded by the UNIX auditing mechanism. USTAT 
is implemented on a SunOS system that provides audit records on 239 events. Of 
these, only 28 are used by a preprocessor, which maps these onto 10 general actions 
(Table 11.2). Using just these actions and the parameters that are invoked with each 
action, a state transition diagram is developed that characterizes suspicious activ-
ity. Because a number of different auditable events map into a smaller number of 
actions, the rule-creation process is simpler. Furthermore, the state transition dia-
gram model is easily modified to accommodate newly learned intrusion behaviors.
The Base-Rate Fallacy
To be of practical use, an intrusion detection system should detect a substantial 
percentage of intrusions while keeping the false alarm rate at an acceptable level. 
If only a modest percentage of actual intrusions are detected, the system provides a 
false sense of security. On the other hand, if the system frequently triggers an alert 
when there is no intrusion (a false alarm), then either system managers will begin to 
ignore the alarms or much time will be wasted analyzing the false alarms.
Unfortunately, because of the nature of the probabilities involved, it is very 
difficult to meet the standard of high rate of detections with a low rate of false 
alarms. In general, if the actual numbers of intrusions is low compared to the

number of legitimate uses of a system, then the false alarm rate will be high unless 
the test is extremely discriminating. This is an example of a phenomenon known as 
the base-rate fallacy. A study of existing intrusion detection systems, reported in 
[AXEL00], indicated that current systems have not overcome the problem of the 
base-rate  fallacy. See Appendix J for a brief background on the mathematics of 
this¬†problem.
Distributed Intrusion Detection
Traditionally, work on intrusion detection systems focused on single-system stand-
alone facilities. The typical organization, however, needs to defend a distributed 
collection of hosts supported by a LAN or internetwork. Although it is possible to 
mount a defense by using stand-alone intrusion detection systems on each host, a 
more effective defense can be achieved by coordination and cooperation among 
intrusion detection systems across the network.
Porras points out the following major issues in the design of a distributed 
intrusion detection system [PORR92]:
‚ñ†
‚ñ† A distributed intrusion detection system may need to deal with different audit 
record formats. In a heterogeneous environment, different systems will employ 
different native audit collection systems and, if using intrusion detection, may 
employ different formats for security-related audit records.
‚ñ†
‚ñ† One or more nodes in the network will serve as collection and analysis points 
for the data from the systems on the network. Thus, either raw audit data or 
summary data must be transmitted across the network. Therefore, there is a 
requirement to assure the integrity and confidentiality of these data. Integrity 
is required to prevent an intruder from masking his or her activities by alter-
ing the transmitted audit information. Confidentiality is required because the 
transmitted audit information could be valuable.
USTAT Action
SunOS Event Type
Read
open_r, open_rc, open_rtc, open_rwc, open_rwtc, open_rt, open_rw, 
open_rwt
Write
truncate, ftruncate, creat, open_rtc, open_rwc, open_rwtc, open_rt, 
open_rw, open_rwt, open_w, open_wt, open_wc, open_wct
Create
mkdir, creat, open_rc, open_rtc, open_rwc, open_rwtc, open_wc, 
open_wtc, mknod
Delete
rmdir, unlink
Execute
exec, execve
Exit
exit
Modify_Owner
chown, fchown
Modify_Perm
chmod, fchmod
Rename
rename
Hardlink
link
Table 11.2 USTAT Actions Versus SunOS Event Types

‚ñ†
‚ñ† Either a centralized or decentralized architecture can be used. With a central-
ized architecture, there is a single central point of collection and analysis of 
all audit data. This eases the task of correlating incoming reports but creates a 
potential bottleneck and single point of failure. With a decentralized architec-
ture, there are more than one analysis centers, but these must coordinate their 
activities and exchange information.
A good example of a distributed intrusion detection system is one developed 
at the University of California at Davis [HEBE92, SNAP91]. A similar approach 
has been taken for a project at Purdue [SPAF00, BALA98]. Figure 11.2 shows the 
overall architecture, which consists of three main components:
‚ñ†
‚ñ† Host agent module: An audit collection module operating as a background 
process on a monitored system. Its purpose is to collect data on security-
related events on the host and transmit these to the central manager.
‚ñ†
‚ñ† LAN monitor agent module: Operates in the same fashion as a host agent 
module except that it analyzes LAN traffic and reports the results to the cen-
tral manager.
‚ñ†
‚ñ† Central manager module: Receives reports from LAN monitor and host 
agents, and processes and correlates these reports to detect intrusion.
The scheme is designed to be independent of any operating system or system 
auditing implementation. Figure 11.3 shows the general approach that is taken. The 
agent captures each audit record produced by the native audit collection system. 
A¬† filter is applied that retains only those records that are of security interest. These 
records are then reformatted into a standardized format referred to as the host audit 
Figure 11.2 Architecture for Distributed Intrusion Detection
Central manager
LAN monitor
Host
Host
Agent
module
Router
WAN
Manager
module

record (HAR). Next, a template-driven logic module analyzes the records for suspi-
cious activity. At the lowest level, the agent scans for notable events that are of interest 
independent of any past events. Examples include failed file accesses, accessing sys-
tem files, and changing a file‚Äôs access control. At the next higher level, the agent looks 
for sequences of events, such as known attack patterns (signatures). Finally, the agent 
looks for anomalous behavior of an individual user based on a historical profile of that 
user, such as number of programs executed, number of files accessed, and the like.
When suspicious activity is detected, an alert is sent to the central manager. 
The central manager includes an expert system that can draw inferences from 
received data. The manager may also query individual systems for copies of HARs 
to correlate with those from other agents.
The LAN monitor agent also supplies information to the central manager. 
The LAN monitor agent audits host-host connections, services used, and volume of 
traffic. It searches for significant events, such as sudden changes in network load, 
the use of security-related services, and network activities such as rlogin.
The architecture depicted in Figures 11.2 and 11.3 is quite general and flexible. 
It offers a foundation for a machine-independent approach that can expand from 
stand-alone intrusion detection to a system that is able to correlate activity from 
a number of sites and networks to detect suspicious activity that would otherwise 
remain undetected.
Honeypots
A relatively recent innovation in intrusion detection technology is the honeypot. 
Honeypots are decoy systems that are designed to lure a potential attacker away 
from critical systems. Honeypots are designed to
‚ñ†
‚ñ† divert an attacker from accessing critical systems
‚ñ†
‚ñ† collect information about the attacker‚Äôs activity
‚ñ†
‚ñ† encourage the attacker to stay on the system long enough for administrators 
to respond
Figure 11.3 Agent Architecture
OS audit
information
Alerts
Modifcations
Query/
response
Notable
activity;
Signatures;
Noteworthy
sessions
Host audit record (HAR)
Filter for
security
interest
Reformat
function
OS audit
function
Analysis
module
Templates
Central
manager
Logic
module

These systems are filled with fabricated information designed to appear valu-
able but that a legitimate user of the system wouldn‚Äôt access. Thus, any access to the 
honeypot is suspect. The system is instrumented with sensitive monitors and event 
loggers that detect these accesses and collect information about the attacker‚Äôs activ-
ities. Because any attack against the honeypot is made to seem successful, adminis-
trators have time to mobilize and log and track the attacker without ever exposing 
productive systems.
The honeypot is a resource that has no production value. There is no legiti-
mate reason for anyone outside the network to interact with a honeypot. Thus, any 
attempt to communicate with the system is most likely a probe, scan, or attack. 
Conversely, if a honeypot initiates outbound communication, the system has prob-
ably been compromised.
Initial efforts involved a single honeypot computer with IP addresses designed 
to attract hackers. More recent research has focused on building entire honeypot 
networks that emulate an enterprise, possibly with actual or simulated traffic and 
data. Once hackers are within the network, administrators can observe their behav-
ior in detail and figure out defenses.
Honeypots can be deployed in a variety of locations. Figure 11.4 illustrates 
some possibilities. The location depends on a number of factors, such as the type 
of information the organization is interested in gathering and the level of risk that 
organizations can tolerate to obtain the maximum amount of data.
A honeypot outside the external firewall (location 1) is useful for tracking 
attempts to connect to unused IP addresses within the scope of the network. A hon-
eypot at this location does not increase the risk for the internal network. The danger 
of having a compromised system behind the firewall is avoided. Further, because 
the honeypot attracts many potential attacks, it reduces the alerts issued by the fire-
wall and by internal IDS sensors, easing the management burden. The disadvantage 
of an external honeypot is that it has little or no ability to trap internal attackers, 
especially if the external firewall filters traffic in both directions.
The network of externally available services, such as Web and mail, often 
called the DMZ (demilitarized zone), is another candidate for locating a honeypot 
(location 2). The security administrator must assure that the other systems in the 
DMZ are secure against any activity generated by the honeypot. A disadvantage of 
this location is that a typical DMZ is not fully accessible, and the firewall typically 
blocks traffic to the DMZ that attempts to access unneeded services. Thus, the fire-
wall either has to open up the traffic beyond what is permissible, which is risky, or 
limit the effectiveness of the honeypot.
A fully internal honeypot (location 3) has several advantages. Its most impor-
tant advantage is that it can catch internal attacks. A honeypot at this location can 
also detect a misconfigured firewall that forwards impermissible traffic from the 
Internet to the internal network. There are several disadvantages. The most seri-
ous of these is if the honeypot is compromised so that it can attack other internal 
systems. Any further traffic from the Internet to the attacker is not blocked by the 
firewall because it is regarded as traffic to the honeypot only. Another difficulty for 
this honeypot location is that, as with location 2, the firewall must adjust its filtering 
to allow traffic to the honeypot, thus complicating firewall configuration and poten-
tially compromising the internal network.

Intrusion Detection Exchange Format
To facilitate the development of distributed intrusion detection systems that can 
function across a wide range of platforms and environments, standards are needed 
to support interoperability. Such standards are the focus of the IETF Intrusion 
Detection Working Group. The purpose of the working group is to define data 
formats and exchange procedures for sharing information of interest to intrusion 
detection and response systems and to management systems that may need to inter-
act with them.
The working group issued the following RFCs in 2007:
‚ñ†
‚ñ† Intrusion Detection Message Exchange Requirements (RFC 4766): This docu-
ment defines requirements for the Intrusion Detection Message Exchange 
Format (IDMEF). The document also specifies requirements for a communi-
cation protocol for communicating IDMEF.
Figure 11.4 Example of Honeypot Deployment
Internet
External
frewall
Honeypot
Honeypot
Honeypot
LAN switch
or router
LAN switch
or router
Internal
network
Service network
(Web, Mail, DNS, etc.)
2
 1
3

‚ñ†
‚ñ† The Intrusion Detection Message Exchange Format (RFC 4765): This docu-
ment describes a data model to represent information exported by intrusion 
detection systems and explains the rationale for using this model. An imple-
mentation of the data model in the Extensible Markup Language (XML) is 
presented, an XML Document Type Definition is developed, and examples 
are provided.
‚ñ†
‚ñ† The Intrusion Detection Exchange Protocol (RFC 4767): This document 
describes the Intrusion Detection Exchange Protocol (IDXP), an application-
level protocol for exchanging data between intrusion detection entities. IDXP 
supports mutual authentication, integrity, and confidentiality over a connec-
tion-oriented protocol.
Figure 11.5 illustrates the key elements of the model on which the intrusion 
detection message exchange approach is based. This model does not correspond 
to any particular product or implementation, but its functional components are the 
key elements of any IDS. The functional components are as follows:
Figure 11.5 Model for Intrusion Detection Message Exchange
Response
Activity
Event
Event
Alert
Notifcation
Operator
Administrator
Security
policy
Security
policy

‚ñ†
‚ñ† Data source: The raw data that an IDS uses to detect unauthorized or 
 undesired activity. Common data sources include network packets, operat-
ing system audit logs, application audit logs, and system-generated check-
sum data.
‚ñ†
‚ñ† Sensor: Collects data from the data source. The sensor forwards events to the 
analyzer.
‚ñ†
‚ñ† Analyzer: The ID component or process that analyzes the data collected by 
the sensor for signs of unauthorized or undesired activity or for events that 
might be of interest to the security administrator. In many existing IDSs, the 
sensor and the analyzer are part of the same component.
‚ñ†
‚ñ† Administrator: The human with overall responsibility for setting the security 
policy of the organization, and, thus, for decisions about deploying and config-
uring the IDS. This may or may not be the same person as the operator of the 
IDS. In some organizations, the administrator is associated with the network 
or systems administration groups. In other organizations, it‚Äôs an independent 
position.
‚ñ†
‚ñ† Manager: The ID component or process from which the operator manages the 
various components of the ID system. Management functions typically include 
sensor configuration, analyzer configuration, event notification management, 
data consolidation, and reporting.
‚ñ†
‚ñ† Operator: The human that is the primary user of the IDS manager. The opera-
tor often monitors the output of the IDS and initiates or recommends further 
action.
In this model, intrusion detection proceeds in the following manner. The sensor 
monitors data sources looking for suspicious activity, such as network sessions 
showing unexpected telnet activity, operating system log file entries showing a user 
attempting to access files to which he or she is not authorized to have access, and 
application log files showing persistent login failures. The sensor communicates sus-
picious activity to the analyzer as an event, which characterizes an activity within a 
given period of time. If the analyzer determines that the event is of interest, it sends 
an alert to the manager component that contains information about the unusual 
activity that was detected, as well as the specifics of the occurrence. The manager 
component issues a notification to the human operator. A response can be initiated 
automatically by the manager component or by the human operator. Examples of 
responses include logging the activity; recording the raw data (from the data source) 
that characterized the event; terminating a network, user, or application session; or 
altering network or system access controls. The security policy is the predefined, 
formally documented statement that defines what activities are allowed to take 
place on an organization‚Äôs network or on particular hosts to support the organiza-
tion‚Äôs requirements. This includes, but is not limited to, which hosts are to be denied 
external network access.
The specification defines formats for event and alert messages, message types, 
and exchange protocols for communication of intrusion detection information.

11.3 PasswOrd ManageMent
The front line of defense against intruders is the password system. Virtually all mul-
tiuser systems require that a user provide not only a name or identifier (ID) but also 
a password. The password serves to authenticate the ID of the individual logging on 
to the system. In turn, the ID provides security in the following ways:
‚ñ†
‚ñ† The ID determines whether the user is authorized to gain access to a system. 
In some systems, only those who already have an ID filed on the system are 
allowed to gain access.
‚ñ†
‚ñ† The ID determines the privileges accorded to the user. A few users may have 
supervisory or ‚Äúsuperuser‚Äù status that enables them to read files and perform 
functions that are especially protected by the operating system. Some systems 
have guest or anonymous accounts, and users of these accounts have more 
limited privileges than others.
‚ñ†
‚ñ† The ID is used in what is referred to as discretionary access control. 
For¬† example, by listing the IDs of the other users, a user may grant permission 
to them to read files owned by that user.
The Vulnerability of Passwords
In this subsection, we outline the main forms of attack against password-based 
authentication and briefly outline a countermeasure strategy. The remainder of 
Section 11.3 goes into more detail on the key countermeasures.
Typically, a system that uses password-based authentication maintains a 
password file indexed by user ID. One technique that is typically used is to store 
not the user‚Äôs password but a one-way hash function of the password, as described 
subsequently.
We can identify the following attack strategies and countermeasures:
‚ñ†
‚ñ† Offline dictionary attack: Typically, strong access controls are used to protect 
the system‚Äôs password file. However, experience shows that determined hack-
ers can frequently bypass such controls and gain access to the file. The attacker 
obtains the system password file and compares the password hashes against 
hashes of commonly used passwords. If a match is found, the attacker can gain 
access by that ID/password combination. Countermeasures include controls to 
prevent unauthorized access to the password file, intrusion detection measures 
to identify a compromise, and rapid reissuance of passwords should the pass-
word file be compromised.
‚ñ†
‚ñ† Specific account attack: The attacker targets a specific account and submits 
password guesses until the correct password is discovered. The standard coun-
termeasure is an account lockout mechanism, which locks out access to the 
account after a number of failed login attempts. Typical practice is no more 
than five access attempts.
‚ñ†
‚ñ† Popular password attack: A variation of the preceding attack is to use a popu-
lar password and try it against a wide range of user IDs. A user‚Äôs tendency is 
to choose a password that is easily remembered; this unfortunately makes the

password easy to guess. Countermeasures include policies to inhibit the selec-
tion by users of common passwords and scanning the IP addresses of authenti-
cation requests and client cookies for submission patterns.
‚ñ†
‚ñ† Password guessing against single user: The attacker attempts to gain knowl-
edge about the account holder and system password policies and uses that 
knowledge to guess the password. Countermeasures include training in and 
enforcement of password policies that make passwords difficult to guess. 
Such policies address the secrecy, minimum length of the password, character 
set, prohibition against using well-known user identifiers, and length of time 
before the password must be changed.
‚ñ†
‚ñ† Workstation hijacking: The attacker waits until a logged-in workstation is 
unattended. The standard countermeasure is automatically logging the work-
station out after a period of inactivity. Intrusion detection schemes can be used 
to detect changes in user behavior.
‚ñ†
‚ñ† Exploiting user mistakes: If the system assigns a password, then the user is 
more likely to write it down because it is difficult to remember. This situation 
creates the potential for an adversary to read the written password. A user 
may intentionally share a password, to enable a colleague to share files, for 
example. Also, attackers are frequently successful in obtaining passwords by 
using social engineering tactics that trick the user or an account manager into 
revealing a password. Many computer systems are shipped with preconfigured 
passwords for system administrators. Unless these preconfigured passwords 
are changed, they are easily guessed. Countermeasures include user training, 
intrusion detection, and simpler passwords combined with another authentica-
tion mechanism.
‚ñ†
‚ñ† Exploiting multiple password use: Attacks can also become much more effec-
tive or damaging if different network devices share the same or a similar pass-
word for a given user. Countermeasures include a policy that forbids the same 
or similar password on particular network devices.
‚ñ†
‚ñ† Electronic monitoring: If a password is communicated across a network to log 
on to a remote system, it is vulnerable to eavesdropping. Simple encryption 
will not fix this problem, because the encrypted password is, in effect, the pass-
word and can be observed and reused by an adversary.
The Use of Hashed Passwords
A widely used password security technique is the use of hashed passwords and a salt 
value. This scheme is found on virtually all UNIX variants as well as on a number 
of other operating systems. The following procedure is employed (Figure 11.6a). 
To¬†load a new password into the system, the user selects or is assigned a password. 
This password is combined with a fixed-length salt value [MORR79]. In older 
 implementations, this value is related to the time at which the password is assigned 
to the user. Newer implementations use a pseudorandom or random number. The 
password and salt serve as inputs to a hashing algorithm to produce a fixed-length 
hash code. The hash algorithm is designed to be slow to execute to thwart attacks. 
The hashed password is then stored, together with a plaintext copy of the salt, in

the password file for the corresponding user ID. The hashed-password method has 
been shown to be secure against a variety of cryptanalytic attacks [WAGN00].
When a user attempts to log on to a UNIX system, the user provides an ID 
and a password (Figure 11.6b). The operating system uses the ID to index into the 
password file and retrieve the plaintext salt and the encrypted password. The salt 
and user-supplied password are used as input to the encryption routine. If the result 
matches the stored value, the password is accepted.
Figure 11.6 UNIX Password Scheme
User ID
Salt
Password
Load
Select
(a) Loading a new password
(b) Verifying a password
Salt
‚Ä¢
‚Ä¢
‚Ä¢
Password fle
Hash code
User ID
User ID
Salt
Password fle
Slow hash
function
Salt
Hashed password
Password
Slow hash
function
Compare
Hash code

The salt serves three purposes:
‚ñ†
‚ñ† It prevents duplicate passwords from being visible in the password file. Even if 
two users choose the same password, those passwords will be assigned differ-
ent salt values. Hence, the hashed passwords of the two users will differ.
‚ñ†
‚ñ† It greatly increases the difficulty of offline dictionary attacks. For a salt of 
length b bits, the number of possible passwords is increased by a factor of 2b, 
increasing the difficulty of guessing a password in a dictionary attack.
‚ñ†
‚ñ† It becomes nearly impossible to find out whether a person with passwords on 
two or more systems has used the same password on all of them.
To see the second point, consider the way that an offline dictionary attack 
would work. The attacker obtains a copy of the password file. Suppose first that the 
salt is not used. The attacker‚Äôs goal is to guess a single password. To that end, the 
attacker submits a large number of likely passwords to the hashing function. If any 
of the guesses matches one of the hashes in the file, then the attacker has found a 
password that is in the file. But faced with the UNIX scheme, the attacker must take 
each guess and submit it to the hash function once for each salt value in the diction-
ary file, multiplying the number of guesses that must be checked.
There are two threats to the UNIX password scheme. First, a user can gain 
access on a machine using a guest account or by some other means and then run 
a password guessing program, called a password cracker, on that machine. The 
attacker should be able to check many thousands of possible passwords with little 
resource consumption. In addition, if an opponent is able to obtain a copy of the 
password file, then a cracker program can be run on another machine at leisure. 
This enables the opponent to run through millions of possible passwords in a rea-
sonable period.
UniX implementations Since the original development of UNIX, most implemen-
tations have relied on the following password scheme. Each user selects a password 
of up to eight printable characters in length. This is converted into a 56-bit value 
(using 7-bit ASCII) that serves as the key input to an encryption routine. The hash 
routine, known as crypt(3), is based on DES. A 12-bit salt value is used. The modi-
fied DES algorithm is executed with a data input consisting of a 64-bit block of 
zeros. The output of the algorithm then serves as input for a second encryption. 
This process is repeated for a total of 25 encryptions. The resulting 64-bit output is 
then translated into an 11-character sequence. The modification of the DES algo-
rithm converts it into a one-way hash function. The crypt(3) routine is designed to 
discourage guessing attacks. Software implementations of DES are slow compared 
to hardware versions, and the use of 25 iterations multiplies the time required by 25.
This particular implementation is now considered woefully inadequate. For 
example, [PERR03] reports the results of a dictionary attack using a supercom-
puter. The attack was able to process over 50 million password guesses in about 80 
minutes. Further, the results showed that for about $10,000 anyone should be able 
to do the same in a few months using one uniprocessor machine. Despite its known 
weaknesses, this UNIX scheme is still often required for compatibility with existing 
account management software or in multivendor environments.

There are other, much stronger, hash/salt schemes available for UNIX. 
The¬†recommended hash function for many UNIX systems, including Linux, Solaris, 
and FreeBSD (a widely used open source UNIX implementation), is based on the 
MD5 secure hash algorithm (which is similar to, but not as secure as SHA-1). The 
MD5 crypt routine uses a salt of up to 48 bits and effectively has no limitations on 
password length. It produces a 128-bit hash value. It is also far slower than crypt(3). 
To achieve the slowdown, MD5 crypt uses an inner loop with 1000 iterations.
Probably the most secure version of the UNIX hash/salt scheme was devel-
oped for OpenBSD, another widely used open source UNIX. This scheme, reported 
in [PROV99], uses a hash function based on the Blowfish symmetric block cipher. 
The hash function, called Bcrypt, is quite slow to execute. Bcrypt allows passwords 
of up to 55 characters in length and requires a random salt value of 128 bits, to 
produce a 192-bit hash value. Bcrypt also includes a cost variable; an increase in 
the cost variable causes a corresponding increase in the time required to perform a 
Bcyrpt hash. The cost assigned to a new password is configurable, so that adminis-
trators can assign a higher cost to privileged users.
password cracking approacHes The traditional approach to password guess-
ing, or password cracking as it is called, is to develop a large dictionary of pos-
sible passwords and to try each of these against the password file. This means that 
each password must be hashed using each available salt value and then compared 
to stored hash values. If no match is found, then the cracking program tries varia-
tions on all the words in its dictionary of likely passwords. Such variations include 
backwards spelling of words, additional numbers or special characters, or sequence 
of characters,
An alternative is to trade off space for time by precomputing potential hash 
values. In this approach the attacker generates a large dictionary of possible pass-
words. For each password, the attacker generates the hash values associated with 
each possible salt value. The result is a mammoth table of hash values known as a 
rainbow table. For example, [OECH03] showed that using 1.4 GB of data, he could 
crack 99.9% of all alphanumeric Windows password hashes in 13.8 seconds. This 
approach can be countered by using a sufficiently large salt value and a sufficiently 
large hash length. Both the FreeBSD and OpenBSD approaches should be secure 
from this attack for the foreseeable future.
User Password Choices
Even the stupendous guessing rates referenced in the preceding section do not 
yet make it feasible for an attacker to use a dumb brute-force technique of trying 
all possible combinations of characters to discover a password. Instead, password 
crackers rely on the fact that some people choose easily guessable passwords.
Some users, when permitted to choose their own password, pick one that is 
absurdly short. One study at Purdue University [SPAF92a] observed password 
change choices on 54 machines, representing approximately 7000 user accounts. 
Almost 3% of the passwords were three characters or fewer in length. An attacker 
could begin the attack by exhaustively testing all possible passwords of length 3 or

fewer. A simple remedy is for the system to reject any password choice of fewer 
than, say, six characters or even to require that all passwords be exactly eight char-
acters in length. Most users would not complain about such a restriction.
Password length is only part of the problem. Many people, when permitted 
to choose their own password, pick a password that is guessable, such as their own 
name, their street name, a common dictionary word, and so forth. This makes the 
job of password cracking straightforward. The cracker simply has to test the pass-
word file against lists of likely passwords. Because many people use guessable pass-
words, such a strategy should succeed on virtually all systems.
One demonstration of the effectiveness of guessing is reported in [KLEI90]. 
From a variety of sources, the author collected UNIX password files, containing 
nearly 14,000 encrypted passwords. The result, which the author rightly character-
izes as frightening, is shown in Table 11.3. In all, nearly one-fourth of the passwords 
were guessed. The following strategy was used:
1. Try the user‚Äôs name, initials, account name, and other relevant personal infor-
mation. In all, 130 different permutations for each user were tried.
2. Try words from various dictionaries. The author compiled a dictionary of over 
60,000 words, including the online dictionary on the system itself, and various 
other lists as shown.
3. Try various permutations on the words from step 2. This included making the 
first letter uppercase or a control character, making the entire word uppercase, 
reversing the word, changing the letter ‚Äúo‚Äù to the digit ‚Äúzero,‚Äù and so on. These 
permutations added another 1 million words to the list.
4. Try various capitalization permutations on the words from step 2 that were not 
considered in step 3. This added almost 2 million additional words to the list.
Thus, the test involved in the neighborhood of 3 million words. Using the fastest 
Thinking Machines implementation listed earlier, the time to encrypt all these 
words for all possible salt values is under an hour. Keep in mind that such a thor-
ough search could produce a success rate of about 25%, whereas even a single hit 
may be enough to gain a wide range of privileges on a system.
access control One way to thwart a password attack is to deny the opponent 
access to the password file. If the encrypted password portion of the file is acces-
sible only by a privileged user, then the opponent cannot read it without already 
knowing the password of a privileged user. [SPAF92a] points out several flaws in 
this strategy:
‚ñ†
‚ñ† Many systems, including most UNIX systems, are susceptible to unanticipated 
break-ins. Once an attacker has gained access by some means, he or she may 
wish to obtain a collection of passwords in order to use different accounts for 
different logon sessions to decrease the risk of detection. Or a user with an 
account may desire another user‚Äôs account to access privileged data or to sabo-
tage the system.
‚ñ†
‚ñ† An accident of protection might render the password file readable, thus 
 compromising all the accounts.

Type of Password
Search Size
Number of 
Matches
Percentage of Passwords 
Matched
Cost/Benefit 
Ratioa
User/account name
130
368
2.7%
2.830
Character sequences
866
22
0.2%
0.025
Numbers
427
9
0.1%
0.021
Chinese
392
56
0.4%
0.143
Place names
628
82
0.6%
0.131
Common names
2239
548
4.0%
0.245
Female names
4280
161
1.2%
0.038
Male names
2866
140
1.0%
0.049
Uncommon names
4955
130
0.9%
0.026
Myths & legends
1246
66
0.5%
0.053
Shakespearean
473
11
0.1%
0.023
Sports terms
238
32
0.2%
0.134
Science fiction
691
59
0.4%
0.085
Movies and actors
99
12
0.1%
0.121
Cartoons
92
9
0.1%
0.098
Famous people
290
55
0.4%
0.190
Phrases and patterns
933
253
1.8%
0.271
Surnames
33
9
0.1%
0.273
Biology
58
1
0.0%
0.017
System dictionary
19683
1027
7.4%
0.052
Machine names
9018
132
1.0%
0.015
Mnemonics
14
2
0.0%
0.143
King James bible
7525
83
0.6%
0.011
Miscellaneous words
3212
54
0.4%
0.017
Yiddish words
56
0
0.0%
0.000
Asteroids
2407
19
0.1%
0.007
Total
62727
3340
24.2%
0.053
aComputed as the number of matches divided by the search size. The more words that needed to be tested for 
a match, the lower the cost/benefit ratio.
Table 11.3 Passwords Cracked from a Sample Set of 13,797 Accounts [KLEI90]
‚ñ†
‚ñ† Some of the users have accounts on other machines in other protection 
domains, and they use the same password. Thus, if the passwords could be 
read by anyone on one machine, a machine in another location might be 
compromised.
Thus, a more effective strategy would be to force users to select passwords that are 
difficult to guess.

Password Selection Strategies
The lesson from the two experiments just described ([SPAF92a], [KLEI90]) is that, 
left to their own devices, many users choose a password that is too short or too easy 
to guess. At the other extreme, if users are assigned passwords consisting of eight 
randomly selected printable characters, password cracking is effectively  impossible. 
But it would be almost as impossible for most users to remember their passwords. 
Fortunately, even if we limit the password universe to strings of characters that are 
reasonably memorable, the size of the universe is still too large to permit practical 
cracking. Our goal, then, is to eliminate guessable passwords while allowing the 
user to select a password that is memorable. Four basic techniques are¬†in use:
‚ñ†
‚ñ† User education
‚ñ†
‚ñ† Computer-generated passwords
‚ñ†
‚ñ† Reactive password checking
‚ñ†
‚ñ† Proactive password checking
Users can be told the importance of using hard-to-guess passwords and can be 
provided with guidelines for selecting strong passwords. This user education strat-
egy is unlikely to succeed at most installations, particularly where there is a large 
user population or a lot of turnover. Many users will simply ignore the guidelines. 
Others may not be good judges of what is a strong password. For example, many 
users (mistakenly) believe that reversing a word or capitalizing the last letter makes 
a password unguessable.
Computer-generated passwords also have problems. If the passwords are quite 
random in nature, users will not be able to remember them. Even if the password 
is pronounceable, the user may have difficulty remembering it and so be tempted 
to write it down. In general, computer-generated password schemes have a history 
of poor acceptance by users. FIPS PUB 181 defines one of the best-designed auto-
mated password generators. The standard includes not only a description of the 
approach but also a complete listing of the C source code of the algorithm. The 
algorithm generates words by forming pronounceable syllables and concatenating 
them to form a word. A random number generator produces a random stream of 
characters used to construct the syllables and words.
A reactive password checking strategy is one in which the system periodically 
runs its own password cracker to find guessable passwords. The system cancels any 
passwords that are guessed and notifies the user. This tactic has a number of draw-
backs. First, it is resource intensive if the job is done right. Because a determined 
opponent who is able to steal a password file can devote full CPU time to the task 
for hours or even days, an effective reactive password checker is at a distinct disad-
vantage. Furthermore, any existing passwords remain vulnerable until the reactive 
password checker finds them.
The most promising approach to improved password security is a proactive 
password checker. In this scheme, a user is allowed to select his or her own pass-
word. However, at the time of selection, the system checks to see if the password is 
allowable and, if not, rejects it. Such checkers are based on the philosophy that, with 
sufficient guidance from the system, users can select memorable passwords from a 
fairly large password space that are not likely to be guessed in a dictionary attack.

The trick with a proactive password checker is to strike a balance between 
user acceptability and strength. If the system rejects too many passwords, users will 
complain that it is too hard to select a password. If the system uses some simple 
algorithm to define what is acceptable, this provides guidance to password crackers 
to refine their guessing technique. In the remainder of this subsection, we look at 
possible approaches to proactive password checking.
The first approach is a simple system for rule enforcement. For example, the 
following rules could be enforced:
‚ñ†
‚ñ† All passwords must be at least eight characters long.
‚ñ†
‚ñ† In the first eight characters, the passwords must include at least one each of 
uppercase, lowercase, numeric digits, and punctuation marks.
These rules could be coupled with advice to the user. Although this approach is 
superior to simply educating users, it may not be sufficient to thwart password 
crackers. This scheme alerts crackers as to which passwords not to try but may still 
make it possible to do password cracking.
Another possible procedure is simply to compile a large dictionary of pos-
sible ‚Äúbad‚Äù passwords. When a user selects a password, the system checks to 
make sure that it is not on the disapproved list. There are two problems with this 
approach:
‚ñ†
‚ñ† Space: The dictionary must be very large to be effective. For example, the dic-
tionary used in the Purdue study [SPAF92a] occupies more than 30 megabytes 
of storage.
‚ñ†
‚ñ† Time: The time required to search a large dictionary may itself be large. In 
addition, to check for likely permutations of dictionary words, either those 
words most be included in the dictionary, making it truly huge, or each search 
must also involve considerable processing.
Bloom Filter
A technique [SPAF92a, SPAF92b] for developing an effective and efficient  proactive 
password checker that is based on rejecting words on a list has been implemented 
on a number of systems, including Linux. It is based on the use of a Bloom filter 
[BLOO70]. To begin, we explain the operation of the Bloom  filter. A Bloom filter of 
order k consists of a set of k independent hash functions H1(x), H2(x), c , Hk(x), 
where each function maps a password into a hash value in the range 0 to N - 1. 
That is,
Hi(Xj) = y  1 ‚Ä¶ i ‚Ä¶ k;  1 ‚Ä¶ j ‚Ä¶ D;  0 ‚Ä¶ y ‚Ä¶ N - 1
where
Xj = jth word in password dictionary
D = number of words in password dictionary
The following procedure is then applied to the dictionary:
1. A hash table of N bits is defined, with all bits initially set to 0.

2. For each password, its k hash values are calculated, and the corresponding bits 
in the hash table are set to 1. Thus, if Hi(Xj) = 67 for some (i, j), then the 
sixty-seventh bit of the hash table is set to 1; if the bit already has the value 1, 
it¬†remains at 1.
When a new password is presented to the checker, its k hash values are 
 calculated. If all the corresponding bits of the hash table are equal to 1, then the 
password is rejected. All passwords in the dictionary will be rejected. But there will 
also be some ‚Äúfalse positives‚Äù (i.e., passwords that are not in the dictionary but 
that produce a match in the hash table). To see this, consider a scheme with two 
hash functions. Suppose that the passwords undertaker and hulkhogan are in the 
 dictionary, but xG%#jj98 is not. Further suppose that
 H1(undertaker) = 25    H1(hulkhogan) = 83    H1(xG%#jj98) = 665
 H2(undertaker) = 998  H2(hulkhogan) = 665  H2(xG%#jj98) = 998
If the password xG%#jj98 is presented to the system, it will be rejected even 
though it is not in the dictionary. If there are too many such false positives, it will be 
difficult for users to select passwords. Therefore, we would like to design the hash 
scheme to minimize false positives. It can be shown that the probability of a false 
positive can be approximated by:
P ‚âà (1 - ekD/N)k = (1 - ek/R)k
or, equivalently,
R ‚âà
-k
ln(1 - P1/k)
where
k = number of hash functions
N = number of bits in hash table
D = number of words in dictionary
R = N/D, ratio of hash table size (bits) to dictionary size (words)
Figure 11.7 plots P as a function of R for various values of k. Suppose we have 
a dictionary of 1 million words and we wish to have a 0.01 probability of rejecting a 
password not in the dictionary. If we choose six hash functions, the required ratio is 
R = 9.6. Therefore, we need a hash table of 9.6 * 106 bits or about 1.2 MBytes of 
storage. In contrast, storage of the entire dictionary would require on the order of 8 
MBytes. Thus, we achieve a compression of almost a factor of 7. Furthermore, pass-
word checking involves the straightforward calculation of six hash functions and is 
independent of the size of the dictionary, whereas with the use of the full dictionary, 
there is substantial searching.1
1The Bloom filter involves the use of probabilistic techniques. There is a small probability that some 
passwords not in the dictionary will be rejected. It is often the case in designing algorithms that the use of 
probabilistic techniques results in a less time-consuming or less complex solution, or both.

Review Questions 
 
11.1 
List and briefly define three classes of intruders.
 
11.2 
Give examples of intrusion.
 
11.3 
List the direct approaches that can be implemented to counter insider attacks.
 
11.4 
Explain how statistical anomaly detection and rule-based intrusion detection are 
used to detect different types of intruders.
 
11.5 
List the tests that can be performed to determine if a user‚Äôs current activity is statisti-
cally anomalous or whether it is within acceptable parameters.
 
11.6 
What is the base- rate fallacy?
Figure 11.7 Performance of Bloom Filter
0.001
0.01
0.1
1
Pr [false positive]
20
15
10
5
0
Ratio of hash table size (bits) to dictionary size (words) 
4 hash functions
2 hash functions
6 hash functions
 11.4 Key terMs, revIew QuestIOns, and PrObLeMs
Key Terms 
audit record
base-rate fallacy
bloom filter
distributed intrusion detection
honeypot
intruder
intrusion detection
intrusion detection exchange 
format
password
rainbow table
rule-based intrusion detection
salt value
signature detection
statistical anomaly detection

Conservativeness
of signatures
Frequency
of alerts
More specifc
or stricter
Less specifc
or looser
 
11.7 
List the possible locations where a honeypot can be deployed.
 
11.8 
Briefly explain the purposes that a salt serves in the context of UNIX password 
management.
 
11.9 
Discuss the threats to the UNIX password scheme.
Problems 
 
11.1 
In the context of an IDS, we define a false positive to be an alarm generated by an 
IDS in which the IDS alerts to a condition that is actually benign. A false negative 
occurs when an IDS fails to generate an alarm when an alert-worthy condition is 
in effect. Using the following diagram, depict two curves that roughly indicate false 
positives and false negatives, respectively.
 
11.2 
The overlapping area of the two probability density functions of Figure 11.1 repre-
sents the region in which there is the potential for false positives and false negatives. 
Further, Figure 11.1 is an idealized and not necessarily representative depiction of 
the relative shapes of the two density functions. Suppose there is 1 actual intrusion 
for every 1000 authorized users, and the overlapping area covers 1% of the autho-
rized users and 50% of the intruders.
a. Sketch such a set of density functions and argue that this is not an unreasonable 
depiction.
b. Observe, that the overlap region equally covers authorized users and intruders. 
Does it always mean there is equal probability that events in this region are by 
authorized users and intruders? Justify your answer.
 
11.3 
An example of a host-based intrusion detection tool is the tripwire program. This 
is a file integrity checking tool that scans files and directories on the system on a 
regular basis and notifies the administrator of any changes. It uses a protected data-
base of cryptographic checksums for each file checked and compares this value with 
that recomputed on each file as it is scanned. It must be configured with a list of 
files and directories to check, and what changes, if any, are permissible to each. It 
can allow, for example, log files to have new entries appended, but not for existing 
entries to be changed. What are the advantages and disadvantages of using such a 
tool? Consider the problem of determining which files should only change rarely, 
which files may change more often and how, and which change frequently and hence 
cannot be checked. Hence consider the amount of work in both the configuration of 
the program and on the system administrator monitoring the responses generated.
 
11.4 
A taxicab was involved in a fatal hit-and-run accident at night. Two cab companies, 
the Yellow and the Red, operate in the city. You are told that:
‚ñ† 85% of the cabs in the city are Yellow and 15% are Red.
‚ñ† A witness identified the cab as Red.

The court tested the reliability of the witness under the same circumstances that 
existed on the night of the accident and concluded that the witness was correct in 
identifying the color of the cab 90% of the time. What is the probability that the cab 
involved in the incident was Red rather than Yellow?
 
11.5 
Explain the suitability or unsuitability of the following passwords:
a. anu 1998
b. 5mimf2a3c (for 5 members in 
my family 2 adults 3 children)
c. Coimbatore16
d. Windows
e. Olympics
f. msk@123
g. g.0987654
h. iamking
 
11.6 
An early attempt to force users to use less predictable passwords involved computer-
supplied passwords. The passwords were eight characters long and were taken from 
the character set consisting of lowercase letters and digits. They were generated by a 
pseudorandom number generator with 215 possible starting values. Using the technol-
ogy of the time, the time required to search through all character strings of length 8 
from a 36-character alphabet was 112 years. Unfortunately, this is not a true reflec-
tion of the actual security of the system. Explain the problem.
 
11.7 
Assume that passwords are selected from five-character combinations of 26 alpha-
betic characters. Assume that an adversary is able to attempt passwords at a rate of 
one per second.
a. Assuming no feedback to the adversary until each attempt has been completed, 
what is the expected time to discover the correct password?
b. Assuming feedback to the adversary flagging an error as each incorrect character 
is entered, what is the expected time to discover the correct password?
 
11.8 
Assume that source elements of length k are mapped in some uniform fashion into a 
target elements of length p. If each digit can take on one of r values, then the number 
of source elements is r k and the number of target elements is the smaller number r p. 
A particular source element xi is mapped to a particular target element yj.
a. What is the probability that the correct source element can be selected by an 
 adversary on one try?
b. What is the probability that a different source element xk(xi ‚â† xk) that results 
in¬†the same target element, yj, could be produced by an adversary?
c. What is the probability that the correct target element can be produced by an 
adversary on one try?
 
11.9 
A phonetic password generator picks two segments randomly for each six-letter 
 password. The form of each segment is C9VC (consonant, digit, vowel, consonant), 
where V = 6a, e, i, o, u7 and C ‚â† V.
a. What is the total password population?
b. What is the probability of an adversary guessing a password correctly?
 
11.10 
Assume that passwords are limited to the use of the 95 printable ASCII characters 
and that all passwords are 12 characters in length. Assume a password cracker with 
an encryption rate of 6.4 million encryptions per second. How long will it take to test 
exhaustively all possible passwords on a UNIX system?
 
11.11 
Because of the known risks of the UNIX password system, the SunOS-4.0 documen-
tation recommends that the password file be removed and replaced with a publicly 
readable file called /etc/publickey. An entry in the file for user A consists of a user‚Äôs 
identifier IDA, the user‚Äôs public key, PUa, and the corresponding private key PRa. 
This private key is encrypted using DES with a key derived from the user‚Äôs login 
password Pa. When A logs in, the system decrypts E(Pa, PRa) to obtain PRa.
a. The system then verifies that Pa was correctly supplied. How?
b. How can an opponent attack this system?
 
11.12 
The encryption scheme used for UNIX passwords is one way; it is not possible to 
reverse it. Therefore, would it be accurate to say that this is, in fact, a hash code 
rather than an encryption of the password?

11.13 
It was stated that the inclusion of the salt in the UNIX password scheme increases 
the difficulty of guessing by a factor of 4096. But the salt is stored in plaintext in the 
same entry as the corresponding ciphertext password. Therefore, those two charac-
ters are known to the attacker and need not be guessed. Why is it asserted that the 
salt increases security?
 
11.14 
Assuming that you have successfully answered the preceding problem and under-
stand the significance of the salt, here is another question. Wouldn‚Äôt it be possible to 
thwart completely all password crackers by dramatically increasing the salt size to, 
say, 24 or 48 bits?
 
11.15 
Consider the Bloom filter discussed in Section 11.3. Define k = number of hash 
functions; N = number of bits in hash table; and D = number of words in dictionary.
a. Show that the expected number of bits in the hash table that are equal to zero is 
expressed as
f = a1 - k
Nb
D
b. Show that the probability that an input word, not in the dictionary, will be falsely 
accepted as being in the dictionary is
P = (1 - f)k
c. Show that the preceding expression can be approximated as
P ‚âà (1 - e-kD/N)k
 
11.16 
Design a file access system to allow certain users read and write access to files, 
depending on authorization set up by the system. The instructions should be of the 
format:
ReadFile(F1, User A):  User A has read access to file F1
WriteFile(F2, User A): User A has write access to file F2
ExecuteFile(F3, User B): User B has execute access to file F3
 
 
Each file has a header record, which contains authorization privileges; that is, a list of 
users who can read and write. The file is to be encrypted by a key that is not shared by 
the users but known only to the system.

410
12.1 The Need for Firewalls
12.2 Firewall Characteristics and Access Policy
12.3 Types of Firewalls
Packet Filtering Firewall
Stateful Inspection Firewalls
Application-Level Gateway
Circuit-Level Gateway
12.4 Firewall Basing
Bastion Host
Host-Based Firewalls
Personal Firewall
12.5 Firewall Location and Configurations
DMZ Networks
Virtual Private Networks
Distributed Firewalls
Summary of Firewall Locations and Topologies
12.6 Key Terms, Review Questions, and Problems
Chapter
Firewalls

Firewalls can be an effective means of protecting a local system or network of systems 
from network-based security threats while at the same time affording access to the 
outside world via wide area networks and the Internet.
 12.1 The Need for firewalls
Information systems in corporations, government agencies, and other organizations 
have undergone a steady evolution. The following are notable developments:
‚ñ†
‚ñ† Centralized data processing system, with a central mainframe supporting a 
number of directly connected terminals
‚ñ†
‚ñ† Local area networks (LANs) interconnecting PCs and terminals to each other 
and the mainframe
‚ñ†
‚ñ† Premises network, consisting of a number of LANs, interconnecting PCs, 
servers, and perhaps a mainframe or two
‚ñ†
‚ñ† Enterprise-wide network, consisting of multiple, geographically distributed 
premises networks interconnected by a private wide area network (WAN)
‚ñ†
‚ñ† Internet connectivity, in which the various premises networks all hook into 
the Internet and may or may not also be connected by a private WAN
Internet connectivity is no longer optional for organizations. The information 
and services available are essential to the organization. Moreover, individual users 
within the organization want and need Internet access, and if this is not provided via 
their LAN, they will use dial-up capability from their PC to an Internet service pro-
vider (ISP). However, while Internet access provides benefits to the organization, it 
enables the outside world to reach and interact with local network assets. This cre-
ates a threat to the organization. While it is possible to equip each workstation and 
server on the premises network with strong security features, such as intrusion pro-
tection, this may not be sufficient and in some cases is not cost-effective. Consider 
a network with hundreds or even thousands of systems, running various operating 
systems, such as different versions of UNIX and Windows. When a security flaw 
learNiNg objecTives
After studying this chapter, you should be able to:
‚óÜ‚ñ†
Explain the role of firewalls as part of a computer and network security 
strategy.
‚óÜ‚ñ†
List the key characteristics of firewalls.
‚óÜ‚ñ†
Discuss the various basing options for firewalls.
‚óÜ‚ñ†
Understand the relative merits of various choices for firewall location and 
configurations.

is discovered, each potentially affected system must be upgraded to fix that flaw. 
This requires scaleable configuration management and aggressive patching to func-
tion effectively. While difficult, this is possible and is necessary if only host-based 
security is used. A widely accepted alternative or at least complement to host-based 
security services is the firewall. The firewall is inserted between the premises net-
work and the Internet to establish a controlled link and to erect an outer security 
wall or perimeter. The aim of this perimeter is to protect the premises network from 
Internet-based attacks and to provide a single choke point where security and audit-
ing can be imposed. The firewall may be a single computer system or a set of two or 
more systems that cooperate to perform the firewall function.
The firewall, then, provides an additional layer of defense, insulating the inter-
nal systems from external networks. This follows the classic military doctrine of 
‚Äúdefense in depth,‚Äù which is just as applicable to IT security.
 12.2 firewall characTerisTics aNd access Policy
[BELL94b] lists the following design goals for a firewall:
1. All traffic from inside to outside, and vice versa, must pass through the fire-
wall. This is achieved by physically blocking all access to the local network 
except via the firewall. Various configurations are possible, as explained later 
in this chapter.
2. Only authorized traffic, as defined by the local security policy, will be allowed 
to pass. Various types of firewalls are used, which implement various types of 
security policies, as explained later in this chapter.
3. The firewall itself is immune to penetration. This implies the use of a hardened 
system with a secured operating system. Trusted computer systems are suitable 
for hosting a firewall and often required in government applications.
A critical component in the planning and implementation of a firewall is 
specifying a suitable access policy. This lists the types of traffic authorized to pass 
through the firewall, including address ranges, protocols, applications, and content 
types. This policy should be developed from the organization‚Äôs information security 
risk assessment and policy. This policy should be developed from a broad specifica-
tion of which traffic types the organization needs to support. It is then refined to 
detail the filter elements we discuss next, which can then be implemented within an 
appropriate firewall topology.
SP 800-41-1 (Guidelines on Firewalls and Firewall Policy, September 2009) 
lists a range of characteristics that a firewall access policy could use to filter traffic, 
including:
‚ñ†
‚ñ† IP Address and Protocol Values: Controls access based on the source or 
 destination addresses and port numbers, direction of flow being inbound or 
outbound, and other network and transport layer characteristics. This type of 
filtering is used by packet filter and stateful inspection firewalls. It is typically 
used to limit access to specific services.

‚ñ†
‚ñ† Application Protocol: Controls access on the basis of authorized application 
protocol data. This type of filtering is used by an application-level gateway 
that relays and monitors the exchange of information for specific applica-
tion protocols, for example, checking SMTP e-mail for spam, or HTPP Web 
requests to authorized sites only.
‚ñ†
‚ñ† User Identity: Controls access based on the users identity, typically for inside 
users who identify themselves using some form of secure authentication tech-
nology, such as IPSec (Chapter 9).
‚ñ†
‚ñ† Network Activity: Controls access based on considerations such as the time 
or request, for example, only in business hours; rate of requests, for example, 
to detect scanning attempts; or other activity patterns.
Before proceeding to the details of firewall types and configurations, it is best 
to summarize what one can expect from a firewall. The following capabilities are 
within the scope of a firewall:
1. A firewall defines a single choke point that keeps unauthorized users out of 
the protected network, prohibits potentially vulnerable services from enter-
ing or leaving the network, and provides protection from various kinds of IP 
spoofing and routing attacks. The use of a single choke point simplifies security 
management because security capabilities are consolidated on a single system 
or set of systems.
2. A firewall provides a location for monitoring security-related events. Audits 
and alarms can be implemented on the firewall system.
3. A firewall is a convenient platform for several Internet functions that are not 
security related. These include a network address translator, which maps local 
addresses to Internet addresses, and a network management function that 
 audits or logs Internet usage.
4. A firewall can serve as the platform for IPsec. Using the tunnel mode capa-
bility described in Chapter 9, the firewall can be used to implement virtual 
private networks.
Firewalls have their limitations, including the following:
1. The firewall cannot protect against attacks that bypass the firewall. Internal 
systems may have dial-out capability to connect to an ISP. An internal LAN 
may support a modem pool that provides dial-in capability for traveling 
 employees and telecommuters.
2. The firewall may not protect fully against internal threats, such as a disgrun-
tled employee or an employee who unwittingly cooperates with an external 
attacker.
3. An improperly secured wireless LAN may be accessed from outside the 
 organization. An internal firewall that separates portions of an enterprise 
 network cannot guard against wireless communications between local systems 
on  different sides of the internal firewall.
4. A laptop, PDA, or portable storage device may be used and infected outside 
the corporate network, and then attached and used internally.

12.3 TyPes of firewalls
A firewall can monitor network traffic at a number of levels, from low-level net-
work packets either individually or as part of a flow, to all traffic within a transport 
connection, up to inspecting details of application protocols. The choice of which 
level is appropriate is determined by the desired firewall access policy. It can oper-
ate as a positive filter, allowing to pass only packets that meet specific criteria, or as 
a negative filter, rejecting any packet that meets certain criteria. The criteria imple-
ment the access policy for the firewall, that we discussed in the previous section. 
Depending on the type of firewall, it may examine one or more protocol headers in 
each packet, the payload of each packet, or the pattern generated by a sequence of 
packets. In this section, we look at the principal types of firewalls.
Packet Filtering Firewall
A packet filtering firewall applies a set of rules to each incoming and outgoing IP 
packet and then forwards or discards the packet (Figure 12.1b). The firewall is typi-
cally configured to filter packets going in both directions (from and to the internal 
network). Filtering rules are based on information contained in a network packet:
‚ñ†
‚ñ† Source IP address: The IP address of the system that originated the IP packet 
(e.g., 192.178.1.1)
‚ñ†
‚ñ† Destination IP address: The IP address of the system the IP packet is trying 
to reach (e.g., 192.168.1.2)
‚ñ†
‚ñ† Source and destination transport-level address: The transport-level (e.g., TCP 
or UDP) port number, which defines applications such as SNMP or TELNET
‚ñ†
‚ñ† IP protocol field: Defines the transport protocol
‚ñ†
‚ñ† Interface: For a firewall with three or more ports, which interface of the fire-
wall the packet came from or which interface of the firewall the packet is 
destined for
The packet filter is typically set up as a list of rules based on matches to fields 
in the IP or TCP header. If there is a match to one of the rules, that rule is invoked 
to determine whether to forward or discard the packet. If there is no match to any 
rule, then a default action is taken. Two default policies are possible:
‚ñ†‚ñ†
Default = discard: That which is not expressly permitted is prohibited.
‚ñ†‚ñ†
Default = forward: That which is not expressly prohibited is permitted.
The default discard policy is more conservative. Initially, everything is blocked, 
and services must be added on a case-by-case basis. This policy is more visible to 
users, who are more likely to see the firewall as a hindrance. However, this is the 
 policy likely to be preferred by businesses and government organizations. Further, 
visibility to users diminishes as rules are created. The default forward policy increases 
ease of use for end users but provides reduced security; the security administrator 
must, in essence, react to each new security threat as it becomes known. This policy 
may be used by generally more open organizations, such as universities.

Figure 12.1 Types of Firewalls
External (untrusted) network
(e.g., Internet)
Internal (protected) network
(e.g., enterprise network)
Firewall
(a) General model
(d) Application proxy frewall
Physical 
Network
access
Internet
Transport
Application
Physical 
Network
access
Internet
Transport
Application
Application proxy
External
transport
connection
Internal
transport
connection
(b) Packet fltering frewall
Physical 
Network
access
Internet
Transport
Application
End-to-end
transport
connection
End-to-end
transport
connection
(c) Stateful inspection frewall
Physical 
Network
access
Internet
Transport
Application
End-to-end
transport
connection
End-to-end
transport
connection
(e) Circuit-level proxy frewall
Physical 
Network
access
Internet
Transport
Application
Physical 
Network
access
Internet
Transport
Application
Circuit-level proxy
External
transport
connection
Internal
transport
connection
State
info
Table 12.1 is a simplified example of a ruleset for SMTP traffic. The goal is to 
allow inbound and outbound e-mail traffic but to block all other traffic. The rules 
are applied top to bottom to each packet.

A. Inbound mail from an external source is allowed (port 25 is for SMTP 
 incoming).
B. This rule is intended to allow a response to an inbound SMTP connection.
C. Outbound mail to an external source is allowed.
D. This rule is intended to allow a response to an inbound SMTP connection.
E. This is an explicit statement of the default policy. All rulesets include this rule 
implicitly as the last rule.
There are several problems with this ruleset. Rule D allows external traffic 
to any destination port above 1023. As an example of an exploit of this rule, an 
external attacker can open a connection from the attacker‚Äôs port 5150 to an internal 
Web proxy server on port 8080. This is supposed to be forbidden and could allow an 
attack on the server. To counter this attack, the firewall ruleset can be configured 
with a source port field for each row. For rules B and D, the source port is set to 25; 
for rules A and C, the source port is set to 7 1023.
But a vulnerability remains. Rules C and D are intended to specify that any 
inside host can send mail to the outside. A TCP packet with a destination port of 
25 is routed to the SMTP server on the destination machine. The problem with 
this rule is that the use of port 25 for SMTP receipt is only a default; an outside 
machine could be configured to have some other application linked to port 25. As 
the revised rule D is written, an attacker could gain access to internal machines 
by sending packets with a TCP source port number of 25. To counter this threat, 
we can add an ACK flag field to each row. For rule D, the field would indicate 
that the ACK flag must be set on the incoming packet. Rule D would now look 
like this:
Rule
Direction
Source 
Address
Source 
Port
Dest 
Address
Protocol
Dest 
Port
Flag
Action
D
In
External
25
Internal
TCP
7 1023
ACK
Permit
The rule takes advantage of a feature of TCP connections. Once a connection 
is set up, the ACK flag of a TCP segment is set to acknowledge segments sent from 
the other side. Thus, this rule allows incoming packets with a source port number of 
25 that include the ACK flag in the TCP segment.
Rule
Direction
Source 
Address
Destination 
Address
Protocol
Destination 
Port
Action
A
In
External
Internal
TCP
25
Permit
B
Out
Internal
External
TCP
7 1023
Permit
C
Out
Internal
External
TCP
25
Permit
D
In
External
Internal
TCP
7 1023
Permit
E
Either
Any
Any
Any
Any
Deny
Table 12.1 Packet-Filtering Example

One advantage of a packet filtering firewall is its simplicity. Also, packet  filters 
typically are transparent to users and are very fast. [SP 800-41-1] lists the  following 
weaknesses of packet filter firewalls:
‚ñ†
‚ñ† Because packet filter firewalls do not examine upper-layer data, they can-
not prevent attacks that employ application-specific vulnerabilities or func-
tions. For example, a packet filter firewall cannot block specific application 
commands; if a packet filter firewall allows a given application, all functions 
 available within that application will be permitted.
‚ñ†
‚ñ† Because of the limited information available to the firewall, the logging func-
tionality present in packet filter firewalls is limited. Packet filter logs normally 
contain the same information used to make access control decisions (source 
address, destination address, and traffic type).
‚ñ†
‚ñ† Most packet filter firewalls do not support advanced user authentication 
schemes. Once again, this limitation is mostly due to the lack of upper-layer 
functionality by the firewall.
‚ñ†
‚ñ† Packet filter firewalls are generally vulnerable to attacks and exploits that 
take advantage of problems within the TCP/IP specification and protocol 
stack, such as network layer address spoofing. Many packet filter firewalls 
cannot detect a network packet in which the OSI Layer 3 addressing informa-
tion has been altered. Spoofing attacks are generally employed by intruders 
to bypass the security controls implemented in a firewall platform.
‚ñ†
‚ñ† Finally, due to the small number of variables used in access control decisions, 
packet filter firewalls are susceptible to security breaches caused by improper 
configurations. In other words, it is easy to accidentally configure a packet 
filter firewall to allow traffic types, sources, and destinations that should be 
denied based on an organization‚Äôs information security policy.
Some of the attacks that can be made on packet filtering firewalls and the 
appropriate countermeasures are the following:
‚ñ†
‚ñ† IP address spoofing: The intruder transmits packets from the outside with a 
source IP address field containing an address of an internal host. The attacker 
hopes that the use of a spoofed address will allow penetration of systems that 
employ simple source address security, in which packets from specific trusted 
internal hosts are accepted. The countermeasure is to discard packets with an 
inside source address if the packet arrives on an external interface. In fact, this 
countermeasure is often implemented at the router external to the firewall.
‚ñ†
‚ñ† Source routing attacks: The source station specifies the route that a packet 
should take as it crosses the Internet, in the hopes that this will bypass secu-
rity measures that do not analyze the source routing information. The coun-
termeasure is to discard all packets that use this option.
‚ñ†
‚ñ† Tiny fragment attacks: The intruder uses the IP fragmentation option to 
create extremely small fragments and force the TCP header information 
into a separate packet fragment. This attack is designed to circumvent fil-
tering rules that depend on TCP header information. Typically, a packet 
filter will make a filtering decision on the first fragment of a packet. All

subsequent fragments of that packet are filtered out solely on the basis that 
they are part of the packet whose first fragment was rejected. The attacker 
hopes that the filtering firewall examines only the first fragment and that 
the remaining fragments are passed through. A tiny fragment attack can be 
defeated by enforcing a rule that the first fragment of a packet must contain 
a predefined minimum amount of the transport header. If the first fragment 
is rejected, the filter can remember the packet and discard all subsequent 
fragments.
Stateful Inspection Firewalls
A traditional packet filter makes filtering decisions on an individual packet basis 
and does not take into consideration any higher-layer context. To understand what 
is meant by context and why a traditional packet filter is limited with regard to con-
text, a little background is needed. Most standardized applications that run on top 
of TCP follow a client/server model. For example, for the Simple Mail Transfer 
Protocol (SMTP), e-mail is transmitted from a client system to a server system. 
The client system generates new e-mail messages, typically from user input. The 
server system accepts incoming e-mail messages and places them in the appropri-
ate user mailboxes. SMTP operates by setting up a TCP connection between client 
and server, in which the TCP server port number, which identifies the SMTP server 
application, is 25. The TCP port number for the SMTP client is a number between 
1024 and 65535 that is generated by the SMTP client.
In general, when an application that uses TCP creates a session with a remote 
host, it creates a TCP connection in which the TCP port number for the remote 
(server) application is a number less than 1024 and the TCP port number for the 
local (client) application is a number between 1024 and 65535. The numbers less 
than 1024 are the ‚Äúwell-known‚Äù port numbers and are assigned permanently to 
particular applications (e.g., 25 for server SMTP). The numbers between 1024 and 
65535 are generated dynamically and have temporary significance only for the life-
time of a TCP connection.
A simple packet filtering firewall must permit inbound network traffic on all 
these high-numbered ports for TCP-based traffic to occur. This creates a vulnerabil-
ity that can be exploited by unauthorized users.
A stateful inspection packet firewall tightens up the rules for TCP traffic by 
creating a directory of outbound TCP connections, as shown in Table 12.2. There is 
an entry for each currently established connection. The packet filter will now allow 
incoming traffic to high-numbered ports only for those packets that fit the profile of 
one of the entries in this directory.
A stateful packet inspection firewall reviews the same packet information 
as a packet filtering firewall, but also records information about TCP connections 
(Figure 12.1c). Some stateful firewalls also keep track of TCP sequence numbers 
to prevent attacks that depend on the sequence number, such as session hijack-
ing. Some even inspect limited amounts of application data for some well-known 
protocols like FTP, IM and SIPS commands, in order to identify and track related 
connections.

Application-Level Gateway
An application-level gateway, also called an application proxy, acts as a relay of 
application-level traffic (Figure 12.1d). The user contacts the gateway using a TCP/
IP application, such as Telnet or FTP, and the gateway asks the user for the name 
of the remote host to be accessed. When the user responds and provides a valid 
user ID and authentication information, the gateway contacts the application on 
the remote host and relays TCP segments containing the application data between 
the two endpoints. If the gateway does not implement the proxy code for a specific 
application, the service is not supported and cannot be forwarded across the fire-
wall. Further, the gateway can be configured to support only specific features of an 
application that the network administrator considers acceptable while denying all 
other features.
Application-level gateways tend to be more secure than packet filters. Rather 
than trying to deal with the numerous possible combinations that are to be allowed 
and forbidden at the TCP and IP level, the application-level gateway need only 
scrutinize a few allowable applications. In addition, it is easy to log and audit all 
incoming traffic at the application level.
A prime disadvantage of this type of gateway is the additional processing 
overhead on each connection. In effect, there are two spliced connections between 
the end users, with the gateway at the splice point, and the gateway must examine 
and forward all traffic in both directions.
Circuit-Level Gateway
A fourth type of firewall is the circuit-level gateway or circuit-level proxy 
(Figure¬†12.1e). This can be a stand-alone system or it can be a specialized function 
performed by an application-level gateway for certain applications. As with an 
Source Address
Source Port
Destination 
Address
Destination Port
Connection State
192.168.1.100
1030
210.22.88.29
80
Established
192.168.1.102
1031
216.32.42.123
80
Established
192.168.1.101
1033
173.66.32.122
25
Established
192.168.1.106
1035
177.231.32.12
79
Established
223.43.21.231
1990
192.168.1.6
80
Established
2122.22.123.32
2112
192.168.1.6
80
Established
210.922.212.18
3321
192.168.1.6
80
Established
24.102.32.23
1025
192.168.1.6
80
Established
223.21.22.12
1046
192.168.1.6
80
Established
Table 12.2 Example Stateful Firewall Connection State Table (SP 800-41-1)

application gateway, a circuit-level gateway does not permit an end-to-end TCP 
connection; rather, the gateway sets up two TCP connections, one between itself 
and a TCP user on an inner host and one between itself and a TCP user on an out-
side host. Once¬†the two connections are established, the gateway typically relays 
TCP segments from one connection to the other without examining the contents. 
The security function consists of determining which connections will be allowed.
A typical use of circuit-level gateways is a situation in which the system admin-
istrator trusts the internal users. The gateway can be configured to support appli-
cation-level or proxy service on inbound connections and circuit-level functions for 
outbound connections. In this configuration, the gateway can incur the processing 
overhead of examining incoming application data for forbidden functions but does 
not incur that overhead on outgoing data.
An example of a circuit-level gateway implementation is the SOCKS package 
[KOBL92]; version 5 of SOCKS is specified in RFC 1928. The SOCKS protocol pro-
vides a framework for client-server applications in both the TCP and UDP domains. 
It is designed to provide convenient and secure access to a network-level firewall. The 
protocol occupies a thin layer between the application and either TCP or UDP but 
does not provide network-level routing services, such as forwarding of ICMP messages.
SOCKS consists of the following components:
‚ñ†
‚ñ† The SOCKS server, which often runs on a UNIX-based firewall. SOCKS is 
also implemented on Windows systems.
‚ñ†
‚ñ† The SOCKS client library, which runs on internal hosts protected by the firewall.
‚ñ†
‚ñ† SOCKS-ified versions of several standard client programs such as FTP and 
TELNET. The implementation of the SOCKS protocol typically involves 
either the recompilation or relinking of TCP-based client applications or the 
use of alternate dynamically loaded libraries, to use the appropriate encapsu-
lation routines in the SOCKS library.
When a TCP-based client wishes to establish a connection to an object that is 
reachable only via a firewall (such determination is left up to the implementation), 
it must open a TCP connection to the appropriate SOCKS port on the SOCKS 
server system. The SOCKS service is located on TCP port 1080. If the connection 
request succeeds, the client enters a negotiation for the authentication method to 
be used, authenticates with the chosen method, and then sends a relay request. The 
SOCKS server evaluates the request and either establishes the appropriate connec-
tion or denies it. UDP exchanges are handled in a similar fashion. In essence, a TCP 
connection is opened to authenticate a user to send and receive UDP segments, and 
the UDP segments are forwarded as long as the TCP connection is open.
 12.4 firewall basiNg
It is common to base a firewall on a stand-alone machine running a common operat-
ing system, such as UNIX or Linux. Firewall functionality can also be implemented 
as a software module in a router or LAN switch. In this section, we look at some 
additional firewall basing considerations.

Bastion Host
A bastion host is a system identified by the firewall administrator as a critical strong 
point in the network‚Äôs security. Typically, the bastion host serves as a platform for 
an application-level or circuit-level gateway. Common characteristics of a bastion 
host are as follows:
‚ñ†
‚ñ† The bastion host hardware platform executes a secure version of its operating 
system, making it a hardened system.
‚ñ†
‚ñ† Only the services that the network administrator considers essential are 
installed on the bastion host. These could include proxy applications for DNS, 
FTP, HTTP, and SMTP.
‚ñ†
‚ñ† The bastion host may require additional authentication before a user is 
allowed access to the proxy services. In addition, each proxy service may 
require its own authentication before granting user access.
‚ñ†
‚ñ† Each proxy is configured to support only a subset of the standard applica-
tion‚Äôs command set.
‚ñ†
‚ñ† Each proxy is configured to allow access only to specific host systems. This 
means that the limited command/feature set may be applied only to a subset 
of systems on the protected network.
‚ñ†
‚ñ† Each proxy maintains detailed audit information by logging all traffic, each 
connection, and the duration of each connection. The audit log is an essential 
tool for discovering and terminating intruder attacks.
‚ñ†
‚ñ† Each proxy module is a very small software package specifically designed for 
network security. Because of its relative simplicity, it is easier to check such 
modules for security flaws. For example, a typical UNIX mail application may 
contain over 20,000 lines of code, while a mail proxy may contain fewer than 
1000.
‚ñ†
‚ñ† Each proxy is independent of other proxies on the bastion host. If there is a 
problem with the operation of any proxy, or if a future vulnerability is discov-
ered, it can be uninstalled without affecting the operation of the other proxy 
applications. Also, if the user population requires support for a new service, 
the network administrator can easily install the required proxy on the bastion 
host.
‚ñ†
‚ñ† A proxy generally performs no disk access other than to read its initial con-
figuration file. Hence, the portions of the file system containing executable 
code can be made read only. This makes it difficult for an intruder to install 
Trojan horse sniffers or other dangerous files on the bastion host.
‚ñ†
‚ñ† Each proxy runs as a nonprivileged user in a private and secured directory on 
the bastion host.
Host-Based Firewalls
A host-based firewall is a software module used to secure an individual host. Such 
modules are available in many operating systems or can be provided as an add-
on package. Like conventional stand-alone firewalls, host-resident firewalls filter

and restrict the flow of packets. A common location for such firewalls is a server. 
There are several advantages to the use of a server-based or workstation-based 
firewall:
‚ñ†
‚ñ† Filtering rules can be tailored to the host environment. Specific corporate 
security policies for servers can be implemented, with different filters for 
servers used for different application.
‚ñ†
‚ñ† Protection is provided independent of topology. Thus both internal and exter-
nal attacks must pass through the firewall.
‚ñ†
‚ñ† Used in conjunction with stand-alone firewalls, the host-based firewall pro-
vides an additional layer of protection. A new type of server can be added to 
the network, with its own firewall, without the necessity of altering the net-
work firewall configuration.
Personal Firewall
A personal firewall controls the traffic between a personal computer or workstation 
on one side and the Internet or enterprise network on the other side. Personal fire-
wall functionality can be used in the home environment and on corporate intranets. 
Typically, the personal firewall is a software module on the personal computer. In 
a home environment with multiple computers connected to the Internet, firewall 
functionality can also be housed in a router that connects all of the home computers 
to a DSL, cable modem, or other Internet interface.
Personal firewalls are typically much less complex than either server-based 
firewalls or stand-alone firewalls. The primary role of the personal firewall is to 
deny unauthorized remote access to the computer. The firewall can also monitor 
outgoing activity in an attempt to detect and block worms and other malware.
Personal firewall capabilities are provided by the netfilter package on Linux 
systems, or the pf package on BSD and Mac OS X systems. These packages may be 
configured on the command-line, or with a GUI front-end. When such a personal 
firewall is enabled, all inbound connections are usually denied except for those 
the user explicitly permits. Outbound connections are usually allowed. The list of 
 inbound services that can be selectively re-enabled, with their port numbers, may 
include the following common services:
‚ñ†
‚ñ† Personal file sharing (548, 427)
‚ñ†
‚ñ† Windows sharing (139)
‚ñ†
‚ñ† Personal Web sharing (80, 427)
‚ñ†
‚ñ† Remote login‚ÄîSSH (22)
‚ñ†
‚ñ† FTP access (20‚Äì21, 1024-65535 from 20‚Äì21)
‚ñ†
‚ñ† Printer sharing (631, 515)
‚ñ†
‚ñ† iChat Rendezvous (5297, 5298)
‚ñ†
‚ñ† iTunes Music Sharing (3869)
‚ñ†
‚ñ† CVS (2401)
‚ñ†
‚ñ† Gnutella/Limewire (6346)

‚ñ†
‚ñ† ICQ (4000)
‚ñ†
‚ñ† IRC (194)
‚ñ†
‚ñ† MSN Messenger (6891‚Äì6900)
‚ñ†
‚ñ† Network Time (123)
‚ñ†
‚ñ† Retrospect (497)
‚ñ†
‚ñ† SMB (without netbios; 445)
‚ñ†
‚ñ† Timbuktu (407)
‚ñ†
‚ñ† VNC (5900‚Äì5902)
‚ñ†
‚ñ† WebSTAR Admin (1080, 1443)
When FTP access is enabled, ports 20 and 21 on the local machine are opened 
for FTP; if others connect this computer from ports 20 or 21, the ports 1024 through 
65535 are open.
For increased protection, advanced firewall features may be configured. For 
example, stealth mode hides the system on the Internet by dropping unsolicited 
communication packets, making it appear as though the system is not present. 
UDP packets can be blocked, restricting network traffic to TCP packets only for 
open ports. The firewall also supports logging, an important tool for checking on 
unwanted activity. Other types of personal firewall allow the user to specify that 
only selected applications, or applications signed by a valid certificate authority, 
may provide services accessed from the network.
 12.5 firewall locaTioN aNd coNfiguraTioNs
As Figure 12.1a indicates, a firewall is positioned to provide a protective barrier 
between an external, potentially untrusted source of traffic and an internal network. 
With that general principle in mind, a security administrator must decide on the 
location and on the number of firewalls needed. In this section, we look at some 
common options.
DMZ Networks
Figure 12.2 suggests the most common distinction, that between an internal and an 
external firewall. An external firewall is placed at the edge of a local or enterprise 
network, just inside the boundary router that connects to the Internet or some wide 
area network (WAN). One or more internal firewalls protect the bulk of the enter-
prise network. Between these two types of firewalls are one or more networked 
devices in a region referred to as a DMZ (demilitarized zone) network. Systems 
that are externally accessible but need some protections are usually located on 
DMZ networks. Typically, the systems in the DMZ require or foster external con-
nectivity, such as a corporate Web site, an e-mail server, or a DNS (domain name 
system) server.
The external firewall provides a measure of access control and protection for 
the DMZ systems consistent with their need for external connectivity. The external

firewall also provides a basic level of protection for the remainder of the enterprise 
network. In this type of configuration, internal firewalls serve three purposes:
1. The internal firewall adds more stringent filtering capability, compared to the 
external firewall, in order to protect enterprise servers and workstations from 
external attack.
2. The internal firewall provides two-way protection with respect to the DMZ. 
First, the internal firewall protects the remainder of the network from attacks 
launched from DMZ systems. Such attacks might originate from worms, root-
kits, bots, or other malware lodged in a DMZ system. Second, an internal fire-
wall can protect the DMZ systems from attack from the internal protected 
network.
Figure 12.2 Example Firewall Configuration
Workstations
Application and database servers
Web
server(s)
E-mail
server
Internal DMZ network
Boundary
router
External
frewall
LAN
switch
LAN
switch
Internal
frewall
Internal protected network
DNS
server
Internet
Remote
users

3. Multiple internal firewalls can be used to protect portions of the internal net-
work from each other. For example, firewalls can be configured so that internal 
servers are protected from internal workstations and vice versa. A common 
practice is to place the DMZ on a different network interface on the external 
firewall from that used to access the internal networks.
Virtual Private Networks
In today‚Äôs distributed computing environment, the virtual private network (VPN) 
offers an attractive solution to network managers. In essence, a VPN consists of a 
set of computers that interconnect by means of a relatively unsecure network and 
that make use of encryption and special protocols to provide security. At each cor-
porate site, workstations, servers, and databases are linked by one or more local 
area networks (LANs). The Internet or some other public network can be used to 
interconnect sites, providing a cost savings over the use of a private network and 
offloading the wide area network management task to the public network provider. 
That same public network provides an access path for telecommuters and other mo-
bile employees to log on to corporate systems from remote sites.
But the manager faces a fundamental requirement: security. Use of a public 
network exposes corporate traffic to eavesdropping and provides an entry point for 
unauthorized users. To counter this problem, a VPN is needed. In essence, a VPN 
uses encryption and authentication in the lower protocol layers to provide a secure 
connection through an otherwise insecure network, typically the Internet. VPNs are 
generally cheaper than real private networks using private lines but rely on having 
the same encryption and authentication system at both ends. The encryption may 
be performed by firewall software or possibly by routers. The most common proto-
col mechanism used for this purpose is at the IP level and is known as IPsec.
Figure 12.3 (Compare Figure 9.1) is a typical scenario of IPSec usage.1 An 
organization maintains LANs at dispersed locations. Nonsecure IP traffic is con-
ducted on each LAN. For traffic off site, through some sort of private or public 
WAN, IPSec protocols are used. These protocols operate in networking devices, 
such as a router or firewall, that connect each LAN to the outside world. The IPSec 
networking device will typically encrypt and compress all traffic going into the 
WAN and decrypt and uncompress traffic coming from the WAN; authentication 
may also be provided. These operations are transparent to workstations and servers 
on the LAN. Secure transmission is also possible with individual users who dial into 
the WAN. Such user workstations must implement the IPSec protocols to provide 
security. They must also implement high levels of host security, as they are directly 
connected to the wider Internet. This makes them an attractive target for attackers 
attempting to access the corporate network.
A logical means of implementing an IPSec is in a firewall, as shown in 
Figure¬†12.3. If IPSec is implemented in a separate box behind (internal to) the fire-
wall, then VPN traffic passing through the firewall in both directions is encrypted. 
In this case, the firewall is unable to perform its filtering function or other security 
1Details of IPSec are provided in Chapter 9. For this discussion, all that we need to know is that IPSec 
adds one or more additional headers to the IP packet to support encryption and authentication functions.

functions, such as access control, logging, or scanning for viruses. IPSec could be 
implemented in the boundary router, outside the firewall. However, this device is 
likely to be less secure than the firewall and thus less desirable as an IPSec platform.
Distributed Firewalls
A distributed firewall configuration involves stand-alone firewall devices plus host-
based firewalls working together under a central administrative control. Figure¬†12.4 
suggests a distributed firewall configuration. Administrators can configure host-
resident firewalls on hundreds of servers and workstations as well as configure per-
sonal firewalls on local and remote user systems. Tools let the network adminis-
trator set policies and monitor security across the entire network. These firewalls 
protect against internal attacks and provide protection tailored to specific machines 
and applications. Stand-alone firewalls provide global protection, including internal 
firewalls and an external firewall, as discussed previously.
With distributed firewalls, it may make sense to establish both an internal and an 
external DMZ. Web servers that need less protection because they have less critical 
information on them could be placed in an external DMZ, outside the external fire-
wall. What protection is needed is provided by host-based firewalls on these servers.
An important aspect of a distributed firewall configuration is security moni-
toring. Such monitoring typically includes log aggregation and analysis, firewall sta-
tistics, and fine-grained remote monitoring of individual hosts if needed.
Figure 12.3 A VPN Security Scenario
IP
header
IP
payload
IP
header
IPSec
header
Secure IP
payload
IP
header
IPSec
header
Secure IP
payload
IP
header
IPSec
header
Secure IP
payload
IP
header
IP
payload
Firewall
with IPSec
Ethernet
switch
Ethernet
switch
User system
with IPSec
Firewall
with IPSec
Public (Internet)
or private
network

Summary of Firewall Locations and Topologies
We can now summarize the discussion from Sections 12.4 and 12.5 to define a 
spectrum of firewall locations and topologies. The following alternatives can be 
identified:
‚ñ†
‚ñ† Host-resident firewall: This category includes personal firewall software and 
firewall software on servers. Such firewalls can be used alone or as part of an 
in-depth firewall deployment.
Figure 12.4 Example Distributed Firewall Configuration
Workstations
Application and database servers
Web
server(s)
E-mail
server
Internal DMZ network
Boundary
router
External
frewall
LAN
switch
LAN
switch
host-resident
frewall
Internal
frewall
Internal protected network
DNS
server
Internet
Web
server(s)
External
DMZ network
Remote
users

‚ñ†
‚ñ† Screening router: A single router between internal and external networks 
with stateless or full packet filtering. This arrangement is typical for small 
office/home office (SOHO) applications.
‚ñ†
‚ñ† Single bastion inline: A single firewall device between an internal and exter-
nal router (e.g., Figure 12.1a). The firewall may implement stateful filters and/
or application proxies. This is the typical firewall appliance configuration for 
small- to medium-sized organizations.
‚ñ†
‚ñ† Single bastion T: Similar to single bastion inline but has a third network inter-
face on bastion to a DMZ where externally visible servers are placed. Again, 
this is a common appliance configuration for medium to large organizations.
‚ñ†
‚ñ† Double bastion inline: Figure 12.2 illustrates this configuration, where the 
DMZ is sandwiched between bastion firewalls. This configuration is common 
for large businesses and government organizations.
‚ñ†
‚ñ† Double bastion T: The DMZ is on a separate network interface on the bas-
tion firewall. This configuration is also common for large businesses and 
government organizations and may be required. For example, this configu-
ration is required for Australian government use (Australian Government 
Information Technology Security Manual‚ÄîACSI33).
‚ñ†
‚ñ† Distributed firewall configuration: Illustrated in Figure 12.4. This configura-
tion is used by some large businesses and government organizations.
 12.6 Key Terms, review QuesTioNs, aNd Problems
Key Terms 
application-level gateway
bastion host
circuit-level gateway
distributed firewalls
DMZ
firewall
host-based firewall
IP address spoofing
IP security (IPSec)
packet filtering firewall
personal firewall
proxy
stateful inspection firewall
tiny fragment attack
virtual private network (VPN)
Review Questions 
 
12.1 
List three design goals for a firewall.
 
12.2 
List four techniques used by firewalls to control access and enforce a security policy.
 
12.3 
When does a packet filtering firewall resort to default actions? List these default 
policies.
 
12.4 
What are some weaknesses of a packet filtering firewall?
 
12.5 
Explain three attacks that can be made on packet filtering firewalls. What measures 
can be taken to counter these attacks?
 
12.6 
What is an application-level gateway?
 
12.7 
What is a circuit-level gateway?
 
12.8 
What are the differences among the firewalls of Figure 12.1?
 
12.9 
What are the common characteristics of a bastion host?

12.10 
Why is it useful to have host-based firewalls?
 
12.11 
What is a virtual private network? How does it ensure a secure connection?
 
12.12 
Describe the spectrum of firewall locations and topologies.
Problems 
 
12.1 
As was mentioned in Section 12.3, one approach to defeating the tiny fragment attack 
is to enforce a minimum length of the transport header that must be contained in the 
first fragment of an IP packet. If the first fragment is rejected, all subsequent frag-
ments can be rejected. However, the nature of IP is such that fragments may arrive 
out of order. Thus, an intermediate fragment may pass through the filter before the 
initial fragment is rejected. How can this situation be handled?
 
12.2 
In an IPv4 packet, the size of the payload in the first fragment, in octets, is equal to 
Total Length - (4 * IHL). If this value is less than the required minimum (8 octets 
for TCP), then this fragment and the entire packet are rejected. Suggest an alterna-
tive method of achieving the same result using only the Fragment Offset field.
 
12.3 
RFC 791, the IPv4 protocol specification, describes a reassembly algorithm that 
results in new fragments overwriting any overlapped portions of previously received 
fragments. Given such a reassembly implementation, an attacker could construct a 
series of packets in which the lowest (zero-offset) fragment would contain innocu-
ous data (and thereby be passed by administrative packet filters), and in which some 
subsequent packet having a non-zero offset would overlap TCP header informa-
tion (destination port, for instance) and cause it to be modified. The second packet 
would be passed through most filter implementations because it does not have a zero 
fragment offset. Suggest a method that could be used by a packet filter to counter 
this¬†attack.
 
12.4 
Table 12.3 shows a sample of a packet filter firewall ruleset for an imaginary network 
of IP address that range from 192.168.1.0 to 192.168.1.254. Describe the effect of each 
rule.
 
12.5 
SMTP (Simple Mail Transfer Protocol) is the standard protocol for transferring mail 
between hosts over TCP. A TCP connection is set up between a user agent and a 
server program. The server listens on TCP port 25 for incoming connection requests. 
The user end of the connection is on a TCP port number above 1023. Suppose you 
wish to build a packet filter ruleset allowing inbound and outbound SMTP traffic. 
You generate the following ruleset:
Source Address
Source Port
Dest Address
Dest Port
Action
1
Any
Any
192.168.1.0
7 1023
Allow
2
192.168.1.1
Any
Any
Any
Deny
3
Any
Any
192.168.1.1
Any
Deny
4
192.168.1.0
Any
Any
Any
Allow
5
Any
Any
192.168.1.2
SMTP
Allow
6
Any
Any
192.168.1.3
HTTP
Allow
7
Any
Any
Any
Any
Deny
Table 12.3 Sample Packet Filter Firewall Ruleset

Rule
Direction
Src Addr
Dest Addr
Protocol
Dest Port
Action
A
In
External
Internal
TCP
25
Permit
B
Out
Internal
External
TCP
71023
Permit
C
Out
Internal
External
TCP
25
Permit
D
In
External
Internal
TCP
71023
Permit
E
Either
Any
Any
Any
Any
Deny
a. Describe the effect of each rule.
b. Your host in this example has IP address 172.16.1.1. Someone tries to send e-mail 
from a remote host with IP address 192.168.3.4. If successful, this generates an 
SMTP dialogue between the remote user and the SMTP server on your host con-
sisting of SMTP commands and mail. Additionally, assume that a user on your host 
tries to send e-mail to the SMTP server on the remote system. Four typical packets 
for this scenario are as shown:
Packet
Direction
Src Addr
Dest Addr
Protocol
Dest Port
Action
1
In
192.168.3.4
172.16.1.1
TCP
25
?
2
Out
172.16.1.1
192.168.3.4
TCP
1234
?
3
Out
172.16.1.1
192.168.3.4
TCP
25
?
4
In
192.168.3.4
172.16.1.1
TCP
1357
?
Indicate which packets are permitted or denied and which rule is used in each case.
c. Someone from the outside world (10.1.2.3) attempts to open a connection from 
port 5150 on a remote host to the Web proxy server on port 8080 on one of your 
local hosts (172.16.3.4), in order to carry out an attack. Typical packets are as 
follows:
Packet
Direction
Src Addr
Dest Addr
Protocol
Dest Port
Action
5
In
10.1.2.3
172.16.3.4
TCP
8080
?
6
Out
172.16.3.4
10.1.2.3
TCP
5150
?
Will the attack succeed? Give details.
 
12.6 
To provide more protection, the ruleset from the preceding problem is modified as 
follows:
Rule
Direction
Src Addr
Dest Addr
Protocol
Src Port
Dest Port
Action
A
In
External
Internal
TCP
71023
25
Permit
B
Out
Internal
External
TCP
25
71023
Permit
C
Out
Internal
External
TCP
71023
25
Permit
D
In
External
Internal
TCP
25
71023
Permit
E
Either
Any
Any
Any
Any
Any
Deny
a. Describe the change.
b. Apply this new ruleset to the same six packets of the preceding problem. Indicate 
which packets are permitted or denied and which rule is used in each case.

12.7 
A hacker uses port 25 as the client port on his or her end to attempt to open a connec-
tion to your Web proxy server.
a. The following packets might be generated:
Packet
Direction
Src Addr
Dest Addr
Protocol
Src Port
Dest Port
Action
7
In
10.1.2.3
172.16.3.4
TCP
25
8080
?
8
Out
172.16.3.4
10.1.2.3
TCP
8080
25
?
Explain why this attack will succeed, using the ruleset of the preceding problem.
b. When a TCP connection is initiated, the ACK bit in the TCP header is not set. 
Subsequently, all TCP headers sent over the TCP connection have the ACK bit set. 
Use this information to modify the ruleset of the preceding problem to prevent 
the attack just described.
 
12.8 
A common management requirement is that ‚Äúall external Web traffic must flow 
via the organization‚Äôs Web proxy.‚Äù However, that requirement is easier stated than 
implemented. Discuss the various problems and issues, possible solutions, and limita-
tions with supporting this requirement. In particular consider issues such as identify-
ing exactly what constitutes ‚ÄúWeb traffic‚Äù and how it may be monitored, given the 
large range of ports and various protocols used by Web browsers and servers.
 
12.9 
Consider the threat of ‚Äútheft/breach of proprietary or confidential information held 
in key data files on the system.‚Äù One method by which such a breach might occur is 
the accidental/deliberate e-mailing of information to a user outside of the organiza-
tion. A possible countermeasure to this is to require all external e-mail to be given a 
sensitivity tag (classification if you like) in its subject and for external e-mail to have 
the lowest sensitivity tag. Discuss how this measure could be implemented in a fire-
wall and what components and architecture would be needed to do this.
 
12.10 
You are given the following ‚Äúinformal firewall policy‚Äù details to be implemented 
using a firewall like that in Figure 12.2:
1. E-mail may be sent using SMTP in both directions through the firewall, but it must 
be relayed via the DMZ mail gateway that provides header sanitization and con-
tent filtering. External e-mail must be destined for the DMZ mail server.
2. Users inside may retrieve their e-mail from the DMZ mail gateway, using either 
POP3 or POP3S, and authenticate themselves.
3. Users outside may retrieve their e-mail from the DMZ mail gateway, but only if 
they use the secure POP3 protocol, and authenticate themselves.
4. Web requests (both insecure and secure) are allowed from any internal user out 
through the firewall but must be relayed via the DMZ Web proxy, which provides 
content filtering (noting this is not possible for secure requests), and users must 
authenticate with the proxy for logging.
5. Web requests (both insecure and secure) are allowed from anywhere on the 
Internet to the DMZ Web server.
6. DNS lookup requests by internal users allowed via the DMZ DNS server, which 
queries to the Internet.
7. External DNS requests are provided by the DMZ DNS server.
8. Management and update of information on the DMZ servers is allowed using 
secure shell connections from relevant authorized internal users (may have dif-
ferent sets of users on each system as appropriate).
9. SNMP management requests are permitted from the internal management hosts 
to the firewalls, with the firewalls also allowed to send management traps (i.e., 
notification of some event occurring) to the management hosts.
Design suitable packet filter rulesets (similar to those shown in Table 12.1) to be 
implemented on the ‚ÄúExternal Firewall‚Äù and the ‚ÄúInternal Firewall‚Äù to satisfy the 
aforementioned policy requirements.

432
Appendix A
Some ASpectS of number theory
A.1 Prime and Relatively Prime Numbers
Divisors
Prime Numbers
Relatively Prime Numbers
A.2 Modular Arithmetic

In this appendix, we provide some background on two concepts referenced in this 
book: prime numbers and modular arithmetic.
 A.1 Prime And relAtively Prime numbers
In this section, unless otherwise noted, we deal only with nonnegative integers. The 
use of negative integers would introduce no essential differences.
Divisors
We say that b ‚â† 0 divides a if a = mb for some m, where a, b, and m are integers. 
That is, b divides a if there is no remainder on division. The notation bÔøΩ a is com-
monly used to mean b divides a. Also, if bÔøΩ a, we say that b is a divisor of a. For 
example, the positive divisors of 24 are 1, 2, 3, 4, 6, 8, 12, and 24.
The following relations hold:
‚ñ†
‚ñ† If aÔøΩ 1, then a = {1
‚ñ†
‚ñ† If aÔøΩ b and bÔøΩ a, then a = {b
‚ñ†
‚ñ† Any b ‚â† 0 divides 0
‚ñ†
‚ñ† If bÔøΩ g and bÔøΩ h, then bÔøΩ (mg + nh) for arbitrary integers m and n
To see this last point, note that
If bÔøΩ g, then g is of the form g = b * g1 for some integer g1.
If bÔøΩ h, then h is of the form h = b * h1 for some integer h1.
So
mg + nh = mbg1 + nbh1 = b * (mg1 + nh1)
and therefore b divides mg + nh.
Prime Numbers
An integer p 7 1 is a prime number if its only divisors are {1 and {p. Prime 
 numbers play a critical role in number theory and in the techniques discussed in 
Chapter 3.
Any integer a 7 1 can be factored in a unique way as
a = p1
a1 * p2
a2 * c * pt
at
where p1 6 p2 6 c 6 pt are prime numbers and where each ai is a positive inte-
ger. For example, 91 = 7 * 13 and 11011 = 7 * 112 * 13.
It is useful to cast this another way. If P is the set of all prime numbers, then 
any positive integer can be written uniquely in the following form:
a = q
p‚ààP
pap where each ap √ö 0
The right-hand side is the product over all possible prime numbers p; for any par-
ticular value of a, most of the exponents ap will be 0.

The value of any given positive integer can be specified by simply listing all 
the nonzero exponents in the foregoing formulation. Thus, the integer 12 is repre-
sented by {a2 = 2, a3 = 1}, and the integer 18 is represented by {a2 = 1, a3 = 2}. 
Multiplication of two numbers is equivalent to adding the corresponding exponents:
k = mn S kp = mp + np  for all p
What does it mean, in terms of these prime factors, to say that aÔøΩ b? Any inte-
ger of the form pk can be divided only by an integer that is of a lesser or equal power 
of the same prime number, pj with j ‚Ä¶ k. Thus, we can say
aÔøΩ b S ap ‚Ä¶ bp  for all p
Relatively Prime Numbers
We will use the notation gcd(a, b) to mean the greatest common divisor of a and b. 
The positive integer c is said to be the greatest common divisor of a and b if
1. c is a divisor of a and of b.
2. Any divisor of a and b is a divisor of c.
An equivalent definition is the following:
gcd(a, b) = max[k, such that kÔøΩ a and kÔøΩ b]
Because we require that the greatest common divisor be positive, gcd(a, b) 
= gcd(a, -b) = gcd(-a, b) = gcd(-a, -b). In general, gcd(a, b) = gcd(ÔøΩ aÔøΩ , ÔøΩ bÔøΩ ). 
For example, gcd(60, 24) = gcd(60, -24) = 12. Also, because all nonzero integers 
divide 0, we have gcd(a, 0) = ÔøΩ aÔøΩ .
It is easy to determine the greatest common divisor of two positive integers if 
we express each integer as the product of primes. For example,
 300 = 22 * 31 * 52
 18 = 21 * 32
 gcd(18, 300) = 21 * 31 * 50 = 6
In general,
k = gcd(a, b)  S  kp = min(ap, bp) for all p
Determining the prime factors of a large number is no easy task, so the pre-
ceding relationship does not directly lead to a way of calculating the greatest com-
mon divisor.
The integers a and b are relatively prime if they have no prime factors in com-
mon, that is, if their only common factor is 1. This is equivalent to saying that a and 
b are relatively prime if gcd(a, b) = 1. For example, 8 and 15 are relatively prime 
because the divisors of 8 are 1, 2, 4, and 8, and the divisors of 15 are 1, 3, 5, and 15, 
so 1 is the only number on both lists.

A.2 modulAr Arithmetic
Given any positive integer n and any nonnegative integer a, if we divide a by n, 
we get an integer quotient q and an integer remainder r that obey the following 
relationship:
a = qn + r  0 ‚Ä¶ r 6 n;  q = :a/n;
where :x; is the largest integer less than or equal to x.
Figure A.1 a demonstrates that, given a and positive n, it is always possible to 
find q and r that satisfy the preceding relationship. Represent the integers on the 
number line; a will fall somewhere on that line (positive a is shown, a similar dem-
onstration can be made for negative a). Starting at 0, proceed to n, 2n, up to qn such 
that qn ‚Ä¶ a and (q + 1)n 7 a. The distance from qn to a is r, and we have found 
the unique values of q and r. The remainder r is often referred to as a residue.
If a is an integer and n is a positive integer, we define a mod n to be the re-
mainder when a is divided by n. Thus, for any integer a, we can always write:
a = :a/n; * n + (a mod n)
Two integers a and b are said to be congruent modulo n, if  (a mod n) = (b mod n). 
This is written a K b mod n. For example, 73 K 4 mod 23 and 21 K -9 mod 10. 
Note that if a K 0 mod n, then nÔøΩ a.
The modulo operator has the following properties:
1. a K b mod n if nÔøΩ (a - b)
2. (a mod n) = (b mod n) implies a K b mod n
0
n
2n
3n
qn
(q + 1)n
a
n
r
(a) General relationship
0
15
15
10
30
= 2    15
70
(b) Example: 70 = (4    15) + 10
45
= 3    15
60
= 4    15
75
= 5    15
Figure A.1 The Relationship a = qn + r; 0 ‚Ä¶ r 6 n

3. a K b mod n implies b K a mod n.
4. a K b mod n and b K c mod n imply a K c mod n.
To demonstrate the first point, if nÔøΩ (a - b), then (a - b) = kn for some k. 
So we can write a = b + kn. Therefore, (a mod n) = (remainder when  b + kn  
is divided by n) = (remainder when b is divided by n) = (b mod n). The remaining 
points are as easily proved.
The (mod n) operator maps all integers into the set of integers 
{0, 1, c , (n - 1)}. This suggests the question: Can we perform arithmetic opera-
tions within the confines of this set? It turns out that we can; the technique is known 
as modular arithmetic.
Modular arithmetic exhibits the following properties:
1. [(a mod n) + (b mod n)] mod n = (a + b) mod n
2. [(a mod n) - (b mod n)] mod n = (a - b) mod n
3. [(a mod n) * (b mod n)] mod n = (a * b) mod n
We demonstrate the first property. Define (a mod n) = ra and (b mod n) = rb. 
Then we can write a = ra + jn for some integer j and b = rb + kn for some integer 
k. Then
 (a + b) mod n = (ra + jn + rb + kn) mod n
 = (ra + rb + (k + j)n) mod n
 = (ra + rb) mod n
 = [(a mod n) + (b mod n)] mod n
The remaining properties are as easily proved.




Today‚Äôs Internet is arguably the largest engineered system ever created by  mankind, 
with hundreds of millions of connected computers, communication links, and  
switches; with billions of users who connect via laptops, tablets, and smartphones; 
and with an array of new Internet-connected ‚Äúthings‚Äù including game consoles, sur-
veillance systems, watches, eye glasses, thermostats, and cars. Given that the Inter-
net is so large and has so many diverse components and uses, is there any hope of 
understanding how it works? Are there guiding principles and structure that can 
provide a foundation for understanding such an amazingly large and complex sys-
tem? And if so, is it possible that it actually could be both interesting and fun to 
learn about computer networks? Fortunately, the answer to all of these questions is 
a resounding YES! Indeed, it‚Äôs our aim in this book to provide you with a modern 
introduction to the dynamic field of computer networking, giving you the princi-
ples and practical insights you‚Äôll need to understand not only today‚Äôs networks, but 
tomorrow‚Äôs as well.
This first chapter presents a broad overview of computer networking and the 
Internet. Our goal here is to paint a broad picture and set the context for the rest 
of this book, to see the forest through the trees. We‚Äôll cover a lot of ground in this 
introductory chapter and discuss a lot of the pieces of a computer network, without 
losing sight of the big picture.
We‚Äôll structure our overview of computer networks in this chapter as follows. 
After introducing some basic terminology and concepts, we‚Äôll first examine the basic 
hardware and software components that make up a network. We‚Äôll begin at the net-
work‚Äôs edge and look at the end systems and network applications running in the 
network. We‚Äôll then explore the core of a computer network, examining the links 
1
CHAPTER
Computer 
Networks and 
the Internet
1

and the switches that transport data, as well as the access networks and physical 
media that connect end systems to the network core. We‚Äôll learn that the Internet is 
a network of networks, and we‚Äôll learn how these networks connect with each other.
After having completed this overview of the edge and core of a computer net-
work, we‚Äôll take the broader and more abstract view in the second half of this chap-
ter. We‚Äôll examine delay, loss, and throughput of data in a computer network and 
provide simple quantitative models for end-to-end throughput and delay: models 
that take into account transmission, propagation, and queuing delays. We‚Äôll then 
introduce some of the key architectural principles in computer networking, namely, 
protocol layering and service models. We‚Äôll also learn that computer networks are 
vulnerable to many different types of attacks; we‚Äôll survey some of these attacks and 
consider how computer networks can be made more secure. Finally, we‚Äôll close this 
chapter with a brief history of computer networking.
1.1 What Is the Internet?
In this book, we‚Äôll use the public Internet, a specific computer network, as our prin-
cipal vehicle for discussing computer networks and their protocols. But what is the 
Internet? There are a couple of ways to answer this question. First, we can describe 
the nuts and bolts of the Internet, that is, the basic hardware and software components 
that make up the Internet. Second, we can describe the Internet in terms of a network-
ing infrastructure that provides services to distributed applications. Let‚Äôs begin with 
the nuts-and-bolts description, using Figure 1.1 to illustrate our discussion.
1.1.1 A Nuts-and-Bolts Description
The Internet is a computer network that interconnects billions of computing devices 
throughout the world. Not too long ago, these computing devices were primarily 
traditional desktop computers, Linux workstations, and so-called servers that store 
and transmit information such as Web pages and e-mail messages. Increasingly, 
however, users  connect to the Internet with smartphones and tablets‚Äîtoday, close 
to half of the world‚Äôs population are active mobile Internet users with the percentage 
expected to increase to 75% by 2025 [Statista 2019]. Furthermore, nontraditional 
Internet ‚Äúthings‚Äù such as TVs, gaming consoles, thermostats, home security systems, 
home appliances, watches, eye glasses, cars, traffic control systems, and more are 
being connected to the Internet. Indeed, the term computer network is beginning to 
sound a bit dated, given the many nontraditional devices that are being hooked up to 
the Internet. In Internet jargon, all of these devices are called hosts or end systems. 
By some estimates, there were about 18 billion devices connected to the Internet in 
2017, and the number will reach 28.5 billion by 2022 [Cisco VNI 2020].

Figure 1.1 ‚ô¶ Some pieces of the Internet
Key:
TrafÔ¨Åc light
Thermostat
Fridge
Datacenter
Workstation
Host
(= end system)
Mobile
Computer
Base
station
Router
Cell phone
tower
Smartphone
or tablet
Link-layer
switch
Server
Content Provider Network
National or
Global ISP
Datacenter Network
Datacenter Network
Mobile Network
Enterprise Network
Home Network
Local or
Regional ISP

End systems are connected together by a network of communication links and 
packet switches. We‚Äôll see in Section 1.2 that there are many types of communica-
tion links, which are made up of different types of physical media, including coaxial 
cable, copper wire, optical fiber, and radio spectrum. Different links can transmit 
data at different rates, with the transmission rate of a link measured in bits/second. 
When one end system has data to send to another end system, the sending end system 
segments the data and adds header bytes to each segment. The resulting packages 
of information, known as packets in the jargon of computer networks, are then sent 
through the network to the destination end system, where they are reassembled into 
the original data.
A packet switch takes a packet arriving on one of its incoming communication 
links and forwards that packet on one of its outgoing communication links. Packet 
switches come in many shapes and flavors, but the two most prominent types in 
today‚Äôs Internet are routers and link-layer switches. Both types of switches forward 
packets toward their ultimate destinations. Link-layer switches are typically used in 
access networks, while routers are typically used in the network core. The sequence 
of communication links and packet switches traversed by a packet from the send-
ing end system to the receiving end system is known as a route or path through 
the¬†network. Cisco predicts annual global IP traffic will reach nearly five zettabytes  
(1021¬†bytes) by 2022 [Cisco VNI 2020].
Packet-switched networks (which transport packets) are in many ways 
similar to transportation networks of highways, roads, and intersections (which 
transport vehicles). Consider, for example, a factory that needs to move a large 
amount of cargo to some destination warehouse located thousands of kilometers 
away. At the factory, the cargo is segmented and loaded into a fleet of trucks. 
Each of the trucks then independently travels through the network of highways, 
roads, and intersections to the destination warehouse. At the destination ware-
house, the cargo is unloaded and grouped with the rest of the cargo arriving 
from the same shipment. Thus, in many ways, packets are analogous to trucks, 
communication links are analogous to highways and roads, packet switches are 
analogous to intersections, and end systems are analogous to buildings. Just as 
a truck takes a path through the transportation network, a packet takes a path 
through a computer network.
End systems access the Internet through Internet Service Providers (ISPs), 
including residential ISPs such as local cable or telephone companies; corpo-
rate ISPs; university ISPs; ISPs that provide WiFi access in airports, hotels, cof-
fee shops, and other public places; and cellular data ISPs, providing mobile access 
to our  smartphones and other devices. Each ISP is in itself a network of packet 
switches and communication links. ISPs provide a variety of types of network access 
to the end systems, including residential broadband access such as cable modem 
or DSL, high-speed local area network access, and mobile wireless access. ISPs 
also  provide¬† Internet access to content providers, connecting servers directly to 
the  Internet. The Internet is all about connecting end systems to each other, so the

ISPs¬†that  provide access to end systems must also be interconnected. These lower-
tier ISPs are thus interconnected through national and international upper-tier ISPs 
and these upper-tier ISPs are connected  directly to each other. An upper-tier ISP 
consists of high-speed routers interconnected with high-speed fiber-optic links. Each 
ISP network, whether upper-tier or lower-tier, is managed independently, runs the 
IP protocol (see below), and conforms to certain naming and address conventions. 
We‚Äôll examine ISPs and their interconnection more closely in Section 1.3.
End systems, packet switches, and other pieces of the Internet run protocols that 
control the sending and receiving of information within the Internet. The Transmission  
Control Protocol (TCP) and the Internet Protocol (IP) are two of the most impor-
tant protocols in the Internet. The IP protocol specifies the format of the packets 
that are sent and received among routers and end systems. The Internet‚Äôs principal 
protocols are collectively known as TCP/IP. We‚Äôll begin looking into protocols in 
this introductory chapter. But that‚Äôs just a start‚Äîmuch of this book is concerned with 
networking protocols!
Given the importance of protocols to the Internet, it‚Äôs important that everyone 
agree on what each and every protocol does, so that people can create systems and 
products that interoperate. This is where standards come into play. Internet  standards 
are developed by the Internet Engineering Task Force (IETF) [IETF 2020]. The IETF 
standards documents are called requests for comments (RFCs). RFCs started out 
as general requests for comments (hence the name) to resolve network and protocol 
design problems that faced the precursor to the Internet [Allman 2011]. RFCs tend 
to¬†be quite technical and detailed. They define protocols such as TCP, IP, HTTP (for 
the Web), and SMTP (for e-mail). There are currently nearly 9000 RFCs. Other bod-
ies also specify standards for network components, most notably for network links. 
The IEEE 802 LAN Standards Committee [IEEE 802 2020], for example, specifies 
the Ethernet and wireless WiFi standards.
1.1.2 A Services Description
Our discussion above has identified many of the pieces that make up the Internet. 
But we can also describe the Internet from an entirely different angle‚Äînamely, as 
an¬†infrastructure that provides services to applications. In addition to traditional 
applications such as e-mail and Web surfing, Internet applications include mobile 
smartphone and tablet applications, including Internet messaging, mapping with 
real-time road-traffic information, music streaming movie and television streaming, 
online social media, video conferencing, multi-person games, and location-based 
recommendation systems. The applications are said to be distributed applications, 
since they involve multiple end systems that exchange data with each other. Impor-
tantly, Internet applications run on end systems‚Äîthey do not run in the packet 
switches in the network core. Although packet switches facilitate the exchange of 
data among end systems, they are not concerned with the application that is the 
source or sink of data.

Let‚Äôs explore a little more what we mean by an infrastructure that provides 
 services to applications. To this end, suppose you have an exciting new idea for a dis-
tributed Internet application, one that may greatly benefit humanity or one that may 
simply make you rich and famous. How might you go about transforming this idea 
into an actual Internet application? Because applications run on end systems, you are 
going to need to write programs that run on the end systems. You might, for example, 
write your programs in Java, C, or Python. Now, because you are developing a dis-
tributed Internet application, the programs running on the different end systems will 
need to send data to each other. And here we get to a central issue‚Äîone that leads 
to the alternative way of describing the Internet as a platform for applications. How 
does one program running on one end system instruct the Internet to deliver data to 
another program running on another end system?
End systems attached to the Internet provide a socket interface that speci-
fies how a program running on one end system asks the Internet infrastructure to 
deliver data to a specific destination program running on another end system. This 
Internet socket interface is a set of rules that the sending program must follow so 
that the Internet can deliver the data to the destination program. We‚Äôll discuss the 
Internet socket interface in detail in Chapter 2. For now, let‚Äôs draw upon a simple 
analogy, one that we will frequently use in this book. Suppose Alice wants to send 
a letter to Bob using the postal service. Alice, of course, can‚Äôt just write the letter 
(the data) and drop the letter out her window. Instead, the postal service requires 
that Alice put the letter in an envelope; write Bob‚Äôs full name, address, and zip 
code in the center of the envelope; seal the envelope; put a stamp in the upper-
right-hand corner of the envelope; and finally, drop the envelope into an official 
postal service mailbox. Thus, the postal service has its own ‚Äúpostal service inter-
face,‚Äù or set of rules, that Alice must follow to have the postal service deliver her 
letter to Bob. In a similar manner, the Internet has a socket interface that the pro-
gram sending data must follow to have the Internet deliver the data to the program 
that will receive the data.
The postal service, of course, provides more than one service to its custom-
ers. It provides express delivery, reception confirmation, ordinary use, and many 
more services. In a similar manner, the Internet provides multiple services to its 
applications. When you develop an Internet application, you too must choose one 
of the Internet‚Äôs services for your application. We‚Äôll describe the Internet‚Äôs ser-
vices in Chapter 2.
We have just given two descriptions of the Internet; one in terms of its hardware 
and software components, the other in terms of an infrastructure for providing ser-
vices to distributed applications. But perhaps you are still confused as to what the 
Internet is. What are packet switching and TCP/IP? What are routers? What kinds of 
communication links are present in the Internet? What is a distributed application? 
How can a thermostat or body scale be attached to the Internet? If you feel a bit over-
whelmed by all of this now, don‚Äôt worry‚Äîthe purpose of this book is to introduce 
you to both the nuts and bolts of the Internet and the principles that govern how and

why it works. We‚Äôll explain these important terms and questions in the following 
sections and chapters.
1.1.3 What Is a Protocol?
Now that we‚Äôve got a bit of a feel for what the Internet is, let‚Äôs consider another 
important buzzword in computer networking: protocol. What is a protocol? What 
does a protocol do?
A Human Analogy
It is probably easiest to understand the notion of a computer network protocol by 
first considering some human analogies, since we humans execute protocols all of 
the time. Consider what you do when you want to ask someone for the time of day. 
A typical exchange is shown in Figure 1.2. Human protocol (or good manners, at 
Figure 1.2 ‚ô¶ A human protocol and a computer network protocol
GET http://www.pearsonhighered.com/
cs-resources/
TCP connection request
Time
Time
TCP connection reply
<Ô¨Åle>
Hi
Got the time?
Time
Time
Hi
2:00

least) dictates that one first offer a greeting (the first ‚ÄúHi‚Äù in Figure 1.2) to initiate 
communication with someone else. The typical response to a ‚ÄúHi‚Äù is a returned 
‚ÄúHi‚Äù message. Implicitly, one then takes a cordial ‚ÄúHi‚Äù response as an indication 
that one can proceed and ask for the time of day. A different response to the initial 
‚ÄúHi‚Äù (such as ‚ÄúDon‚Äôt bother me!‚Äù or ‚ÄúI don‚Äôt speak English,‚Äù or some unprintable 
reply) might indicate an unwillingness or inability to communicate. In this case, 
the human protocol would be not to ask for the time of day. Sometimes one gets no 
response at all to a question, in which case one typically gives up asking that person 
for the time. Note that in our human protocol, there are specific messages we send, 
and specific actions we take in response to the received reply messages or other 
events (such as no reply within some given amount of time). Clearly, transmitted 
and received messages, and actions taken when these messages are sent or received 
or other events occur, play a central role in a human protocol. If people run differ-
ent protocols (for example, if one person has manners but the other does not, or if 
one understands the concept of time and the other does not) the protocols do not 
interoperate and no useful work can be accomplished. The same is true in network-
ing‚Äîit takes two (or more) communicating entities running the same protocol in 
order to accomplish a task.
Let‚Äôs consider a second human analogy. Suppose you‚Äôre in a college class (a 
computer networking class, for example!). The teacher is droning on about protocols 
and you‚Äôre confused. The teacher stops to ask, ‚ÄúAre there any questions?‚Äù (a message 
that is transmitted to, and received by, all students who are not sleeping). You raise 
your hand (transmitting an implicit message to the teacher). Your teacher acknowl-
edges you with a smile, saying ‚ÄúYes . . .‚Äù (a transmitted message encouraging you 
to ask your question‚Äîteachers love to be asked questions), and you then ask your 
question (that is, transmit your message to your teacher). Your teacher hears your 
question (receives your question message) and answers (transmits a reply to you). 
Once again, we see that the transmission and receipt of messages, and a set of con-
ventional actions taken when these messages are sent and received, are at the heart 
of this question-and-answer protocol.
Network Protocols
A network protocol is similar to a human protocol, except that the entities exchang-
ing messages and taking actions are hardware or software components of some 
device (for example, computer, smartphone, tablet, router, or other network-capable 
device). All activity in the Internet that involves two or more communicating remote 
entities is governed by a protocol. For example, hardware-implemented protocols in 
two physically connected computers control the flow of bits on the ‚Äúwire‚Äù between 
the two network interface cards; congestion-control protocols in end systems control 
the rate at which packets are transmitted between sender and receiver; protocols in 
routers determine a packet‚Äôs path from source to destination. Protocols are running

everywhere in the Internet, and consequently much of this book is about computer 
network protocols.
As an example of a computer network protocol with which you are probably 
familiar, consider what happens when you make a request to a Web server, that 
is, when you type the URL of a Web page into your Web browser. The scenario is 
illustrated in the right half of Figure 1.2. First, your computer will send a connec-
tion request message to the Web server and wait for a reply. The Web server will 
eventually receive your connection request message and return a connection reply 
message. Knowing that it is now OK to request the Web document, your computer 
then sends the name of the Web page it wants to fetch from that Web server in a 
GET message. Finally, the Web server returns the Web page (file) to your computer.
Given the human and networking examples above, the exchange of messages 
and the actions taken when these messages are sent and received are the key defining 
elements of a protocol:
A protocol defines the format and the order of messages exchanged between two 
or more communicating entities, as well as the actions taken on the transmission 
and/or receipt of a message or other event.
The Internet, and computer networks in general, make extensive use of pro-
tocols. Different protocols are used to accomplish different communication tasks. 
As you read through this book, you will learn that some protocols are simple and 
straightforward, while others are complex and intellectually deep. Mastering the 
field of computer networking is equivalent to understanding the what, why, and how 
of networking protocols.
1.2 The Network Edge
In the previous section, we presented a high-level overview of the Internet and 
 networking protocols. We are now going to delve a bit more deeply into the com-
ponents of the Internet. We begin in this section at the edge of the network and 
look at the components with which we are most  familiar‚Äînamely, the computers, 
smartphones and other devices that we use on a daily basis. In the next section, we‚Äôll 
move from the network edge to the network core and examine switching and routing 
in computer networks.
Recall from the previous section that in computer networking jargon, the com-
puters and other devices connected to the Internet are often referred to as end sys-
tems. They are referred to as end systems because they sit at the edge of the Internet,  
as shown in Figure 1.3. The Internet‚Äôs end systems include desktop computers

Figure 1.3 ‚ô¶ End-system interaction
Content Provider Network
National or
Global ISP
Datacenter Network
Datacenter Network
Mobile Network
Enterprise Network
Home Network
Local or
Regional ISP
(e.g., desktop PCs, Macs, and Linux boxes), servers (e.g., Web and e-mail servers),  
and mobile devices (e.g., laptops, smartphones, and tablets). Furthermore, an 
increasing number of non-traditional ‚Äúthings‚Äù are being attached to the Internet as 
end  systems (see the Case History feature).
End systems are also referred to as hosts because they host (that is, run) appli-
cation programs such as a Web browser program, a Web server program, an e-mail

client program, or an e-mail server program. Throughout this book we will use the 
terms hosts and end systems interchangeably; that is, host = end system. Hosts 
are sometimes further divided into two categories: clients and servers. Infor-
mally, clients tend to be desktops, laptops, smartphones, and so on, whereas 
servers tend to be more powerful machines that store and distribute Web pages, 
stream video, relay e-mail, and so on. Today, most of the servers from which we 
receive search results, e-mail, Web pages, videos and mobile app content reside 
in large data centers. For example, as of 2020, Google has 19 data centers on four 
continents, collectively containing several million servers. Figure 1.3 includes 
two such data centers, and the Case History sidebar describes data centers in 
more detail.
DATA CENTERS AND CLOUD COMPUTING
Internet companies such as Google, Microsoft, Amazon, and Alibaba have built 
massive data centers, each housing tens to hundreds of thousands of hosts. These 
data centers are not only connected to the Internet, as shown in Figure 1.1, but also 
internally include complex computer networks that interconnect the datacenter‚Äôs hosts. 
The data centers are the engines behind the Internet applications that we use on a 
daily basis.
Broadly speaking, data centers serve three purposes, which we describe here in 
the context of Amazon for concreteness. First, they serve Amazon e-commerce pages 
to users, for example, pages describing products and purchase information. Second, 
they serve as massively parallel computing infrastructures for Amazon-specific data 
processing tasks. Third, they provide cloud computing to other companies. Indeed, 
today a major trend in computing is for companies to use a cloud provider such as 
Amazon to handle essentially all of their IT needs. For example, Airbnb and many 
other Internet-based companies do not own and manage their own data centers but 
instead run their entire Web-based services in the Amazon cloud, called Amazon 
Web Services (AWS).
The worker bees in a data center are the hosts. They serve content (e.g., Web 
pages and videos), store e-mails and documents, and collectively perform massively 
distributed computations. The hosts in data centers, called blades¬†and resembling 
pizza boxes, are generally commodity hosts that include CPU, memory, and disk 
storage. The hosts are stacked in racks, with each rack typically having 20 to 
40¬†blades. The racks are then interconnected using sophisticated and evolving data 
center network designs. Data center networks are discussed in greater detail in 
Chapter 6.
CASE HISTORY

1.2.1 Access Networks
Having considered the applications and end systems at the ‚Äúedge of the network,‚Äù 
let‚Äôs next consider the access network‚Äîthe network that physically connects an end 
system to the first router (also known as the ‚Äúedge router‚Äù) on a path from the end 
system to any other distant end system. Figure 1.4 shows several types of access 
Figure 1.4 ‚ô¶ Access networks
Content Provider Network
National or
Global ISP
Datacenter Network
Datacenter Network
Mobile Network
Enterprise Network
Home Network
Local or
Regional ISP

networks with thick, shaded lines and the settings (home, enterprise, and wide-area 
mobile wireless) in which they are used.
Home Access: DSL, Cable, FTTH, and 5G Fixed Wireless
As of 2020, more than 80% of the households in Europe and the USA have Internet 
access [Statista 2019]. Given this widespread use of home access networks let‚Äôs begin 
our overview of access networks by considering how homes connect to the Internet.
Today, the two most prevalent types of broadband residential access are 
digital subscriber line (DSL) and cable. A residence typically obtains DSL 
Internet access from the same local telephone company (telco) that provides its 
wired local phone access. Thus, when DSL is used, a customer‚Äôs telco is also 
its ISP. As shown in Figure 1.5, each customer‚Äôs DSL modem uses the existing 
telephone line exchange data with a digital subscriber line access multiplexer 
(DSLAM) located in the telco‚Äôs local central office (CO). The home‚Äôs DSL 
modem takes digital data and translates it to high- frequency tones for transmis-
sion over telephone wires to the CO; the analog signals from many such houses 
are translated back into digital format at the DSLAM.
The residential telephone line carries both data and traditional telephone signals 
simultaneously, which are encoded at different frequencies:
‚Ä¢ A high-speed downstream channel, in the 50 kHz to 1 MHz band
‚Ä¢ A medium-speed upstream channel, in the 4 kHz to 50 kHz band
‚Ä¢ An ordinary two-way telephone channel, in the 0 to 4 kHz band
This approach makes the single DSL link appear as if there were three separate 
links, so that a telephone call and an Internet connection can share the DSL link at 
Figure 1.5 ‚ô¶ DSL Internet access
Home PC
Home
phone
DSL
modem
Internet
Telephone
network
Splitter
Existing phone line:
0-4KHz phone; 4-50KHz
upstream data; 50KHz‚Äì
1MHz downstream data
Central
ofÔ¨Åce
DSLAM

the same time. (We‚Äôll describe this technique of frequency-division multiplexing 
in Section 1.3.1.) On the customer side, a splitter separates the data and telephone 
signals arriving to the home and forwards the data signal to the DSL modem. On the 
telco side, in the CO, the DSLAM separates the data and phone signals and sends 
the data into the Internet. Hundreds or even thousands of households connect to a 
single DSLAM.
The DSL standards define multiple transmission rates, including downstream 
transmission rates of 24 Mbs and 52 Mbs, and upstream rates of 3.5 Mbps and 
16¬†Mbps; the newest standard provides for aggregate upstream plus downstream 
rates of 1 Gbps [ITU 2014]. Because the downstream and upstream rates are dif-
ferent, the access is said to be asymmetric. The actual downstream and upstream 
transmission rates achieved may be less than the rates noted above, as the DSL 
provider may purposefully limit a residential rate when tiered service (different 
rates, available at different prices) are offered. The maximum rate is also limited 
by the distance between the home and the CO, the gauge of the twisted-pair line 
and the degree of electrical interference. Engineers have expressly designed DSL 
for short distances between the home and the CO; generally, if the residence is not 
located within 5 to 10 miles of the CO, the residence must resort to an alternative 
form of Internet access.
While DSL makes use of the telco‚Äôs existing local telephone infrastructure, 
cable Internet access makes use of the cable television company‚Äôs existing cable 
television infrastructure. A residence obtains cable Internet access from the same 
company that provides its cable television. As illustrated in Figure 1.6, fiber optics 
Figure 1.6 ‚ô¶ A hybrid fiber-coaxial access network
Fiber
cable
Coaxial cable
Hundreds
of homes
Cable head end
Hundreds
of homes
Fiber
node
Fiber
node
Internet
CMTS

connect the cable head end to neighborhood-level junctions, from which tradi-
tional coaxial cable is then used to reach individual houses and apartments. Each 
neighborhood junction typically supports 500 to 5,000 homes. Because both fiber 
and coaxial cable are employed in this system, it is often referred to as hybrid fiber 
coax (HFC).
Cable internet access requires special modems, called cable modems. As 
with a DSL modem, the cable modem is typically an external device and con-
nects to the home PC through an Ethernet port. (We will discuss Ethernet in 
great detail in Chapter 6.) At the cable head end, the cable modem termination 
system (CMTS) serves a similar function as the DSL network‚Äôs DSLAM‚Äî
turning the analog signal sent from the cable modems in many downstream 
homes back into digital format. Cable modems divide the HFC network into two 
channels, a downstream and an upstream channel. As with DSL, access is typi-
cally asymmetric, with the downstream channel typically allocated a higher 
transmission rate than the upstream channel. The DOCSIS 2.0 and 3.0 standards 
define downstream bitrates of 40 Mbps and 1.2 Gbps, and upstream rates 
of 30 Mbps and 100 Mbps, respectively. As in the case of DSL networks, the 
 maximum achievable rate may not be realized due to lower contracted data rates 
or media impairments.
One important characteristic of cable Internet access is that it is a shared broad-
cast medium. In particular, every packet sent by the head end travels downstream on 
every link to every home and every packet sent by a home travels on the upstream 
channel to the head end. For this reason, if several users are simultaneously down-
loading a video file on the downstream channel, the actual rate at which each user 
receives its video file will be significantly lower than the aggregate cable down-
stream rate. On the other hand, if there are only a few active users and they are all 
Web surfing, then each of the users may actually receive Web pages at the full cable 
downstream rate, because the users will rarely request a Web page at exactly the 
same time. Because the upstream channel is also shared, a distributed multiple access 
protocol is needed to coordinate transmissions and avoid collisions. (We‚Äôll discuss 
this collision issue in some detail in Chapter 6.)
Although DSL and cable networks currently represent the majority of residential 
broadband access in the United States, an up-and-coming technology that provides 
even higher speeds is fiber to the home (FTTH) [Fiber Broadband 2020]. As the 
name suggests, the FTTH concept is simple‚Äîprovide an optical fiber path from 
the¬†CO directly to the home. FTTH can potentially provide Internet access rates in 
the gigabits per second range.
There are several competing technologies for optical distribution from the CO 
to the homes. The simplest optical distribution network is called direct fiber, with 
one fiber leaving the CO for each home. More commonly, each fiber leaving the 
central office is actually shared by many homes; it is not until the fiber gets rela-
tively close to the homes that it is split into individual customer-specific fibers. 
There are two competing optical-distribution network architectures that perform

this splitting: active optical networks (AONs) and passive optical networks (PONs). 
AON is essentially switched Ethernet, which is discussed in Chapter 6.
Here, we briefly discuss PON, which is used in Verizon‚Äôs FiOS service. 
Figure 1.7 shows FTTH using the PON distribution architecture. Each home has 
an optical network terminator (ONT), which is connected by dedicated optical 
fiber to a neighborhood splitter. The splitter combines a number of homes (typi-
cally less than 100) onto a single, shared optical fiber, which connects to an optical 
line  terminator (OLT) in the telco‚Äôs CO. The OLT, providing conversion between 
optical and electrical signals, connects to the Internet via a telco router. At home, 
users connect a home router (typically a wireless router) to the ONT and access the 
 Internet via this home router. In the PON architecture, all packets sent from OLT to 
the splitter are replicated at the splitter (similar to a cable head end).
In addition to DSL, Cable, and FTTH, 5G fixed wireless is beginning to be 
deployed. 5G fixed wireless not only promises high-speed residential access, but 
will do so without installing costly and failure-prone cabling from the telco‚Äôs 
CO to the home. With 5G fixed wireless, using beam-forming technology, data 
is sent wirelessly from a provider‚Äôs base station to the a modem in the home. 
A WiFi wireless router is connected to the modem (possibly bundled together), 
similar to how a WiFi wireless router is connected to a cable or DSL modem. 
5G cellular networks are covered in Chapter 7.
Access in the Enterprise (and the Home): Ethernet and WiFi
On corporate and university campuses, and increasingly in home settings, a local 
area¬†network (LAN) is used to connect an end system to the edge router. Although 
there are many types of LAN technologies, Ethernet is by far the most preva-
lent access technology in corporate, university, and home networks. As shown in 
Figure 1.7 ‚ô¶ FTTH Internet access
Internet
Central ofÔ¨Åce
Optical
splitter
ONT
ONT
ONT
OLT
Optical
Ô¨Åbers

Figure 1.8, Ethernet users use twisted-pair copper wire to connect to an Ethernet 
switch, a¬†technology discussed in detail in Chapter 6. The Ethernet switch, or a 
network of such interconnected switches, is then in turn connected into the larger 
Internet. With Ethernet access, users typically have 100 Mbps to tens of Gbps 
access to the Ethernet switch, whereas servers may have 1 Gbps 10 Gbps access.
Increasingly, however, people are accessing the Internet wirelessly from lap-
tops, smartphones, tablets, and other ‚Äúthings‚Äù. In a wireless LAN setting, wireless 
users transmit/receive packets to/from an access point that is connected into the 
enterprise‚Äôs network (most likely using wired Ethernet), which in turn is connected 
to the wired Internet. A wireless LAN user must typically be within a few tens of 
meters of the access point. Wireless LAN access based on IEEE 802.11 technol-
ogy, more colloquially known as WiFi, is now just about everywhere‚Äîuniversities, 
business offices, cafes, airports, homes, and even in airplanes. As discussed in detail 
in Chapter 7, 802.11 today provides a shared transmission rate of up to more than 
100 Mbps.
Even though Ethernet and WiFi access networks were initially deployed in 
enterprise (corporate, university) settings, they are also common components of 
home networks. Many homes combine broadband residential access (that is, cable 
modems or DSL) with these inexpensive wireless LAN technologies to create pow-
erful home networks Figure 1.9 shows a typical home network. This home network 
consists of a roaming laptop, multiple Internet-connected home appliances, as well 
as a wired PC; a base station (the wireless access point), which communicates with 
the wireless PC and other wireless devices in the home; and a home router that con-
nects the wireless access point, and any other wired home devices, to the Internet. 
This network allows household members to have broadband access to the Internet 
with one member roaming from the kitchen to the backyard to the bedrooms.
Figure 1.8 ‚ô¶ Ethernet Internet access
Ethernet
switch
Institutional
router
1 Gbps
1 Gbps
1 Gbps
Server
To Institution‚Äôs
ISP

Wide-Area Wireless Access: 3G and LTE 4G and 5G
Mobile devices such as iPhones and Android devices are being used to message, share 
photos in social networks, make mobile payments, watch movies, stream music, and 
much more while on the run. These devices employ the same wireless infrastructure 
used for cellular telephony to send/receive packets through a base station that is oper-
ated by the cellular network provider. Unlike WiFi, a user need only be within a few 
tens of kilometers (as opposed to a few tens of meters) of the base station.
Telecommunications companies have made enormous investments in so-called 
fourth-generation (4G) wireless, which provides real-world download speeds of up to 
60 Mbps. But even higher-speed wide-area access technologies‚Äîa fifth-generation 
(5G) of wide-area wireless networks‚Äîare already being deployed. We‚Äôll cover the 
basic principles of wireless networks and mobility, as well as WiFi, 4G and 5G tech-
nologies (and more!) in Chapter 7.
1.2.2 Physical Media
In the previous subsection, we gave an overview of some of the most important 
network access technologies in the Internet. As we described these technologies, 
we also indicated the physical media used. For example, we said that HFC uses a 
combination of fiber cable and coaxial cable. We said that DSL and Ethernet use 
copper wire. And we said that mobile access networks use the radio spectrum. In this 
subsection, we provide a brief overview of these and other transmission media that 
are commonly used in the Internet.
In order to define what is meant by a physical medium, let us reflect on the 
brief life of a bit. Consider a bit traveling from one end system, through a series 
of links and routers, to another end system. This poor bit gets kicked around 
and transmitted many, many times! The source end system first transmits the 
Figure 1.9 ‚ô¶ A typical home network
Cable
head end
Internet
Home Network

bit, and shortly thereafter the first router in the series receives the bit; the first 
router then transmits the bit, and shortly thereafter the second router receives the 
bit; and so on. Thus our bit, when traveling from source to destination, passes 
through a series of transmitter-receiver pairs. For each transmitter-receiver pair, 
the bit is sent by propagating electromagnetic waves or optical pulses across a 
physical medium. The physical medium can take many shapes and forms and 
does not have to be of the same type for each transmitter-receiver pair along 
the path. Examples of physical media include twisted-pair copper wire, coaxial 
cable, multimode fiber-optic cable, terrestrial radio spectrum, and satellite radio 
spectrum. Physical media fall into two categories: guided media and unguided 
media. With guided media, the waves are guided along a solid medium, such as 
a fiber-optic cable, a twisted-pair copper wire, or a coaxial cable. With unguided 
media, the waves propagate in the atmosphere and in outer space, such as in a 
wireless LAN or a digital satellite channel.
But before we get into the characteristics of the various media types, let us say a 
few words about their costs. The actual cost of the physical link (copper wire, fiber-
optic cable, and so on) is often relatively minor compared with other networking 
costs. In particular, the labor cost associated with the installation of the physical link 
can be orders of magnitude higher than the cost of the material. For this reason, many 
builders install twisted pair, optical fiber, and coaxial cable in every room in a build-
ing. Even if only one medium is initially used, there is a good chance that another 
medium could be used in the near future, and so money is saved by not having to lay 
additional wires in the future.
Twisted-Pair Copper Wire
The least expensive and most commonly used guided transmission medium is 
twisted-pair copper wire. For over a hundred years it has been used by telephone 
networks. In fact, more than 99 percent of the wired connections from the telephone 
handset to the local telephone switch use twisted-pair copper wire. Most of us have 
seen twisted pair in our homes (or those of our parents or grandparents!) and work 
environments. Twisted pair consists of two insulated copper wires, each about 1 mm 
thick, arranged in a regular spiral pattern. The wires are twisted together to reduce the 
electrical interference from similar pairs close by. Typically, a number of pairs are 
bundled together in a cable by wrapping the pairs in a protective shield. A wire pair 
constitutes a single communication link. Unshielded twisted pair (UTP) is com-
monly used for computer networks within a building, that is, for LANs. Data rates 
for LANs using twisted pair today range from 10 Mbps to 10 Gbps. The data rates 
that can be achieved depend on the thickness of the wire and the distance between 
transmitter and receiver.
When fiber-optic technology emerged in the 1980s, many people dispar-
aged twisted pair because of its relatively low bit rates. Some people even felt

that fiber-optic technology would completely replace twisted pair. But twisted 
pair did not give up so easily. Modern twisted-pair technology, such as category 
6a cable, can achieve data rates of 10 Gbps for distances up to a hundred meters. 
In the end, twisted pair has emerged as the dominant solution for high-speed 
LAN networking.
As discussed earlier, twisted pair is also commonly used for residential Inter-
net access. We saw that dial-up modem technology enables access at rates of up to 
56¬†kbps over twisted pair. We also saw that DSL (digital subscriber line) technology 
has enabled residential users to access the Internet at tens of Mbps over twisted pair 
(when users live close to the ISP‚Äôs central office).
Coaxial Cable
Like twisted pair, coaxial cable consists of two copper conductors, but the two con-
ductors are concentric rather than parallel. With this construction and special insula-
tion and shielding, coaxial cable can achieve high data transmission rates. Coaxial 
cable is quite common in cable television systems. As we saw earlier, cable televi-
sion systems have recently been coupled with cable modems to provide residential 
users with Internet access at rates of hundreds of Mbps. In cable television and cable 
Internet access, the transmitter shifts the digital signal to a specific frequency band, 
and the resulting analog signal is sent from the transmitter to one or more receivers. 
Coaxial cable can be used as a guided shared medium. Specifically, a number of 
end systems can be connected directly to the cable, with each of the end systems 
receiving whatever is sent by the other end systems.
Fiber Optics
An optical fiber is a thin, flexible medium that conducts pulses of light, with each 
pulse representing a bit. A single optical fiber can support tremendous bit rates, up 
to tens or even hundreds of gigabits per second. They are immune to electromagnetic 
interference, have very low signal attenuation up to 100 kilometers, and are very hard 
to tap. These characteristics have made fiber optics the preferred long-haul guided 
transmission media, particularly for overseas links. Many of the long-distance tele-
phone networks in the United States and elsewhere now use fiber optics exclusively. 
Fiber optics is also prevalent in the backbone of the Internet. However, the high cost 
of optical devices‚Äîsuch as transmitters, receivers, and switches‚Äîhas hindered their 
deployment for short-haul transport, such as in a LAN or into the home in a resi-
dential access network. The Optical Carrier (OC) standard link speeds range from 
51.8¬†Mbps to 39.8 Gbps; these specifications are often referred to as OC-n, where 
the link speed equals n √ó 51.8 Mbps. Standards in use today include OC-1, OC-3, 
OC-12, OC-24, OC-48, OC-96, OC-192, OC-768.

Terrestrial Radio Channels
Radio channels carry signals in the electromagnetic spectrum. They are an attrac-
tive medium because they require no physical wire to be installed, can penetrate 
walls, provide connectivity to a mobile user, and can potentially carry a signal 
for long distances. The characteristics of a radio channel depend significantly 
on the propagation environment and the distance over which a signal is to be 
carried. Environmental considerations determine path loss and shadow fad-
ing (which decrease the signal strength as the signal travels over a distance and 
around/through obstructing objects), multipath fading (due to signal reflection off 
of interfering objects), and interference (due to other transmissions and electro-
magnetic signals).
Terrestrial radio channels can be broadly classified into three groups: those that 
operate over very short distance (e.g., with one or two meters); those that operate in 
local areas, typically spanning from ten to a few hundred meters; and those that oper-
ate in the wide area, spanning tens of kilometers. Personal devices such as wireless 
headsets, keyboards, and medical devices operate over short distances; the wireless 
LAN technologies described in Section 1.2.1 use local-area radio channels; the cel-
lular access technologies use wide-area radio channels. We‚Äôll discuss radio channels 
in detail in Chapter 7.
Satellite Radio Channels
A communication satellite links two or more Earth-based microwave transmitter/ 
receivers, known as ground stations. The satellite receives transmissions on 
one frequency band, regenerates the signal using a repeater (discussed below), 
and transmits the signal on another frequency. Two types of satellites are used 
in¬† communications: geostationary satellites and low-earth orbiting (LEO) 
satellites.
Geostationary satellites permanently remain above the same spot on Earth. 
This stationary presence is achieved by placing the satellite in orbit at 36,000 kilo-
meters above Earth‚Äôs surface. This huge distance from ground station through 
satellite back to ground station introduces a substantial signal propagation delay 
of 280 milliseconds. Nevertheless, satellite links, which can operate at speeds of 
hundreds of Mbps, are often used in areas without access to DSL or cable-based 
Internet access.
LEO satellites are placed much closer to Earth and do not remain permanently 
above one spot on Earth. They rotate around Earth (just as the Moon does) and may 
communicate with each other, as well as with ground stations. To provide continuous 
coverage to an area, many satellites need to be placed in orbit. There are currently  
many low-altitude communication systems in development. LEO satellite  technology 
may be used for Internet access sometime in the future.

Figure 1.10 ‚ô¶ The network core
Content Provider Network
National or
Global ISP
Datacenter Network
Datacenter Network
Mobile Network
Enterprise Network
Home Network
Local or
Regional ISP
1.3 The Network Core
Having examined the Internet‚Äôs edge, let us now delve more deeply inside the 
network core‚Äîthe mesh of packet switches and links that interconnects the 
Internet‚Äôs end systems. Figure 1.10 highlights the network core with thick, 
shaded lines.

1.3.1 Packet Switching
In a network application, end systems exchange messages with each other. Mes-
sages can contain anything the application designer wants. Messages may perform 
a control function (for example, the ‚ÄúHi‚Äù messages in our handshaking example in 
Figure 1.2) or can contain data, such as an e-mail message, a JPEG image, or an 
MP3 audio file. To send a message from a source end system to a destination end 
system, the source breaks long messages into smaller chunks of data known as pack-
ets. Between source and destination, each packet travels through communication 
links and packet switches (for which there are two predominant types, routers and 
link-layer switches). Packets are transmitted over each communication link at a rate 
equal to the full transmission rate of the link. So, if a source end system or a packet 
switch is sending a packet of L bits over a link with transmission rate R bits/sec, then 
the time to transmit the packet is L / R seconds.
Store-and-Forward Transmission
Most packet switches use store-and-forward transmission at the inputs to the 
links. Store-and-forward transmission means that the packet switch must receive 
the entire packet before it can begin to transmit the first bit of the packet onto the 
outbound link. To explore store-and-forward transmission in more detail, consider 
a simple network consisting of two end systems connected by a single router, as 
shown in Figure 1.11. A router will typically have many incident links, since its 
job is to switch an incoming packet onto an outgoing link; in this simple example, 
the router has the rather simple task of transferring a packet from one (input) link 
to the only other attached link. In this example, the source has three packets, each 
consisting of L bits, to send to the destination. At the snapshot of time shown in 
Figure 1.11, the source has transmitted some of packet 1, and the front of packet 1 
has already arrived at the router. Because the router employs store-and-forwarding, 
at this instant of time, the router cannot transmit the bits it has received; instead it 
must first buffer (i.e., ‚Äústore‚Äù) the packet‚Äôs bits. Only after the router has received 
all of the packet‚Äôs bits can it begin to transmit (i.e., ‚Äúforward‚Äù) the packet onto the 
outbound link. To gain some insight into store-and-forward transmission, let‚Äôs now 
calculate the amount of time that elapses from when the source begins to send the 
packet until the destination has received the entire packet. (Here we will ignore 
propagation delay‚Äîthe time it takes for the bits to travel across the wire at near 
the speed of light‚Äîwhich will be discussed in Section 1.4.) The source begins to 
transmit at time 0; at time L/R seconds, the source has transmitted the entire packet, 
and the entire packet has been received and stored at the router (since there is no 
propagation delay). At time L/R seconds, since the router has just received the entire 
packet, it can begin to transmit the packet onto the outbound link towards the des-
tination; at time 2L/R, the router has transmitted the entire packet, and the entire 
packet has been received by the destination. Thus, the total delay is 2L/R. If the

switch instead forwarded bits as soon as they arrive (without first receiving the entire 
packet), then the total delay would be L/R since bits are not held up at the router. 
But, as we will discuss in Section 1.4, routers need to receive, store, and process the 
entire packet before forwarding.
Now let‚Äôs calculate the amount of time that elapses from when the source begins 
to send the first packet until the destination has received all three packets. As before, 
at time L/R, the router begins to forward the first packet. But also at time L/R the 
source will begin to send the second packet, since it has just finished sending the 
entire first packet. Thus, at time 2L/R, the destination has received the first packet 
and the router has received the second packet. Similarly, at time 3L/R, the destina-
tion has received the first two packets and the router has received the third packet. 
Finally, at time 4L/R the destination has received all three packets!
Let‚Äôs now consider the general case of sending one packet from source to des-
tination over a path consisting of N links each of rate R (thus, there are N-1 routers 
between source and destination). Applying the same logic as above, we see that the 
end-to-end delay is:
 
dend@to@end = N L
R 
(1.1)
You may now want to try to determine what the delay would be for P packets sent 
over a series of N links.
Queuing Delays and Packet Loss
Each packet switch has multiple links attached to it. For each attached link, the 
packet switch has an output buffer (also called an output queue), which stores 
packets that the router is about to send into that link. The output buffers play a key 
role in packet switching. If an arriving packet needs to be transmitted onto a link but 
finds the link busy with the transmission of another packet, the arriving packet must 
wait in the output buffer. Thus, in addition to the store-and-forward delays, packets 
suffer output buffer queuing delays. These delays are variable and depend on the 
Figure 1.11 ‚ô¶ Store-and-forward packet switching
Source
R bps
1
2
Destination
Front of packet 1
stored in router,
awaiting remaining
bits before forwarding
3

level of congestion in the network. Since the amount of buffer space is finite, an 
arriving packet may find that the buffer is completely full with other packets waiting 
for transmission. In this case, packet loss will occur‚Äîeither the arriving packet or 
one of the already-queued packets will be dropped.
Figure 1.12 illustrates a simple packet-switched network. As in Figure 1.11, 
packets are represented by three-dimensional slabs. The width of a slab represents 
the number of bits in the packet. In this figure, all packets have the same width and 
hence the same length. Suppose Hosts A and B are sending packets to Host E. Hosts 
A and B first send their packets along 100 Mbps Ethernet links to the first router. 
The router then directs these packets to the 15 Mbps link. If, during a short interval 
of time, the arrival rate of packets to the router (when converted to bits per second) 
exceeds 15 Mbps, congestion will occur at the router as packets queue in the link‚Äôs 
output buffer before being transmitted onto the link. For example, if Host A and B 
each send a burst of five packets back-to-back at the same time, then most of these 
packets will spend some time waiting in the queue. The situation is, in fact, entirely 
analogous to many common-day situations‚Äîfor example, when we wait in line for a 
bank teller or wait in front of a tollbooth. We‚Äôll examine this queuing delay in more 
detail in Section 1.4.
Forwarding Tables and Routing Protocols
Earlier, we said that a router takes a packet arriving on one of its attached com-
munication links and forwards that packet onto another one of its attached 
communication links. But how does the router determine which link it should 
Figure 1.12 ‚ô¶ Packet switching
100 Mbps Ethernet
Key:
Packets
A
B
C
D
E
15 Mbps
Queue of
packets waiting
for output link

forward the packet onto? Packet forwarding is actually done in different ways in 
different types of computer networks. Here, we briefly describe how it is done 
in the Internet.
In the Internet, every end system has an address called an IP address. When 
a source end system wants to send a packet to a destination end system, the 
source includes the destination‚Äôs IP address in the packet‚Äôs header. As with postal 
addresses, this address has a hierarchical structure. When a packet arrives at a router 
in the network, the router examines a portion of the packet‚Äôs destination address 
and forwards the packet to an adjacent router. More specifically, each router has 
a forwarding table that maps destination addresses (or portions of the destination 
addresses) to that router‚Äôs outbound links. When a packet arrives at a router, the 
router examines the address and searches its forwarding table, using this destination 
address, to find the appropriate outbound link. The router then directs the packet to 
this outbound link.
The end-to-end routing process is analogous to a car driver who does not 
use maps but instead prefers to ask for directions. For example, suppose Joe is 
driving from Philadelphia to 156 Lakeside Drive in Orlando, Florida. Joe first 
drives to his neighborhood gas station and asks how to get to 156 Lakeside Drive 
in Orlando, Florida. The gas station attendant extracts the Florida portion of the 
address and tells Joe that he needs to get onto the interstate highway I-95 South, 
which has an entrance just next to the gas station. He also tells Joe that once he 
enters Florida, he should ask someone else there. Joe then takes I-95 South until he 
gets to Jacksonville, Florida, at which point he asks another gas station attendant 
for directions. The attendant extracts the Orlando portion of the address and tells 
Joe that he should continue on I-95 to Daytona Beach and then ask someone else. 
In Daytona Beach, another gas station attendant also extracts the Orlando portion 
of the address and tells Joe that he should take I-4 directly to Orlando. Joe takes 
I-4 and gets off at the Orlando exit. Joe goes to another gas station attendant, and 
this time the attendant extracts the Lakeside Drive portion of the address and tells 
Joe the road he must follow to get to Lakeside Drive. Once Joe reaches Lakeside 
Drive, he asks a kid on a bicycle how to get to his destination. The kid extracts the 
156 portion of the address and points to the house. Joe finally reaches his ultimate 
destination. In the above analogy, the gas station attendants and kids on bicycles 
are analogous to routers.
We just learned that a router uses a packet‚Äôs destination address to index a for-
warding table and determine the appropriate outbound link. But this statement begs 
yet another question: How do forwarding tables get set? Are they configured by hand 
in each and every router, or does the Internet use a more automated procedure? This 
issue will be studied in depth in Chapter 5. But to whet your appetite here, we‚Äôll note 
now that the Internet has a number of special routing protocols that are used to auto-
matically set the forwarding tables. A routing protocol may, for example, determine 
the shortest path from each router to each destination and use the shortest path results 
to configure the forwarding tables in the routers.

1.3.2 Circuit Switching
There are two fundamental approaches to moving data through a network of links 
and switches: circuit switching and packet switching. Having covered packet-
switched networks in the previous subsection, we now turn our attention to circuit-
switched networks.
In circuit-switched networks, the resources needed along a path (buffers, link 
transmission rate) to provide for communication between the end systems are 
reserved for the duration of the communication session between the end systems.  
In packet-switched networks, these resources are not reserved; a session‚Äôs messages 
use the resources on demand and, as a consequence, may have to wait (that is, queue) 
for access to a communication link. As a simple analogy, consider two restaurants, 
one that requires reservations and another that neither requires reservations nor 
accepts them. For the restaurant that requires reservations, we have to go through 
the hassle of calling before we leave home. But when we arrive at the restaurant we 
can, in principle, immediately be seated and order our meal. For the restaurant that 
does not require reservations, we don‚Äôt need to bother to reserve a table. But when 
we arrive at the restaurant, we may have to wait for a table before we can be seated.
Traditional telephone networks are examples of circuit-switched networks. 
 Consider what happens when one person wants to send information (voice or facsimile)  
to another over a telephone network. Before the sender can send the information, 
the network must establish a connection between the sender and the receiver. This 
is a bona fide connection for which the switches on the path between the sender and 
receiver maintain connection state for that connection. In the jargon of telephony, 
this connection is called a circuit. When the network establishes the circuit, it also 
reserves a constant transmission rate in the network‚Äôs links (representing a fraction 
of each link‚Äôs transmission capacity) for the duration of the connection. Since a given 
transmission rate has been reserved for this sender-to-receiver connection, the sender 
can transfer the data to the receiver at the guaranteed constant rate.
Figure 1.13 illustrates a circuit-switched network. In this network, the four 
circuit switches are interconnected by four links. Each of these links has four cir-
cuits, so that each link can support four simultaneous connections. The hosts (for 
example, PCs and workstations) are each directly connected to one of the switches. 
When two hosts want to communicate, the network establishes a dedicated end-
to-end connection between the two hosts. Thus, in order for Host A to communi-
cate with Host B, the network must first reserve one circuit on each of two links. 
In this example, the dedicated end-to-end connection uses the second circuit in 
the first link and the fourth circuit in the second link. Because each link has four 
circuits, for each link used by the end-to-end connection, the connection gets one 
fourth of the link‚Äôs total transmission capacity for the duration of the connection. 
Thus, for example, if each link between adjacent switches has a transmission rate of  
1 Mbps, then each end-to-end circuit-switch connection gets 250 kbps of dedicated 
transmission rate.

Figure 1.13 ‚ô¶  A simple circuit-switched network consisting of four switches 
and four links
In contrast, consider what happens when one host wants to send a packet to 
another host over a packet-switched network, such as the Internet. As with circuit 
switching, the packet is transmitted over a series of communication links. But dif-
ferent from circuit switching, the packet is sent into the network without reserving 
any link resources whatsoever. If one of the links is congested because other packets 
need to be transmitted over the link at the same time, then the packet will have to 
wait in a buffer at the sending side of the transmission link and suffer a delay. The 
Internet makes its best effort to deliver packets in a timely manner, but it does not 
make any guarantees.
Multiplexing in Circuit-Switched Networks
A circuit in a link is implemented with either frequency-division multiplexing 
(FDM) or time-division multiplexing (TDM). With FDM, the frequency spectrum 
of a link is divided up among the connections established across the link. Specifi-
cally, the link dedicates a frequency band to each connection for the  duration of the 
connection. In telephone networks, this frequency band typically has a width of 
4¬†kHz (that is, 4,000 hertz or 4,000 cycles per second). The width of the band is 
called, not surprisingly, the bandwidth. FM radio stations also use FDM to share 
the frequency spectrum between 88 MHz and 108 MHz, with each station being 
allocated a specific frequency band.
For a TDM link, time is divided into frames of fixed duration, and each frame is 
divided into a fixed number of time slots. When the network establishes a connection 
across a link, the network dedicates one time slot in every frame to this connection. 
These slots are dedicated for the sole use of that connection, with one time slot avail-
able for use (in every frame) to transmit the connection‚Äôs data.

Figure 1.14 illustrates FDM and TDM for a specific network link supporting 
up to four circuits. For FDM, the frequency domain is segmented into four bands, 
each of bandwidth 4 kHz. For TDM, the time domain is segmented into frames, with 
four time slots in each frame; each circuit is assigned the same dedicated slot in the 
revolving TDM frames. For TDM, the transmission rate of a circuit is equal to the 
frame rate multiplied by the number of bits in a slot. For example, if the link trans-
mits 8,000 frames per second and each slot consists of 8 bits, then the transmission 
rate of each circuit is 64 kbps.
Proponents of packet switching have always argued that circuit switching is waste-
ful because the dedicated circuits are idle during silent periods. For example, when one 
person in a telephone call stops talking, the idle network resources (frequency bands or 
time slots in the links along the connection‚Äôs route) cannot be used by other ongoing 
connections. As another example of how these resources can be underutilized, consider 
a radiologist who uses a circuit-switched network to remotely access a series of x-rays. 
The radiologist sets up a connection, requests an image, contemplates the image, and 
then requests a new image. Network resources are allocated to the connection but are 
not used (i.e., are wasted) during the radiologist‚Äôs contemplation periods. Proponents 
of packet switching also enjoy pointing out that establishing end-to-end circuits and 
reserving end-to-end transmission capacity is complicated and requires complex sign-
aling software to coordinate the operation of the switches along the end-to-end path.
Figure 1.14 ‚ô¶  With FDM, each circuit continuously gets a fraction of the 
bandwidth. With TDM, each circuit gets all of the bandwidth 
periodically during brief intervals of time (that is, during slots)
4KHz
TDM
FDM
Link
Frequency
4KHz
Slot
Key:
All slots labeled ‚Äú2‚Äù are dedicated
to a speciÔ¨Åc sender-receiver pair.
Frame
1
2
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4
Time

Before we finish our discussion of circuit switching, let‚Äôs work through a numer-
ical example that should shed further insight on the topic. Let us consider how long 
it takes to send a file of 640,000 bits from Host A to Host B over a circuit-switched 
network. Suppose that all links in the network use TDM with 24 slots and have a bit 
rate of 1.536 Mbps. Also suppose that it takes 500 msec to establish an end-to-end 
circuit before Host A can begin to transmit the file. How long does it take to send 
the file? Each circuit has a transmission rate of (1.536 Mbps)/24 = 64 kbps, so it 
takes (640,000 bits)/(64 kbps) = 10 seconds to transmit the file. To this 10 seconds 
we add the circuit establishment time, giving 10.5 seconds to send the file. Note 
that the transmission time is independent of the number of links: The transmission 
time would be 10 seconds if the end-to-end circuit passed through one link or a 
hundred links. (The actual end-to-end delay also includes a propagation delay; see 
Section 1.4.)
Packet Switching Versus Circuit Switching
Having described circuit switching and packet switching, let us compare the two. 
Critics of packet switching have often argued that packet switching is not suita-
ble for real-time services (for example, telephone calls and video conference calls) 
because of its variable and unpredictable end-to-end delays (due primarily to vari-
able and unpredictable queuing delays). Proponents of packet switching argue that 
(1) it offers better sharing of transmission capacity than circuit switching and (2) it 
is simpler, more efficient, and less costly to implement than circuit switching. An  
interesting discussion of packet switching versus circuit switching is [Molinero- 
Fernandez 2002]. Generally speaking, people who do not like to hassle with  restaurant 
reservations prefer packet switching to circuit switching.
Why is packet switching more efficient? Let‚Äôs look at a simple example. Sup-
pose users share a 1 Mbps link. Also suppose that each user alternates between peri-
ods of activity, when a user generates data at a constant rate of 100 kbps, and periods 
of inactivity, when a user generates no data. Suppose further that a user is active only 
10 percent of the time (and is idly drinking coffee during the remaining 90 percent 
of the time). With circuit switching, 100 kbps must be reserved for each user at all 
times. For example, with circuit-switched TDM, if a one-second frame is divided 
into 10 time slots of 100 ms each, then each user would be allocated one time slot 
per frame.
Thus, the circuit-switched link can support only 10 (= 1 Mbps/100 kbps) simul-
taneous users. With packet switching, the probability that a specific user is active 
is 0.1 (that is, 10 percent). If there are 35 users, the probability that there are 11 or 
more simultaneously active users is approximately 0.0004. (Homework Problem P8 
outlines how this probability is obtained.) When there are 10 or fewer simultane-
ously active users (which happens with probability 0.9996), the aggregate arrival 
rate of data is less than or equal to 1 Mbps, the output rate of the link. Thus, when 
there are 10 or fewer active users, users‚Äô packets flow through the link essentially

without delay, as is the case with circuit switching. When there are more than 10 
simultaneously active users, then the aggregate arrival rate of packets exceeds the 
output capacity of the link, and the output queue will begin to grow. (It continues to 
grow until the aggregate input rate falls back below 1 Mbps, at which point the queue 
will begin to diminish in length.) Because the probability of having more than 10 
simultaneously active users is minuscule in this example, packet switching provides 
essentially the same performance as circuit switching, but does so while allowing for 
more than three times the number of users.
Let‚Äôs now consider a second simple example. Suppose there are 10 users and 
that one user suddenly generates one thousand 1,000-bit packets, while other users 
remain quiescent and do not generate packets. Under TDM circuit switching with 10 
slots per frame and each slot consisting of 1,000 bits, the active user can only use its 
one time slot per frame to transmit data, while the remaining nine time slots in each 
frame remain idle. It will be 10 seconds before all of the active user‚Äôs one million 
bits of data has been transmitted. In the case of packet switching, the active user can 
continuously send its packets at the full link rate of 1 Mbps, since there are no other 
users generating packets that need to be multiplexed with the active user‚Äôs packets. 
In this case, all of the active user‚Äôs data will be transmitted within 1 second.
The above examples illustrate two ways in which the performance of packet 
switching can be superior to that of circuit switching. They also highlight the cru-
cial difference between the two forms of sharing a link‚Äôs transmission rate among 
multiple data streams. Circuit switching pre-allocates use of the transmission link 
regardless of demand, with allocated but unneeded link time going unused. Packet 
switching on the other hand allocates link use on demand. Link transmission capacity 
will be shared on a packet-by-packet basis only among those users who have packets 
that need to be transmitted over the link.
Although packet switching and circuit switching are both prevalent in today‚Äôs 
telecommunication networks, the trend has certainly been in the direction of packet 
switching. Even many of today‚Äôs circuit-switched telephone networks are slowly 
migrating toward packet switching. In particular, telephone networks often use 
packet switching for the expensive overseas portion of a telephone call.
1.3.3 A Network of Networks
We saw earlier that end systems (PCs, smartphones, Web servers, mail servers, and 
so on) connect into the Internet via an access ISP. The access ISP can provide either 
wired or wireless connectivity, using an array of access technologies including DSL, 
cable, FTTH, Wi-Fi, and cellular. Note that the access ISP does not have to be a 
telco or a cable company; instead it can be, for example, a university (providing 
Internet access to students, staff, and faculty), or a company (providing access for 
its employees). But connecting end users and content providers into an access ISP is 
only a small piece of solving the puzzle of connecting the billions of end systems that 
make up the Internet. To complete this puzzle, the access ISPs themselves must be

interconnected. This is done by creating a network of networks‚Äîunderstanding this 
phrase is the key to understanding the Internet.
Over the years, the network of networks that forms the Internet has evolved into 
a very complex structure. Much of this evolution is driven by economics and national 
policy, rather than by performance considerations. In order to understand today‚Äôs 
Internet network structure, let‚Äôs incrementally build a series of network structures, 
with each new structure being a better approximation of the complex Internet that we 
have today. Recall that the overarching goal is to interconnect the access ISPs so that 
all end systems can send packets to each other. One naive approach would be to have 
each access ISP directly connect with every other access ISP. Such a mesh design is, 
of course, much too costly for the access ISPs, as it would require each access ISP 
to have a separate communication link to each of the hundreds of thousands of other 
access ISPs all over the world.
Our first network structure, Network Structure 1, interconnects all of the access 
ISPs with a single global transit ISP. Our (imaginary) global transit ISP is a network 
of routers and communication links that not only spans the globe, but also has at least 
one router near each of the hundreds of thousands of access ISPs. Of course, it would 
be very costly for the global ISP to build such an extensive network. To be profitable, 
it would naturally charge each of the access ISPs for connectivity, with the pricing 
reflecting (but not necessarily directly proportional to) the amount of traffic an access 
ISP exchanges with the global ISP. Since the access ISP pays the global transit ISP, the 
access ISP is said to be a customer and the global transit ISP is said to be a provider.
Now if some company builds and operates a global transit ISP that is profit-
able, then it is natural for other companies to build their own global transit ISPs 
and compete with the original global transit ISP. This leads to Network Structure 2,  
which consists of the hundreds of thousands of access ISPs and multiple global 
 transit ISPs. The access ISPs certainly prefer Network Structure 2 over Network 
Structure 1 since they can now choose among the competing global transit providers 
as a function of their pricing and services. Note, however, that the global transit ISPs 
themselves must interconnect: Otherwise access ISPs connected to one of the global 
transit providers would not be able to communicate with access ISPs connected to the  
other global transit providers.
Network Structure 2, just described, is a two-tier hierarchy with global transit 
providers residing at the top tier and access ISPs at the bottom tier. This assumes 
that global transit ISPs are not only capable of getting close to each and every access 
ISP, but also find it economically desirable to do so. In reality, although some ISPs 
do have impressive global coverage and do directly connect with many access ISPs, 
no ISP has presence in each and every city in the world. Instead, in any given region, 
there may be a regional ISP to which the access ISPs in the region connect. Each 
regional ISP then connects to tier-1 ISPs. Tier-1 ISPs are similar to our (imaginary) 
global transit ISP; but tier-1 ISPs, which actually do exist, do not have a presence 
in every city in the world. There are approximately a dozen tier-1 ISPs, including 
Level 3 Communications, AT&T, Sprint, and NTT. Interestingly, no group officially

sanctions tier-1 status; as the saying goes‚Äîif you have to ask if you‚Äôre a member of 
a group, you‚Äôre probably not.
Returning to this network of networks, not only are there multiple competing 
tier-1 ISPs, there may be multiple competing regional ISPs in a region. In such a 
hierarchy, each access ISP pays the regional ISP to which it connects, and each 
regional ISP pays the tier-1 ISP to which it connects. (An access ISP can also connect 
directly to a tier-1 ISP, in which case it pays the tier-1 ISP). Thus, there is customer-
provider relationship at each level of the hierarchy. Note that the tier-1 ISPs do not 
pay anyone as they are at the top of the hierarchy. To further complicate matters, in 
some regions, there may be a larger regional ISP (possibly spanning an entire coun-
try) to which the smaller regional ISPs in that region connect; the larger regional 
ISP then connects to a tier-1 ISP. For example, in China, there are access ISPs in  
each city, which connect to provincial ISPs, which in turn connect to national ISPs, 
which finally connect to tier-1 ISPs [Tian 2012]. We refer to this multi-tier hierarchy, 
which is still only a crude approximation of today‚Äôs Internet, as Network Structure 3.
To build a network that more closely resembles today‚Äôs Internet, we must add 
points of presence (PoPs), multi-homing, peering, and Internet exchange points 
(IXPs) to the hierarchical Network Structure 3. PoPs exist in all levels of the hier-
archy, except for the bottom (access ISP) level. A PoP is simply a group of one or 
more routers (at the same location) in the provider‚Äôs network where customer ISPs 
can connect into the provider ISP. For a customer network to connect to a provider‚Äôs 
PoP, it can lease a high-speed link from a third-party telecommunications provider 
to directly connect one of its routers to a router at the PoP. Any ISP (except for tier-1 
ISPs) may choose to multi-home, that is, to connect to two or more provider ISPs. So, 
for example, an access ISP may multi-home with two regional ISPs, or it may multi-
home with two regional ISPs and also with a tier-1 ISP. Similarly, a regional ISP may 
multi-home with multiple tier-1 ISPs. When an ISP multi-homes, it can continue to 
send and receive packets into the Internet even if one of its providers has a failure.
As we just learned, customer ISPs pay their provider ISPs to obtain global Inter-
net interconnectivity. The amount that a customer ISP pays a provider ISP reflects 
the amount of traffic it exchanges with the provider. To reduce these costs, a pair 
of nearby ISPs at the same level of the hierarchy can peer, that is, they can directly 
connect their networks together so that all the traffic between them passes over the 
direct connection rather than through upstream intermediaries. When two ISPs peer, 
it is typically settlement-free, that is, neither ISP pays the other. As noted earlier, 
tier-1 ISPs also peer with one another, settlement-free. For a readable discussion of 
peering and customer-provider relationships, see [Van der Berg 2008]. Along these 
same lines, a third-party company can create an Internet Exchange Point (IXP), 
which is a meeting point where multiple ISPs can peer together. An IXP is typically 
in a stand-alone building with its own switches [Ager 2012]. There are over 600 IXPs 
in the Internet today [PeeringDB 2020]. We refer to this ecosystem‚Äîconsisting of 
access ISPs, regional ISPs, tier-1 ISPs, PoPs, multi-homing, peering, and IXPs‚Äîas 
Network Structure 4.

We now finally arrive at Network Structure 5, which describes today‚Äôs Internet. 
Network Structure 5, illustrated in Figure 1.15, builds on top of Network Structure 4 
by adding content-provider networks. Google is currently one of the leading exam-
ples of such a content-provider network. As of this writing, it Google has 19 major data 
centers distributed across North America, Europe, Asia, South America, and Australia 
with each data center having tens or hundreds of thousands of servers. Additionally, 
Google has smaller data centers, each with a few hundred servers; these smaller data 
centers are often located within IXPs. The Google data centers are all interconnected 
via Google‚Äôs private TCP/IP network, which spans the entire globe but is neverthe-
less separate from the public Internet. Importantly, the Google private network only 
carries traffic to/from Google servers. As shown in Figure 1.15, the Google private 
network attempts to ‚Äúbypass‚Äù the upper tiers of the Internet by peering (settlement 
free) with lower-tier ISPs, either by directly connecting with them or by connecting 
with them at IXPs [Labovitz 2010]. However, because many access ISPs can still only 
be reached by transiting through tier-1 networks, the Google network also connects 
to tier-1 ISPs, and pays those ISPs for the traffic it exchanges with them. By creating 
its own network, a content provider not only reduces its payments to upper-tier ISPs, 
but also has greater control of how its services are ultimately delivered to end users. 
Google‚Äôs network infrastructure is described in greater detail in Section 2.6.
In summary, today‚Äôs Internet‚Äîa network of networks‚Äîis complex, consisting 
of a dozen or so tier-1 ISPs and hundreds of thousands of lower-tier ISPs. The ISPs 
are diverse in their coverage, with some spanning multiple continents and oceans, 
and others limited to narrow geographic regions. The lower-tier ISPs connect to the 
higher-tier ISPs, and the higher-tier ISPs interconnect with one another. Users and 
content providers are customers of lower-tier ISPs, and lower-tier ISPs are customers 
of higher-tier ISPs. In recent years, major content providers have also created their 
own networks and connect directly into lower-tier ISPs where possible.
Figure 1.15 ‚ô¶ Interconnection of ISPs
access
ISP
access
ISP
access
ISP
access
ISP
access
ISP
access
ISP
access
ISP
access
ISP
Regional
ISP
Tier 1
ISP
Content provider
(e.g., Google)
Tier 1
ISP
IXP
Regional
ISP
IXP
IXP

1.4 Delay, Loss, and Throughput  
in Packet-Switched Networks
Back in Section 1.1 we said that the Internet can be viewed as an infrastructure that 
provides services to distributed applications running on end systems. Ideally, we 
would like Internet services to be able to move as much data as we want between any 
two end systems, instantaneously, without any loss of data. Alas, this is a lofty goal, 
one that is unachievable in reality. Instead, computer networks necessarily constrain 
throughput (the amount of data per second that can be transferred) between end sys-
tems, introduce delays between end systems, and can actually lose packets. On one 
hand, it is unfortunate that the physical laws of reality introduce delay and loss as 
well as constrain throughput. On the other hand, because computer networks have 
these problems, there are many fascinating issues surrounding how to deal with the 
problems‚Äîmore than enough issues to fill a course on computer networking and to 
motivate thousands of PhD theses! In this section, we‚Äôll begin to examine and quan-
tify delay, loss, and throughput in computer networks.
1.4.1 Overview of Delay in Packet-Switched Networks
Recall that a packet starts in a host (the source), passes through a series of routers, 
and ends its journey in another host (the destination). As a packet travels from one 
node (host or router) to the subsequent node (host or router) along this path, the 
packet suffers from several types of delays at each node along the path. The most 
important of these delays are the nodal processing delay, queuing delay, transmis-
sion delay, and propagation delay; together, these delays accumulate to give a total 
nodal delay. The performance of many Internet applications‚Äîsuch as search, Web 
browsing, e-mail, maps, instant messaging, and voice-over-IP‚Äîare greatly affected 
by network delays. In order to acquire a deep understanding of packet switching and 
computer networks, we must understand the nature and importance of these delays.
Types of Delay
Let‚Äôs explore these delays in the context of Figure 1.16. As part of its end-to-end 
route between source and destination, a packet is sent from the upstream node 
through router A to router B. Our goal is to characterize the nodal delay at router A. 
Note that router A has an outbound link leading to router B. This link is preceded 
by a queue (also known as a buffer). When the packet arrives at router A from the 
upstream node, router A examines the packet‚Äôs header to determine the appropriate 
outbound link for the packet and then directs the packet to this link. In this exam-
ple, the outbound link for the packet is the one that leads to router B. A packet can 
be transmitted on a link only if there is no other packet currently being transmitted 
on the link and if there are no other packets preceding it in the queue; if the link is

currently busy or if there are other packets already queued for the link, the newly 
arriving packet will then join the queue.
Processing Delay
The time required to examine the packet‚Äôs header and determine where to direct 
the packet is part of the processing delay. The processing delay can also include 
other factors, such as the time needed to check for bit-level errors in the packet 
that occurred in transmitting the packet‚Äôs bits from the upstream node to router A. 
Processing delays in high-speed routers are typically on the order of microseconds 
or less. After this nodal processing, the router directs the packet to the queue that 
precedes the link to router B. (In Chapter 4 we‚Äôll study the details of how a router 
operates.)
Queuing Delay
At the queue, the packet experiences a queuing delay as it waits to be transmitted 
onto the link. The length of the queuing delay of a specific packet will depend on the 
number of earlier-arriving packets that are queued and waiting for transmission onto 
the link. If the queue is empty and no other packet is currently being transmitted, then 
our packet‚Äôs queuing delay will be zero. On the other hand, if the traffic is heavy and 
many other packets are also waiting to be transmitted, the queuing delay will be long. 
We will see shortly that the number of packets that an arriving packet might expect 
to find is a function of the intensity and nature of the traffic arriving at the queue. 
 Queuing delays can be on the order of microseconds to milliseconds in practice.
Transmission Delay
Assuming that packets are transmitted in a first-come-first-served manner, as is com-
mon in packet-switched networks, our packet can be transmitted only after all the 
packets that have arrived before it have been transmitted. Denote the length of the 
Figure 1.16 ‚ô¶ The nodal delay at router A
A
B
Nodal
processing
Queueing
(waiting for
transmission)
Transmission
Propagation

packet by L bits, and denote the transmission rate of the link from router A to router 
B by R bits/sec. For example, for a 10 Mbps Ethernet link, the rate is R = 10 Mbps; 
for a 100 Mbps Ethernet link, the rate is R = 100 Mbps. The transmission delay is 
L/R. This is the amount of time required to push (that is, transmit) all of the packet‚Äôs 
bits into the link. Transmission delays are typically on the order of microseconds to 
milliseconds in practice.
Propagation Delay
Once a bit is pushed into the link, it needs to propagate to router B. The time required 
to propagate from the beginning of the link to router B is the propagation delay. The 
bit propagates at the propagation speed of the link. The propagation speed depends 
on the physical medium of the link (that is, fiber optics, twisted-pair copper wire, and 
so on) and is in the range of
2 # 108 meters/sec to 3 # 108 meters/sec
which is equal to, or a little less than, the speed of light. The propagation delay is the 
distance between two routers divided by the propagation speed. That is, the propaga-
tion delay is d/s, where d is the distance between router A and router B and s is the 
propagation speed of the link. Once the last bit of the packet propagates to node B, 
it and all the preceding bits of the packet are stored in router B. The whole process 
then continues with router B now performing the forwarding. In wide-area networks, 
propagation delays are on the order of milliseconds.
Comparing Transmission and Propagation Delay
Newcomers to the field of computer networking sometimes have difficulty under-
standing the difference between transmission delay and propagation delay. The dif-
ference is subtle but important. The transmission delay is the amount of time required 
for the router to push out the packet; it is a function of the packet‚Äôs length and the 
transmission rate of the link, but has nothing to do with the distance between the two 
routers. The propagation delay, on the other hand, is the time it takes a bit to propa-
gate from one router to the next; it is a function of the distance between the two rout-
ers, but has nothing to do with the packet‚Äôs length or the transmission rate of the link.
An analogy might clarify the notions of transmission and propagation delay. 
Consider a highway that has a tollbooth every 100 kilometers, as shown in Fig-
ure¬†1.17. You can think of the highway segments between tollbooths as links and 
the tollbooths as routers. Suppose that cars travel (that is, propagate) on the highway 
at a rate of 100 km/hour (that is, when a car leaves a tollbooth, it instantaneously 
accelerates to 100 km/hour and maintains that speed between tollbooths). Suppose 
next that 10 cars, traveling together as a caravan, follow each other in a fixed order. 
You can think of each car as a bit and the caravan as a packet. Also suppose that each 
VideoNote
Exploring propagation 
delay and transmission 
delay

tollbooth services (that is, transmits) a car at a rate of one car per 12 seconds, and that 
it is late at night so that the caravan‚Äôs cars are the only cars on the highway. Finally, 
suppose that whenever the first car of the caravan arrives at a tollbooth, it waits at 
the entrance until the other nine cars have arrived and lined up behind it. (Thus, the 
entire caravan must be stored at the tollbooth before it can begin to be forwarded.) 
The time required for the tollbooth to push the entire caravan onto the highway is  
(10 cars)/(5 cars/minute) = 2 minutes. This time is analogous to the transmission 
delay in a router. The time required for a car to travel from the exit of one tollbooth 
to the next tollbooth is 100 km/(100 km/hour) = 1 hour. This time is analogous to 
propagation delay. Therefore, the time from when the caravan is stored in front of a 
tollbooth until the caravan is stored in front of the next tollbooth is the sum of trans-
mission delay and propagation delay‚Äîin this example, 62 minutes.
Let‚Äôs explore this analogy a bit more. What would happen if the tollbooth ser-
vice time for a caravan were greater than the time for a car to travel between toll-
booths? For example, suppose now that the cars travel at the rate of 1,000 km/hour 
and the tollbooth services cars at the rate of one car per minute. Then the traveling 
delay between two tollbooths is 6 minutes and the time to serve a caravan is 10 min-
utes. In this case, the first few cars in the caravan will arrive at the second tollbooth 
before the last cars in the caravan leave the first tollbooth. This situation also arises 
in packet-switched networks‚Äîthe first bits in a packet can arrive at a router while 
many of the remaining bits in the packet are still waiting to be transmitted by the 
preceding router.
If a picture speaks a thousand words, then an animation must speak a million 
words. The Web site for this textbook provides an interactive animation that nicely 
illustrates and contrasts transmission delay and propagation delay. The reader is 
highly encouraged to visit that animation. [Smith 2009] also provides a very read-
able discussion of propagation, queueing, and transmission delays.
If we let dproc, dqueue, dtrans, and dprop denote the processing, queuing, transmis-
sion, and propagation delays, then the total nodal delay is given by
dnodal = dproc + dqueue + dtrans + dprop
The contribution of these delay components can vary significantly. For example, 
dprop can be negligible (for example, a couple of microseconds) for a link connecting 
two routers on the same university campus; however, dprop is hundreds of millisec-
onds for two routers interconnected by a geostationary satellite link, and can be the 
Figure 1.17 ‚ô¶ Caravan analogy
Ten-car
caravan
Toll
booth
Toll
booth
100 km
100 km

dominant term in dnodal. Similarly, dtrans can range from negligible to significant. Its 
contribution is typically negligible for transmission rates of 10 Mbps and higher (for 
example, for LANs); however, it can be hundreds of milliseconds for large Internet 
packets sent over low-speed dial-up modem links. The processing delay, dproc, is 
often negligible; however, it strongly influences a router‚Äôs maximum throughput, 
which is the maximum rate at which a router can forward packets.
1.4.2 Queuing Delay and Packet Loss
The most complicated and interesting component of nodal delay is the queuing delay, 
 dqueue. In fact, queuing delay is so important and interesting in computer networking 
that thousands of papers and numerous books have been written about it [Bertsekas 
1991; Kleinrock 1975, Kleinrock 1976]. We give only a high-level, intuitive discus-
sion of queuing delay here; the more curious reader may want to browse through 
some of the books (or even eventually write a PhD thesis on the subject!). Unlike the 
other three delays (namely, dproc, dtrans, and dprop), the queuing delay can vary from 
packet to packet. For example, if 10 packets arrive at an empty queue at the same 
time, the first packet transmitted will suffer no queuing delay, while the last packet 
transmitted will suffer a relatively large queuing delay (while it waits for the other 
nine packets to be transmitted). Therefore, when characterizing queuing delay, one 
typically uses statistical measures, such as average queuing delay, variance of queu-
ing delay, and the probability that the queuing delay exceeds some specified value.
When is the queuing delay large and when is it insignificant? The answer to this 
question depends on the rate at which traffic arrives at the queue, the transmission 
rate of the link, and the nature of the arriving traffic, that is, whether the traffic arrives 
periodically or arrives in bursts. To gain some insight here, let a denote the average 
rate at which packets arrive at the queue (a is in units of packets/sec). Recall that R 
is the transmission rate; that is, it is the rate (in bits/sec) at which bits are pushed out 
of the queue. Also suppose, for simplicity, that all packets consist of L bits. Then the 
average rate at which bits arrive at the queue is La bits/sec. Finally, assume that the 
queue is very big, so that it can hold essentially an infinite number of bits. The ratio 
La/R, called the traffic intensity, often plays an important role in estimating the 
extent of the queuing delay. If La/R > 1, then the average rate at which bits arrive at 
the queue exceeds the rate at which the bits can be transmitted from the queue. In this 
unfortunate situation, the queue will tend to increase without bound and the queuing 
delay will approach infinity! Therefore, one of the golden rules in traffic engineering 
is: Design your system so that the traffic intensity is no greater than 1.
Now consider the case La/R ‚â§ 1. Here, the nature of the arriving traffic impacts 
the queuing delay. For example, if packets arrive periodically‚Äîthat is, one packet 
arrives every L/R seconds‚Äîthen every packet will arrive at an empty queue and 
there will be no queuing delay. On the other hand, if packets arrive in bursts but 
periodically, there can be a significant average queuing delay. For example, sup-
pose N packets arrive simultaneously every (L/R)N seconds. Then the first packet 
transmitted has no queuing delay; the second packet transmitted has a queuing delay

of L/R seconds; and more generally, the nth packet transmitted has a queuing delay 
of (n - 1)L/R seconds. We leave it as an exercise for you to calculate the average 
queuing delay in this example.
The two examples of periodic arrivals described above are a bit academic.  Typically, 
the arrival process to a queue is random; that is, the arrivals do not follow any pattern 
and the packets are spaced apart by random amounts of time. In this more realistic case, 
the quantity La/R is not usually sufficient to fully characterize the queuing delay statis-
tics. Nonetheless, it is useful in gaining an intuitive understanding of the extent of the 
queuing delay. In particular, if the traffic intensity is close to zero, then packet arrivals 
are few and far between and it is unlikely that an arriving packet will find another packet 
in the queue. Hence, the average queuing delay will be close to zero. On the other hand, 
when the traffic intensity is close to 1, there will be intervals of time when the arrival 
rate exceeds the transmission capacity (due to variations in packet arrival rate), and 
a queue will form during these periods of time; when the arrival rate is less than the 
transmission capacity, the length of the queue will shrink. Nonetheless, as the traffic 
intensity approaches 1, the average queue length gets larger and larger. The qualitative 
dependence of average queuing delay on the traffic intensity is shown in Figure 1.18.
One important aspect of Figure 1.18 is the fact that as the traffic intensity 
approaches 1, the average queuing delay increases rapidly. A small percentage 
increase in the intensity will result in a much larger percentage-wise increase in 
delay. Perhaps you have experienced this phenomenon on the highway. If you regu-
larly drive on a road that is typically congested, the fact that the road is typically 
congested means that its traffic intensity is close to 1. If some event causes an even 
slightly larger-than-usual amount of traffic, the delays you experience can be huge.
To really get a good feel for what queuing delays are about, you are encouraged 
once again to visit the textbook Web site, which provides an interactive animation 
for a queue. If you set the packet arrival rate high enough so that the traffic intensity 
exceeds 1, you will see the queue slowly build up over time.
Figure 1.18 ‚ô¶ Dependence of average queuing delay on traffic intensity
Average queuing delay
La/R
1

Packet Loss
In our discussions above, we have assumed that the queue is capable of holding an 
infinite number of packets. In reality a queue preceding a link has finite capacity, 
although the queuing capacity greatly depends on the router design and cost. Because 
the queue capacity is finite, packet delays do not really approach infinity as the traf-
fic intensity approaches 1. Instead, a packet can arrive to find a full queue. With no 
place to store such a packet, a router will drop that packet; that is, the packet will be 
lost. This overflow at a queue can again be seen in the interactive animation when 
the traffic intensity is greater than 1.
From an end-system viewpoint, a packet loss will look like a packet having 
been transmitted into the network core but never emerging from the network at the 
destination. The fraction of lost packets increases as the traffic intensity increases. 
Therefore, performance at a node is often measured not only in terms of delay, but 
also in terms of the probability of packet loss. As we‚Äôll discuss in the subsequent 
chapters, a lost packet may be retransmitted on an end-to-end basis in order to ensure 
that all data are eventually transferred from source to destination.
1.4.3 End-to-End Delay
Our discussion up to this point has focused on the nodal delay, that is, the delay at a 
single router. Let‚Äôs now consider the total delay from source to destination. To get a 
handle on this concept, suppose there are N - 1 routers between the source host and 
the destination host. Let‚Äôs also suppose for the moment that the network is uncon-
gested (so that queuing delays are negligible), the processing delay at each router 
and at the source host is dproc, the transmission rate out of each router and out of the 
source host is R bits/sec, and the propagation on each link is dprop. The nodal delays 
accumulate and give an end-to-end delay,
 
dend-end = N (dproc + dtrans + dprop) 
(1.2)
where, once again, dtrans = L/R, where L is the packet size. Note that Equation 1.2 is a 
generalization of Equation 1.1, which did not take into account processing and propaga-
tion delays. We leave it to you to generalize Equation 1.2 to the case of  heterogeneous 
delays at the nodes and to the presence of an average queuing delay at each node.
Traceroute
To get a hands-on feel for end-to-end delay in a computer network, we can make use 
of the Traceroute program. Traceroute is a simple program that can run in any Inter-
net host. When the user specifies a destination hostname, the program in the source 
host sends multiple, special packets toward that destination. As these packets work 
their way toward the destination, they pass through a series of routers. When a router 
receives one of these special packets, it sends back to the source a short message that 
contains the name and address of the router.
VideoNote
Using Traceroute to 
discover network  
paths and measure 
network delay

More specifically, suppose there are N - 1 routers between the source and the 
destination. Then the source will send N special packets into the network, with each 
packet addressed to the ultimate destination. These N special packets are marked 1 
through N, with the first packet marked 1 and the last packet marked N. When the 
nth router receives the nth packet marked n, the router does not forward the packet 
toward its destination, but instead sends a message back to the source. When the 
destination host receives the Nth packet, it too returns a message back to the source. 
The source records the time that elapses between when it sends a packet and when it 
receives the corresponding return message; it also records the name and address of 
the router (or the destination host) that returns the message. In this manner, the source 
can reconstruct the route taken by packets flowing from source to destination, and the 
source can determine the round-trip delays to all the intervening routers. Traceroute 
actually repeats the experiment just described three times, so the source actually 
sends 3 ‚Ä¢ N packets to the destination. RFC 1393 describes Traceroute in detail.
Here is an example of the output of the Traceroute program, where the route was 
being traced from the source host gaia.cs.umass.edu (at the University of  Massachusetts) 
to a host in the computer science department at the University of Sorbonne in Paris 
(formerly the university was known as UPMC). The output has six columns: the first 
column is the n value described above, that is, the number of the router along the route; 
the second column is the name of the router; the third column is the address of the router 
(of the form xxx.xxx.xxx.xxx); the last three columns are the round-trip delays for three 
experiments. If the source receives fewer than three messages from any given router 
(due to packet loss in the network), Traceroute places an asterisk just after the router 
number and reports fewer than three round-trip times for that router.
1  gw-vlan-2451.cs.umass.edu (128.119.245.1)  1.899 ms 3.266 ms  3.280 ms
2   j-cs-gw-int-10-240.cs.umass.edu (10.119.240.254) 1.296 ms 1.276 ms 
1.245 ms
3   n5-rt-1-1-xe-2-1-0.gw.umass.edu (128.119.3.33) 2.237 ms  2.217 ms  
2.187 ms
4  core1-rt-et-5-2-0.gw.umass.edu (128.119.0.9) 0.351 ms 0.392 ms 0.380 ms
5   border1-rt-et-5-0-0.gw.umass.edu (192.80.83.102) 0.345 ms 0.345 ms  
0.344 ms
6  nox300gw1-umass-re.nox.org (192.5.89.101) 3.260 ms  0.416 ms 3.127 ms
7  nox300gw1-umass-re.nox.org (192.5.89.101) 3.165 ms 7.326 ms  7.311 ms
8  198.71.45.237 (198.71.45.237) 77.826 ms 77.246 ms 77.744 ms
9   renater-lb1-gw.mx1.par.fr.geant.net (62.40.124.70) 79.357 ms 77.729 
79.152 ms
10 193.51.180.109 (193.51.180.109) 78.379 ms  79.936 80.042 ms
11 * 193.51.180.109 (193.51.180.109) 80.640 ms *
12 * 195.221.127.182 (195.221.127.182) 78.408 ms *
13 195.221.127.182 (195.221.127.182) 80.686 ms 80.796 ms 78.434 ms
14 r-upmc1.reseau.jussieu.fr (134.157.254.10) 78.399 ms * 81.353 ms

In the trace above, there are 14 routers between the source and the destination. Most 
of these routers have a name, and all of them have addresses. For example, the  
name of Router 4 is core1-rt-et-5-2-0.gw.umass.edu and its address is 
128.119.0.9. Looking at the data provided for this same router, we see that in 
the first of the three trials the round-trip delay between the source and the router 
was 0.351 msec. The round-trip delays for the subsequent two trials were 0.392 
and 0.380 msec. These round-trip delays include all of the delays just discussed, 
including transmission delays, propagation delays, router processing delays, and 
queuing delay. 
Because the queuing delay is varying with time, the round-trip delay of 
packet n sent to a router n can sometimes be longer than the round-trip delay of 
packet n+1 sent to router n+1. Indeed, we observe this phenomenon in the above 
example: the delay to Router 12 is smaller than the delay to Router 11! Also note 
the big increase in the round-trip delay when going from router 7 to router 8. This 
is due to a transatlantic fiber-optic link between routers 7 and 8, giving rise to a 
relatively large propagation delay. There are a number of free software programs 
that provide a graphical interface to Traceroute; one of our favorites is PingPlotter 
[PingPlotter 2020].
End System, Application, and Other Delays
In addition to processing, transmission, and propagation delays, there can be addi-
tional significant delays in the end systems. For example, an end system wanting 
to transmit a packet into a shared medium (e.g., as in a WiFi or cable modem sce-
nario) may purposefully delay its transmission as part of its protocol for sharing the 
medium with other end systems; we‚Äôll consider such protocols in detail in Chapter 6.  
Another important delay is media packetization delay, which is present in Voice-
over-IP (VoIP) applications. In VoIP, the sending side must first fill a packet with 
encoded digitized speech before passing the packet to the Internet. This time to fill a 
packet‚Äîcalled the packetization delay‚Äîcan be significant and can impact the user-
perceived quality of a VoIP call. This issue will be further explored in a homework 
problem at the end of this chapter.
1.4.4 Throughput in Computer Networks
In addition to delay and packet loss, another critical performance measure in com-
puter networks is end-to-end throughput. To define throughput, consider transferring 
a large file from Host A to Host B across a computer network. This transfer might 
be, for example, a large video clip from one computer to another. The instantaneous 
throughput at any instant of time is the rate (in bits/sec) at which Host B is receiving 
the file. (Many applications display the instantaneous throughput during downloads 
in the user interface‚Äîperhaps you have observed this before! You might like to try

measuring the end-to-end delay and download throughput between your and servers 
around the Internet using the speedtest application [Speedtest 2020].) If the file con-
sists of F bits and the transfer takes T seconds for Host B to receive all F bits, then 
the average throughput of the file transfer is F/T bits/sec. For some applications, 
such as Internet telephony, it is desirable to have a low delay and an instantaneous 
throughput consistently above some threshold (for example, over 24 kbps for some 
Internet telephony applications and over 256 kbps for some real-time video applica-
tions). For other applications, including those involving file transfers, delay is not 
critical, but it is desirable to have the highest possible throughput.
To gain further insight into the important concept of throughput, let‚Äôs consider 
a few examples. Figure 1.19(a) shows two end systems, a server and a client, con-
nected by two communication links and a router. Consider the throughput for a file 
transfer from the server to the client. Let Rs denote the rate of the link between the  
server and the router; and Rc denote the rate of the link between the router and  
the client. Suppose that the only bits being sent in the entire network are those 
from the server to the client. We now ask, in this ideal scenario, what is the server- 
to-client throughput? To answer this question, we may think of bits as fluid and com-
munication links as pipes. Clearly, the server cannot pump bits through its link at a 
rate faster than Rs bps; and the router cannot forward bits at a rate faster than Rc bps. 
If Rs 6 Rc, then the bits pumped by the server will ‚Äúflow‚Äù right through the router 
and arrive at the client at a rate of Rs bps, giving a throughput of Rs bps. If, on the 
other hand, Rc 6 Rs, then the router will not be able to forward bits as quickly as it 
receives them. In this case, bits will only leave the router at rate Rc, giving an end-
to-end throughput of Rc. (Note also that if bits continue to arrive at the router at rate 
Rs, and continue to leave the router at Rc, the backlog of bits at the router waiting 
for transmission to the client will grow and grow‚Äîa most undesirable situation!) 
Figure 1.19 ‚ô¶ Throughput for a file transfer from server to client
Server
Rs
R1
R2
RN
Rc
Client
Server
a.
b.
Client

Thus,¬†for this simple two-link network, the throughput is min{Rc, Rs}, that is, it is¬†the 
transmission rate of the bottleneck link. Having determined the throughput, we can 
now approximate the time it takes to transfer a large file of F bits from server to cli-
ent as F/min{Rs, Rc}. For a specific example, suppose that you are downloading an 
MP3 file of F = 32 million bits, the server has a transmission rate of Rs = 2 Mbps, 
and you have an access link of Rc = 1 Mbps. The time needed to transfer the file is 
then 32 seconds. Of course, these expressions for throughput and transfer time are 
only approximations, as they do not account for store-and-forward and processing 
delays as well as protocol issues.
Figure 1.19(b) now shows a network with N links between the server and the 
client, with the transmission rates of the N links being R1, R2, c, RN. Applying 
the same analysis as for the two-link network, we find that the throughput for a file 
transfer from server to client is min{R1, R2, c, RN}, which is once again the trans-
mission rate of the bottleneck link along the path between server and client.
Now consider another example motivated by today‚Äôs Internet. Figure 1.20(a) 
shows two end systems, a server and a client, connected to a computer network. 
Consider the throughput for a file transfer from the server to the client. The server is 
connected to the network with an access link of rate Rs and the client is connected to 
the network with an access link of rate Rc. Now suppose that all the links in the core 
of the communication network have very high transmission rates, much higher than 
Rs and Rc. Indeed, today, the core of the Internet is over-provisioned with high speed 
links that experience little congestion. Also suppose that the only bits being sent in 
the entire network are those from the server to the client. Because the core of the 
computer network is like a wide pipe in this example, the rate at which bits can flow 
from source to destination is again the minimum of Rs and Rc, that is, throughput = 
min{Rs, Rc}. Therefore, the constraining factor for throughput in today‚Äôs Internet is 
typically the access network.
For a final example, consider Figure 1.20(b) in which there are 10 servers and 
10 clients connected to the core of the computer network. In this example, there are 
10 simultaneous downloads taking place, involving 10 client-server pairs. Suppose 
that these 10 downloads are the only traffic in the network at the current time. As 
shown in the figure, there is a link in the core that is traversed by all 10 downloads. 
Denote R for the transmission rate of this link R. Let‚Äôs suppose that all server access 
links have the same rate Rs, all client access links have the same rate Rc, and the 
transmission rates of all the links in the core‚Äîexcept the one common link of rate 
R‚Äîare much larger than Rs, Rc, and R. Now we ask, what are the throughputs of 
the downloads? Clearly, if the rate of the common link, R, is large‚Äîsay a hundred 
times larger than both Rs and Rc‚Äîthen the throughput for each download will once 
again be min{Rs, Rc}. But what if the rate of the common link is of the same order 
as Rs and Rc? What will the throughput be in this case? Let‚Äôs take a look at a spe-
cific example. Suppose Rs = 2 Mbps, Rc = 1 Mbps, R = 5 Mbps, and the com-
mon link divides its transmission rate equally among the 10 downloads. Then the

bottleneck for each download is no longer in the access network, but is now instead 
the shared link in the core, which only provides each download with 500 kbps of 
throughput. Thus, the end-to-end throughput for each download is now reduced to 
500 kbps.
The examples in Figure 1.19 and Figure 1.20(a) show that throughput depends 
on the transmission rates of the links over which the data flows. We saw that when 
there is no other intervening traffic, the throughput can simply be approximated as 
the minimum transmission rate along the path between source and destination. The 
example in Figure 1.20(b) shows that more generally the throughput depends not 
only on the transmission rates of the links along the path, but also on the interven-
ing traffic. In particular, a link with a high transmission rate may nonetheless be the 
bottleneck link for a file transfer if many other data flows are also passing through 
that link. We will examine throughput in computer networks more closely in the 
homework problems and in the subsequent chapters.
Figure 1.20 ‚ô¶  End-to-end throughput: (a) Client downloads a file from 
 server; (b) 10 clients  downloading with 10 servers
Server
Rs
Rc
a.
b.
Client
10 Clients
10 Servers
Bottleneck
link of
capacity R

1.5 Protocol Layers and Their Service Models
From our discussion thus far, it is apparent that the Internet is an extremely com-
plicated system. We have seen that there are many pieces to the Internet: numerous 
applications and protocols, various types of end systems, packet switches, and vari-
ous types of link-level media. Given this enormous complexity, is there any hope of 
organizing a network architecture, or at least our discussion of network architecture? 
Fortunately, the answer to both questions is yes.
1.5.1 Layered Architecture
Before attempting to organize our thoughts on Internet architecture, let‚Äôs look 
for a human analogy. Actually, we deal with complex systems all the time in our 
everyday life. Imagine if someone asked you to describe, for example, the air-
line system. How would you find the structure to describe this complex system 
that has ticketing agents, baggage checkers, gate personnel, pilots, airplanes, 
air traffic control, and a worldwide system for routing airplanes? One way to 
describe this system might be to describe the series of actions you take (or oth-
ers take for you) when you fly on an airline. You purchase your ticket, check 
your bags, go to the gate, and eventually get loaded onto the plane. The plane 
takes off and is routed to its destination. After your plane lands, you deplane at 
the gate and claim your bags. If the trip was bad, you complain about the flight 
to the ticket agent (getting nothing for your effort). This scenario is shown in 
Figure 1.21.
Figure 1.21 ‚ô¶ Taking an airplane trip: actions
Ticket (purchase)
Baggage (check)
Gates (load)
Runway takeoff
Airplane routing
Ticket (complain)
Baggage (claim)
Gates (unload)
Runway landing
Airplane routing
Airplane routing

Already, we can see some analogies here with computer networking: You are 
being shipped from source to destination by the airline; a packet is shipped from 
source host to destination host in the Internet. But this is not quite the analogy we 
are after. We are looking for some structure in Figure 1.21. Looking at Figure 1.21, 
we note that there is a ticketing function at each end; there is also a baggage func-
tion for already-ticketed passengers, and a gate function for already-ticketed and 
already-baggage-checked passengers. For passengers who have made it through the 
gate (that is, passengers who are already ticketed, baggage-checked, and through the 
gate), there is a takeoff and landing function, and while in flight, there is an airplane-
routing function. This suggests that we can look at the functionality in Figure 1.21 in 
a horizontal manner, as shown in Figure 1.22.
Figure 1.22 has divided the airline functionality into layers, providing a frame-
work in which we can discuss airline travel. Note that each layer, combined with the 
layers below it, implements some functionality, some service. At the ticketing layer 
and below, airline-counter-to-airline-counter transfer of a person is accomplished. At 
the baggage layer and below, baggage-check-to-baggage-claim transfer of a person 
and bags is accomplished. Note that the baggage layer provides this service only to an 
already-ticketed person. At the gate layer, departure-gate-to-arrival-gate transfer of 
a person and bags is accomplished. At the takeoff/landing layer, runway-to-runway  
transfer of people and their bags is accomplished. Each layer provides its service 
by (1) performing certain actions within that layer (for example, at the gate layer, 
loading and unloading people from an airplane) and by (2) using the services of the 
layer directly below it (for example, in the gate layer, using the runway-to-runway 
passenger transfer service of the takeoff/landing layer).
A layered architecture allows us to discuss a well-defined, specific part of a 
large and complex system. This simplification itself is of considerable value by 
providing modularity, making it much easier to change the implementation of the 
service provided by the layer. As long as the layer provides the same service to the 
layer above it, and uses the same services from the layer below it, the remainder of 
the system remains unchanged when a layer‚Äôs implementation is changed. (Note 
Figure 1.22 ‚ô¶ Horizontal layering of airline functionality
Ticket (purchase)
Baggage (check)
Gates (load)
Runway takeoff
Airplane routing
Airplane routing
Airplane routing
Ticket (complain)
Baggage (claim)
Gates (unload)
Runway landing
Airplane routing
Ticket
Baggage
Gate
Takeoff/Landing
Airplane routing
Arrival airport
Departure airport
Intermediate air-trafÔ¨Åc
control centers

that changing the implementation of a service is very different from changing the 
service itself!) For example, if the gate functions were changed (for instance, to have 
people board and disembark by height), the remainder of the airline system would 
remain unchanged since the gate layer still provides the same function (loading and 
unloading people); it simply implements that function in a different manner after the 
change. For large and complex systems that are constantly being updated, the ability 
to change the implementation of a service without affecting other components of the 
system is another important advantage of layering.
Protocol Layering
But enough about airlines. Let‚Äôs now turn our attention to network protocols. To 
provide structure to the design of network protocols, network designers organize 
protocols‚Äîand the network hardware and software that implement the protocols‚Äî
in layers. Each protocol belongs to one of the layers, just as each function in the 
airline architecture in Figure 1.22 belonged to a layer. We are again interested in 
the services that a layer offers to the layer above‚Äîthe so-called service model of 
a layer. Just as in the case of our airline example, each layer provides its service by 
(1) performing certain actions within that layer and by (2) using the services of the 
layer directly below it. For example, the services provided by layer n may include 
reliable delivery of messages from one edge of the network to the other. This might 
be implemented by using an unreliable edge-to-edge message delivery service of 
layer n - 1, and adding layer n functionality to detect and retransmit lost messages.
A protocol layer can be implemented in software, in hardware, or in a combina-
tion of the two. Application-layer protocols‚Äîsuch as HTTP and SMTP‚Äîare almost 
always implemented in software in the end systems; so are transport-layer protocols. 
Because the physical layer and data link layers are responsible for handling commu-
nication over a specific link, they are typically implemented in a network interface 
card (for example, Ethernet or WiFi interface cards) associated with a given link. The 
network layer is often a mixed implementation of hardware and software. Also note 
that just as the functions in the layered airline architecture were distributed among 
the various airports and flight control centers that make up the system, so too is a 
layer n protocol distributed among the end systems, packet switches, and other com-
ponents that make up the network. That is, there‚Äôs often a piece of a layer n protocol 
in each of these network components.
Protocol layering has conceptual and structural advantages [RFC 3439]. As 
we have seen, layering provides a structured way to discuss system components. 
Modularity makes it easier to update system components. We mention, however, 
that some researchers and networking engineers are vehemently opposed to layering 
[Wakeman 1992]. One potential drawback of layering is that one layer may duplicate 
lower-layer functionality. For example, many protocol stacks provide error recovery 
on both a per-link basis and an end-to-end basis. A second potential drawback is that 
functionality at one layer may need information (for example, a timestamp value) 
that is present only in another layer; this violates the goal of separation of layers.

When taken together, the protocols of the various layers are called the protocol 
stack. The Internet protocol stack consists of five layers: the physical, link, network, 
transport, and application layers, as shown in Figure 1.23. If you examine the Table 
of Contents, you will see that we have roughly organized this book using the lay-
ers of the Internet protocol stack. We take a top-down approach, first covering the 
application layer and then proceeding downward.
Application Layer
The application layer is where network applications and their application-layer pro-
tocols reside. The Internet‚Äôs application layer includes many protocols, such as the 
HTTP protocol (which provides for Web document request and transfer), SMTP 
(which provides for the transfer of e-mail messages), and FTP (which provides for 
the transfer of files between two end systems). We‚Äôll see that certain network func-
tions, such as the translation of human-friendly names for Internet end systems like 
www.ietf.org to a 32-bit network address, are also done with the help of a specific appli-
cation-layer protocol, namely, the domain name system (DNS). We‚Äôll see in Chap-
ter¬†2 that it is very easy to create and deploy our own new application-layer protocols.
An application-layer protocol is distributed over multiple end systems, with the 
application in one end system using the protocol to exchange packets of information 
with the application in another end system. We‚Äôll refer to this packet of information 
at the application layer as a message.
Transport Layer
The Internet‚Äôs transport layer transports application-layer messages between application  
endpoints. In the Internet, there are two transport protocols, TCP and UDP, either of 
which can transport application-layer messages. TCP provides a  connection-oriented 
service to its applications. This service includes guaranteed delivery of application-layer 
Figure 1.23 ‚ô¶ The Internet protocol stack
Transport
Application
Network
Link
Physical
Five-layer
Internet
protocol stack

messages to the destination and flow control (that is, sender/receiver speed matching). 
TCP also breaks long messages into shorter  segments and provides a congestion-control 
mechanism, so that a source throttles its transmission rate when the network is con-
gested. The UDP protocol provides a connectionless service to its applications. This is a 
no-frills service that provides no reliability, no flow control, and no congestion control. 
In this book, we‚Äôll refer to a transport-layer packet as a segment.
Network Layer
The Internet‚Äôs network layer is responsible for moving network-layer packets known 
as datagrams from one host to another. The Internet transport-layer protocol (TCP 
or UDP) in a source host passes a transport-layer segment and a destination address 
to the network layer, just as you would give the postal service a letter with a destina-
tion address. The network layer then provides the service of delivering the segment 
to the transport layer in the destination host.
The Internet‚Äôs network layer includes the celebrated IP protocol, which defines 
the fields in the datagram as well as how the end systems and routers act on these 
fields. There is only one IP protocol, and all Internet components that have a network 
layer must run the IP protocol. The Internet‚Äôs network layer also contains routing 
protocols that determine the routes that datagrams take between sources and destina-
tions. The Internet has many routing protocols. As we saw in Section 1.3, the Internet 
is a network of networks, and within a network, the network administrator can run 
any routing protocol desired. Although the network layer contains both the IP pro-
tocol and numerous routing protocols, it is often simply referred to as the IP layer, 
reflecting the fact that IP is the glue that binds the Internet together.
Link Layer
The Internet‚Äôs network layer routes a datagram through a series of routers between 
the source and destination. To move a packet from one node (host or router) to the 
next node in the route, the network layer relies on the services of the link layer. In 
particular, at each node, the network layer passes the datagram down to the link 
layer, which delivers the datagram to the next node along the route. At this next node, 
the link layer passes the datagram up to the network layer.
The services provided by the link layer depend on the specific link-layer protocol 
that is employed over the link. For example, some link-layer protocols provide reli-
able delivery, from transmitting node, over one link, to receiving node. Note that this 
reliable delivery service is different from the reliable delivery service of TCP, which 
provides reliable delivery from one end system to another. Examples of link-layer pro-
tocols include Ethernet, WiFi, and the cable access network‚Äôs DOCSIS protocol. As 
datagrams typically need to traverse several links to travel from source to destination, 
a datagram may be handled by different link-layer protocols at different links along its 
route. For example, a datagram may be handled by Ethernet on one link and by PPP on

the next link. The network layer will receive a different service from each of the dif-
ferent link-layer protocols. In this book, we‚Äôll refer to the link-layer packets as frames.
Physical Layer
While the job of the link layer is to move entire frames from one network element to 
an adjacent network element, the job of the physical layer is to move the individual 
bits within the frame from one node to the next. The protocols in this layer are again 
link dependent and further depend on the actual transmission medium of the link (for 
example, twisted-pair copper wire, single-mode fiber optics). For example, Ether-
net has many physical-layer protocols: one for twisted-pair copper wire, another for 
coaxial cable, another for fiber, and so on. In each case, a bit is moved across the link 
in a different way.
1.5.2 Encapsulation
Figure 1.24 shows the physical path that data takes down a sending end system‚Äôs 
protocol stack, up and down the protocol stacks of an intervening link-layer switch 
Figure 1.24 ‚ô¶  Hosts, routers, and link-layer switches; each contains a 
 different set of layers, reflecting their differences in functionality
M
M
M
M
Ht
Ht
Ht
Hn
Hn
Hl
Hn Ht
Hl
Link-layer switch
Router
Application
Transport
Network
Link
Physical
Message
Segment
Datagram
Frame
M
M
M
M
Ht
Ht
Ht
Hn
Hn
Hl
Link
Physical
Source
Network
Link
Physical
Destination
Application
Transport
Network
Link
Physical
M
Hn Ht
Hl
M
Hn Ht
M
Hn Ht
M
Hn Ht
Hl
M
Hn Ht
Hl
M

and router, and then up the protocol stack at the receiving end system. As we dis-
cuss later in this book, routers and link-layer switches are both packet switches. 
Similar to end systems, routers and link-layer switches organize their network-
ing hardware and software into layers. But routers and link-layer switches do not 
implement all of the layers in the protocol stack; they typically implement only 
the bottom layers. As shown in Figure 1.24, link-layer switches implement lay-
ers 1 and 2; routers implement layers 1 through 3. This means, for example, that 
Internet routers are capable of implementing the IP protocol (a layer 3 protocol), 
while link-layer switches are not. We‚Äôll see later that while link-layer switches do 
not recognize IP addresses, they are capable of recognizing layer 2 addresses, such 
as Ethernet addresses. Note that hosts implement all five layers; this is consistent 
with the view that the Internet architecture puts much of its complexity at the edges 
of the network.
Figure 1.24 also illustrates the important concept of encapsulation. At the 
sending host, an application-layer message (M in Figure 1.24) is passed to the 
transport layer. In the simplest case, the transport layer takes the message and 
appends additional information (so-called transport-layer header information, Ht 
in Figure 1.24) that will be used by the receiver-side transport layer. The appli-
cation-layer message and the transport-layer header information together consti-
tute the transport-layer segment. The transport-layer segment thus encapsulates 
the application-layer message. The added information might include information 
allowing the receiver-side transport layer to deliver the message up to the appro-
priate application, and error-detection bits that allow the receiver to determine 
whether bits in the message have been changed in route. The transport layer then 
passes the segment to the network layer, which adds network-layer header infor-
mation (Hn in Figure 1.24) such as source and destination end system addresses, 
creating a network-layer datagram. The datagram is then passed to the link 
layer, which (of course!) will add its own link-layer header information and cre-
ate a link-layer frame. Thus, we see that at each layer, a packet has two types of 
fields: header fields and a payload field. The payload is typically a packet from 
the layer above.
A useful analogy here is the sending of an interoffice memo from one corpo-
rate branch office to another via the public postal service. Suppose Alice, who is in 
one branch office, wants to send a memo to Bob, who is in another branch office. 
The memo is analogous to the application-layer message. Alice puts the memo 
in an interoffice envelope with Bob‚Äôs name and department written on the front 
of the envelope. The interoffice envelope is analogous to a transport-layer seg-
ment‚Äîit contains header information (Bob‚Äôs name and department number) and it 
encapsulates the application-layer message (the memo). When the sending branch-
office mailroom receives the interoffice envelope, it puts the interoffice enve-
lope inside yet another envelope, which is suitable for sending through the public 
postal service. The sending mailroom also writes the postal address of the sending 
and receiving branch offices on the postal envelope. Here, the postal envelope

is analogous to the datagram‚Äîit encapsulates the transport-layer segment (the 
interoffice envelope), which encapsulates the original message (the memo). The 
postal service delivers the postal envelope to the receiving branch-office mail-
room. There, the process of de-encapsulation is begun. The mailroom extracts the 
interoffice memo and forwards it to Bob. Finally, Bob opens the envelope and 
removes the memo.
The process of encapsulation can be more complex than that described above. 
For example, a large message may be divided into multiple transport-layer segments 
(which might themselves each be divided into multiple network-layer datagrams). 
At the receiving end, such a segment must then be reconstructed from its constituent 
datagrams.
1.6 Networks Under Attack
The Internet has become mission critical for many institutions today, including large 
and small companies, universities, and government agencies. Many individuals also 
rely on the Internet for many of their professional, social, and personal activities. 
Billions of ‚Äúthings,‚Äù including wearables and home devices, are currently being con-
nected to the Internet. But behind all this utility and excitement, there is a dark side, 
a side where ‚Äúbad guys‚Äù attempt to wreak havoc in our daily lives by damaging our 
Internet-connected computers, violating our privacy, and rendering inoperable the 
Internet services on which we depend.
The field of network security is about how the bad guys can attack computer 
networks and about how we, soon-to-be experts in computer networking, can 
defend networks against those attacks, or better yet, design new architectures 
that are immune to such attacks in the first place. Given the frequency and vari-
ety of existing attacks as well as the threat of new and more destructive future 
attacks, network security has become a central topic in the field of computer 
networking. One of the features of this textbook is that it brings network security 
issues to the forefront.
Since we don‚Äôt yet have expertise in computer networking and Internet pro-
tocols, we‚Äôll begin here by surveying some of today‚Äôs more prevalent security-
related problems. This will whet our appetite for more substantial discussions in the 
upcoming chapters. So we begin here by simply asking, what can go wrong? How 
are computer networks vulnerable? What are some of the more prevalent types of 
attacks today?
The Bad Guys Can Put Malware into Your Host Via the Internet
We attach devices to the Internet because we want to receive/send data from/to the 
Internet. This includes all kinds of good stuff, including Instagram posts, Internet

search results, streaming music, video conference calls, streaming movies, and 
so on. But, unfortunately, along with all that good stuff comes malicious stuff‚Äî 
collectively known as malware‚Äîthat can also enter and infect our devices. Once 
malware infects our device it can do all kinds of devious things, including delet-
ing our files and installing spyware that collects our private information, such 
as¬†social  security numbers, passwords, and keystrokes, and then sends this (over 
the Internet, of course!) back to the bad guys. Our compromised host may also 
be¬† enrolled in a network of thousands of similarly compromised devices, col-
lectively known as a botnet, which the bad guys control and leverage for spam 
e-mail distribution or distributed denial-of-service attacks (soon to be discussed) 
against targeted hosts.
Much of the malware out there today is self-replicating: once it infects one host, 
from that host it seeks entry into other hosts over the Internet, and from  the newly 
infected hosts, it seeks entry into yet more hosts. In this manner, self- replicating mal-
ware can spread exponentially fast.
The Bad Guys Can Attack Servers and Network Infrastructure
Another broad class of security threats are known as denial-of-service (DoS) 
attacks. As the name suggests, a DoS attack renders a network, host, or other piece 
of infrastructure unusable by legitimate users. Web servers, e-mail servers, DNS 
servers (discussed in Chapter 2), and institutional networks can all be subject to DoS 
attacks. The site Digital Attack Map allows use to visualize the top daily DoS attacks 
worldwide [DAM 2020]. Most Internet DoS attacks fall into one of three categories:
‚Ä¢ Vulnerability attack.  This involves sending a few well-crafted messages to a 
vulnerable application or operating system running on a targeted host. If the right 
sequence of packets is sent to a vulnerable application or operating system, the 
service can stop or, worse, the host can crash.
‚Ä¢ Bandwidth flooding.  The attacker sends a deluge of packets to the targeted 
host‚Äîso many packets that the target‚Äôs access link becomes clogged, preventing 
legitimate packets from reaching the server.
‚Ä¢ Connection flooding. The attacker establishes a large number of half-open or 
fully open TCP connections (TCP connections are discussed in Chapter 3) at the 
target host. The host can become so bogged down with these bogus connections 
that it stops accepting legitimate connections.
Let‚Äôs now explore the bandwidth-flooding attack in more detail. Recalling our 
delay and loss analysis discussion in Section 1.4.2, it‚Äôs evident that if the server 
has an access rate of R bps, then the attacker will need to send traffic at a rate of 
approximately R bps to cause damage. If R is very large, a single attack source 
may not be able to generate enough traffic to harm the server. Furthermore, if all

the traffic emanates from a single source, an upstream router may be able to detect 
the attack and block all traffic from that source before the traffic gets near the 
server. In a distributed DoS (DDoS) attack, illustrated in Figure 1.25, the attacker 
controls multiple sources and has each source blast traffic at the target. With this 
approach, the aggregate traffic rate across all the controlled sources needs to be 
approximately R to cripple the  service. DDoS attacks leveraging botnets with thou-
sands of comprised hosts are a common occurrence today [DAM 2020]. DDos 
attacks are much harder to detect and defend against than a DoS attack from a 
single host.
We encourage you to consider the following question as you work your way 
through this book: What can computer network designers do to defend against 
DoS attacks? We will see that different defenses are needed for the three types of 
DoS attacks.
The Bad Guys Can Sniff Packets
Many users today access the Internet via wireless devices, such as WiFi-connected 
laptops or handheld devices with cellular Internet connections (covered in Chapter 7).  
While ubiquitous Internet access is extremely convenient and enables marvelous 
new applications for mobile users, it also creates a major security vulnerability‚Äîby 
placing a passive receiver in the vicinity of the wireless transmitter, that receiver 
Figure 1.25 ‚ô¶ A distributed denial-of-service attack
Attacker
‚Äústart
    attack‚Äù
zombie
zombie
zombie
Victim
zombie
zombie

can obtain a copy of every packet that is transmitted! These packets can contain all 
kinds of sensitive information, including passwords, social security numbers, trade 
secrets, and private personal messages. A passive receiver that records a copy of 
every packet that flies by is called a packet sniffer.
Sniffers can be deployed in wired environments as well. In wired broadcast 
environments, as in many Ethernet LANs, a packet sniffer can obtain copies of 
broadcast packets sent over the LAN. As described in Section 1.2, cable access 
technologies also broadcast packets and are thus vulnerable to sniffing. Further-
more, a bad guy who gains access to an institution‚Äôs access router or access link 
to the Internet may be able to plant a sniffer that makes a copy of every packet 
going to/from the organization. Sniffed packets can then be analyzed offline for 
sensitive information.
Packet-sniffing software is freely available at various Web sites and as commer-
cial products. Professors teaching a networking course have been known to assign 
lab exercises that involve writing a packet-sniffing and application-layer data recon-
struction program. Indeed, the Wireshark [Wireshark 2020] labs associated with this 
text (see the introductory Wireshark lab at the end of this chapter) use exactly such 
a packet sniffer!
Because packet sniffers are passive‚Äîthat is, they do not inject packets into the 
channel‚Äîthey are difficult to detect. So, when we send packets into a wireless chan-
nel, we must accept the possibility that some bad guy may be recording copies of our 
packets. As you may have guessed, some of the best defenses against packet sniffing 
involve cryptography. We will examine cryptography as it applies to network secu-
rity in Chapter 8.
The Bad Guys Can Masquerade as Someone You Trust
It is surprisingly easy (you will have the knowledge to do so shortly as you proceed 
through this text!) to create a packet with an arbitrary source address, packet content, 
and destination address and then transmit this hand-crafted packet into the Internet, 
which will dutifully forward the packet to its destination. Imagine the unsuspecting 
receiver (say an Internet router) who receives such a packet, takes the (false) source 
address as being truthful, and then performs some command embedded in the pack-
et‚Äôs contents (say modifies its forwarding table). The ability to inject packets into the 
Internet with a false source address is known as IP spoofing, and is but one of many 
ways in which one user can masquerade as another user.
To solve this problem, we will need end-point authentication, that is, a mech-
anism that will allow us to determine with certainty if a message originates from 
where we think it does. Once again, we encourage you to think about how this 
can be done for network applications and protocols as you progress through the 
chapters of this book. We will explore mechanisms for end-point authentication 
in Chapter 8.

In closing this section, it‚Äôs worth considering how the Internet got to be such 
an insecure place in the first place. The answer, in essence, is that the Internet was 
originally designed to be that way, based on the model of ‚Äúa group of mutually trust-
ing users attached to a transparent network‚Äù [Blumenthal 2001]‚Äîa model in which 
(by definition) there is no need for security. Many aspects of the original Internet 
architecture deeply reflect this notion of mutual trust. For example, the ability for 
one user to send a packet to any other user is the default rather than a requested/
granted capability, and user identity is taken at declared face value, rather than being 
authenticated by default.
But today‚Äôs Internet certainly does not involve ‚Äúmutually trusting users.‚Äù None-
theless, today‚Äôs users still need to communicate when they don‚Äôt necessarily trust 
each other, may wish to communicate anonymously, may communicate indirectly 
through third parties (e.g., Web caches, which we‚Äôll study in Chapter 2, or mobility-
assisting agents, which we‚Äôll study in Chapter 7), and may distrust the hardware, 
software, and even the air through which they communicate. We now have many 
security-related challenges before us as we progress through this book: We should 
seek defenses against sniffing, end-point masquerading, man-in-the-middle attacks, 
DDoS attacks, malware, and more. We should keep in mind that communication 
among mutually trusted users is the exception rather than the rule. Welcome to the 
world of modern computer networking!
1.7 History of Computer Networking and  
the Internet
Sections 1.1 through 1.6 presented an overview of the technology of computer net-
working and the Internet. You should know enough now to impress your family and 
friends! However, if you really want to be a big hit at the next cocktail party, you 
should sprinkle your discourse with tidbits about the fascinating history of the Inter-
net [Segaller 1998].
1.7.1 The Development of Packet Switching: 1961‚Äì1972
The field of computer networking and today‚Äôs Internet trace their beginnings 
back to the early 1960s, when the telephone network was the world‚Äôs dominant 
communication network. Recall from Section 1.3 that the telephone network uses 
circuit switching to transmit information from a sender to a receiver‚Äîan appro-
priate choice given that voice is transmitted at a constant rate between sender 
and receiver. Given the increasing importance of computers in the early 1960s 
and the advent of timeshared computers, it was perhaps natural to consider how 
to hook computers together so that they could be shared among geographically

distributed users. The traffic generated by such users was likely to be bursty‚Äî
intervals of activity, such as the sending of a command to a remote computer, 
followed by periods of inactivity while waiting for a reply or while contemplat-
ing the received response.
Three research groups around the world, each unaware of the others‚Äô work 
[Leiner 1998], began inventing packet switching as an efficient and robust alterna-
tive to circuit switching. The first published work on packet-switching techniques 
was that of Leonard Kleinrock [Kleinrock 1961; Kleinrock 1964], then a graduate 
student at MIT. Using queuing theory, Kleinrock‚Äôs work elegantly demonstrated the 
effectiveness of the packet-switching approach for bursty traffic sources. In 1964, 
Paul Baran [Baran 1964] at the Rand Institute had begun investigating the use of 
packet switching for secure voice over military networks, and at the National Physi-
cal Laboratory in England, Donald Davies and Roger Scantlebury were also devel-
oping their ideas on packet switching.
The work at MIT, Rand, and the NPL laid the foundations for today‚Äôs Inter-
net. But the Internet also has a long history of a let‚Äôs-build-it-and-demonstrate-it 
attitude that also dates back to the 1960s. J. C. R. Licklider [DEC 1990] and 
Lawrence Roberts, both colleagues of Kleinrock‚Äôs at MIT, went on to lead the 
computer science program at the Advanced Research Projects Agency (ARPA) 
in the United States. Roberts published an overall plan for the ARPAnet [Roberts 
1967], the first packet-switched computer network and a direct ancestor of today‚Äôs 
public Internet. On Labor Day in 1969, the first packet switch was installed at 
UCLA under Kleinrock‚Äôs supervision, and three additional packet switches were 
installed shortly thereafter at the Stanford Research Institute (SRI), UC Santa 
Barbara, and the University of Utah (Figure 1.26). The fledgling precursor to the 
Internet was four nodes large by the end of 1969. Kleinrock recalls the very first 
use of the network to perform a remote login from UCLA to SRI, crashing the 
system [Kleinrock 2004].
By 1972, ARPAnet had grown to approximately 15 nodes and was given its 
first public demonstration by Robert Kahn. The first host-to-host protocol between 
ARPAnet end systems, known as the network-control protocol (NCP), was com-
pleted [RFC 001]. With an end-to-end protocol available, applications could now be 
written. Ray Tomlinson wrote the first e-mail program in 1972.
1.7.2  Proprietary Networks and Internetworking:  
1972‚Äì1980
The initial ARPAnet was a single, closed network. In order to communicate with an 
ARPAnet host, one had to be actually attached to another ARPAnet IMP. In the early 
to mid-1970s, additional stand-alone packet-switching networks besides ARPAnet 
came into being: ALOHANet, a microwave network linking universities on the 
Hawaiian islands [Abramson 1970], as well as DARPA‚Äôs packet-satellite [RFC 829]  
and packet-radio networks [Kahn 1978]; Telenet, a BBN commercial packet- switching

network based on ARPAnet technology; Cyclades, a French packet-switching net-
work pioneered by Louis Pouzin [Think 2012]; Time-sharing networks such as 
Tymnet and the GE Information Services network, among others, in the late 1960s 
and early 1970s [Schwartz 1977]; IBM‚Äôs SNA (1969‚Äì1974), which paralleled the 
ARPAnet work [Schwartz 1977].
The number of networks was growing. With perfect hindsight we can see that the 
time was ripe for developing an encompassing architecture for connecting networks 
together. Pioneering work on interconnecting networks (under the sponsorship of 
the Defense Advanced Research Projects Agency (DARPA)), in essence creating 
Figure 1.26 ‚ô¶ An early packet switch
Mark J. Terrill/AP Photo

a network of networks, was done by Vinton Cerf and Robert Kahn [Cerf 1974]; the 
term internetting was coined to describe this work.
These architectural principles were embodied in TCP. The early versions of 
TCP, however, were quite different from today‚Äôs TCP. The early versions of TCP 
combined a reliable in-sequence delivery of data via end-system retransmission (still 
part of today‚Äôs TCP) with forwarding functions (which today are performed by IP). 
Early experimentation with TCP, combined with the recognition of the importance 
of an unreliable, non-flow-controlled, end-to-end transport service for applications 
such as packetized voice, led to the separation of IP out of TCP and the development 
of the UDP protocol. The three key Internet protocols that we see today‚ÄîTCP, UDP, 
and IP‚Äîwere conceptually in place by the end of the 1970s.
In addition to the DARPA Internet-related research, many other important net-
working activities were underway. In Hawaii, Norman Abramson was developing  
ALOHAnet, a packet-based radio network that allowed multiple remote sites 
on the Hawaiian Islands to communicate with each other. The ALOHA protocol  
[Abramson 1970] was the first multiple-access protocol, allowing geographically  
distributed users to share a single broadcast communication medium (a radio 
 frequency). Metcalfe and Boggs built on Abramson‚Äôs multiple-access protocol work 
when they developed the Ethernet protocol [Metcalfe 1976] for wire-based shared 
broadcast networks. Interestingly, Metcalfe and Boggs‚Äô Ethernet protocol was moti-
vated by the need to connect multiple PCs, printers, and shared disks [Perkins 1994]. 
Twenty-five years ago, well before the PC revolution and the explosion of networks, 
Metcalfe and Boggs were laying the foundation for today‚Äôs PC LANs.
1.7.3 A Proliferation of Networks: 1980‚Äì1990
By the end of the 1970s, approximately two hundred hosts were connected to the 
ARPAnet. By the end of the 1980s the number of hosts connected to the public 
 Internet, a confederation of networks looking much like today‚Äôs Internet, would 
reach a hundred thousand. The 1980s would be a time of tremendous growth.
Much of that growth resulted from several distinct efforts to create computer 
networks linking universities together. BITNET provided e-mail and file transfers 
among several universities in the Northeast. CSNET (computer science network) 
was formed to link university researchers who did not have access to ARPAnet. In 
1986, NSFNET was created to provide access to NSF-sponsored supercomputing 
centers. Starting with an initial backbone speed of 56 kbps, NSFNET‚Äôs backbone 
would be running at 1.5 Mbps by the end of the decade and would serve as a primary 
backbone linking regional networks.
In the ARPAnet community, many of the final pieces of today‚Äôs Internet archi-
tecture were falling into place. January 1, 1983 saw the official deployment of  
TCP/IP as the new standard host protocol for ARPAnet (replacing the NCP pro-
tocol). The transition [RFC 801] from NCP to TCP/IP was a flag day event‚Äîall 
hosts were required to transfer over to TCP/IP as of that day. In the late 1980s,

important extensions were made to TCP to implement host-based congestion con-
trol [Jacobson 1988]. The DNS, used to map between a human-readable Internet 
name (for example, gaia.cs.umass.edu) and its 32-bit IP address, was also developed 
[RFC 1034].
Paralleling this development of the ARPAnet (which was for the most part a 
US effort), in the early 1980s the French launched the Minitel project, an ambitious 
plan to bring data networking into everyone‚Äôs home. Sponsored by the French gov-
ernment, the Minitel system consisted of a public packet-switched network (based 
on the X.25 protocol suite), Minitel servers, and inexpensive terminals with built-in 
low-speed modems. The Minitel became a huge success in 1984 when the French 
government gave away a free Minitel terminal to each French household that wanted 
one. Minitel sites included free sites‚Äîsuch as a telephone directory site‚Äîas well as 
private sites, which collected a usage-based fee from each user. At its peak in the 
mid 1990s, it offered more than 20,000 services, ranging from home banking to spe-
cialized research databases. The Minitel was in a large proportion of French homes 
10¬†years before most Americans had ever heard of the Internet.
1.7.4 The Internet Explosion: The 1990s
The 1990s were ushered in with a number of events that symbolized the continued 
evolution and the soon-to-arrive commercialization of the Internet. ARPAnet, the 
progenitor of the Internet, ceased to exist. In 1991, NSFNET lifted its restrictions on 
the use of NSFNET for commercial purposes. NSFNET itself would be decommis-
sioned in 1995, with Internet backbone traffic being carried by commercial Internet 
Service Providers.
The main event of the 1990s was to be the emergence of the World Wide Web 
application, which brought the Internet into the homes and businesses of millions 
of people worldwide. The Web served as a platform for enabling and deploying 
hundreds of new applications that we take for granted today, including search (e.g., 
Google and Bing) Internet commerce (e.g., Amazon and eBay) and social networks 
(e.g., Facebook).
The Web was invented at CERN by Tim Berners-Lee between 1989 and 1991 
[Berners-Lee 1989], based on ideas originating in earlier work on hypertext from the 
1940s by Vannevar Bush [Bush 1945] and since the 1960s by Ted Nelson [Xanadu 
2012]. Berners-Lee and his associates developed initial versions of HTML, HTTP, 
a Web server, and a browser‚Äîthe four key components of the Web. Around the end 
of 1993 there were about two hundred Web servers in operation, this collection of 
servers being just a harbinger of what was about to come. At about this time sev-
eral researchers were developing Web browsers with GUI interfaces, including Marc 
Andreessen, who along with Jim Clark, formed Mosaic Communications, which 
later became Netscape Communications Corporation [Cusumano 1998; Quittner 
1998]. By 1995, university students were using Netscape browsers to surf the Web 
on a daily basis. At about this time companies‚Äîbig and small‚Äîbegan to operate

Web servers and transact commerce over the Web. In 1996, Microsoft started to 
make browsers, which started the browser war between Netscape and Microsoft, 
which Microsoft won a few years later [Cusumano 1998].
The second half of the 1990s was a period of tremendous growth and innovation 
for the Internet, with major corporations and thousands of startups creating Internet 
products and services. By the end of the millennium the Internet was supporting 
hundreds of popular applications, including four killer applications:
‚Ä¢ E-mail, including attachments and Web-accessible e-mail
‚Ä¢ The Web, including Web browsing and Internet commerce
‚Ä¢ Instant messaging, with contact lists
‚Ä¢ Peer-to-peer file sharing of MP3s, pioneered by Napster
Interestingly, the first two killer applications came from the research community, 
whereas the last two were created by a few young entrepreneurs.
The period from 1995 to 2001 was a roller-coaster ride for the Internet in the 
financial markets. Before they were even profitable, hundreds of Internet startups 
made initial public offerings and started to be traded in a stock market. Many com-
panies were valued in the billions of dollars without having any significant revenue 
streams. The Internet stocks collapsed in 2000‚Äì2001, and many startups shut down. 
Nevertheless, a number of companies emerged as big winners in the Internet space, 
including Microsoft, Cisco, Yahoo, eBay, Google, and Amazon.
1.7.5 The New Millennium
In the first two decades of the 21st century, perhaps no other technology has trans-
formed society more than the Internet along with Internet-connected smartphones. 
And innovation in computer networking continues at a rapid pace. Advances are 
being made on all fronts, including deployments of faster routers and higher trans-
mission speeds in both access networks and in network backbones. But the following 
developments merit special attention:
‚Ä¢ Since the beginning of the millennium, we have been seeing aggressive deploy-
ment of broadband Internet access to homes‚Äînot only cable modems and DSL 
but also fiber to the home, and now 5G fixed wireless as discussed in Section 1.2. 
This high-speed Internet access has set the stage for a wealth of video applica-
tions, including the distribution of user-generated video (for example, YouTube), 
on-demand streaming of movies and television shows (e.g., Netflix), and multi-
person video conference (e.g., Skype, Facetime, and Google Hangouts).
‚Ä¢ The increasing ubiquity of high-speed wireless Internet access is not only making 
it possible to remain constantly connected while on the move, but also enabling 
new location-specific applications such as Yelp, Tinder, and Waz. The number of 
wireless devices connecting to the Internet surpassed the number of wired devices

in 2011. This high-speed wireless access has set the stage for the rapid emergence 
of hand-held computers (iPhones, Androids, iPads, and so on), which enjoy con-
stant and untethered access to the Internet.
‚Ä¢ Online social networks‚Äîsuch as Facebook, Instagram, Twitter, and WeChat 
(hugely popular in China)‚Äîhave created massive people networks on top of the 
Internet. Many of these social networks are extensively used for messaging as 
well as photo sharing. Many Internet users today ‚Äúlive‚Äù primarily within one or 
more social networks. Through their APIs, the online social networks create plat-
forms for new networked applications, including mobile payments and distrib-
uted games.
‚Ä¢ As discussed in Section 1.3.3, online service providers, such as Google and 
Microsoft, have deployed their own extensive private networks, which not only 
connect together their globally distributed data centers, but are used to bypass the 
Internet as much as possible by peering directly with lower-tier ISPs. As a result, 
Google provides search results and e-mail access almost instantaneously, as if 
their data centers were running within one‚Äôs own computer.
‚Ä¢ Many Internet commerce companies are now running their applications in the 
‚Äúcloud‚Äù‚Äîsuch as in Amazon‚Äôs EC2, in Microsoft‚Äôs Azure, or in the Alibaba 
Cloud. Many companies and universities have also migrated their Internet 
applications (e.g., e-mail and Web hosting) to the cloud. Cloud companies not 
only provide applications scalable computing and storage environments, but 
also provide the applications implicit access to their high-performance private 
networks.
1.8 Summary
In this chapter, we‚Äôve covered a tremendous amount of material! We‚Äôve looked at 
the various pieces of hardware and software that make up the Internet in particular 
and computer networks in general. We started at the edge of the network, look-
ing at end systems and applications, and at the transport service provided to the 
applications running on the end systems. We also looked at the link-layer tech-
nologies and physical media typically found in the access network. We then dove 
deeper inside the network, into the network core, identifying packet switching and 
circuit switching as the two basic approaches for transporting data through a tel-
ecommunication network, and we examined the strengths and weaknesses of each 
approach. We also examined the structure of the global Internet, learning that the 
Internet is a network of networks. We saw that the Internet‚Äôs hierarchical structure, 
consisting of higher- and lower-tier ISPs, has allowed it to scale to include thou-
sands of networks.

In the second part of this introductory chapter, we examined several topics cen-
tral to the field of computer networking. We first examined the causes of delay, 
throughput and packet loss in a packet-switched network. We developed simple 
quantitative models for transmission, propagation, and queuing delays as well as 
for throughput; we‚Äôll make extensive use of these delay models in the homework 
problems throughout this book. Next we examined protocol layering and service 
models, key architectural principles in networking that we will also refer back to 
throughout this book. We also surveyed some of the more prevalent security attacks 
in the Internet day. We finished our introduction to networking with a brief history 
of computer networking. The first chapter in itself constitutes a mini-course in com-
puter networking.
So, we have indeed covered a tremendous amount of ground in this first chapter! 
If you‚Äôre a bit overwhelmed, don‚Äôt worry. In the following chapters, we‚Äôll revisit all 
of these ideas, covering them in much more detail (that‚Äôs a promise, not a threat!). 
At this point, we hope you leave this chapter with a still-developing intuition for the 
pieces that make up a network, a still-developing command of the vocabulary of 
networking (don‚Äôt be shy about referring back to this chapter), and an ever-growing 
desire to learn more about networking. That‚Äôs the task ahead of us for the rest of this 
book.
Road-Mapping This Book
Before starting any trip, you should always glance at a road map in order to 
become familiar with the major roads and junctures that lie ahead. For the trip 
we are about to embark on, the ultimate destination is a deep understanding of 
the how, what, and why of computer networks. Our road map is the sequence of 
chapters of this book:
 1. Computer Networks and the Internet
 2. Application Layer
 3. Transport Layer
 4. Network Layer: Data Plane
 5. Network Layer: Control Plane
 6. The Link Layer and LANs
 7. Wireless and Mobile Networks
 8. Security in Computer Networks
Chapters 2 through 6 are the five core chapters of this book. You should notice 
that these chapters are organized around the top four layers of the five-layer Internet 
protocol. Further note that our journey will begin at the top of the Internet protocol 
stack, namely, the application layer, and will work its way downward. The rationale 
behind this top-down journey is that once we understand the applications, we can

understand the network services needed to support these applications. We can then, 
in turn, examine the various ways in which such services might be implemented by 
a network architecture. Covering applications early thus provides motivation for the 
remainder of the text.
The second half of the book‚ÄîChapters 7 and 8‚Äîzooms in on two enor-
mously important (and somewhat independent) topics in modern computer net-
working. In Chapter 7, we examine wireless and mobile networks, including 
wireless LANs (including WiFi and Bluetooth), Cellular networks (including 
4G and 5G), and mobility. Chapter 8, which addresses security in computer net-
works, first looks at the underpinnings of encryption and network security, and 
then we examine how the basic theory is being applied in a broad range of Inter-
net contexts.
Homework Problems and Questions
Chapter 1 Review Questions
SECTION 1.1
  R1. What is the difference between a host and an end system? List several differ-
ent types of end systems. Is a Web server an end system?
  R2. The word protocol is often used to describe diplomatic relations. How does 
Wikipedia describe diplomatic protocol?
  R3. Why are standards important for protocols?
SECTION 1.2
  R4. List four access technologies. Classify each one as home access, enterprise 
access, or wide-area wireless access.
  R5. Is HFC transmission rate dedicated or shared among users? Are collisions 
possible in a downstream HFC channel? Why or why not?
  R6. List the available residential access technologies in your city. For each 
type of access, provide the advertised downstream rate, upstream rate, and 
monthly price.
  R7. What is the transmission rate of Ethernet LANs?
  R8. What are some of the physical media that Ethernet can run over?
  R9. HFC, DSL, and FTTH are all used for residential access. For each of  
these access technologies, provide a range of  transmission rates and  
comment on whether the transmission rate is shared or dedicated.
 R10. Describe the most popular wireless Internet access technologies today. 
 Compare and contrast them.

SECTION 1.3
 R11. Suppose there is exactly one packet switch between a sending host and a 
receiving host. The transmission rates between the sending host and the 
switch and between the switch and the receiving host are R1 and R2, respec-
tively. Assuming that the switch uses store-and-forward packet switching, 
what is the total end-to-end delay to send a packet of length L? (Ignore queu-
ing, propagation delay, and processing delay.)
 R12. What advantage does a circuit-switched network have over a packet-switched net-
work? What advantages does TDM have over FDM in a circuit-switched network?
 R13. Suppose users share a 2 Mbps link. Also suppose each user transmits contin-
uously at 1 Mbps when transmitting, but each user transmits only 20 percent 
of the time. (See the discussion of statistical multiplexing in Section 1.3.)
a. When circuit switching is used, how many users can be supported?
b. For the remainder of this problem, suppose packet switching is used. Why 
will there be essentially no queuing delay before the link if two or fewer 
users transmit at the same time? Why will there be a queuing delay if 
three users transmit at the same time?
c. Find the probability that a given user is transmitting.
d. Suppose now there are three users. Find the probability that at any given 
time, all three users are transmitting simultaneously. Find the fraction of 
time during which the queue grows.
 R14. Why will two ISPs at the same level of the hierarchy often peer with each 
other? How does an IXP earn money?
 R15. Some content providers have created their own networks. Describe Google‚Äôs 
network. What motivates content providers to create these networks?
SECTION 1.4
 R16. Consider sending a packet from a source host to a destination host over a 
fixed route. List the delay components in the end-to-end delay. Which of 
these delays are constant and which are variable?
 R17. Visit the Transmission Versus Propagation Delay interactive animation at 
the companion Web site. Among the rates, propagation delay, and packet 
sizes available, find a combination for which the sender finishes transmitting 
before the first bit of the packet reaches the receiver. Find another combina-
tion for which the first bit of the packet reaches the receiver before the sender 
finishes transmitting.
 R18. How long does it take a packet of length 1,000 bytes to propagate over a 
link of distance 2,500 km, propagation speed 2.5 # 108 m/s, and transmission 
rate 2 Mbps? More generally, how long does it take a packet of length L to 
propagate over a link of distance d, propagation speed s, and transmission

rate R bps? Does this delay depend on packet length? Does this delay depend 
on transmission rate?
 R19. Suppose Host A wants to send a large file to Host B. The path from Host A to Host 
B has three links, of rates R1 = 500 kbps, R2 = 2 Mbps, and R3 = 1 Mbps.
a. Assuming no other traffic in the network, what is the throughput for the 
file transfer?
b. Suppose the file is 4 million bytes. Dividing the file size by the through-
put, roughly how long will it take to transfer the file to Host B?
c. Repeat (a) and (b), but now with R2 reduced to 100 kbps.
 R20. Suppose end system A wants to send a large file to end system B. At a very 
high level, describe how end system A creates packets from the file. When 
one of these packets arrives to a router, what information in the packet does 
the router use to determine the link onto which the packet is forwarded? 
Why is packet switching in the Internet analogous to driving from one city to 
another and asking directions along the way?
 R21. Visit the Queuing and Loss interactive animation  at the companion Web site. 
What is the maximum emission rate and the minimum transmission rate? 
With those rates, what is the traffic intensity? Run the interactive animation 
with these rates and determine how long it takes for packet loss to occur. 
Then repeat the experiment a second time and determine again how long it 
takes for packet loss to occur. Are the values different? Why or why not?
SECTION 1.5
 R22. List five tasks that a layer can perform. Is it possible that one (or more) of 
these tasks could be performed by two (or more) layers?
 R23. What are the five layers in the Internet protocol stack? What are the principal 
responsibilities of each of these layers?
 R24. What is an application-layer message? A transport-layer segment? A net-
work-layer datagram? A link-layer frame?
 R25. Which layers in the Internet protocol stack does a router process? Which lay-
ers does a link-layer switch process? Which layers does a host process?
SECTION 1.6
 R26. What is self-replicating malware?
 R27. Describe how a botnet can be created and how it can be used for a DDoS attack.
 R28. Suppose Alice and Bob are sending packets to each other over a computer 
network. Suppose Trudy positions herself in the network so that she can 
capture all the packets sent by Alice and send whatever she wants to Bob; she 
can also capture all the packets sent by Bob and send whatever she wants to 
Alice. List some of the malicious things Trudy can do from this position.

Problems
 P1. Design and describe an application-level protocol to be used between an 
automatic teller machine and a bank‚Äôs centralized computer. Your protocol 
should allow a user‚Äôs card and password to be verified, the account bal-
ance (which is maintained at the centralized computer) to be queried, and an 
account withdrawal to be made (that is, money disbursed to the user). Your 
protocol entities should be able to handle the all-too-common case in which 
there is not enough money in the account to cover the withdrawal. Specify 
your protocol by listing the messages exchanged and the action taken by the 
automatic teller machine or the bank‚Äôs centralized computer on transmission 
and receipt of messages. Sketch the operation of your protocol for the case of 
a simple withdrawal with no errors, using a diagram similar to that in Figure 1.2.  
Explicitly state the assumptions made by your protocol about the underlying 
end-to-end transport service.
 P2. Equation 1.1 gives a formula for the end-to-end delay of sending one packet 
of length L over N links of transmission rate R. Generalize this formula for 
sending P such packets back-to-back over the N links.
 P3. Consider an application that transmits data at a steady rate (for example, the 
sender generates an N-bit unit of data every k time units, where k is small 
and fixed). Also, when such an application starts, it will continue running 
for a relatively long period of time. Answer the following questions, briefly 
justifying your answer:
a. Would a packet-switched network or a circuit-switched network be more 
appropriate for this application? Why?
b. Suppose that a packet-switched network is used and the only traffic in 
this network comes from such applications as described above. Further-
more, assume that the sum of the application data rates is less than the 
capacities of each and every link. Is some form of congestion control 
needed? Why?
 P4. Consider the circuit-switched network in Figure 1.13. Recall that there are 
four circuits on each link. Label the four switches A, B, C, and D, going in 
the clockwise direction.
a. What is the maximum number of simultaneous connections that can be in 
progress at any one time in this network?
b. Suppose that all connections are between switches A and C. What is the 
maximum number of simultaneous connections that can be in progress?
c. Suppose we want to make four connections between switches A and C, 
and another four connections between switches B and D. Can we  
route these calls through the four links to accommodate all eight 
 connections?

P5. Review the car-caravan analogy in Section 1.4. Assume a propagation speed 
of 100 km/hour.
a. Suppose the caravan travels 175 km, beginning in front of one tollbooth, 
passing through a second tollbooth, and finishing just after a third toll-
booth. What is the end-to-end delay?
b. Repeat (a), now assuming that there are eight cars in the caravan instead 
of ten.
 P6. This elementary problem begins to explore propagation delay and transmis-
sion delay, two central concepts in data networking. Consider two hosts, A 
and B, connected by a single link of rate R bps. Suppose that the two hosts 
are separated by m meters, and suppose the propagation speed along the link 
is s meters/sec. Host A is to send a packet of size L bits to Host B.
a. Express the propagation delay, dprop, in terms of m and s.
b. Determine the transmission time of the packet, dtrans, in terms of L and R.
c. Ignoring processing and queuing delays, obtain an expression for the end-
to-end delay.
d. Suppose Host A begins to transmit the packet at time t = 0. At time t = 
dtrans, where is the last bit of the packet?
e. Suppose dprop is greater than dtrans. At time t = dtrans, where is the first  
bit of the packet?
f. Suppose dprop is less than dtrans. At time t = dtrans, where is the first bit of 
the packet?
g. Suppose s = 2.5 # 108, L = 1500 bytes, and R = 10 Mbps. Find the 
distance m so that dprop equals dtrans.
 P7. In this problem, we consider sending real-time voice from Host A to Host B 
over a packet-switched network (VoIP). Host A converts analog voice to a 
digital 64 kbps bit stream on the fly. Host A then groups the bits into 56-byte 
packets. There is one link between Hosts A and B; its transmission rate is 
10 Mbps and its propagation delay is 10 msec. As soon as Host A gathers a 
packet, it sends it to Host B. As soon as Host B receives an entire packet, it 
converts the packet‚Äôs bits to an analog signal. How much time elapses from 
the time a bit is created (from the original analog signal at Host A) until the 
bit is decoded (as part of the analog signal at Host B)?
 P8. Suppose users share a 10 Mbps link. Also suppose each user requires 200 kbps 
when transmitting, but each user transmits only 10 percent of the time. (See 
the discussion of packet switching versus circuit switching in Section 1.3.)
a. When circuit switching is used, how many users can be supported?
b. For the remainder of this problem, suppose packet switching is used. Find 
the probability that a given user is transmitting.
Exploring propagation 
delay and transmission 
delay
VideoNote

c. Suppose there are 120 users. Find the probability that at any given time, 
exactly n users are transmitting simultaneously. (Hint: Use the binomial 
distribution.)
d. Find the probability that there are 51 or more users transmitting 
 simultaneously.
 P9. Consider the discussion in Section 1.3 of packet switching versus circuit switch-
ing in which an example is provided with a 1 Mbps link. Users are generating 
data at a rate of 100 kbps when busy, but are busy generating data only with 
probability p = 0.1. Suppose that the 1 Mbps link is replaced by a 1 Gbps link.
a. What is N, the maximum number of users that can be supported simulta-
neously under circuit switching?
b. Now consider packet switching and a user population of M users. Give a 
formula (in terms of p, M, N) for the probability that more than N users 
are sending data.
 P10. Consider a packet of length L that begins at end system A and travels over 
three links to a destination end system. These three links are connected by 
two packet switches. Let di, si, and Ri denote the length, propagation speed, 
and the transmission rate of link i, for i = 1, 2, 3. The packet switch delays 
each packet by dproc. Assuming no queuing delays, in terms of di, si, Ri, 
(i = 1, 2, 3), and L, what is the total end-to-end delay for the packet? Sup-
pose now the packet is 1,500 bytes, the propagation speed on all three links is 
2.5 # 108m/s, the transmission rates of all three links are 2.5 Mbps, the packet 
switch processing delay is 3 msec, the length of the first link is 5,000 km, the 
length of the second link is 4,000 km, and the length of the last link is 1,000 
km. For these values, what is the end-to-end delay?
 P11. In the above problem, suppose R1 = R2 = R3 = R and dproc = 0. Further 
suppose that the packet switch does not store-and-forward packets but instead 
immediately transmits each bit it receives before waiting for the entire packet 
to arrive. What is the end-to-end delay?
 P12. A packet switch receives a packet and determines the outbound link to which 
the packet should be forwarded. When the packet arrives, one other packet is 
halfway done being transmitted on this outbound link and four other packets are 
waiting to be transmitted. Packets are transmitted in order of arrival. Suppose 
all packets are 1,500 bytes and the link rate is 2.5 Mbps. What is the queuing 
delay for the packet? More generally, what is the queuing delay when all packets 
have length L, the transmission rate is R, x bits of the currently-being-transmitted 
packet have been transmitted, and n packets are already in the queue?
 P13. (a)  Suppose N packets arrive simultaneously to a link at which no packets 
are currently being transmitted or queued. Each packet is of length L and 
the link has transmission rate R. What is the average queuing delay for 
the N packets?

(b) Now suppose that N such packets arrive to the link every LN/R seconds. 
What is the average queuing delay of a packet?
 P14. Consider the queuing delay in a router buffer. Let I denote traffic intensity; 
that is, I = La/R. Suppose that the queuing delay takes the form IL/R (1 - I) 
for I 6 1.
a. Provide a formula for the total delay, that is, the queuing delay plus the 
transmission delay.
b. Plot the total delay as a function of L /R.
 P15. Let a denote the rate of packets arriving at a link in packets/sec, and let ¬µ 
denote the link‚Äôs transmission rate in packets/sec. Based on the formula for 
the total delay (i.e., the queuing delay plus the transmission delay) derived 
in the previous problem, derive a formula for the total delay in terms of a 
and ¬µ.
 P16. Consider a router buffer preceding an outbound link. In this problem, you 
will use Little‚Äôs formula, a famous formula from queuing theory. Let N 
denote the average number of packets in the buffer plus the packet being 
transmitted. Let a denote the rate of packets arriving at the link. Let d denote 
the average total delay (i.e., the queuing delay plus the transmission delay) 
experienced by a packet. Little‚Äôs formula is N = a # d. Suppose that on  
average, the buffer contains 100 packets, and the average packet queuing 
delay is 20 msec. The link‚Äôs transmission rate is 100 packets/sec. Using  
Little‚Äôs formula, what is the average packet arrival rate, assuming there is  
no packet loss?
 P17. a.  Generalize Equation 1.2 in Section 1.4.3 for heterogeneous processing 
rates, transmission rates, and propagation delays.
b. Repeat (a), but now also suppose that there is an average queuing delay of 
dqueue at each node.
 P18. Perform a Traceroute between source and destination on the same continent 
at three different hours of the day.
a. Find the average and standard deviation of the round-trip delays at each of 
the three hours.
b. Find the number of routers in the path at each of the three hours. Did the 
paths change during any of the hours?
c. Try to identify the number of ISP networks that the Traceroute packets 
pass through from source to destination. Routers with similar names and/
or similar IP addresses should be considered as part of the same ISP. In 
your experiments, do the largest delays occur at the peering interfaces 
between adjacent ISPs?
d. Repeat the above for a source and destination on different continents. 
Compare the intra-continent and inter-continent results.
Using Traceroute to 
discover network  
paths and measure 
network delay
VideoNote

P19. Metcalfe‚Äôs law states the value of a computer network is proportional to 
the square of the number of connected users of the system. Let n denote the 
number of users in a computer network. Assuming each user sends one mes-
sage to each of the other users, how many messages will be sent? Does your 
answer support Metcalfe‚Äôs law?
 P20. Consider the throughput example corresponding to Figure 1.20(b). Now 
suppose that there are M client-server pairs rather than 10. Denote Rs, Rc, 
and R for the rates of the server links, client links, and network link. Assume 
all other links have abundant capacity and that there is no other traffic in the 
network besides the traffic generated by the M client-server pairs. Derive a 
general expression for throughput in terms of Rs, Rc, R, and M.
 P21. Consider Figure 1.19(b). Now suppose that there are M paths between the 
server and the client. No two paths share any link. Path k (k = 1, c, M) 
consists of N links with transmission rates Rk
1, Rk
2, c, Rk
N. If the server can 
only use one path to send data to the client, what is the maximum throughput 
that the server can achieve? If the server can use all M paths to send data, 
what is the maximum throughput that the server can achieve?
 P22. Consider Figure 1.19(b). Suppose that each link between the server and the 
client has a packet loss probability p, and the packet loss probabilities for 
these links are independent. What is the probability that a packet (sent by the 
server) is successfully received by the receiver? If a packet is lost in the path 
from the server to the client, then the server will re-transmit the packet. On 
average, how many times will the server re-transmit the packet in order for 
the client to successfully receive the packet?
 P23. Consider Figure 1.19(a). Assume that we know the bottleneck link along the 
path from the server to the client is the first link with rate Rs bits/sec. Suppose 
we send a pair of packets back to back from the server to the client, and there 
is no other traffic on this path. Assume each packet of size L bits, and both 
links have the same propagation delay  dprop.
a. What is the packet inter-arrival time at the destination? That is, how much 
time elapses from when the last bit of the first packet arrives until the last 
bit of the second packet arrives?
b. Now assume that the second link is the bottleneck link (i.e., Rc 6 Rs). Is 
it possible that the second packet queues at the input queue of the second 
link? Explain. Now suppose that the server sends the second packet T 
seconds after sending the first packet. How large must T be to ensure no 
queuing before the second link? Explain.
 P24. Suppose you would like to urgently deliver 50 terabytes data from Boston to 
Los Angeles. You have available a 100 Mbps dedicated link for data transfer. 
Would you prefer to transmit the data via this link or instead use FedEx over-
night delivery? Explain.

P25. Suppose two hosts, A and B, are separated by 20,000 kilometers and are con-
nected by a direct link of R = 5 Mbps. Suppose the propagation speed over 
the link is 2.5 # 108 meters/sec.
a. Calculate the bandwidth-delay product, R #  dprop.
b. Consider sending a file of 800,000 bits from Host A to Host B. Suppose 
the file is sent continuously as one large message. What is the maximum 
number of bits that will be in the link at any given time?
c. Provide an interpretation of the bandwidth-delay product.
d. What is the width (in meters) of a bit in the link? Is it longer than a 
 football field?
e. Derive a general expression for the width of a bit in terms of the 
propagation speed s, the transmission rate R, and the length of the  
link m.
 P26. Referring to problem P24, suppose we can modify R. For what value of R is 
the width of a bit as long as the length of the link?
 P27. Consider problem P24 but now with a link of R = 500 Mbps.
a. Calculate the bandwidth-delay product, R #  dprop.
b. Consider sending a file of 800,000 bits from Host A to Host B. Suppose 
the file is sent continuously as one big message. What is the maximum 
number of bits that will be in the link at any given time?
c. What is the width (in meters) of a bit in the link?
 P28. Refer again to problem P24.
a. How long does it take to send the file, assuming it is sent continuously?
b. Suppose now the file is broken up into 20 packets with each packet 
containing 40,000 bits. Suppose that each packet is acknowledged by 
the receiver and the transmission time of an acknowledgment packet is 
negligible. Finally, assume that the sender cannot send a packet until the 
preceding one is acknowledged. How long does it take to send the file?
c. Compare the results from (a) and (b).
 P29. Suppose there is a 10 Mbps microwave link between a geostationary  
satellite and its base station on Earth. Every minute the satellite takes a digi-
tal photo and sends it to the base station. Assume a propagation speed  
of 2.4 # 108 meters/sec.
a. What is the propagation delay of the link?
b. What is the bandwidth-delay product, R #  dprop?
c. Let x denote the size of the photo. What is the minimum value of x for the 
microwave link to be continuously transmitting?

P30. Consider the airline travel analogy in our discussion of layering in Section 1.5, 
and the addition of headers to protocol data units as they flow down the proto-
col stack. Is there an equivalent notion of header information that is added to 
passengers and baggage as they move down the airline protocol stack?
 P31. In modern packet-switched networks, including the Internet, the source host seg-
ments long, application-layer messages (for example, an image or a music file) 
into smaller packets and sends the packets into the network. The receiver then 
reassembles the packets back into the original message. We refer to this process as  
message segmentation. Figure 1.27 illustrates the end-to-end transport of a message 
with and without message segmentation. Consider a message that is 106 bits 
long that is to be sent from source to destination in Figure 1.27. Suppose each 
link in the figure is 5 Mbps. Ignore propagation, queuing, and processing delays.
a. Consider sending the message from source to destination without message 
segmentation. How long does it take to move the message from the source 
host to the first packet switch? Keeping in mind that each switch uses 
store-and-forward packet switching, what is the total time to move the 
message from source host to destination host?
b. Now suppose that the message is segmented into 100 packets, with each 
packet being 10,000 bits long. How long does it take to move the first 
packet from source host to the first switch? When the first packet is being 
sent from the first switch to the second switch, the second packet is being 
sent from the source host to the first switch. At what time will the second 
packet be fully received at the first switch?
c. How long does it take to move the file from source host to destination 
host when message segmentation is used? Compare this result with your 
answer in part (a) and comment.
Figure 1.27 ‚ô¶  End-to-end message transport: (a) without message 
 segmentation; (b) with message segmentation
Source
a.
Packet switch
Packet switch
Destination
Message
Source
b.
Packet switch
Packet
Packet switch
Destination

d. In addition to reducing delay, what are reasons to use message 
 segmentation?
e. Discuss the drawbacks of message segmentation.
 P32. Experiment with the Message Segmentation interactive animation at the book‚Äôs 
Web site. Do the delays in the interactive animation correspond to the delays 
in the previous problem? How do link propagation delays affect the overall 
end-to-end delay for packet switching (with message segmentation) and for 
message switching?
 P33. Consider sending a large file of F bits from Host A to Host B. There are three 
links (and two switches) between A and B, and the links are uncongested 
(that is, no queuing delays). Host A segments the file into segments of S bits 
each and adds 80 bits of header to each segment, forming packets of L = 80 +  
S bits. Each link has a transmission rate of R bps. Find the value of S that 
minimizes the delay of moving the file from Host A to Host B. Disregard 
propagation delay.
 P34. Skype offers a service that allows you to make a phone call from a PC to an 
ordinary phone. This means that the voice call must pass through both the 
Internet and through a telephone network. Discuss how this might be done.
Wireshark Lab
‚ÄúTell me and I forget. Show me and I remember. Involve me and I understand.‚Äù
Chinese proverb
One‚Äôs understanding of network protocols can often be greatly deepened by seeing 
them in action and by playing around with them‚Äîobserving the sequence of mes-
sages exchanged between two protocol entities, delving into the details of protocol 
operation, causing protocols to perform certain actions, and observing these actions 
and their consequences. This can be done in simulated scenarios or in a real network 
environment such as the Internet. The interactive animations at the textbook Web site 
take the first approach. In the Wireshark labs, we‚Äôll take the latter approach. You‚Äôll 
run network applications in various scenarios using a computer on your desk, at 
home, or in a lab. You‚Äôll observe the network protocols in your computer, interacting 
and exchanging messages with protocol entities executing elsewhere in the Inter-
net. Thus, you and your computer will be an integral part of these live labs. You‚Äôll 
observe‚Äîand you‚Äôll learn‚Äîby doing.
The basic tool for observing the messages exchanged between executing pro-
tocol entities is called a packet sniffer. As the name suggests, a packet sniffer pas-
sively copies (sniffs) messages being sent from and received by your computer; it 
also displays the contents of the various protocol fields of these captured messages. 
A screenshot of the Wireshark packet sniffer is shown in Figure 1.28. Wireshark is a

free packet sniffer that runs on Windows, Linux/Unix, and Mac computers. Through-
out the textbook, you will find Wireshark labs that allow you to explore a number 
of the protocols studied in the chapter. In this first Wireshark lab, you‚Äôll obtain and 
install a copy of Wireshark, access a Web site, and capture and examine the protocol 
messages being exchanged between your Web browser and the Web server.
You can find full details about this first Wireshark lab (including instructions 
about how to obtain and install Wireshark) at the Web site www.pearson.com/
cs-resources.
Figure 1.28 ‚ô¶  A Wireshark screenshot (Wireshark screenshot reprinted  
by permission of the Wireshark Foundation.)
Command
menus
Listing of
captured
packets
Details of
selected
packet
header
Packet
contents in
hexadecimal
and ASCII

What made you decide to specialize in networking/Internet technology?
As a PhD student at MIT in 1959, I looked around and found that most of my classmates 
were doing research in the area of information theory and coding theory that had been 
established by the great researcher, Claude Shannon.  I judged that he had solved most   of 
the important problems already. The research problems that were left were hard and seemed 
to me to be of lesser consequence. So I decided to launch out in a new area that no one 
else had yet conceived of.  Happily, at MIT I was surrounded by many computers, and it 
was clear to me that, sooner or later, these machines would need to communicate with each 
other. At the time, there was no effective way for them to do so and that the solution to this 
important problem would have impact.  I had an approach to this problem and so, for my 
PhD research, I decided to create a mathematical theory to model, evaluate, design and  
optimize efficient and reliable data networks.
What was your first job in the computer industry? What did it entail?
I went to the evening session at CCNY from 1951 to 1957 for my bachelor‚Äôs degree 
in electrical engineering. During the day, I worked first as a technician and then as an 
electrical engineer at a small, industrial electronics firm called Photobell. While there, I 
introduced digital technology to their product line. Essentially, we were using photoelec-
tric devices to detect the presence of certain items (boxes, people, etc.) and the use of a 
circuit known then as a bistable multivibrator was just what we needed to bring digital 
processing into this field of detection. These circuits happen to be the building blocks for 
computers, and have come to be known as flip-flops or switches in today‚Äôs vernacular.
What was going through your mind when you sent the first host-to-host message (from 
UCLA to the Stanford Research Institute)?
Frankly, we had no idea of the importance of that event. We had not prepared a special 
message of historic significance, as did so many inventors of the past (Samuel Morse with 
‚ÄúWhat hath God wrought.‚Äù or Alexander Graham Bell with ‚ÄúWatson, come here! I want you.‚Äù  
or Neal Armstrong with ‚ÄúThat‚Äôs one small step for a man, one giant leap for mankind.‚Äù)  
Those guys were smart! They understood media and public relations. All we wanted to do 
was to demonstrate our ability to remotely login to the SRI computer. So we typed the ‚ÄúL‚Äù, 
AN INTERVIEW WITH‚Ä¶
Leonard Kleinrock
Leonard Kleinrock is a professor of computer science at the University 
of California, Los Angeles. In 1969, his computer at UCLA became 
the first node of the Internet. His creation of the  mathematical  theory 
of packet-switching principles in 1961 became the technology behind 
the Internet. He received his B.E.E. from the City College of New York 
(CCNY) and his masters and PhD in electrical engineering from MIT.
Courtesy of Leonard Kleinrock

which was correctly received, we typed the ‚Äúo‚Äù which was correctly received, and then we 
typed the ‚Äúg‚Äù which caused the SRI host computer to crash! So, it turned out that our mes-
sage was the shortest and perhaps the most prophetic message ever, namely ‚ÄúLo!‚Äù as in  
‚ÄúLo and behold!‚Äù
Earlier that year, I was quoted in a UCLA press release saying that once the network 
was up and running, it would be possible to gain access to computer utilities from our 
homes and offices as easily as we gain access to electricity and telephone connectivity. So 
my vision at that time was that the Internet would be ubiquitous, always on, always avail-
able, anyone with any device could connect from any location, and it would be invisible. 
However, I never anticipated that my 99-year-old mother would use the Internet at the same 
time that my 5 year-old granddaughter was‚Äîand indeed she did!
What is your vision for the future of networking?
The easy part of the vision is to predict the infrastructure itself. I anticipate that we will see  
considerable deployment of wireless and mobile devices in smart spaces to produce what 
I like to refer to as the Invisible Internet. This step will enable us to move out from the 
netherworld of cyberspace to the physical world of smart spaces. Our environments (desks, 
walls, vehicles, watches, belts, fingernails, bodies and so on) will come alive with technol-
ogy, through actuators, sensors, logic, processing, storage, cameras, microphones, speak-
ers, displays, and communication. This embedded technology will allow our environment 
to provide the IP services wherever and whenever we want. When I walk into a room, the 
room will know I entered. I will be able to communicate with my environment naturally, 
as in spoken English, haptics, gestures, and eventually through brain-Internet interfaces; 
my requests will generate replies that present Web pages to me from wall displays, through 
my eyeglasses, as speech, holograms, and so forth. Looking a bit further out, I see a net-
working future that includes the following additional key components. I see customized 
intelligent software agents deployed across the network whose function it is to mine data, 
act on that data, observe trends, and carry out tasks dynamically and adaptively. I see the 
deployment of blockchain technology that provides irrefutable, immutable distributed 
ledgers coupled with reputation systems that provide credibility to the contents and func-
tionality. I see considerably more network traffic generated not so much by humans, but 
by the embedded devices, the intelligent software agents and the distributed ledgers. I see 
large collections of self-organizing systems controlling this vast, fast network. I see huge 
amounts of information flashing across this network instantaneously with this information 
undergoing enormous processing and filtering. The Invisible Internet will essentially be 
a pervasive global nervous system . I see all these things and more as we move headlong 
through the twenty-first century.
The harder part of the vision is to predict the applications and services, which have 
consistently surprised us in dramatic ways (e-mail, search technologies, the World Wide 
Web, blogs, peer-to-peer networks, social networks, user generated content, sharing of

music, photos, and videos, etc.). These applications have ‚Äúcome of the blue‚Äù, sudden,  
unanticipated and explosive. What a wonderful world for the next generation to explore!
What people have inspired you professionally?
By far, it was Claude Shannon from MIT, a brilliant researcher who had the ability to relate 
his mathematical ideas to the physical world in highly intuitive ways. He was a superb 
member of my PhD thesis committee.
Do you have any advice for students entering the networking/Internet field?
The Internet and all that it enables is a vast new frontier, continuously full of amazing  
challenges. There is room for great innovation. Don‚Äôt be constrained by today‚Äôs technology. 
Reach out and imagine what could be and then make it happen.

81
Network applications are the raisons d‚Äô√™tre of a computer network‚Äîif we couldn‚Äôt  
conceive of any useful applications, there wouldn‚Äôt be any need for networking infra-
structure and protocols to support them. Since the Internet‚Äôs inception, numerous useful 
and entertaining applications have indeed been created. These applications have been the 
driving force behind the Internet‚Äôs success, motivating people in homes, schools, govern-
ments, and businesses to make the Internet an integral part of their daily activities.
Internet applications include the classic text-based applications that became pop-
ular in the 1970s and 1980s: text e-mail, remote access to computers, file transfers, and  
newsgroups. They include the killer application of the mid-1990s, the World Wide 
Web, encompassing Web surfing, search, and electronic commerce. Since the begin-
ning of new millennium, new and highly compelling applications continue to emerge, 
including voice over IP and video conferencing such as Skype, Facetime, and Google 
Hangouts; user generated video such as YouTube and movies on demand such as 
Netflix; and multiplayer online games such as Second Life and World of Warcraft. 
During this same period, we have seen the emergence of a new generation of social 
networking applications‚Äîsuch as Facebook, Instagram, and Twitter‚Äîwhich have 
created human networks on top of the Internet‚Äôs network or routers and communi-
cation links. And most recently, along with the arrival of the smartphone and the 
ubiquity of 4G/5G wireless Internet access, there has been a profusion of location 
based mobile apps, including popular check-in, dating, and road-traffic forecasting 
apps (such as Yelp, Tinder, and Waz), mobile payment apps (such as WeChat and 
Apple Pay) and messaging apps (such as WeChat and WhatsApp). Clearly, there has 
been no slowing down of new and exciting Internet applications. Perhaps some of 
the readers of this text will create the next generation of killer Internet applications!
Application 
Layer
2
CHAPTER
81

In this chapter, we study the conceptual and implementation aspects of network 
applications. We begin by defining key application-layer concepts, including net-
work services required by applications, clients and servers, processes, and trans-
port-layer interfaces. We examine several network applications in detail, including the 
Web, e-mail, DNS, peer-to-peer (P2P) file distribution, and video streaming. We then 
cover network application development, over both TCP and UDP. In particular, we 
study the socket interface and walk through some simple client-server applications 
in Python. We also provide several fun and interesting socket programming assign-
ments at the end of the chapter.
The application layer is a particularly good place to start our study of protocols. 
It‚Äôs familiar ground. We‚Äôre acquainted with many of the applications that rely on 
the protocols we‚Äôll study. It will give us a good feel for what protocols are all about 
and will introduce us to many of the same issues that we‚Äôll see again when we study 
transport, network, and link layer protocols.
2.1 Principles of Network Applications
Suppose you have an idea for a new network application. Perhaps this application 
will be a great service to humanity, or will please your professor, or will bring you 
great wealth, or will simply be fun to develop. Whatever the motivation may be, let‚Äôs 
now examine how you transform the idea into a real-world network application.
At the core of network application development is writing programs that run on 
different end systems and communicate with each other over the network. For exam-
ple, in the Web application there are two distinct programs that communicate with 
each other: the browser program running in the user‚Äôs host (desktop, laptop, tablet, 
smartphone, and so on); and the Web server program running in the Web server host. 
As another example, in a Video on Demand application such as Netflix (see Sec-
tion¬†2.6), there is a Netflix-provided program running on the user‚Äôs smartphone, tablet, 
or computer; and a Netflix server program running on the Netflix server host. Servers 
often (but certainly not always) are housed in a data center, as shown in Figure 2.1.
Thus, when developing your new application, you need to write software that 
will run on multiple end systems. This software could be written, for example, in 
C, Java, or Python. Importantly, you do not need to write software that runs on net-
work-core devices, such as routers or link-layer switches. Even if you wanted to 
write application software for these network-core devices, you wouldn‚Äôt be able to 
do so. As we learned in Chapter 1, and as shown earlier in Figure 1.24, network-core 
devices do not function at the application layer but instead function at lower layers‚Äî
specifically at the network layer and below. This basic design‚Äînamely, confining 
application software to the end systems‚Äîas shown in Figure 2.1, has facilitated the 
rapid development and deployment of a vast array of network applications.

Transport
Network
Data Link
Physical
Transport
Network
Data Link
Physical
Transport
Network
Data Link
Physical
Content Provider Network
National or
Global ISP
Datacenter Network
Datacenter Network
Mobile Network
Enterprise Network
Local or
Regional ISP
Application
Application
Application
Home Network
Figure 2.1 ‚ô¶  Communication for a network application takes place  
between end systems at the application layer

2.1.1 Network Application Architectures
Before diving into software coding, you should have a broad architectural plan for 
your application. Keep in mind that an application‚Äôs architecture is distinctly differ-
ent from the network architecture (e.g., the five-layer Internet architecture discussed 
in Chapter 1). From the application developer‚Äôs perspective, the network architec-
ture is fixed and provides a specific set of services to applications. The application  
architecture, on the other hand, is designed by the application developer and dic-
tates how the application is structured over the various end systems. In choosing 
the application architecture, an application developer will likely draw on one of the 
two predominant architectural paradigms used in modern network applications: the 
client-server architecture or the peer-to-peer (P2P) architecture.
In a client-server architecture, there is an always-on host, called the server, 
which services requests from many other hosts, called clients. A classic example 
is the Web application for which an always-on Web server services requests from 
browsers running on client hosts. When a Web server receives a request for an object 
from a client host, it responds by sending the requested object to the client host. Note 
that with the client-server architecture, clients do not directly communicate with each 
other; for example, in the Web application, two browsers do not directly communi-
cate. Another characteristic of the client-server architecture is that the server has a 
fixed, well-known address, called an IP address (which we‚Äôll discuss soon). Because 
the server has a fixed, well-known address, and because the server is always on, a cli-
ent can always contact the server by sending a packet to the server‚Äôs IP address. Some 
of the better-known applications with a client-server architecture include the Web, 
FTP, Telnet, and e-mail. The client-server architecture is shown in Figure 2.2(a).
Often in a client-server application, a single-server host is incapable of keep-
ing up with all the requests from clients. For example, a popular social-networking 
site can quickly become overwhelmed if it has only one server handling all of its 
requests. For this reason, a data center, housing a large number of hosts, is often 
used to create a powerful virtual server. The most popular Internet services‚Äîsuch 
as search engines (e.g., Google, Bing, Baidu), Internet commerce (e.g., Amazon, 
eBay, Alibaba), Web-based e-mail (e.g., Gmail and Yahoo Mail), social media (e.g., 
Facebook, Instagram, Twitter, and WeChat)‚Äîrun in one or more data centers. As 
discussed in Section 1.3.3, Google has 19 data centers distributed around the world, 
which collectively handle search, YouTube, Gmail, and other services. A data center 
can have hundreds of thousands of servers, which must be powered and maintained. 
Additionally, the service providers must pay recurring interconnection and band-
width costs for sending data from their data centers.
In a P2P architecture, there is minimal (or no) reliance on dedicated servers in 
data centers. Instead the application exploits direct communication between pairs of 
intermittently connected hosts, called peers. The peers are not owned by the service 
provider, but are instead desktops and laptops controlled by users, with most of the 
peers residing in homes, universities, and offices. Because the peers communicate

without passing through a dedicated server, the architecture is called peer-to-peer. 
An example of a popular P2P application is the file-sharing application BitTorrent.
One of the most compelling features of P2P architectures is their self- 
scalability. For example, in a P2P file-sharing application, although each peer 
 generates workload by requesting files, each peer also adds service capacity to the 
system by distributing files to other peers. P2P architectures are also cost effective, 
since they normally don‚Äôt require significant server infrastructure and server band-
width (in contrast with clients-server designs with datacenters). However, P2P appli-
cations face challenges of security, performance, and reliability due to their highly 
 decentralized structure.
2.1.2 Processes Communicating
Before building your network application, you also need a basic understanding of 
how the programs, running in multiple end systems, communicate with each other. 
In the jargon of operating systems, it is not actually programs but processes that 
a. Client-server architecture
b. Peer-to-peer architecture
Figure 2.2 ‚ô¶ (a) Client-server architecture; (b) P2P architecture

communicate. A process can be thought of as a program that is running within an end 
system. When processes are running on the same end system, they can communicate 
with each other with interprocess communication, using rules that are governed by 
the end system‚Äôs operating system. But in this book, we are not particularly interested 
in how processes in the same host communicate, but instead in how processes run-
ning on different hosts (with potentially different operating systems) communicate.
Processes on two different end systems communicate with each other by 
exchanging messages across the computer network. A sending process creates and 
sends messages into the network; a receiving process receives these messages and 
possibly responds by sending messages back. Figure 2.1 illustrates that processes 
communicating with each other reside in the application layer of the five-layer pro-
tocol stack.
Client and Server Processes
A network application consists of pairs of processes that send messages to each 
other over a network. For example, in the Web application a client browser process 
exchanges messages with a Web server process. In a P2P file-sharing system, a file 
is transferred from a process in one peer to a process in another peer. For each pair of 
communicating processes, we typically label one of the two processes as the client 
and the other process as the server. With the Web, a browser is a client process and 
a Web server is a server process. With P2P file sharing, the peer that is downloading 
the file is labeled as the client, and the peer that is uploading the file is labeled as 
the server.
You may have observed that in some applications, such as in P2P file sharing, 
a process can be both a client and a server. Indeed, a process in a P2P file-sharing 
system can both upload and download files. Nevertheless, in the context of any given 
communication session between a pair of processes, we can still label one process 
as the client and the other process as the server. We define the client and server pro-
cesses as follows:
In the context of a communication session between a pair of processes, the pro-
cess that initiates the communication (that is, initially contacts the other process 
at the beginning of the session) is labeled as the client. The process that waits to 
be contacted to begin the session is the server.
In the Web, a browser process initializes contact with a Web server process; 
hence the browser process is the client and the Web server process is the server. In 
P2P file sharing, when Peer A asks Peer B to send a specific file, Peer A is the cli-
ent and Peer B is the server in the context of this specific communication session. 
When there‚Äôs no confusion, we‚Äôll sometimes also use the terminology ‚Äúclient side 
and server side of an application.‚Äù At the end of this chapter, we‚Äôll step through sim-
ple code for both the client and server sides of network applications.

The Interface Between the Process and the Computer Network
As noted above, most applications consist of pairs of communicating processes, with 
the two processes in each pair sending messages to each other. Any message sent 
from one process to another must go through the underlying network. A process 
sends messages into, and receives messages from, the network through a software 
interface called a socket. Let‚Äôs consider an analogy to help us understand processes 
and sockets. A process is analogous to a house and its socket is analogous to its door. 
When a process wants to send a message to another process on another host, it shoves 
the message out its door (socket). This sending process assumes that there is a trans-
portation infrastructure on the other side of its door that will transport the message to 
the door of the destination process. Once the message arrives at the destination host, 
the message passes through the receiving process‚Äôs door (socket), and the receiving 
process then acts on the message.
Figure 2.3 illustrates socket communication between two processes that com-
municate over the Internet. (Figure 2.3 assumes that the underlying transport protocol 
used by the processes is the Internet‚Äôs TCP protocol.) As shown in this figure, a socket 
is the interface between the application layer and the transport layer within a host. It 
is also referred to as the Application Programming Interface (API) between the 
application and the network, since the socket is the programming interface with which 
network applications are built. The application developer has control of everything on 
the application-layer side of the socket but has little control of the transport-layer side 
of the socket. The only control that the application developer has on the transport-
layer side is (1) the choice of transport protocol and (2) perhaps the ability to fix a few 
Process
Host or
server
Host or
server
Controlled
by application
developer
Controlled
by application
developer
Process
TCP with
buffers,
variables
Internet
Controlled
by operating
system
Controlled
by operating
system
TCP with
buffers,
variables
Socket
Socket
Figure 2.3 ‚ô¶  Application processes, sockets, and underlying transport protocol

transport-layer parameters such as maximum buffer and maximum segment sizes (to 
be covered in Chapter 3). Once the application developer chooses a transport protocol 
(if a choice is available), the application is built using the transport-layer services 
provided by that protocol. We‚Äôll explore sockets in some detail in Section 2.7.
Addressing Processes
In order to send postal mail to a particular destination, the destination needs to have 
an address. Similarly, in order for a process running on one host to send packets to 
a process running on another host, the receiving process needs to have an address. 
To identify the receiving process, two pieces of information need to be specified: 
(1)¬†the address of the host and (2) an identifier that specifies the receiving process in 
the destination host.
In the Internet, the host is identified by its IP address. We‚Äôll discuss IP addresses 
in great detail in Chapter 4. For now, all we need to know is that an IP address is a 32-bit 
quantity that we can think of as uniquely identifying the host. In addition to know-
ing the address of the host to which a message is destined, the sending process must 
also identify the receiving process (more specifically, the receiving socket) running in 
the host. This information is needed because in general a host could be running many 
network applications. A destination port number serves this purpose. Popular applica-
tions have been assigned specific port numbers. For example, a Web server is identified 
by port number 80. A mail server process (using the SMTP protocol) is identified by 
port number 25. A list of well-known port numbers for all Internet standard protocols 
can be found at www.iana.org. We‚Äôll examine port numbers in detail in Chapter 3.
2.1.3 Transport Services Available to Applications
Recall that a socket is the interface between the application process and the trans-
port-layer protocol. The application at the sending side pushes messages through the 
socket. At the other side of the socket, the transport-layer protocol has the responsi-
bility of getting the messages to the socket of the receiving process.
Many networks, including the Internet, provide more than one transport-layer 
protocol. When you develop an application, you must choose one of the available 
transport-layer protocols. How do you make this choice? Most likely, you would 
study the services provided by the available transport-layer protocols, and then pick 
the protocol with the services that best match your application‚Äôs needs. The situation 
is similar to choosing either train or airplane transport for travel between two cities. 
You have to choose one or the other, and each transportation mode offers different 
services. (For example, the train offers downtown pickup and drop-off, whereas the 
plane offers shorter travel time.)
What are the services that a transport-layer protocol can offer to applications 
invoking it? We can broadly classify the possible services along four dimensions: 
reliable data transfer, throughput, timing, and security.

Reliable Data Transfer
As discussed in Chapter 1, packets can get lost within a computer network. For exam-
ple, a packet can overflow a buffer in a router, or can be discarded by a host or router 
after having some of its bits corrupted. For many applications‚Äîsuch as electronic 
mail, file transfer, remote host access, Web document transfers, and financial appli-
cations‚Äîdata loss can have devastating consequences (in the latter case, for either 
the bank or the customer!). Thus, to support these applications, something has to be 
done to guarantee that the data sent by one end of the application is delivered cor-
rectly and completely to the other end of the application. If a protocol provides such 
a guaranteed data delivery service, it is said to provide reliable data transfer. One 
important service that a transport-layer protocol can potentially provide to an applica-
tion is process-to-process reliable data transfer. When a transport protocol provides 
this service, the sending process can just pass its data into the socket and know with 
complete confidence that the data will arrive without errors at the receiving process.
When a transport-layer protocol doesn‚Äôt provide reliable data transfer, some of 
the data sent by the sending process may never arrive at the receiving process. This 
may be acceptable for loss-tolerant applications, most notably multimedia applica-
tions such as conversational audio/video that can tolerate some amount of data loss. 
In these multimedia applications, lost data might result in a small glitch in the audio/
video‚Äînot a crucial impairment.
Throughput
In Chapter 1, we introduced the concept of available throughput, which, in the 
context of a communication session between two processes along a network path, 
is the rate at which the sending process can deliver bits to the receiving process. 
Because other sessions will be sharing the bandwidth along the network path, and 
because these other sessions will be coming and going, the available throughput 
can fluctuate with time. These observations lead to another natural service that a 
transport-layer protocol could provide, namely, guaranteed available throughput at 
some specified rate. With such a service, the application could request a guaranteed 
throughput of r bits/sec, and the transport protocol would then ensure that the avail-
able throughput is always at least r bits/sec. Such a guaranteed throughput service 
would appeal to many applications. For example, if an Internet telephony applica-
tion encodes voice at 32 kbps, it needs to send data into the network and have data 
delivered to the receiving application at this rate. If the transport protocol cannot 
provide this throughput, the application would need to encode at a lower rate (and 
receive enough throughput to sustain this lower coding rate) or may have to give 
up, since receiving, say, half of the needed throughput is of little or no use to this 
Internet telephony application. Applications that have throughput requirements are 
said to be bandwidth-sensitive applications. Many current multimedia applications 
are bandwidth sensitive, although some multimedia applications may use adaptive

coding techniques to encode digitized voice or video at a rate that matches the cur-
rently available throughput.
While bandwidth-sensitive applications have specific throughput requirements, 
elastic applications can make use of as much, or as little, throughput as happens to 
be available. Electronic mail, file transfer, and Web transfers are all elastic applica-
tions. Of course, the more throughput, the better. There‚Äôs an adage that says that one 
cannot be too rich, too thin, or have too much throughput!
Timing
A transport-layer protocol can also provide timing guarantees. As with throughput 
guarantees, timing guarantees can come in many shapes and forms. An example 
guarantee might be that every bit that the sender pumps into the socket arrives 
at the receiver‚Äôs socket no more than 100 msec later. Such a service would be 
appealing to interactive real-time applications, such as Internet telephony, virtual 
environments, teleconferencing, and multiplayer games, all of which require tight 
timing constraints on data delivery in order to be effective, see [Gauthier 1999; 
Ramjee 1994]. Long delays in Internet telephony, for example, tend to result in 
unnatural pauses in the conversation; in a multiplayer game or virtual interactive 
environment, a long delay between taking an action and seeing the response from 
the environment (for example, from another player at the end of an end-to-end con-
nection) makes the application feel less realistic. For non-real-time applications, 
lower delay is always preferable to higher delay, but no tight constraint is placed 
on the end-to-end delays.
Security
Finally, a transport protocol can provide an application with one or more security 
services. For example, in the sending host, a transport protocol can encrypt all data 
transmitted by the sending process, and in the receiving host, the transport-layer pro-
tocol can decrypt the data before delivering the data to the receiving process. Such a 
service would provide confidentiality between the two processes, even if the data is 
somehow observed between sending and receiving processes. A transport protocol 
can also provide other security services in addition to confidentiality, including data 
integrity and end-point authentication, topics that we‚Äôll cover in detail in Chapter 8.
2.1.4 Transport Services Provided by the Internet
Up until this point, we have been considering transport services that a computer net-
work could provide in general. Let‚Äôs now get more specific and examine the type of 
transport services provided by the Internet. The Internet (and, more generally, TCP/
IP networks) makes two transport protocols available to applications, UDP and TCP. 
When you (as an application developer) create a new network application for the

Internet, one of the first decisions you have to make is whether to use UDP or TCP. 
Each of these protocols offers a different set of services to the invoking applications. 
Figure 2.4 shows the service requirements for some selected applications.
TCP Services
The TCP service model includes a connection-oriented service and a reliable data 
transfer service. When an application invokes TCP as its transport protocol, the 
application receives both of these services from TCP.
‚Ä¢ Connection-oriented service. TCP has the client and server exchange transport-
layer control information with each other before the application-level mes-
sages begin to flow. This so-called handshaking procedure alerts the client 
and server, allowing them to prepare for an onslaught of packets. After the 
handshaking phase, a TCP connection is said to exist between the sockets 
of the two processes. The connection is a full-duplex connection in that the two 
processes can send messages to each other over the connection at the same time. 
When the application finishes sending messages, it must tear down the connec-
tion. In Chapter 3, we‚Äôll discuss connection-oriented service in detail and examine 
how it is implemented.
‚Ä¢ Reliable data transfer service. The communicating processes can rely on TCP to 
deliver all data sent without error and in the proper order. When one side of the 
application passes a stream of bytes into a socket, it can count on TCP to deliver the 
same stream of bytes to the receiving socket, with no missing or duplicate bytes.
Application
Data Loss
Throughput
Time-Sensitive
File transfer/download
No loss
Elastic
No
E-mail
No loss
Elastic
No
Web documents
No loss
Elastic (few kbps)
No
Internet telephony/
Video conferencing
Loss-tolerant
Audio: few kbps‚Äì1 Mbps
Video: 10 kbps‚Äì5 Mbps
Yes: 100s of msec
Streaming stored 
Loss-tolerant
Same as above
Yes: few seconds
audio/video
Interactive games
Loss-tolerant
Few kbps‚Äì10 kbps
Yes: 100s of msec
Smartphone messaging 
No loss 
Elastic 
Yes and no
Figure 2.4 ‚ô¶ Requirements of selected network applications

TCP also includes a congestion-control mechanism, a service for the  general 
welfare of the Internet rather than for the direct benefit of the communicating pro-
cesses. The TCP congestion-control mechanism throttles a sending process (client or 
server) when the network is congested between sender and receiver. As we will see 
in Chapter 3, TCP congestion control also attempts to limit each TCP connection to 
its fair share of network bandwidth.
UDP Services
UDP is a no-frills, lightweight transport protocol, providing minimal services. UDP 
is connectionless, so there is no handshaking before the two processes start to com-
municate. UDP provides an unreliable data transfer service‚Äîthat is, when a process 
sends a message into a UDP socket, UDP provides no guarantee that the message 
will ever reach the receiving process. Furthermore, messages that do arrive at the 
receiving process may arrive out of order.
SECURING TCP
Neither TCP nor UDP provides any encryption‚Äîthe data that the sending process 
passes into its socket is the same data that travels over the network to the destina-
tion process. So, for example, if the sending process sends a password in cleartext 
(i.e., unencrypted) into its socket, the cleartext password will travel over all the links 
between sender and receiver, potentially getting sniffed and discovered at any of the 
intervening links. Because privacy and other security issues have become critical for 
many applications, the Internet community has developed an enhancement for TCP, 
called Transport Layer Security (TLS) [RFC 5246]. TCP-enhanced-with-TLS not 
only does everything that traditional TCP does but also provides critical process-to-
process security services, including encryption, data integrity, and end-point authenti-
cation. We emphasize that TLS is not a third Internet transport protocol, on the same 
level as TCP and UDP, but instead is an enhancement of TCP, with the enhancements 
being implemented in the application layer. In particular, if an application wants to 
use the services of TLS, it needs to include TLS code (existing, highly optimized librar-
ies and classes) in both the client and server sides of the application. TLS has its own 
socket API that is similar to the traditional TCP socket API. When an application uses 
TLS, the sending process passes cleartext data to the TLS socket; TLS in the sending 
host then encrypts the data and passes the encrypted data to the TCP socket. The 
encrypted data travels over the Internet to the TCP socket in the receiving process. 
The receiving socket passes the encrypted data to TLS, which decrypts the data. 
Finally, TLS passes the cleartext data through its TLS socket to the receiving process. 
We‚Äôll cover TLS in some detail in Chapter 8.
FOCUS ON SECURITY

UDP does not include a congestion-control mechanism, so the sending side of 
UDP can pump data into the layer below (the network layer) at any rate it pleases. 
(Note, however, that the actual end-to-end throughput may be less than this rate due 
to the limited transmission capacity of intervening links or due to congestion).
Services Not Provided by Internet Transport Protocols
We have organized transport protocol services along four dimensions: reliable data 
transfer, throughput, timing, and security. Which of these services are provided by 
TCP and UDP? We have already noted that TCP provides reliable end-to-end data 
transfer. And we also know that TCP can be easily enhanced at the application 
layer with TLS to provide security services. But in our brief description of TCP and 
UDP, conspicuously missing was any mention of throughput or timing guarantees‚Äî 
services not provided by today‚Äôs Internet transport protocols. Does this mean that time-
sensitive applications such as Internet telephony cannot run in today‚Äôs Internet? The 
answer is clearly no‚Äîthe Internet has been hosting time-sensitive applications for 
many years. These applications often work fairly well because they have been designed 
to cope, to the greatest extent possible, with this lack of guarantee. Nevertheless, clever 
design has its limitations when delay is excessive, or the end-to-end throughput is 
limited. In summary, today‚Äôs Internet can often provide satisfactory service to time-
sensitive applications, but it cannot provide any timing or throughput guarantees.
Figure 2.5 indicates the transport protocols used by some popular Internet appli-
cations. We see that e-mail, remote terminal access, the Web, and file transfer all use 
TCP. These applications have chosen TCP primarily because TCP provides reliable 
data transfer, guaranteeing that all data will eventually get to its destination. Because 
Internet telephony applications (such as Skype) can often tolerate some loss but 
require a minimal rate to be effective, developers of Internet telephony applications 
Application
Application-Layer Protocol
Underlying Transport Protocol
Electronic mail
Remote terminal access
Web
File transfer
Streaming multimedia
Internet telephony
SMTP [RFC 5321]
Telnet [RFC 854]
HTTP 1.1 [RFC 7230]
FTP [RFC 959]
HTTP (e.g., YouTube), DASH
SIP [RFC 3261], RTP [RFC 3550], or proprietary
(e.g., Skype)
TCP
TCP
TCP
TCP
TCP
UDP or TCP
Figure 2.5 ‚ô¶  Popular Internet applications, their application-layer  
protocols, and their underlying transport protocols

usually prefer to run their applications over UDP, thereby circumventing TCP‚Äôs 
congestion control mechanism and packet overheads. But because many firewalls 
are configured to block (most types of) UDP traffic, Internet telephony applications 
often are designed to use TCP as a backup if UDP communication fails.
2.1.5 Application-Layer Protocols
We have just learned that network processes communicate with each other by sending 
messages into sockets. But how are these messages structured? What are the meanings 
of the various fields in the messages? When do the processes send the messages? These 
questions bring us into the realm of application-layer protocols. An application-layer 
protocol defines how an application‚Äôs processes, running on different end systems, 
pass messages to each other. In particular, an application-layer protocol defines:
‚Ä¢ The types of messages exchanged, for example, request messages and response 
messages
‚Ä¢ The syntax of the various message types, such as the fields in the message and 
how the fields are delineated
‚Ä¢ The semantics of the fields, that is, the meaning of the information in the fields
‚Ä¢ Rules for determining when and how a process sends messages and responds to 
messages
Some application-layer protocols are specified in RFCs and are therefore in the 
public domain. For example, the Web‚Äôs application-layer protocol, HTTP (the 
HyperText Transfer Protocol [RFC 7230]), is available as an RFC. If a browser 
developer follows the rules of the HTTP RFC, the browser will be able to retrieve 
Web pages from any Web server that has also followed the rules of the HTTP RFC. 
Many other application-layer protocols are proprietary and intentionally not avail-
able in the public domain. For example, Skype uses proprietary application-layer 
protocols.
It is important to distinguish between network applications and application-layer 
protocols. An application-layer protocol is only one piece of a network application 
(albeit, a very important piece of the application from our point of view!). Let‚Äôs look 
at a couple of examples. The Web is a client-server application that allows users to 
obtain documents from Web servers on demand. The Web application consists of 
many components, including a standard for document formats (that is, HTML), Web 
browsers (for example, Chrome and Microsoft Internet Explorer), Web servers 
(for example, Apache and Microsoft servers), and an application-layer protocol. 
The Web‚Äôs application-layer protocol, HTTP, defines the format and sequence 
of messages exchanged between browser and Web server. Thus, HTTP is only 
one piece (albeit, an important piece) of the Web application. As another example, 
we‚Äôll see in Section 2.6 that Netflix‚Äôs video service also has many components,

including servers that store and transmit videos, other servers that manage billing 
and other  client functions, clients (e.g., the Netflix app on your smartphone, tablet, or 
 computer), and an application-level DASH protocol defines the format and sequence 
of messages exchanged between a Netflix server and client. Thus, DASH is only one 
piece (albeit, an important piece) of the Netflix application.
2.1.6 Network Applications Covered in This Book
New applications are being developed every day. Rather than covering a large 
number of Internet applications in an encyclopedic manner, we have chosen to 
focus on a small number of applications that are both pervasive and important. 
In this chapter, we discuss five important applications: the Web, electronic mail, 
directory service, video streaming, and P2P applications. We first discuss the 
Web, not only because it is an enormously popular application, but also because 
its application-layer protocol, HTTP, is straightforward and easy to understand. 
We then discuss electronic mail, the Internet‚Äôs first killer application. E-mail is 
more complex than the Web in the sense that it makes use of not one but sev-
eral application-layer protocols. After e-mail, we cover DNS, which provides a 
directory service for the Internet. Most users do not interact with DNS directly; 
instead, users invoke DNS indirectly through other applications (including the 
Web, file transfer, and electronic mail). DNS illustrates nicely how a piece of 
core network functionality (network-name to network-address translation) can 
be implemented at the application layer in the Internet. We then discuss P2P file 
sharing applications, and complete our application study by discussing video 
streaming on demand, including distributing stored video over content distribu-
tion networks. 
2.2 The Web and HTTP
Until the early 1990s, the Internet was used primarily by researchers, academics, 
and university students to log in to remote hosts, to transfer files from local hosts to 
remote hosts and vice versa, to receive and send news, and to receive and send elec-
tronic mail. Although these applications were (and continue to be) extremely useful, 
the Internet was essentially unknown outside of the academic and research commu-
nities. Then, in the early 1990s, a major new application arrived on the scene‚Äîthe 
World Wide Web [Berners-Lee 1994]. The Web was the first Internet application 
that caught the general public‚Äôs eye. It dramatically changed how people interact 
inside and outside their work environments. It elevated the Internet from just one of 
many data networks to essentially the one and only data network.
Perhaps what appeals the most to users is that the Web operates on demand. 
Users receive what they want, when they want it. This is unlike traditional broadcast

radio and television, which force users to tune in when the content provider makes 
the content available. In addition to being available on demand, the Web has many 
other wonderful features that people love and cherish. It is enormously easy for any 
individual to make information available over the Web‚Äîeveryone can become a 
publisher at extremely low cost. Hyperlinks and search engines help us navigate 
through an ocean of information. Photos and videos stimulate our senses. Forms, 
JavaScript, video, and many other devices enable us to interact with pages and sites. 
And the Web and its protocols serve as a platform for YouTube, Web-based e-mail 
(such as Gmail), and most mobile Internet applications, including Instagram and 
Google Maps.
2.2.1 Overview of HTTP
The HyperText Transfer Protocol (HTTP), the Web‚Äôs application-layer protocol, 
is at the heart of the Web. It is defined in [RFC 1945], [RFC 7230] and [RFC 7540]. 
HTTP is implemented in two programs: a client program and a server program. The 
client program and server program, executing on different end systems, talk to each 
other by exchanging HTTP messages. HTTP defines the structure of these messages 
and how the client and server exchange the messages. Before explaining HTTP in 
detail, we should review some Web terminology.
A Web page (also called a document) consists of objects. An object is 
 simply a file‚Äîsuch as an HTML file, a JPEG image, a Javascrpt file, a CCS 
style sheet file, or a video clip‚Äîthat is addressable by a single URL. Most Web 
pages consist of a base HTML file and several referenced objects. For example, 
if a Web page contains HTML text and five JPEG images, then the Web page has 
six objects: the base HTML file plus the five images. The base HTML file refer-
ences the other objects in the page with the objects‚Äô URLs. Each URL has two 
components: the hostname of the server that houses the object and the object‚Äôs 
path name. For example, the URL
http://www.someSchool.edu/someDepartment/picture.gif
has www.someSchool.edu for a hostname and /someDepartment/picture.
gif for a path name. Because Web browsers (such as Internet Explorer and Chrome) 
implement the client side of HTTP, in the context of the Web, we will use the words 
browser and client interchangeably. Web servers, which implement the server side 
of HTTP, house Web objects, each addressable by a URL. Popular Web servers 
include Apache and Microsoft Internet Information Server.
HTTP defines how Web clients request Web pages from Web servers and how 
servers transfer Web pages to clients. We discuss the interaction between client 
and server in detail later, but the general idea is illustrated in Figure 2.6. When a  
user requests a Web page (for example, clicks on a hyperlink), the browser sends

HTTP request messages for the objects in the page to the server. The server receives 
the requests and responds with HTTP response messages that contain the objects.
HTTP uses TCP as its underlying transport protocol (rather than running on top 
of UDP). The HTTP client first initiates a TCP connection with the server. Once the 
connection is established, the browser and the server processes access TCP through 
their socket interfaces. As described in Section 2.1, on the client side the socket inter-
face is the door between the client process and the TCP connection; on the server side 
it is the door between the server process and the TCP connection. The client sends 
HTTP request messages into its socket interface and receives HTTP response mes-
sages from its socket interface. Similarly, the HTTP server receives request messages 
from its socket interface and sends response messages into its socket interface. Once 
the client sends a message into its socket interface, the message is out of the client‚Äôs 
hands and is ‚Äúin the hands‚Äù of TCP. Recall from Section 2.1 that TCP provides a 
reliable data transfer service to HTTP. This implies that each HTTP request message 
sent by a client process eventually arrives intact at the server; similarly, each HTTP 
response message sent by the server process eventually arrives intact at the client. 
Here we see one of the great advantages of a layered architecture‚ÄîHTTP need not 
worry about lost data or the details of how TCP recovers from loss or reordering of 
data within the network. That is the job of TCP and the protocols in the lower layers 
of the protocol stack.
It is important to note that the server sends requested files to clients without 
storing any state information about the client. If a particular client asks for the same 
object twice in a period of a few seconds, the server does not respond by saying 
that it just served the object to the client; instead, the server resends the object, as 
it has completely forgotten what it did earlier. Because an HTTP server maintains 
HTTP request
HTTP response
HTTP response
HTTP request
PC running
Internet Explorer
Android smartphone
running Google Chrome
Server running
Apache Web server
Figure 2.6 ‚ô¶ HTTP request-response behavior

no information about the clients, HTTP is said to be a stateless protocol. We also 
remark that the Web uses the client-server application architecture, as described in 
Section 2.1. A Web server is always on, with a fixed IP address, and it services 
requests from potentially millions of different browsers.
The original version of HTTP is called HTTP/1.0 and dates back to the early 
1990‚Äôs [RFC 1945]. As of 2020, the majority of HTTP transactions take place over 
HTTP/1.1 [RFC 7230]. However, increasingly browsers and Web servers also sup-
port a new version of HTTP called HTTP/2 [RFC 7540]. At the end of this section, 
we‚Äôll provide an introduction to HTTP/2.
2.2.2 Non-Persistent and Persistent Connections
In many Internet applications, the client and server communicate for an extended 
period of time, with the client making a series of requests and the server respond-
ing to each of the requests. Depending on the application and on how the 
application is being used, the series of requests may be made back-to-back, peri-
odically at regular intervals, or intermittently. When this client-server interaction 
is taking place over TCP, the application developer needs to make an important  
decision‚Äîshould each request/response pair be sent over a separate TCP connec-
tion, or should all of the requests and their corresponding responses be sent over 
the same TCP connection? In the former approach, the application is said to use  
non-persistent connections; and in the latter approach, persistent connections. To 
gain a deep understanding of this design issue, let‚Äôs examine the advantages and dis-
advantages of persistent connections in the context of a specific application, namely, 
HTTP, which can use both non-persistent connections and persistent connections. 
Although HTTP uses persistent connections in its default mode, HTTP clients and 
servers can be configured to use non-persistent connections instead.
HTTP with Non-Persistent Connections
Let‚Äôs walk through the steps of transferring a Web page from server to client for the 
case of non-persistent connections. Let‚Äôs suppose the page consists of a base HTML 
file and 10 JPEG images, and that all 11 of these objects reside on the same server. 
Further suppose the URL for the base HTML file is
http://www.someSchool.edu/someDepartment/home.index
Here is what happens:
 1. The HTTP client process initiates a TCP connection to the server  
www.someSchool.edu on port number 80, which is the default port number 
for HTTP. Associated with the TCP connection, there will be a socket at the  
client and a socket at the server.

2. The HTTP client sends an HTTP request message to the server via its socket. 
The request message includes the path name /someDepartment/home 
.index. (We will discuss HTTP messages in some detail below.)
 3. The HTTP server process receives the request message via its socket, retrieves 
the object /someDepartment/home.index from its storage (RAM or 
disk), encapsulates the object in an HTTP response message, and sends the 
response message to the client via its socket.
 4. The HTTP server process tells TCP to close the TCP connection. (But TCP 
doesn‚Äôt actually terminate the connection until it knows for sure that the client 
has received the response message intact.)
 5. The HTTP client receives the response message. The TCP connection termi-
nates. The message indicates that the encapsulated object is an HTML file. The 
client extracts the file from the response message, examines the HTML file, and 
finds references to the 10 JPEG objects.
 6. The first four steps are then repeated for each of the referenced JPEG objects.
As the browser receives the Web page, it displays the page to the user. Two 
different browsers may interpret (that is, display to the user) a Web page in some-
what different ways. HTTP has nothing to do with how a Web page is interpreted 
by a client. The HTTP specifications ([RFC 1945] and [RFC 7540]) define only the 
communication protocol between the client HTTP program and the server HTTP 
program.
The steps above illustrate the use of non-persistent connections, where each 
TCP connection is closed after the server sends the object‚Äîthe connection does not 
persist for other objects. HTTP/1.0 employes non-persistent TCP connections. Note 
that each non-persistent TCP connection transports exactly one request message and 
one response message. Thus, in this example, when a user requests the Web page, 11 
TCP connections are generated.
In the steps described above, we were intentionally vague about whether the  
client obtains the 10 JPEGs over 10 serial TCP connections, or whether some of the 
JPEGs are obtained over parallel TCP connections. Indeed, users can configure some 
browsers to control the degree of parallelism. Browsers may open multiple TCP con-
nections and request different parts of the Web page over the multiple connections. As 
we‚Äôll see in the next chapter, the use of parallel connections shortens the response time.
Before continuing, let‚Äôs do a back-of-the-envelope calculation to estimate the 
amount of time that elapses from when a client requests the base HTML file until 
the entire file is received by the client. To this end, we define the round-trip time 
(RTT), which is the time it takes for a small packet to travel from client to server 
and then back to the client. The RTT includes packet-propagation delays, packet-
queuing delays in intermediate routers and switches, and packet-processing delays. 
(These delays were discussed in Section 1.4.) Now consider what happens when 
a user clicks on a hyperlink. As shown in Figure 2.7, this causes the browser to 
initiate a TCP connection between the browser and the Web server; this involves

a ‚Äúthree-way handshake‚Äù‚Äîthe client sends a small TCP segment to the server, the 
server acknowledges and responds with a small TCP segment, and, finally, the cli-
ent acknowledges back to the server. The first two parts of the three-way handshake 
take one RTT. After completing the first two parts of the handshake, the client sends 
the HTTP request message combined with the third part of the three-way handshake 
(the acknowledgment) into the TCP connection. Once the request message arrives at  
the server, the server sends the HTML file into the TCP connection. This HTTP 
request/response eats up another RTT. Thus, roughly, the total response time is two 
RTTs plus the transmission time at the server of the HTML file.
HTTP with Persistent Connections
Non-persistent connections have some shortcomings. First, a brand-new connection 
must be established and maintained for each requested object. For each of these 
connections, TCP buffers must be allocated and TCP variables must be kept in both 
the client and server. This can place a significant burden on the Web server, which 
may be serving requests from hundreds of different clients simultaneously. Second, 
Time
at client
Time
at server
Initiate TCP
connection
RTT
Request Ô¨Åle
RTT
Entire Ô¨Åle received
Time to transmit Ô¨Åle
Figure 2.7 ‚ô¶  Back-of-the-envelope calculation for the time needed  
to request and receive an HTML file

as we just described, each object suffers a delivery delay of two RTTs‚Äîone RTT to 
establish the TCP connection and one RTT to request and receive an object.
With HTTP/1.1 persistent connections, the server leaves the TCP connection 
open after sending a response. Subsequent requests and responses between the same 
client and server can be sent over the same connection. In particular, an entire Web 
page (in the example above, the base HTML file and the 10 images) can be sent over 
a single persistent TCP connection. Moreover, multiple Web pages residing on the 
same server can be sent from the server to the same client over a single persistent 
TCP connection. These requests for objects can be made back-to-back, without wait-
ing for replies to pending requests (pipelining). Typically, the HTTP server closes 
a connection when it isn‚Äôt used for a certain time (a configurable timeout interval). 
When the server receives the back-to-back requests, it sends the objects back-to-
back. The default mode of HTTP uses persistent connections with pipelining. We‚Äôll 
quantitatively compare the performance of non-persistent and persistent connections 
in the homework problems of Chapters 2 and 3. You are also encouraged to see  
[Heidemann 1997; Nielsen 1997; RFC 7540].
2.2.3 HTTP Message Format
The HTTP specifications [RFC 1945; RFC 7230; RFC 7540] include the definitions 
of the HTTP message formats. There are two types of HTTP messages, request mes-
sages and response messages, both of which are discussed below.
HTTP Request Message
Below we provide a typical HTTP request message:
GET /somedir/page.html HTTP/1.1
Host: www.someschool.edu
Connection: close
User-agent: Mozilla/5.0
Accept-language: fr
We can learn a lot by taking a close look at this simple request message. First of 
all, we see that the message is written in ordinary ASCII text, so that your ordinary 
computer-literate human being can read it. Second, we see that the message consists 
of five lines, each followed by a carriage return and a line feed. The last line is fol-
lowed by an additional carriage return and line feed. Although this particular request 
message has five lines, a request message can have many more lines or as few as 
one line. The first line of an HTTP request message is called the request line; the 
subsequent lines are called the header lines. The request line has three fields: the 
method field, the URL field, and the HTTP version field. The method field can take 
on several different values, including GET, POST, HEAD, PUT, and DELETE.

The great majority of HTTP request messages use the GET method. The GET method 
is used when the browser requests an object, with the requested object identified in 
the URL field. In this example, the browser is requesting the object /somedir/
page.html. The version is self-explanatory; in this example, the browser imple-
ments version HTTP/1.1.
Now let‚Äôs look at the header lines in the example. The header line Host:  
www.someschool.edu specifies the host on which the object resides. You might 
think that this header line is unnecessary, as there is already a TCP connection in 
place to the host. But, as we‚Äôll see in Section 2.2.5, the information provided by the 
host header line is required by Web proxy caches. By including the Connection: 
close header line, the browser is telling the server that it doesn‚Äôt want to bother 
with persistent connections; it wants the server to close the connection after sending 
the requested object. The User-agent: header line specifies the user agent, that 
is, the browser type that is making the request to the server. Here the user agent is 
Mozilla/5.0, a Firefox browser. This header line is useful because the server can actu-
ally send different versions of the same object to different types of user agents. (Each 
of the versions is addressed by the same URL.) Finally, the Accept-language: 
header indicates that the user prefers to receive a French version of the object, if such 
an object exists on the server; otherwise, the server should send its default version. 
The Accept-language: header is just one of many content negotiation headers 
available in HTTP.
Having looked at an example, let‚Äôs now look at the general format of a request 
message, as shown in Figure 2.8. We see that the general format closely follows our 
earlier example. You may have noticed, however, that after the header lines (and the 
additional carriage return and line feed) there is an ‚Äúentity body.‚Äù The entity body 
method
sp
sp
cr
lf
cr
lf
header Ô¨Åeld name:
Header lines
Blank line
Entity body
Request line
value
sp
cr
lf
cr
lf
header Ô¨Åeld name:
value
sp
URL
Version
Figure 2.8 ‚ô¶ General format of an HTTP request message

is empty with the GET method, but is used with the POST method. An HTTP client 
often uses the POST method when the user fills out a form‚Äîfor example, when a 
user provides search words to a search engine. With a POST message, the user is still 
requesting a Web page from the server, but the specific contents of the Web page 
depend on what the user entered into the form fields. If the value of the method field 
is POST, then the entity body contains what the user entered into the form fields.
We would be remiss if we didn‚Äôt mention that a request generated with a form 
does not necessarily have to use the POST method. Instead, HTML forms often use 
the GET method and include the inputted data (in the form fields) in the requested 
URL. For example, if a form uses the GET method, has two fields, and the inputs to 
the two fields are monkeys and bananas, then the URL will have the structure 
www.somesite.com/animalsearch?monkeys&bananas. In your day-to-
day Web surfing, you have probably noticed extended URLs of this sort.
The HEAD method is similar to the GET method. When a server receives a 
request with the HEAD method, it responds with an HTTP message but it leaves out 
the requested object. Application developers often use the HEAD method for debug-
ging. The PUT method is often used in conjunction with Web publishing tools. It 
allows a user to upload an object to a specific path (directory) on a specific Web 
server. The PUT method is also used by applications that need to upload objects 
to Web servers. The DELETE method allows a user, or an application, to delete an 
object on a Web server.
HTTP Response Message
Below we provide a typical HTTP response message. This response message could 
be the response to the example request message just discussed.
HTTP/1.1 200 OK
Connection: close
Date: Tue, 18 Aug 2015 15:44:04 GMT
Server: Apache/2.2.3 (CentOS)
Last-Modified: Tue, 18 Aug 2015 15:11:03 GMT
Content-Length: 6821
Content-Type: text/html¬†
(data data data data data ...)
Let‚Äôs take a careful look at this response message. It has three sections: an initial 
status line, six header lines, and then the entity body. The entity body is the meat 
of the message‚Äîit contains the requested object itself (represented by data data 
data data data ...). The status line has three fields: the protocol version 
field, a status code, and a corresponding status message. In this example, the status 
line indicates that the server is using HTTP/1.1 and that everything is OK (that is, the 
server has found, and is sending, the requested object).

Now let‚Äôs look at the header lines. The server uses the Connection: close 
header line to tell the client that it is going to close the TCP connection after sending 
the message. The Date: header line indicates the time and date when the HTTP 
response was created and sent by the server. Note that this is not the time when 
the object was created or last modified; it is the time when the server retrieves the 
object from its file system, inserts the object into the response message, and sends the 
response message. The Server: header line indicates that the message was gener-
ated by an Apache Web server; it is analogous to the User-agent: header line in 
the HTTP request message. The Last-Modified: header line indicates the time 
and date when the object was created or last modified. The Last-Modified: 
header, which we will soon cover in more detail, is critical for object caching, both 
in the local client and in network cache servers (also known as proxy servers). The 
Content-Length: header line indicates the number of bytes in the object being 
sent. The Content-Type: header line indicates that the object in the entity body 
is HTML text. (The object type is officially indicated by the Content-Type: 
header and not by the file extension.)
Having looked at an example, let‚Äôs now examine the general format of a response 
message, which is shown in Figure 2.9. This general format of the response message 
matches the previous example of a response message. Let‚Äôs say a few additional words 
about status codes and their phrases. The status code and associated phrase indicate 
the result of the request. Some common status codes and associated phrases include:
‚Ä¢ 200 OK: Request succeeded and the information is returned in the response.
‚Ä¢ 301 Moved Permanently: Requested object has been permanently moved; 
the new URL is specified in Location: header of the response message. The 
client software will automatically retrieve the new URL.
version
sp
sp
cr
lf
cr
lf
header Ô¨Åeld name:
Header lines
Blank line
Entity body
Status line
value
cr
sp
sp
lf
cr
lf
header Ô¨Åeld name:
value
status code
phrase
Figure 2.9 ‚ô¶ General format of an HTTP response message

‚Ä¢ 400 Bad Request: This is a generic error code indicating that the request 
could not be understood by the server.
‚Ä¢ 404 Not Found: The requested document does not exist on this server.
‚Ä¢ 505 HTTP Version Not Supported: The requested HTTP protocol ver-
sion is not supported by the server.
How would you like to see a real HTTP response message? This is highly rec-
ommended and very easy to do! First Telnet into your favorite Web server. Then 
type in a one-line request message for some object that is housed on the server. For 
example, if you have access to a command prompt, type:
telnet gaia.cs.umass.edu 80¬†
GET /kurose_ross/interactive/index.php HTTP/1.1
Host: gaia.cs.umass.edu
(Press the carriage return twice after typing the last line.) This opens a TCP con-
nection to port 80 of the host gaia.cs.umass.edu and then sends the HTTP 
request message. You should see a response message that includes the base HTML 
file for the interactive homework problems for this textbook. If you‚Äôd rather just see 
the HTTP message lines and not receive the object itself, replace GET with HEAD.
In this section, we discussed a number of header lines that can be used within 
HTTP request and response messages. The HTTP specification defines many, 
many more header lines that can be inserted by browsers, Web servers, and net-
work cache servers. We have covered only a small number of the totality of header 
lines. We‚Äôll cover a few more below and another small number when we discuss 
network Web caching in Section 2.2.5. A highly readable and comprehensive dis-
cussion of the HTTP protocol, including its headers and status codes, is given in 
[Krishnamurthy 2001].
How does a browser decide which header lines to include in a request message? 
How does a Web server decide which header lines to include in a response mes-
sage? A browser will generate header lines as a function of the browser type and 
version, the user configuration of the browser and whether the browser currently 
has a cached, but possibly out-of-date, version of the object. Web servers behave 
similarly: There are different products, versions, and configurations, all of which 
influence which header lines are included in response messages.
2.2.4 User-Server Interaction: Cookies
We mentioned above that an HTTP server is stateless. This simplifies server design 
and has permitted engineers to develop high-performance Web servers that can han-
dle thousands of simultaneous TCP connections. However, it is often desirable for 
a Web site to identify users, either because the server wishes to restrict user access 
Using Wireshark to 
investigate the HTTP 
protocol
VideoNote

or because it wants to serve content as a function of the user identity. For these pur-
poses, HTTP uses cookies. Cookies, defined in [RFC 6265], allow sites to keep track 
of users. Most major commercial Web sites use cookies today.
As shown in Figure 2.10, cookie technology has four components: (1) a cookie 
header line in the HTTP response message; (2) a cookie header line in the HTTP 
request message; (3) a cookie file kept on the user‚Äôs end system and managed by 
the user‚Äôs browser; and (4) a back-end database at the Web site. Using Figure 2.10, 
let‚Äôs walk through an example of how cookies work. Suppose Susan, who always 
Client host
Server host
usual http request msg
usual http response
Set-cookie: 1678
usual http request msg
cookie: 1678
usual http response msg
usual http request msg
cookie: 1678
usual http response msg
Time
One week later
ebay: 8734
Server creates
ID 1678 for user
Time
Cookie Ô¨Åle
Key:
amazon: 1678
ebay: 8734
amazon: 1678
ebay: 8734
Cookie-speciÔ¨Åc
action
access
access
entry in backend
database
Cookie-speciÔ¨Åc
action
Figure 2.10 ‚ô¶ Keeping user state with cookies

accesses the Web using Internet Explorer from her home PC, contacts Amazon.com 
for the first time. Let us suppose that in the past she has already visited the eBay site. 
When the request comes into the Amazon Web server, the server creates a unique 
identification number and creates an entry in its back-end database that is indexed 
by the identification number. The Amazon Web server then responds to Susan‚Äôs 
browser, including in the HTTP response a Set-cookie: header, which contains 
the identification number. For example, the header line might be:
Set-cookie: 1678
When Susan‚Äôs browser receives the HTTP response message, it sees the  
Set-cookie: header. The browser then appends a line to the special cookie file 
that it manages. This line includes the hostname of the server and the identification 
number in the Set-cookie: header. Note that the cookie file already has an entry 
for eBay, since Susan has visited that site in the past. As Susan continues to browse 
the Amazon site, each time she requests a Web page, her browser consults her cookie 
file, extracts her identification number for this site, and puts a cookie header line that 
includes the identification number in the HTTP request. Specifically, each of her 
HTTP requests to the Amazon server includes the header line:
Cookie: 1678
In this manner, the Amazon server is able to track Susan‚Äôs activity at the Amazon 
site. Although the Amazon Web site does not necessarily know Susan‚Äôs name, it 
knows exactly which pages user 1678 visited, in which order, and at what times! 
Amazon uses cookies to provide its shopping cart service‚ÄîAmazon can maintain a 
list of all of Susan‚Äôs intended purchases, so that she can pay for them collectively at 
the end of the session.
If Susan returns to Amazon‚Äôs site, say, one week later, her browser will con-
tinue to put the header line Cookie: 1678 in the request messages. Amazon also 
recommends products to Susan based on Web pages she has visited at Amazon in 
the past. If Susan also registers herself with Amazon‚Äîproviding full name, e-mail 
address, postal address, and credit card information‚ÄîAmazon can then include this 
information in its database, thereby associating Susan‚Äôs name with her identifica-
tion number (and all of the pages she has visited at the site in the past!). This is how  
Amazon and other e-commerce sites provide ‚Äúone-click shopping‚Äù‚Äîwhen Susan 
chooses to purchase an item during a subsequent visit, she doesn‚Äôt need to re-enter 
her name, credit card number, or address.
From this discussion, we see that cookies can be used to identify a user. The first 
time a user visits a site, the user can provide a user identification (possibly his or her 
name). During the subsequent sessions, the browser passes a cookie header to the 
server, thereby identifying the user to the server. Cookies can thus be used to create 
a user session layer on top of stateless HTTP. For example, when a user logs in to

a Web-based e-mail application (such as Hotmail), the browser sends cookie infor-
mation to the server, permitting the server to identify the user throughout the user‚Äôs 
session with the application.
Although cookies often simplify the Internet shopping experience for the user, 
they are controversial because they can also be considered as an invasion of privacy. 
As we just saw, using a combination of cookies and user-supplied account informa-
tion, a Web site can learn a lot about a user and potentially sell this information to a 
third party.
2.2.5 Web Caching
A Web cache‚Äîalso called a proxy server‚Äîis a network entity that satisfies HTTP 
requests on the behalf of an origin Web server. The Web cache has its own disk 
storage and keeps copies of recently requested objects in this storage. As shown in  
Figure 2.11, a user‚Äôs browser can be configured so that all of the user‚Äôs HTTP requests 
are first directed to the Web cache [RFC 7234]. Once a browser is configured, each 
browser request for an object is first directed to the Web cache. As an example, 
suppose a browser is requesting the object http://www.someschool.edu/
campus.gif. Here is what happens:
 1. The browser establishes a TCP connection to the Web cache and sends an HTTP 
request for the object to the Web cache.
 2. The Web cache checks to see if it has a copy of the object stored locally. If it 
does, the Web cache returns the object within an HTTP response message to the 
client browser.
 3. If the Web cache does not have the object, the Web cache opens a TCP connec-
tion to the origin server, that is, to www.someschool.edu. The Web cache 
HTTP request
HTTP response
HTTP request
HTTP response
HTTP request
HTTP response
HTTP request
HTTP response
Client
Origin
server
Origin
server
Client
Proxy
server
Figure 2.11 ‚ô¶ Clients requesting objects through a Web cache

then sends an HTTP request for the object into the cache-to-server TCP connec-
tion. After receiving this request, the origin server sends the object within an 
HTTP response to the Web cache.
 4. When the Web cache receives the object, it stores a copy in its local storage and 
sends a copy, within an HTTP response message, to the client browser (over the 
existing TCP connection between the client browser and the Web cache).
Note that a cache is both a server and a client at the same time. When it receives 
requests from and sends responses to a browser, it is a server. When it sends requests 
to and receives responses from an origin server, it is a client.
Typically a Web cache is purchased and installed by an ISP. For example, a uni-
versity might install a cache on its campus network and configure all of the campus 
browsers to point to the cache. Or a major residential ISP (such as Comcast) might 
install one or more caches in its network and preconfigure its shipped browsers to 
point to the installed caches.
Web caching has seen deployment in the Internet for two reasons. First, a Web 
cache can substantially reduce the response time for a client request, particularly if 
the bottleneck bandwidth between the client and the origin server is much less than 
the bottleneck bandwidth between the client and the cache. If there is a high-speed 
connection between the client and the cache, as there often is, and if the cache has 
the requested object, then the cache will be able to deliver the object rapidly to the 
client. Second, as we will soon illustrate with an example, Web caches can sub-
stantially reduce traffic on an institution‚Äôs access link to the Internet. By reducing 
traffic, the institution (for example, a company or a university) does not have to 
upgrade bandwidth as quickly, thereby reducing costs. Furthermore, Web caches 
can substantially reduce Web traffic in the Internet as a whole, thereby improving 
performance for all applications.
To gain a deeper understanding of the benefits of caches, let‚Äôs consider an exam-
ple in the context of Figure 2.12. This figure shows two networks‚Äîthe institutional 
network and the rest of the public Internet. The institutional network is a high-speed 
LAN. A router in the institutional network and a router in the Internet are connected 
by a 15 Mbps link. The origin servers are attached to the Internet but are located all 
over the globe. Suppose that the average object size is 1 Mbits and that the average 
request rate from the institution‚Äôs browsers to the origin servers is 15 requests per 
second. Suppose that the HTTP request messages are negligibly small and thus cre-
ate no traffic in the networks or in the access link (from institutional router to Internet 
router). Also suppose that the amount of time it takes from when the router on the 
Internet side of the access link in Figure 2.12 forwards an HTTP request (within an 
IP datagram) until it receives the response (typically within many IP datagrams) is 
two seconds on average. Informally, we refer to this last delay as the ‚ÄúInternet delay.‚Äù
The total response time‚Äîthat is, the time from the browser‚Äôs request of an 
object until its receipt of the object‚Äîis the sum of the LAN delay, the access delay 
(that is, the delay between the two routers), and the Internet delay. Let‚Äôs now do

Public Internet
Institutional network
15 Mbps access link
100 Mbps LAN
Origin servers
Figure 2.12 ‚ô¶ Bottleneck between an institutional network and the Internet
a very crude calculation to estimate this delay. The traffic intensity on the LAN  
(see Section 1.4.2) is
(15 requests/sec) # (1 Mbits/request)/(100 Mbps) = 0.15
whereas the traffic intensity on the access link (from the Internet router to institution 
router) is
(15 requests/sec) # (1 Mbits/request)/(15 Mbps) = 1
A traffic intensity of 0.15 on a LAN typically results in, at most, tens of millisec-
onds of delay; hence, we can neglect the LAN delay. However, as discussed in 
Section 1.4.2, as the traffic intensity approaches 1 (as is the case of the access link 
in Figure 2.12), the delay on a link becomes very large and grows without bound. 
Thus, the average response time to satisfy requests is going to be on the order of 
minutes, if not more, which is unacceptable for the institution‚Äôs users. Clearly 
something must be done.

One possible solution is to increase the access rate from 15 Mbps to, say, 100¬†Mbps. 
This will lower the traffic intensity on the access link to 0.15, which translates to neg-
ligible delays between the two routers. In this case, the total response time will roughly 
be two seconds, that is, the Internet delay. But this solution also means that the institu-
tion must upgrade its access link from 15 Mbps to 100 Mbps, a costly proposition.
Now consider the alternative solution of not upgrading the access link but 
instead installing a Web cache in the institutional network. This solution is illustrated 
in Figure 2.13. Hit rates‚Äîthe fraction of requests that are satisfied by a cache‚Äî 
typically range from 0.2 to 0.7 in practice. For illustrative purposes, let‚Äôs suppose 
that the cache provides a hit rate of 0.4 for this institution. Because the clients and 
the cache are connected to the same high-speed LAN, 40 percent of the requests will 
be satisfied almost immediately, say, within 10 milliseconds, by the cache. Neverthe-
less, the remaining 60 percent of the requests still need to be satisfied by the origin 
servers. But with only 60 percent of the requested objects passing through the access 
link, the traffic intensity on the access link is reduced from 1.0 to 0.6. Typically, a 
Public Internet
Institutional network
15 Mbps access link
Institutional
cache
100 Mbps LAN
Origin servers
Figure 2.13 ‚ô¶ Adding a cache to the institutional network

traffic intensity less than 0.8 corresponds to a small delay, say, tens of milliseconds, 
on a 15 Mbps link. This delay is negligible compared with the two-second Internet 
delay. Given these considerations, average delay therefore is
0.4 # (0.01 seconds) + 0.6 # (2.01 seconds)
which is just slightly greater than 1.2 seconds. Thus, this second solution provides an 
even lower response time than the first solution, and it doesn‚Äôt require the institution 
to upgrade its link to the Internet. The institution does, of course, have to purchase 
and install a Web cache. But this cost is low‚Äîmany caches use public-domain soft-
ware that runs on inexpensive PCs.
Through the use of Content Distribution Networks (CDNs), Web caches are 
increasingly playing an important role in the Internet. A CDN company installs many 
geographically distributed caches throughout the Internet, thereby localizing much of 
the traffic. There are shared CDNs (such as Akamai and Limelight) and dedicated CDNs 
(such as Google and Netflix). We will discuss CDNs in more detail in Section 2.6.
The Conditional GET
Although caching can reduce user-perceived response times, it introduces a new 
problem‚Äîthe copy of an object residing in the cache may be stale. In other words, 
the object housed in the Web server may have been modified since the copy was 
cached at the client. Fortunately, HTTP has a mechanism that allows a cache to 
verify that its objects are up to date. This mechanism is called the conditional GET 
[RFC 7232]. An HTTP request message is a so-called conditional GET message if 
(1) the request message uses the GET method and (2) the request message includes an  
If-Modified-Since: header line.
To illustrate how the conditional GET operates, let‚Äôs walk through an example. 
First, on the behalf of a requesting browser, a proxy cache sends a request message 
to a Web server:
GET /fruit/kiwi.gif HTTP/1.1
Host: www.exotiquecuisine.com
Second, the Web server sends a response message with the requested object to the 
cache:
HTTP/1.1 200 OK
Date: Sat, 3 Oct 2015 15:39:29
Server: Apache/1.3.0 (Unix)
Last-Modified: Wed, 9 Sep 2015 09:23:24
Content-Type: image/gif¬†
(data data data data data ...)

The cache forwards the object to the requesting browser but also caches the object 
locally. Importantly, the cache also stores the last-modified date along with the 
object. Third, one week later, another browser requests the same object via the cache, 
and the object is still in the cache. Since this object may have been modified at the 
Web server in the past week, the cache performs an up-to-date check by issuing a 
conditional GET. Specifically, the cache sends:
GET /fruit/kiwi.gif HTTP/1.1
Host: www.exotiquecuisine.com
If-modified-since: Wed, 9 Sep 2015 09:23:24
Note that the value of the If-modified-since: header line is exactly equal 
to the value of the Last-Modified: header line that was sent by the server one 
week ago. This conditional GET is telling the server to send the object only if the 
object has been modified since the specified date. Suppose the object has not been 
modified since 9 Sep 2015 09:23:24. Then, fourth, the Web server sends a response 
message to the cache:
HTTP/1.1 304 Not Modified
Date: Sat, 10 Oct 2015 15:39:29
Server: Apache/1.3.0 (Unix)¬†
(empty entity body)
We see that in response to the conditional GET, the Web server still sends a 
response message but does not include the requested object in the response message.  
Including the requested object would only waste bandwidth and increase user- 
perceived response time, particularly if the object is large. Note that this last response 
message has 304 Not Modified in the status line, which tells the cache that it 
can go ahead and forward its (the proxy cache‚Äôs) cached copy of the object to the 
requesting browser.
2.2.6 HTTP/2
HTTP/2 [RFC 7540], standardized in 2015, was the first new version of HTTP since 
HTTP/1.1, which was standardized in 1997. Since standardization, HTTP/2 has 
taken off, with over 40% of the top 10 million websites supporting HTTP/2 in 2020 
[W3Techs]. Most browsers‚Äîincluding Google Chrome, Internet Explorer, Safari, 
Opera, and Firefox‚Äîalso support HTTP/2.
The primary goals for HTTP/2 are to reduce perceived latency by enabling request 
and response multiplexing over a single TCP connection, provide request prioritization 
and server push, and provide efficient compression of HTTP header fields. HTTP/2 
does not change HTTP methods, status codes, URLs, or header fields. Instead, HTTP/2 
changes how the data is formatted and transported between the client and server.

To motivate the need for HTTP/2, recall that HTTP/1.1 uses persistent TCP 
connections, allowing a Web page to be sent from server to client over a single TCP 
connection. By having only one TCP connection per Web page, the number of sock-
ets at the server is reduced and each transported Web page gets a fair share of the 
network bandwidth (as discussed below). But developers of Web browsers quickly 
discovered that sending all the objects in a Web page over a single TCP connec-
tion has a Head of Line (HOL) blocking problem. To understand HOL blocking, 
consider a Web page that includes an HTML base page, a large video clip near the 
top of Web page, and many small objects below the video. Further suppose there is 
a low-to-medium speed bottleneck link (for example, a low-speed wireless link) on 
the path between server and client. Using a single TCP connection, the video clip 
will take a long time to pass through the bottleneck link, while the small objects are 
delayed as they wait behind the video clip; that is, the video clip at the head of the 
line blocks the small objects behind it. HTTP/1.1 browsers typically work around this 
problem by opening multiple parallel TCP connections, thereby having objects in the 
same web page sent in parallel to the browser. This way, the small objects can arrive 
at and be rendered in the browser much faster, thereby reducing user-perceived delay.
TCP congestion control, discussed in detail in Chapter 3, also provides brows-
ers an unintended incentive to use multiple parallel TCP connections rather than a 
single persistent connection. Very roughly speaking, TCP congestion control aims to 
give each TCP connection sharing a bottleneck link an equal share of the available 
bandwidth of that link; so if there are n TCP connections operating over a bottleneck 
link, then each connection approximately gets 1/nth of the bandwidth. By opening 
multiple parallel TCP connections to transport a single Web page, the browser can 
‚Äúcheat‚Äù and grab a larger portion of the link bandwidth. Many HTTP/1.1 browsers 
open up to six parallel TCP connections not only to circumvent HOL blocking but 
also to obtain more bandwidth.
One of the primary goals of HTTP/2 is to get rid of (or at least reduce the num-
ber of) parallel TCP connections for transporting a single Web page. This not only 
reduces the number of sockets that need to be open and maintained at servers, but 
also allows TCP congestion control to operate as intended. But with only one TCP 
connection to transport a Web page, HTTP/2 requires carefully designed mecha-
nisms to avoid HOL blocking.
HTTP/2 Framing
The HTTP/2 solution for HOL blocking is to break each message into small frames, and 
interleave the request and response messages on the same TCP connection. To under-
stand this, consider again the example of a Web page consisting of one large video clip 
and, say, 8 smaller objects. Thus the server will receive 9 concurrent requests from any 
browser wanting to see this Web page. For each of these requests, the server needs to 
send 9 competing HTTP response messages to the browser.  Suppose all frames are of

fixed length, the video clip consists of 1000 frames, and each of the smaller objects 
consists of two frames. With frame interleaving, after sending one frame from the 
video clip, the first frames of each of the small objects are sent. Then after sending the 
second frame of the video clip, the last frames of each of the small objects are sent. 
Thus, all of the smaller objects are sent after sending a total of 18 frames. If interleav-
ing were not used, the smaller objects would be sent only after sending 1016 frames. 
Thus the HTTP/2 framing mechanism can significantly decrease user-perceived delay.
The ability to break down an HTTP message into independent frames, inter-
leave them, and then reassemble them on the other end is the single most important 
enhancement of HTTP/2. The framing is done by the framing sub-layer of the 
HTTP/2 protocol. When a server wants to send an HTTP response, the response 
is processed by the framing sub-layer, where it is broken down into frames. The 
header field of the response becomes one frame, and the body of the message is 
broken down into one for more additional frames. The frames of the response are 
then interleaved by the framing sub-layer in the server with the frames of other 
responses and sent over the single persistent TCP connection. As the frames arrive 
at the client, they are first reassembled into the original response messages at the 
framing sub-layer and then processed by the browser as usual. Similarly, a client‚Äôs 
HTTP requests are broken into frames and interleaved.
In addition to breaking down each HTTP message into independent frames, the 
framing sublayer also binary encodes the frames. Binary protocols are more efficient 
to parse, lead to slightly smaller frames, and are less error-prone.
Response Message Prioritization and Server Pushing
Message prioritization allows developers to customize the relative priority of 
requests to better optimize application performance. As we just learned, the fram-
ing sub-layer organizes messages into parallel streams of data destined to the same 
requestor. When a client sends concurrent requests to a server, it can prioritize the 
responses it is requesting by assigning a weight between 1 and 256 to each message. 
The higher number indicates higher priority. Using these weights, the server can 
send first the frames for the responses with the highest priority. In addition to this, 
the client also states each message‚Äôs dependency on other messages by specifying 
the ID of the message on which it depends.
Another feature of HTTP/2 is the ability for a server to send multiple responses 
for a single client request. That is, in addition to the response to the original request, 
the server can push additional objects to the client, without the client having to 
request each one. This is possible since the HTML base page indicates the objects 
that will be needed to fully render the Web page. So instead of waiting for the 
HTTP requests for these objects, the server can analyze the HTML page, identify 
the objects that are needed, and send them to the client before receiving explicit 
requests for these objects. Server push eliminates the extra latency due to waiting 
for the requests.

HTTP/3
QUIC, discussed in Chapter 3, is a new ‚Äútransport‚Äù protocol that is implemented in 
the application layer over the bare-bones UDP protocol. QUIC has several features 
that are desirable for HTTP, such as message multiplexing (interleaving), per-stream 
flow control, and low-latency connection establishment. HTTP/3 is yet a new HTTP 
protocol that is designed to operate over QUIC. As of 2020, HTTP/3 is described 
in Internet drafts and has not yet been fully standardized. Many of the HTTP/2 fea-
tures (such as message interleaving) are subsumed by QUIC, allowing for a simpler, 
streamlined design for HTTP/3.
2.3 Electronic Mail in the Internet
Electronic mail has been around since the beginning of the Internet. It was the most 
popular application when the Internet was in its infancy [Segaller 1998], and has 
become more elaborate and powerful over the years. It remains one of the Internet‚Äôs 
most important and utilized applications.
As with ordinary postal mail, e-mail is an asynchronous communication 
medium‚Äîpeople send and read messages when it is convenient for them, without 
having to coordinate with other people‚Äôs schedules. In contrast with postal mail, 
electronic mail is fast, easy to distribute, and inexpensive. Modern e-mail has 
many powerful features, including messages with attachments, hyperlinks, HTML- 
formatted text, and embedded photos.
In this section, we examine the application-layer protocols that are at the heart 
of Internet e-mail. But before we jump into an in-depth discussion of these protocols, 
let‚Äôs take a high-level view of the Internet mail system and its key components.
Figure 2.14 presents a high-level view of the Internet mail system. We see from 
this diagram that it has three major components: user agents, mail servers, and the 
Simple Mail Transfer Protocol (SMTP). We now describe each of these compo-
nents in the context of a sender, Alice, sending an e-mail message to a recipient, 
Bob. User agents allow users to read, reply to, forward, save, and compose  messages. 
Examples of user agents for e-mail include Microsoft Outlook, Apple Mail, Web-
based Gmail, the Gmail App running in a smartphone, and so on. When Alice is 
finished composing her message, her user agent sends the message to her mail server, 
where the message is placed in the mail server‚Äôs outgoing message queue. When Bob 
wants to read a message, his user agent retrieves the message from his mailbox in his 
mail server.
Mail servers form the core of the e-mail infrastructure. Each recipient, such 
as Bob, has a mailbox located in one of the mail servers. Bob‚Äôs mailbox manages 
and maintains the messages that have been sent to him. A typical message starts its 
journey in the sender‚Äôs user agent, then travels to the sender‚Äôs mail server, and then

travels to the recipient‚Äôs mail server, where it is deposited in the recipient‚Äôs mailbox. 
When Bob wants to access the messages in his mailbox, the mail server containing 
his mailbox authenticates Bob (with his username and password). Alice‚Äôs mail server 
must also deal with failures in Bob‚Äôs mail server. If Alice‚Äôs server cannot deliver 
mail to Bob‚Äôs server, Alice‚Äôs server holds the message in a message queue and 
attempts to transfer the message later. Reattempts are often done every 30 minutes 
or so; if there is no success after several days, the server removes the message and 
notifies the sender (Alice) with an e-mail message.
SMTP is the principal application-layer protocol for Internet electronic mail. It 
uses the reliable data transfer service of TCP to transfer mail from the sender‚Äôs mail 
server to the recipient‚Äôs mail server. As with most application-layer protocols, SMTP 
has two sides: a client side, which executes on the sender‚Äôs mail server, and a server 
side, which executes on the recipient‚Äôs mail server. Both the client and server sides of 
Outgoing
message queue 
Key:
User mailbox
SMTP
User agent
User agent
User agent
User agent
User agent
User agent
Mail server
Mail server
Mail server
SMTP
SMTP
Figure 2.14 ‚ô¶ A high-level view of the Internet e-mail system

SMTP run on every mail server. When a mail server sends mail to other mail servers, 
it acts as an SMTP client. When a mail server receives mail from other mail servers, 
it acts as an SMTP server.
2.3.1 SMTP
SMTP, defined in RFC 5321, is at the heart of Internet electronic mail. As men-
tioned above, SMTP transfers messages from senders‚Äô mail servers to the recipients‚Äô 
mail servers. SMTP is much older than HTTP. (The original SMTP RFC dates back 
to 1982, and SMTP was around long before that.) Although SMTP has numerous 
wonderful qualities, as evidenced by its ubiquity in the Internet, it is nevertheless 
a legacy technology that possesses certain archaic characteristics. For example, it 
restricts the body (not just the headers) of all mail messages to simple 7-bit ASCII. 
This restriction made sense in the early 1980s when transmission capacity was scarce 
and no one was e-mailing large attachments or large image, audio, or video files. But 
today, in the multimedia era, the 7-bit ASCII restriction is a bit of a pain‚Äîit requires 
binary multimedia data to be encoded to ASCII before being sent over SMTP; and it 
requires the corresponding ASCII message to be decoded back to binary after SMTP 
transport. Recall from Section 2.2 that HTTP does not require multimedia data to be 
ASCII encoded before transfer.
To illustrate the basic operation of SMTP, let‚Äôs walk through a common sce-
nario. Suppose Alice wants to send Bob a simple ASCII message.
 1. Alice invokes her user agent for e-mail, provides Bob‚Äôs e-mail address (for 
example, bob@someschool.edu), composes a message, and instructs the 
user agent to send the message.
 2. Alice‚Äôs user agent sends the message to her mail server, where it is placed in a 
message queue.
 3. The client side of SMTP, running on Alice‚Äôs mail server, sees the message in the 
message queue. It opens a TCP connection to an SMTP server, running on Bob‚Äôs 
mail server.
 4. After some initial SMTP handshaking, the SMTP client sends Alice‚Äôs message 
into the TCP connection.
 5. At Bob‚Äôs mail server, the server side of SMTP receives the message. Bob‚Äôs mail 
server then places the message in Bob‚Äôs mailbox.
 6. Bob invokes his user agent to read the message at his convenience.
The scenario is summarized in Figure 2.15.
It is important to observe that SMTP does not normally use intermediate mail serv-
ers for sending mail, even when the two mail servers are located at opposite ends of 
the world. If Alice‚Äôs server is in Hong Kong and Bob‚Äôs server is in St. Louis, the TCP 
connection is a direct connection between the Hong Kong and St. Louis servers. In

particular, if Bob‚Äôs mail server is down, the message remains in Alice‚Äôs mail server and 
waits for a new attempt‚Äîthe message does not get placed in some intermediate mail 
server.
Let‚Äôs now take a closer look at how SMTP transfers a message from a send-
ing mail server to a receiving mail server. We will see that the SMTP proto-
col has many similarities with protocols that are used for face-to-face human 
interaction. First, the client SMTP (running on the sending mail server host) has 
TCP establish a connection to port 25 at the server SMTP (running on the receiv-
ing mail server host). If the server is down, the client tries again later. Once 
this connection is established, the server and client perform some application-
layer handshaking‚Äîjust as humans often introduce themselves before trans-
ferring information from one to another, SMTP clients and servers introduce 
themselves before transferring information. During this SMTP handshaking phase,  
the SMTP client indicates the e-mail address of the sender (the person who gener-
ated the message) and the e-mail address of the recipient. Once the SMTP client and 
server have introduced themselves to each other, the client sends the message. SMTP 
can count on the reliable data transfer service of TCP to get the message to the server 
without errors. The client then repeats this process over the same TCP connection if 
it has other messages to send to the server; otherwise, it instructs TCP to close the 
connection.
Let‚Äôs next take a look at an example transcript of messages exchanged between an 
SMTP client (C) and an SMTP server (S). The hostname of the client is crepes.fr  
and the hostname of the server is hamburger.edu. The ASCII text lines prefaced 
with C: are exactly the lines the client sends into its TCP socket, and the ASCII text 
lines prefaced with S: are exactly the lines the server sends into its TCP socket. The 
following transcript begins as soon as the TCP connection is established.
S:¬†¬†220 hamburger.edu
C:¬†¬†HELO crepes.fr
SMTP
Alice‚Äôs
mail server
Bob‚Äôs
mail server
Alice‚Äôs
agent
Bob‚Äôs
agent
1
2
4
6
5
Message queue 
Key:
User mailbox
3
Figure 2.15 ‚ô¶ Alice sends a message to Bob

S:¬†¬†250 Hello crepes.fr, pleased to meet you
C:¬†¬†MAIL FROM: <alice@crepes.fr>
S:¬†¬†250 alice@crepes.fr ... Sender ok
C:¬†¬†RCPT TO: <bob@hamburger.edu>
S:¬†¬†250 bob@hamburger.edu ... Recipient ok
C:¬†¬†DATA
S:¬†¬†354 Enter mail, end with ‚Äù.‚Äù on a line by itself
C:¬†¬†Do you like ketchup?
C:¬†¬†How about pickles?
C:¬†¬†.
S:¬†¬†250 Message accepted for delivery
C:¬†¬†QUIT
S:¬†¬†221 hamburger.edu closing connection
In the example above, the client sends a message (‚ÄúDo you like ketchup? 
How about pickles?‚Äù) from mail server crepes.fr to mail server  
hamburger.edu. As part of the dialogue, the client issued five commands: 
HELO (an abbreviation for HELLO), MAIL FROM, RCPT TO, DATA, and QUIT. 
These commands are self-explanatory. The client also sends a line consisting of a 
single period, which indicates the end of the message to the server. (In ASCII jar-
gon, each message ends with CRLF.CRLF, where CR and LF stand for carriage 
return and line feed, respectively.) The server issues replies to each command, 
with each reply having a reply code and some (optional) English-language expla-
nation. We mention here that SMTP uses persistent connections: If the sending 
mail server has several messages to send to the same receiving mail server, it can 
send all of the messages over the same TCP connection. For each message, the 
client begins the process with a new MAIL FROM: crepes.fr, designates the 
end of message with an isolated period, and issues QUIT only after all messages 
have been sent.
It is highly recommended that you use Telnet to carry out a direct dialogue with 
an SMTP server. To do this, issue
telnet serverName 25
where serverName is the name of a local mail server. When you do this, you are 
simply establishing a TCP connection between your local host and the mail server. 
After typing this line, you should immediately receive the 220 reply from the 
server. Then issue the SMTP commands HELO, MAIL FROM, RCPT TO, DATA, 
CRLF.CRLF, and QUIT at the appropriate times. It is also highly recommended 
that you do Programming Assignment 3 at the end of this chapter. In that assign-
ment, you‚Äôll build a simple user agent that implements the client side of SMTP. It 
will allow you to send an e-mail message to an arbitrary recipient via a local mail 
server.

2.3.2 Mail Message Formats
When Alice writes an ordinary snail-mail letter to Bob, she may include all kinds 
of peripheral header information at the top of the letter, such as Bob‚Äôs address, her 
own return address, and the date. Similarly, when an e-mail message is sent from 
one person to another, a header containing peripheral information precedes the 
body of the message itself. This peripheral information is contained in a series of 
header lines, which are defined in RFC 5322. The header lines and the body of the 
message are separated by a blank line (that is, by CRLF). RFC 5322 specifies the 
exact format for mail header lines as well as their semantic interpretations. As with 
HTTP, each header line contains readable text, consisting of a keyword followed 
by a colon followed by a value. Some of the keywords are required and others are 
optional. Every header must have a From: header line and a To: header line; 
a header may include a Subject: header line as well as other optional header 
lines. It is important to note that these header lines are different from the SMTP 
commands we studied in Section 2.3.1 (even though they contain some common 
words such as ‚Äúfrom‚Äù and ‚Äúto‚Äù). The commands in that section were part of the 
SMTP handshaking protocol; the header lines examined in this section are part of 
the mail message itself.
A typical message header looks like this:
From: alice@crepes.fr
To: bob@hamburger.edu
Subject: Searching for the meaning of life.
After the message header, a blank line follows; then the message body (in ASCII) 
follows. You should use Telnet to send a message to a mail server that contains 
some header lines, including the Subject: header line. To do this, issue telnet 
serverName 25, as discussed in Section 2.3.1.
2.3.3 Mail Access Protocols
Once SMTP delivers the message from Alice‚Äôs mail server to Bob‚Äôs mail server, the 
message is placed in Bob‚Äôs mailbox. Given that Bob (the recipient) executes his user 
agent on his local host (e.g., smartphone or PC), it is natural to consider placing a mail 
server on his local host as well. With this approach, Alice‚Äôs mail server would dia-
logue directly with Bob‚Äôs PC. There is a problem with this approach, however. Recall 
that a mail server manages mailboxes and runs the client and server sides of SMTP. 
If Bob‚Äôs mail server were to reside on his local host, then Bob‚Äôs host would have to 
remain always on, and connected to the Internet, in order to receive new mail, which 
can arrive at any time. This is impractical for many Internet users. Instead, a typical 
user runs a user agent on the local host but accesses its mailbox stored on an always-
on shared mail server. This mail server is shared with other users.

Now let‚Äôs consider the path an e-mail message takes when it is sent from Alice 
to Bob. We just learned that at some point along the path the e-mail message needs to 
be deposited in Bob‚Äôs mail server. This could be done simply by having Alice‚Äôs user 
agent send the message directly to Bob‚Äôs mail server. However, typically the send-
er‚Äôs user agent does not dialogue directly with the recipient‚Äôs mail server. Instead, as 
shown in Figure 2.16, Alice‚Äôs user agent uses SMTP or HTTP to deliver the e-mail 
message into her mail server, then Alice‚Äôs mail server uses SMTP (as an SMTP cli-
ent) to relay the e-mail message to Bob‚Äôs mail server. Why the two-step procedure? 
Primarily because without relaying through Alice‚Äôs mail server, Alice‚Äôs user agent 
doesn‚Äôt have any recourse to an unreachable destination mail server. By having Alice 
first deposit the e-mail in her own mail server, Alice‚Äôs mail server can repeatedly try 
to send the message to Bob‚Äôs mail server, say every 30 minutes, until Bob‚Äôs mail 
server becomes operational. (And if Alice‚Äôs mail server is down, then she has the 
recourse of complaining to her system administrator!) 
But there is still one missing piece to the puzzle! How does a recipient like Bob, 
running a user agent on his local host , obtain his messages, which are sitting in a mail 
server? Note that Bob‚Äôs user agent can‚Äôt use SMTP to obtain the messages because 
obtaining the messages is a pull operation, whereas SMTP is a push protocol. 
Today, there are two common ways for Bob to retrieve his e-mail from a mail 
server. If Bob is using Web-based e-mail or a smartphone app (such as Gmail), then 
the user agent will use HTTP to retrieve Bob‚Äôs e-mail. This case requires Bob‚Äôs mail 
server to have an HTTP interface as well as an SMTP interface (to communicate with 
Alice‚Äôs mail server). The alternative method, typically used with mail clients such 
as Microsoft Outlook, is to use the Internet Mail Access Protocol (IMAP) defined 
in RFC 3501. Both the HTTP and IMAP approaches allow Bob to manage folders, 
maintained in Bob‚Äôs mail server. Bob can move messages into the folders he creates, 
delete messages, mark messages as important, and so on.
2.4 DNS‚ÄîThe Internet‚Äôs Directory Service
We human beings can be identified in many ways. For example, we can be iden-
tified by the names that appear on our birth certificates. We can be identified by 
our social security numbers. We can be identified by our driver‚Äôs license numbers. 
SMTP
Alice‚Äôs
mail server
Bob‚Äôs
mail server
Alice‚Äôs
agent
Bob‚Äôs
agent
SMTP
or
HTTP
HTTP
or
IMAP
Figure 2.16 ‚ô¶ E-mail protocols and their communicating entities

Although each can be used to identify people, within a given context one identifier 
may be more appropriate than another. For example, the computers at the IRS (the 
infamous tax-collecting agency in the United States) prefer to use fixed-length social 
security numbers rather than birth certificate names. On the other hand, ordinary 
people  prefer the more mnemonic birth certificate names rather than social security 
numbers. (Indeed, can you imagine saying, ‚ÄúHi. My name is 132-67-9875. Please 
meet my husband, 178-87-1146.‚Äù)
Just as humans can be identified in many ways, so too can Internet hosts. One 
identifier for a host is its hostname. Hostnames‚Äîsuch as www.facebook.com, 
www.google.com, gaia.cs.umass.edu‚Äîare mnemonic and are therefore 
appreciated by humans. However, hostnames provide little, if any, information about 
the location within the Internet of the host. (A hostname such as www.eurecom.
fr, which ends with the country code .fr, tells us that the host is probably in 
France, but doesn‚Äôt say much more.) Furthermore, because hostnames can consist of 
variable-length alphanumeric characters, they would be difficult to process by rout-
ers. For these reasons, hosts are also identified by so-called IP addresses.
We discuss IP addresses in some detail in Chapter 4, but it is useful to say a 
few brief words about them now. An IP address consists of four bytes and has a 
rigid hierarchical structure. An IP address looks like 121.7.106.83, where each 
period separates one of the bytes expressed in decimal notation from 0 to 255. An IP 
address is hierarchical because as we scan the address from left to right, we obtain 
more and more specific information about where the host is located in the Internet 
(that is, within which network, in the network of networks). Similarly, when we scan 
a postal address from bottom to top, we obtain more and more specific information 
about where the addressee is located.
2.4.1 Services Provided by DNS
We have just seen that there are two ways to identify a host‚Äîby a hostname and 
by an IP address. People prefer the more mnemonic hostname identifier, while 
routers prefer fixed-length, hierarchically structured IP addresses. In order to rec-
oncile these preferences, we need a directory service that translates hostnames to 
IP addresses. This is the main task of the Internet‚Äôs domain name system (DNS). 
The DNS is (1) a distributed database implemented in a hierarchy of DNS servers,  
and (2) an application-layer protocol that allows hosts to query the distributed 
database. The DNS servers are often UNIX machines running the Berkeley Inter-
net Name Domain (BIND) software [BIND 2020]. The DNS protocol runs over 
UDP and uses port 53.
DNS is commonly employed by other application-layer protocols, including 
HTTP and SMTP, to translate user-supplied hostnames to IP addresses. As an exam-
ple, consider what happens when a browser (that is, an HTTP client), running on 
some user‚Äôs host, requests the URL www.someschool.edu/index.html. In 
order for the user‚Äôs host to be able to send an HTTP request message to the Web

server www.someschool.edu, the user‚Äôs host must first obtain the IP address of 
www.someschool.edu. This is done as follows.
 1. The same user machine runs the client side of the DNS application.
 2. The browser extracts the hostname, www.someschool.edu, from the URL 
and passes the hostname to the client side of the DNS application.
 3. The DNS client sends a query containing the hostname to a DNS server.
 4. The DNS client eventually receives a reply, which includes the IP address for 
the hostname.
 5. Once the browser receives the IP address from DNS, it can initiate a TCP con-
nection to the HTTP server process located at port 80 at that IP address.
We see from this example that DNS adds an additional delay‚Äîsometimes  
substantial‚Äîto the Internet applications that use it. Fortunately, as we discuss below, 
the desired IP address is often cached in a ‚Äúnearby‚Äù DNS server, which helps to 
reduce DNS network traffic as well as the average DNS delay.
DNS provides a few other important services in addition to translating host-
names to IP addresses:
‚Ä¢ Host aliasing. A host with a complicated hostname can have one or more  
alias names. For example, a hostname such as relay1.west-coast 
.enterprise.com could have, say, two aliases such as enterprise.com  
and www.enterprise.com. In this case, the hostname relay1 
.west-coast.enterprise.com is said to be a canonical hostname. Alias 
hostnames, when present, are typically more mnemonic than canonical host-
names. DNS can be invoked by an application to obtain the canonical hostname 
for a supplied alias hostname as well as the IP address of the host.
‚Ä¢ Mail server aliasing. For obvious reasons, it is highly desirable that e-mail 
addresses be mnemonic. For example, if Bob has an account with Yahoo Mail, 
Bob‚Äôs e-mail address might be as simple as bob@yahoo.com. However, the 
hostname of the Yahoo mail server is more complicated and much less mnemonic 
than simply yahoo.com (for example, the canonical hostname might be some-
thing like relay1.west-coast.yahoo.com). DNS can be invoked by a 
mail application to obtain the canonical hostname for a supplied alias hostname 
as well as the IP address of the host. In fact, the MX record (see below) permits a 
company‚Äôs mail server and Web server to have identical (aliased) hostnames; for 
example, a company‚Äôs Web server and mail server can both be called enter-
prise.com.
‚Ä¢ Load distribution. DNS is also used to perform load distribution among repli-
cated servers, such as replicated Web servers. Busy sites, such as cnn.com, are 
replicated over multiple servers, with each server running on a different end sys-
tem and each having a different IP address. For replicated Web servers, a set of IP

addresses is thus associated with one alias hostname. The DNS database contains 
this set of IP addresses. When clients make a DNS query for a name mapped to a 
set of addresses, the server responds with the entire set of IP addresses, but rotates 
the ordering of the addresses within each reply. Because a client typically sends 
its HTTP request message to the IP address that is listed first in the set, DNS rota-
tion distributes the traffic among the replicated servers. DNS rotation is also used 
for e-mail so that multiple mail servers can have the same alias name. Also, con-
tent distribution companies such as Akamai have used DNS in more sophisticated 
ways [Dilley 2002] to provide Web content distribution (see Section 2.6.3).
The DNS is specified in RFC 1034 and RFC 1035, and updated in several addi-
tional RFCs. It is a complex system, and we only touch upon key aspects of its 
operation here. The interested reader is referred to these RFCs and the book by Albitz 
and Liu [Albitz 1993]; see also the retrospective paper [Mockapetris 1988], which 
provides a nice description of the what and why of DNS, and [Mockapetris 2005].
2.4.2 Overview of How DNS Works
We now present a high-level overview of how DNS works. Our discussion will focus 
on the hostname-to-IP-address translation service.
Suppose that some application (such as a Web browser or a mail client) running 
in a user‚Äôs host needs to translate a hostname to an IP address. The application will 
invoke the client side of DNS, specifying the hostname that needs to be translated. 
(On many UNIX-based machines, gethostbyname() is the function call that 
an application calls in order to perform the translation.) DNS in the user‚Äôs host then 
DNS: CRITICAL NETWORK FUNCTIONS VIA THE CLIENT-SERVER PARADIGM
Like HTTP, FTP, and SMTP, the DNS protocol is an application-layer protocol since it 
(1)¬†runs between communicating end systems using the client-server paradigm and 
(2)¬†relies on an underlying end-to-end transport protocol to transfer DNS messages between 
communicating end systems. In another sense, however, the role of the DNS is quite differ-
ent from Web, file transfer, and e-mail applications. Unlike these applications, the DNS is 
not an application with which a user directly interacts. Instead, the DNS provides a core 
Internet function‚Äînamely, translating hostnames to their underlying IP addresses, for user 
applications and other software in the Internet. We noted in Section 1.2 that much of the 
complexity in the Internet architecture is located at the ‚Äúedges‚Äù of the network. The DNS, 
which implements the critical name-to-address translation process using clients and servers 
located at the edge of the network, is yet another example of that design philosophy.
PRINCIPLES IN PRACTICE

takes over, sending a query message into the network. All DNS query and reply mes-
sages are sent within UDP datagrams to port 53. After a delay, ranging from millisec-
onds to seconds, DNS in the user‚Äôs host receives a DNS reply message that provides 
the desired mapping. This mapping is then passed to the invoking application. Thus, 
from the perspective of the invoking application in the user‚Äôs host, DNS is a black 
box providing a simple, straightforward translation service. But in fact, the black box 
that implements the service is complex, consisting of a large number of DNS servers 
distributed around the globe, as well as an application-layer protocol that specifies 
how the DNS servers and querying hosts communicate.
A simple design for DNS would have one DNS server that contains all the map-
pings. In this centralized design, clients simply direct all queries to the single DNS 
server, and the DNS server responds directly to the querying clients. Although the 
simplicity of this design is attractive, it is inappropriate for today‚Äôs Internet, with its 
vast (and growing) number of hosts. The problems with a centralized design include:
‚Ä¢ A single point of failure. If the DNS server crashes, so does the entire Internet!
‚Ä¢ Traffic volume. A single DNS server would have to handle all DNS queries (for 
all the HTTP requests and e-mail messages generated from hundreds of millions 
of hosts).
‚Ä¢ Distant centralized database. A single DNS server cannot be ‚Äúclose to‚Äù all the 
querying clients. If we put the single DNS server in New York City, then all que-
ries from Australia must travel to the other side of the globe, perhaps over slow 
and congested links. This can lead to significant delays.
‚Ä¢ Maintenance. The single DNS server would have to keep records for all Internet 
hosts. Not only would this centralized database be huge, but it would have to be 
updated frequently to account for every new host.
In summary, a centralized database in a single DNS server simply doesn‚Äôt scale. 
Consequently, the DNS is distributed by design. In fact, the DNS is a wonderful 
example of how a distributed database can be implemented in the Internet.
A Distributed, Hierarchical Database
In order to deal with the issue of scale, the DNS uses a large number of servers, 
organized in a hierarchical fashion and distributed around the world. No single DNS 
server has all of the mappings for all of the hosts in the Internet. Instead, the map-
pings are distributed across the DNS servers. To a first approximation, there are three 
classes of DNS servers‚Äîroot DNS servers, top-level domain (TLD) DNS servers, 
and authoritative DNS servers‚Äîorganized in a hierarchy as shown in Figure 2.17. 
To understand how these three classes of servers interact, suppose a DNS client 
wants to determine the IP address for the hostname www.amazon.com. To a first 
approximation, the following events will take place. The client first contacts one of

the root servers, which returns IP addresses for TLD servers for the top-level domain 
com. The client then contacts one of these TLD servers, which returns the IP address 
of an authoritative server for amazon.com. Finally, the client contacts one of the 
authoritative servers for amazon.com, which returns the IP address for the host-
name www.amazon.com. We‚Äôll soon examine this DNS lookup process in more 
detail. But let‚Äôs first take a closer look at these three classes of DNS servers:
‚Ä¢ Root DNS servers. There are more than 1000 root servers instances scattered all 
over the world, as shown in Figure 2.18. These root servers are copies of 13¬†dif-
ferent root servers, managed by 12 different organizations, and coordinated 
through the Internet Assigned Numbers Authority [IANA 2020]. The full list 
of root name servers, along with the organizations that manage them and their 
IP¬†addresses can be found at [Root Servers 2020]. Root name servers provide 
the¬†IP addresses of the TLD servers.
‚Ä¢ Top-level domain (TLD) servers. For each of the top-level domains‚Äîtop-level 
domains such as com, org, net, edu, and gov, and all of the country top-level 
domains such as uk, fr, ca, and jp‚Äîthere is TLD server (or server cluster). The 
company Verisign Global Registry Services maintains the TLD servers for the 
com top-level domain, and the company Educause maintains the TLD servers for 
the edu top-level domain. The network infrastructure supporting a TLD can be 
large and complex; see [Osterweil 2012] for a nice overview of the Verisign net-
work. See [TLD list 2020] for a list of all top-level domains. TLD servers provide 
the IP addresses for authoritative DNS servers.
‚Ä¢ Authoritative DNS servers. Every organization with publicly accessible hosts 
(such as Web servers and mail servers) on the Internet must provide publicly 
accessible DNS records that map the names of those hosts to IP addresses. An 
organization‚Äôs authoritative DNS server houses these DNS records. An organi-
zation can choose to implement its own authoritative DNS server to hold these 
records; alternatively, the organization can pay to have these records stored in an 
edu DNS servers
org DNS servers
com DNS servers
nyu.edu
DNS servers
facebook.com
DNS servers
amazon.com
DNS servers
pbs.org
DNS servers
umass.edu
DNS servers
Root DNS servers
Figure 2.17 ‚ô¶ Portion of the hierarchy of DNS servers

authoritative DNS server of some service provider. Most universities and large 
companies implement and maintain their own primary and secondary (backup) 
authoritative DNS server.
The root, TLD, and authoritative DNS servers all belong to the hierarchy of 
DNS servers, as shown in Figure 2.17. There is another important type of DNS 
server called the local DNS server. A local DNS server does not strictly belong to 
the hierarchy of servers but is nevertheless central to the DNS architecture. Each 
ISP‚Äîsuch as a residential ISP or an institutional ISP‚Äîhas a local DNS server (also 
called a default name server). When a host connects to an ISP, the ISP provides 
the host with the IP addresses of one or more of its local DNS servers (typically 
through DHCP, which is discussed in Chapter 4). You can easily determine the IP 
address of your local DNS server by accessing network status windows in Win-
dows or UNIX. A host‚Äôs local DNS server is typically ‚Äúclose to‚Äù the host. For an 
institutional ISP, the local DNS server may be on the same LAN as the host; for a 
residential ISP, it is typically separated from the host by no more than a few rout-
ers. When a host makes a DNS query, the query is sent to the local DNS server, 
which acts a proxy,  forwarding the query into the DNS server hierarchy, as we‚Äôll 
discuss in more detail below.
Let‚Äôs take a look at a simple example. Suppose the host cse.nyu.edu desires 
the IP address of gaia.cs.umass.edu. Also suppose that NYU‚Äôs local DNS 
server for cse.nyu.edu is called dns.nyu.edu and that an authoritative DNS 
server for gaia.cs.umass.edu is called dns.umass.edu. As shown in 
0 Servers
1‚Äì10 Servers
11‚Äì20 Servers
21+ Servers
Key:
Figure 2.18 ‚ô¶ DNS root servers in 2020

Figure 2.19, the host cse.nyu.edu first sends a DNS query message to its local 
DNS server, dns.nyu.edu. The query message contains the hostname to be trans-
lated, namely, gaia.cs.umass.edu. The local DNS server forwards the query 
message to a root DNS server. The root DNS server takes note of the edu suffix and 
returns to the local DNS server a list of IP addresses for TLD servers responsible 
for edu. The local DNS server then resends the query message to one of these TLD 
servers. The TLD server takes note of the umass.edu suffix and responds with 
the IP address of the authoritative DNS server for the University of Massachusetts, 
namely, dns.umass.edu. Finally, the local DNS server resends the query mes-
sage directly to dns.umass.edu, which responds with the IP address of gaia 
.cs.umass.edu. Note that in this example, in order to obtain the mapping for one 
hostname, eight DNS messages were sent: four query messages and four reply mes-
sages! We‚Äôll soon see how DNS caching reduces this query traffic.
Our previous example assumed that the TLD server knows the authoritative DNS 
server for the hostname. In general, this is not always true. Instead, the TLD server 
Requesting host
cse.nyu.edu
Local DNS server
TLD DNS server
dns.nyu.edu
Root DNS server
1
8
2
7
4
5
3
6
Authoritative DNS server
dns.umass.edu
gaia.cs.umass.edu
Figure 2.19 ‚ô¶ Interaction of the various DNS servers

may know only of an intermediate DNS server, which in turn knows the authorita-
tive DNS server for the hostname. For example, suppose again that the University of 
Massachusetts has a DNS server for the university, called dns.umass.edu. Also 
suppose that each of the departments at the University of Massachusetts has its own 
DNS server, and that each departmental DNS server is authoritative for all hosts in 
the department. In this case, when the intermediate DNS server, dns.umass.edu, 
receives a query for a host with a hostname ending with cs.umass.edu, it returns 
to dns.nyu.edu the IP address of dns.cs.umass.edu, which is authoritative 
for all hostnames ending with cs.umass.edu. The local DNS server dns.nyu 
.edu then sends the query to the authoritative DNS server, which returns the desired 
mapping to the local DNS server, which in turn returns the mapping to the requesting 
host. In this case, a total of 10 DNS messages are sent!
The example shown in Figure 2.19 makes use of both recursive queries and 
iterative queries. The query sent from cse.nyu.edu to dns.nyu.edu is a 
recursive query, since the query asks dns.nyu.edu to obtain the mapping on its 
behalf. However, the subsequent three queries are iterative since all of the replies 
are directly returned to dns.nyu.edu. In theory, any DNS query can be itera-
tive or recursive. For example, Figure 2.20 shows a DNS query chain for which all  
of the queries are recursive. In practice, the queries typically follow the pattern in 
Figure 2.19: The query from the requesting host to the local DNS server is recursive, 
and the remaining queries are iterative.
DNS Caching
Our discussion thus far has ignored DNS caching, a critically important feature 
of the DNS system. In truth, DNS extensively exploits DNS caching in order to 
improve the delay performance and to reduce the number of DNS messages  
ricocheting around the Internet. The idea behind DNS caching is very simple. In a 
query chain, when a DNS server receives a DNS reply (containing, for example, a 
mapping from a hostname to an IP address), it can cache the mapping in its local 
memory. For example, in Figure 2.19, each time the local DNS server dns.nyu.edu 
receives a reply from some DNS server, it can cache any of the information contained 
in the reply. If a hostname/IP address pair is cached in a DNS server and another 
query arrives to the DNS server for the same hostname, the DNS server can provide 
the desired IP address, even if it is not authoritative for the hostname. Because hosts 
and mappings between hostnames and IP addresses are by no means permanent, 
DNS servers discard cached information after a period of time (often set to two days).
As an example, suppose that a host apricot.nyu.edu queries dns.nyu.edu  
for the IP address for the hostname cnn.com. Furthermore,  suppose that a few hours 
later, another NYU host, say, kiwi.nyu.edu, also queries dns.nyu.edu  
with the same hostname. Because of caching, the local DNS server will be able 
to immediately return the IP address of cnn.com to this second requesting 
host¬†without having to query any other DNS servers. A local DNS server can

Requesting host
cse.nyu.edu
Local DNS server
TLD DNS server
dns.nyu.edu
Root DNS server
1
8
5
4
2
7
Authoritative DNS server
dns.umass.edu
gaia.cs.umass.edu
6
3
Figure 2.20 ‚ô¶ Recursive queries in DNS
also cache the IP addresses of TLD servers, thereby allowing the local DNS server 
to bypass the root DNS servers in a query chain. In fact, because of caching, root 
servers are bypassed for all but a very small fraction of DNS queries.
2.4.3 DNS Records and Messages
The DNS servers that together implement the DNS distributed database store 
resource records (RRs), including RRs that provide hostname-to-IP address map-
pings. Each DNS reply message carries one or more resource records. In this and 
the following subsection, we provide a brief overview of DNS resource records and 
messages; more details can be found in [Albitz 1993] or in the DNS RFCs [RFC 
1034; RFC 1035].

A resource record is a four-tuple that contains the following fields:
(Name, Value, Type, TTL)
TTL is the time to live of the resource record; it determines when a resource should 
be removed from a cache. In the example records given below, we ignore the TTL 
field. The meaning of Name and Value depend on Type:
‚Ä¢ If Type=A, then Name is a hostname and Value is the IP address for the host-
name. Thus, a Type A record provides the standard hostname-to-IP address map-
ping. As an example, (relay1.bar.foo.com, 145.37.93.126, A) is 
a Type A record.
‚Ä¢ If Type=NS, then Name is a domain (such as foo.com) and Value is the host-
name of an authoritative DNS server that knows how to obtain the IP addresses 
for hosts in the domain. This record is used to route DNS queries further along in 
the query chain. As an example, (foo.com, dns.foo.com, NS) is a Type 
NS record.
‚Ä¢ If Type=CNAME, then Value is a canonical hostname for the alias hostname 
Name. This record can provide querying hosts the canonical name for a host-
name. As an example, (foo.com, relay1.bar.foo.com, CNAME) is a 
CNAME record.
‚Ä¢ If Type=MX, then Value is the canonical name of a mail server that has an alias 
hostname Name. As an example, (foo.com, mail.bar.foo.com, MX) 
is an MX record. MX records allow the hostnames of mail servers to have simple 
aliases. Note that by using the MX record, a company can have the same aliased 
name for its mail server and for one of its other servers (such as its Web server). 
To obtain the canonical name for the mail server, a DNS client would query for 
an MX record; to obtain the canonical name for the other server, the DNS client 
would query for the CNAME record.
If a DNS server is authoritative for a particular hostname, then the DNS server 
will contain a Type A record for the hostname. (Even if the DNS server is not author-
itative, it may contain a Type A record in its cache.) If a server is not authoritative 
for a hostname, then the server will contain a Type NS record for the domain that 
includes the hostname; it will also contain a Type A record that provides the IP address 
of the DNS server in the Value field of the NS record. As an example, suppose an 
edu TLD server is not authoritative for the host gaia.cs.umass.edu. Then this 
server will contain a record for a domain that includes the host gaia.cs.umass 
.edu, for example, (umass.edu, dns.umass.edu, NS). The edu 
TLD server would also contain a Type A record, which maps the DNS server 
dns.umass.edu to an IP address, for example, (dns.umass.edu, 
128.119.40.111, A).

DNS Messages
Earlier in this section, we referred to DNS query and reply messages. These are the 
only two kinds of DNS messages. Furthermore, both query and reply messages have 
the same format, as shown in Figure 2.21.The semantics of the various fields in a 
DNS message are as follows:
‚Ä¢ The first 12 bytes is the header section, which has a number of fields. The first 
field is a 16-bit number that identifies the query. This identifier is copied into the 
reply message to a query, allowing the client to match received replies with sent 
queries. There are a number of flags in the flag field. A 1-bit query/reply flag indi-
cates whether the message is a query (0) or a reply (1). A 1-bit authoritative flag is 
set in a reply message when a DNS server is an authoritative server for a queried 
name. A 1-bit recursion-desired flag is set when a client (host or DNS server) 
desires that the DNS server perform recursion when it doesn‚Äôt have the record. A 
1-bit recursion-available field is set in a reply if the DNS server supports recur-
sion. In the header, there are also four number-of fields. These fields indicate the 
number of occurrences of the four types of data sections that follow the header.
‚Ä¢ The question section contains information about the query that is being made. 
This section includes (1) a name field that contains the name that is being que-
ried, and (2) a type field that indicates the type of question being asked about the 
name‚Äîfor example, a host address associated with a name (Type A) or the mail 
server for a name (Type MX).
IdentiÔ¨Åcation
Number of questions
Number of authority RRs
Name, type Ô¨Åelds for
a query
12 bytes
RRs in response to query
Records for
authoritative servers
Additional ‚Äúhelpful‚Äù
info that may be used
Flags
Number of answer RRs
Number of additional RRs
Authority
(variable number of resource records)
Additional information
(variable number of resource records)
Answers
(variable number of resource records)
Questions
(variable number of questions)
Figure 2.21 ‚ô¶ DNS message format

‚Ä¢ In a reply from a DNS server, the answer section contains the resource records for 
the name that was originally queried. Recall that in each resource record there is the 
Type (for example, A, NS, CNAME, and MX), the Value, and the TTL. A reply can 
return multiple RRs in the answer, since a hostname can have multiple IP addresses 
(for example, for replicated Web servers, as discussed earlier in this section).
‚Ä¢ The authority section contains records of other authoritative servers.
‚Ä¢ The additional section contains other helpful records. For example, the answer 
field in a reply to an MX query contains a resource record providing the canoni-
cal hostname of a mail server. The additional section contains a Type A record 
providing the IP address for the canonical hostname of the mail server.
How would you like to send a DNS query message directly from the host you‚Äôre 
working on to some DNS server? This can easily be done with the nslookup program, 
which is available from most Windows and UNIX platforms. For example, from a Win-
dows host, open the Command Prompt and invoke the nslookup program by simply typ-
ing ‚Äúnslookup.‚Äù After invoking nslookup, you can send a DNS query to any DNS server 
(root, TLD, or authoritative). After receiving the reply message from the DNS server, 
nslookup will display the records included in the reply (in a human-readable format). As 
an alternative to running nslookup from your own host, you can visit one of many Web 
sites that allow you to remotely employ nslookup. (Just type ‚Äúnslookup‚Äù into a search 
engine and you‚Äôll be brought to one of these sites.) The DNS Wireshark lab at the end of 
this chapter will allow you to explore the DNS in much more detail.
Inserting Records into the DNS Database
The discussion above focused on how records are retrieved from the DNS database. 
You might be wondering how records get into the database in the first place. Let‚Äôs look 
at how this is done in the context of a specific example. Suppose you have just created 
an exciting new startup company called Network Utopia. The first thing you‚Äôll surely 
want to do is register the domain name networkutopia.com at a registrar. A reg-
istrar is a commercial entity that verifies the uniqueness of the domain name, enters 
the domain name into the DNS database (as discussed below), and collects a small fee 
from you for its services. Prior to 1999, a single registrar, Network Solutions, had a 
monopoly on domain name registration for com, net, and org domains. But now 
there are many registrars competing for customers, and the Internet Corporation for 
Assigned Names and Numbers (ICANN) accredits the various registrars. A complete 
list of accredited registrars is available at http://www.internic.net.
When you register the domain name networkutopia.com with some reg-
istrar, you also need to provide the registrar with the names and IP addresses of 
your primary and secondary authoritative DNS servers. Suppose the names and IP 
addresses are dns1.networkutopia.com, dns2.networkutopia.com, 
212.2.212.1, and 212.212.212.2. For each of these two authoritative DNS

DNS VULNERABILITIES
We have seen that DNS is a critical component of the Internet infrastructure, with 
many important services‚Äîincluding the Web and e-mail‚Äîsimply incapable of func-
tioning without it. We therefore naturally ask, how can DNS be attacked? Is DNS a 
sitting duck, waiting to be knocked out of service, while taking most Internet applica-
tions down with it?
The first type of attack that comes to mind is a DDoS bandwidth-flooding attack 
(see Section 1.6) against DNS servers. For example, an attacker could attempt to 
send to each DNS root server a deluge of packets, so many that the majority of 
legitimate DNS queries never get answered. Such a large-scale DDoS attack against 
DNS root servers actually took place on October 21, 2002. In this attack, the attack-
ers leveraged a botnet to send truck loads of ICMP ping messages to each of the 
13 DNS root IP addresses. (ICMP messages are discussed in Section 5.6. For now, 
it suffices to know that ICMP packets are special types of IP datagrams.) Fortunately, 
this large-scale attack caused minimal damage, having little or no impact on users‚Äô 
Internet experience. The attackers did succeed at directing a deluge of packets at the 
root servers. But many of the DNS root servers were protected by packet filters, con-
figured to always block all ICMP ping messages directed at the root servers. These 
protected servers were thus spared and functioned as normal. Furthermore, most local 
DNS servers cache the IP addresses of top-level-domain servers, allowing the query 
process to often bypass the DNS root servers.
A potentially more effective DDoS attack against DNS is send a deluge of DNS 
queries to top-level-domain servers, for example, to top-level-domain servers that 
handle the .com domain. It is harder to filter DNS queries directed to DNS servers; 
and top-level-domain servers are not as easily bypassed as are root servers. Such an 
attack took place against the top-level-domain service provider Dyn on October 21, 
2016. This DDoS attack was accomplished through a large number of DNS lookup 
requests from a botnet consisting of about one hundred thousand IoT devices such as 
printers, IP cameras, residential gateways and baby monitors that had been infected 
with Mirai malware. For almost a full day, Amazon, Twitter, Netflix, Github and 
Spotify were disturbed.
DNS could potentially be attacked in other ways. In a man-in-the-middle attack, 
the attacker intercepts queries from hosts and returns bogus replies. In the DNS poi-
soning attack, the attacker sends bogus replies to a DNS server, tricking the server 
into accepting bogus records into its cache. Either of these attacks could be used, 
for example, to redirect an unsuspecting Web user to the attacker‚Äôs Web site. The 
DNS Security Extensions (DNSSEC [Gieben 2004; RFC 4033] have been designed 
and deployed to protect against such exploits. DNSSEC, a secured version of DNS, 
addresses many of these possible attacks and is gaining popularity in the Internet.
FOCUS ON SECURITY

servers, the registrar would then make sure that a Type NS and a Type A record are 
entered into the TLD com servers. Specifically, for the primary authoritative server 
for networkutopia.com, the registrar would insert the following two resource 
records into the DNS system:
(networkutopia.com, dns1.networkutopia.com, NS)
(dns1.networkutopia.com, 212.212.212.1, A)
You‚Äôll also have to make sure that the Type A resource record for your Web server 
www.networkutopia.com and the Type MX resource record for your mail 
server mail.networkutopia.com are entered into your authoritative DNS 
servers. (Until recently, the contents of each DNS server were configured statically, 
for example, from a configuration file created by a system manager. More recently, 
an UPDATE option has been added to the DNS protocol to allow data to be dynami-
cally added or deleted from the database via DNS messages. [RFC 2136] and [RFC 
3007] specify DNS dynamic updates.)
Once all of these steps are completed, people will be able to visit your Web site 
and send e-mail to the employees at your company. Let‚Äôs conclude our discussion of 
DNS by verifying that this statement is true. This verification also helps to solidify 
what we have learned about DNS. Suppose Alice in Australia wants to view the Web 
page www.networkutopia.com. As discussed earlier, her host will first send a 
DNS query to her local DNS server. The local DNS server will then contact a TLD 
com server. (The local DNS server will also have to contact a root DNS server if the 
address of a TLD com server is not cached.) This TLD server contains the Type NS 
and Type A resource records listed above, because the registrar had these resource 
records inserted into all of the TLD com servers. The TLD com server sends a reply 
to Alice‚Äôs local DNS server, with the reply containing the two resource records. The 
local DNS server then sends a DNS query to 212.212.212.1, asking for the Type 
A record corresponding to www.networkutopia.com. This record provides the 
IP address of the desired Web server, say, 212.212.71.4, which the local DNS 
server passes back to Alice‚Äôs host. Alice‚Äôs browser can now initiate a TCP connec-
tion to the host 212.212.71.4 and send an HTTP request over the connection. 
Whew! There‚Äôs a lot more going on than what meets the eye when one surfs the Web!
2.5 Peer-to-Peer File Distribution
The applications described in this chapter thus far‚Äîincluding the Web, e-mail, and 
DNS‚Äîall employ client-server architectures with significant reliance on always-on 
infrastructure servers. Recall from Section 2.1.1 that with a P2P architecture, there 
is minimal (or no) reliance on always-on infrastructure servers. Instead, pairs of 
intermittently connected hosts, called peers, communicate directly with each other. 
The peers are not owned by a service provider, but are instead PCs, laptops, and 
smartpones controlled by users.

In this section, we consider a very natural P2P application, namely, distributing a 
large file from a single server to a large number of hosts (called peers). The file might 
be a new version of the Linux operating system, a software patch for an existing 
operating system or an MPEG video file. In client-server file distribution, the server 
must send a copy of the file to each of the peers‚Äîplacing an enormous burden on the 
server and consuming a large amount of server bandwidth. In P2P file distribution, 
each peer can redistribute any portion of the file it has received to any other peers, 
thereby assisting the server in the distribution process. As of 2020, the most popular 
P2P file distribution protocol is BitTorrent. Originally developed by Bram Cohen, 
there are now many different independent BitTorrent clients conforming to the Bit-
Torrent protocol, just as there are a number of Web browser clients that conform to 
the HTTP protocol. In this subsection, we first examine the self-scalability of P2P 
architectures in the context of file distribution. We then describe BitTorrent in some 
detail, highlighting its most important characteristics and features.
Scalability of P2P Architectures
To compare client-server architectures with peer-to-peer architectures, and illustrate 
the inherent self-scalability of P2P, we now consider a simple quantitative model 
for distributing a file to a fixed set of peers for both architecture types. As shown 
in Figure 2.22, the server and the peers are connected to the Internet with access 
Internet
File: F
Server
us
u1
u2
u3
d1
d2
d3
u4
u5
u6
d4
d5
d6
uN
dN
Figure 2.22 ‚ô¶ An illustrative file distribution problem

links. Denote the upload rate of the server‚Äôs access link by us, the upload rate of the 
ith peer‚Äôs access link by ui, and the download rate of the ith peer‚Äôs access link by 
di. Also denote the size of the file to be distributed (in bits) by F and the number of 
peers that want to obtain a copy of the file by N. The distribution time is the time it 
takes to get a copy of the file to all N peers. In our analysis of the distribution time 
below, for both client-server and P2P architectures, we make the simplifying (and 
generally accurate [Akella 2003]) assumption that the Internet core has abundant 
bandwidth, implying that all of the bottlenecks are in access networks. We also sup-
pose that the server and clients are not participating in any other network applica-
tions, so that all of their upload and download access bandwidth can be fully devoted 
to distributing this file.
Let‚Äôs first determine the distribution time for the client-server architecture, 
which we denote by Dcs. In the client-server architecture, none of the peers aids in 
distributing the file. We make the following observations:
‚Ä¢ The server must transmit one copy of the file to each of the N peers. Thus, the 
server must transmit NF bits. Since the server‚Äôs upload rate is us, the time to dis-
tribute the file must be at least NF/us.
‚Ä¢ Let dmin denote the download rate of the peer with the lowest download rate, that 
is, dmin = min5d1, dp, . . . , dN6. The peer with the lowest download rate cannot 
obtain all F bits of the file in less than F/dmin seconds. Thus, the minimum distri-
bution time is at least F/dmin.
Putting these two observations together, we obtain
Dcs √ö maxb NF
us
 , F
dmin
r.
This provides a lower bound on the minimum distribution time for the client-server 
architecture. In the homework problems, you will be asked to show that the server 
can schedule its transmissions so that the lower bound is actually achieved. So let‚Äôs 
take this lower bound provided above as the actual distribution time, that is,
 
Dcs = maxb NF
us
, F
dmin
r 
(2.1)
We see from Equation 2.1 that for N large enough, the client-server distribution time 
is given by NF/us. Thus, the distribution time increases linearly with the number of 
peers N. So, for example, if the number of peers from one week to the next increases 
a thousand-fold from a thousand to a million, the time required to distribute the file 
to all peers increases by 1,000.

Let‚Äôs now go through a similar analysis for the P2P architecture, where each peer 
can assist the server in distributing the file. In particular, when a peer receives some 
file data, it can use its own upload capacity to redistribute the data to other peers. Cal-
culating the distribution time for the P2P architecture is somewhat more complicated 
than for the client-server architecture, since the distribution time depends on how 
each peer distributes portions of the file to the other peers. Nevertheless, a simple 
expression for the minimal distribution time can be obtained [Kumar 2006]. To this 
end, we first make the following observations:
‚Ä¢ At the beginning of the distribution, only the server has the file. To get this file 
into the community of peers, the server must send each bit of the file at least once 
into its access link. Thus, the minimum distribution time is at least F/us. (Unlike 
the client-server scheme, a bit sent once by the server may not have to be sent by 
the server again, as the peers may redistribute the bit among themselves.)
‚Ä¢ As with the client-server architecture, the peer with the lowest download rate 
cannot obtain all F bits of the file in less than F/dmin seconds. Thus, the minimum 
distribution time is at least F/dmin.
‚Ä¢ Finally, observe that the total upload capacity of the system as a whole is equal 
to the upload rate of the server plus the upload rates of each of the individual 
peers, that is, utotal = us + u1 + g+ uN. The system must deliver (upload) F 
bits to each of the N peers, thus delivering a total of NF bits. This cannot be done 
at a rate faster than utotal. Thus, the minimum distribution time is also at least 
NF/(us + u1 + g+ uN).
Putting these three observations together, we obtain the minimum distribution 
time for P2P, denoted by DP2P.
 
DP2P √ö max c
F
us
, F
dmin
, 
NF
us + a
N
i=1
ui
s 
(2.2)
Equation 2.2 provides a lower bound for the minimum distribution time for the P2P 
architecture. It turns out that if we imagine that each peer can redistribute a bit as 
soon as it receives the bit, then there is a redistribution scheme that actually achieves 
this lower bound [Kumar 2006]. (We will prove a special case of this result in the 
homework.) In reality, where chunks of the file are redistributed rather than indi-
vidual bits, Equation 2.2 serves as a good approximation of the actual minimum 
distribution time. Thus, let‚Äôs take the lower bound provided by Equation 2.2 as the 
actual minimum distribution time, that is,
 
DP2P = max c
F
us
, F
dmin
, 
NF
us + a
N
i=1
ui
s 
(2.3)

Figure 2.23 compares the minimum distribution time for the client-server and 
P2P architectures assuming that all peers have the same upload rate u. In Figure 2.23, 
we have set F/u = 1 hour, us = 10u, and dmin √ö us. Thus, a peer can transmit the 
entire file in one hour, the server transmission rate is 10 times the peer upload rate, 
and (for simplicity) the peer download rates are set large enough so as not to have 
an effect. We see from Figure 2.23 that for the client-server architecture, the distri-
bution time increases linearly and without bound as the number of peers increases. 
However, for the P2P architecture, the minimal distribution time is not only always 
less than the distribution time of the client-server architecture; it is also less than one 
hour for any number of peers N. Thus, applications with the P2P architecture can be 
self-scaling. This scalability is a direct consequence of peers being redistributors as 
well as consumers of bits.
BitTorrent
BitTorrent is a popular P2P protocol for file distribution [Chao 2011]. In BitTorrent 
lingo, the collection of all peers participating in the distribution of a particular file is 
called a torrent. Peers in a torrent download equal-size chunks of the file from one 
another, with a typical chunk size of 256 KBytes. When a peer first joins a torrent, it 
has no chunks. Over time it accumulates more and more chunks. While it downloads 
chunks it also uploads chunks to other peers. Once a peer has acquired the entire 
file, it may (selfishly) leave the torrent, or (altruistically) remain in the torrent and 
continue to upload chunks to other peers. Also, any peer may leave the torrent at any 
time with only a subset of chunks, and later rejoin the torrent.
0
5
10
15
20
25
30
0
N
Minimum distribution time
35
0.5
1.5
2.5
1.0
3.0
2.0
3.5
Client-Server
P2P
Figure 2.23 ‚ô¶ Distribution time for P2P and client-server architectures

Let‚Äôs now take a closer look at how BitTorrent operates. Since BitTorrent is 
a rather complicated protocol and system, we‚Äôll only describe its most important 
mechanisms, sweeping some of the details under the rug; this will allow us to see 
the forest through the trees. Each torrent has an infrastructure node called a tracker. 
When a peer joins a torrent, it registers itself with the tracker and periodically informs 
the tracker that it is still in the torrent. In this manner, the tracker keeps track of the 
peers that are participating in the torrent. A given torrent may have fewer than ten or 
more than a thousand peers participating at any instant of time.
As shown in Figure 2.24, when a new peer, Alice, joins the torrent, the tracker 
randomly selects a subset of peers (for concreteness, say 50) from the set of partici-
pating peers, and sends the IP addresses of these 50 peers to Alice. Possessing this 
list of peers, Alice attempts to establish concurrent TCP connections with all the 
peers on this list. Let‚Äôs call all the peers with which Alice succeeds in establishing a 
TCP connection ‚Äúneighboring peers.‚Äù (In Figure 2.24, Alice is shown to have only 
three neighboring peers. Normally, she would have many more.) As time evolves, 
some of these peers may leave and other peers (outside the initial 50) may attempt to 
establish TCP connections with Alice. So a peer‚Äôs neighboring peers will fluctuate 
over time.
Tracker
Trading chunks
Peer
Obtain
list of
peers
Alice
Figure 2.24 ‚ô¶ File distribution with BitTorrent

At any given time, each peer will have a subset of chunks from the file, with dif-
ferent peers having different subsets. Periodically, Alice will ask each of her neigh-
boring peers (over the TCP connections) for the list of the chunks they have. If Alice 
has L different neighbors, she will obtain L lists of chunks. With this knowledge, 
Alice will issue requests (again over the TCP connections) for chunks she currently 
does not have.
So at any given instant of time, Alice will have a subset of chunks and will know 
which chunks her neighbors have. With this information, Alice will have two impor-
tant decisions to make. First, which chunks should she request first from her neigh-
bors? And second, to which of her neighbors should she send requested chunks? In 
deciding which chunks to request, Alice uses a technique called rarest first. The 
idea is to determine, from among the chunks she does not have, the chunks that are 
the rarest among her neighbors (that is, the chunks that have the fewest repeated cop-
ies among her neighbors) and then request those rarest chunks first. In this manner, 
the rarest chunks get more quickly redistributed, aiming to (roughly) equalize the 
numbers of copies of each chunk in the torrent.
To determine which requests she responds to, BitTorrent uses a clever trading 
algorithm. The basic idea is that Alice gives priority to the neighbors that are cur-
rently supplying her data at the highest rate. Specifically, for each of her neighbors, 
Alice continually measures the rate at which she receives bits and determines the four 
peers that are feeding her bits at the highest rate. She then reciprocates by sending 
chunks to these same four peers. Every 10 seconds, she recalculates the rates and pos-
sibly modifies the set of four peers. In BitTorrent lingo, these four peers are said to be 
unchoked. Importantly, every 30 seconds, she also picks one additional neighbor at 
random and sends it chunks. Let‚Äôs call the randomly chosen peer Bob. In BitTorrent 
lingo, Bob is said to be optimistically unchoked. Because Alice is sending data to 
Bob, she may become one of Bob‚Äôs top four uploaders, in which case Bob would start 
to send data to Alice. If the rate at which Bob sends data to Alice is high enough, Bob 
could then, in turn, become one of Alice‚Äôs top four uploaders. In other words, every 
30 seconds, Alice will randomly choose a new trading partner and initiate trading 
with that partner. If the two peers are satisfied with the trading, they will put each 
other in their top four lists and continue trading with each other until one of the peers 
finds a better partner. The effect is that peers capable of uploading at compatible 
rates tend to find each other. The random neighbor selection also allows new peers 
to get chunks, so that they can have something to trade. All other neighboring peers 
besides these five peers (four ‚Äútop‚Äù peers and one probing peer) are ‚Äúchoked,‚Äù that 
is, they do not receive any chunks from Alice. BitTorrent has a number of interesting 
mechanisms that are not discussed here, including pieces (mini-chunks), pipelining, 
random first selection, endgame mode, and anti-snubbing [Cohen 2003].
The incentive mechanism for trading just described is often referred to as tit-for-
tat [Cohen 2003]. It has been shown that this incentive scheme can be circumvented 
[Liogkas 2006; Locher 2006; Piatek 2008]. Nevertheless, the BitTorrent ecosystem 
is wildly successful, with millions of simultaneous peers actively sharing files in

hundreds of thousands of torrents. If BitTorrent had been designed without tit-for-tat 
(or a variant), but otherwise exactly the same, BitTorrent would likely not even exist 
now, as the majority of the users would have been freeriders [Saroiu 2002].
We close our discussion on P2P by briefly mentioning another application of P2P, 
namely, Distributed Hast Table (DHT). A distributed hash table is a simple database, 
with the database records being distributed over the peers in a P2P system. DHTs have 
been widely implemented (e.g., in BitTorrent) and have been the subject of extensive 
research. An overview is provided in a Video Note in the companion website. 
2.6 Video Streaming and Content Distribution 
Networks
By many estimates, streaming video‚Äîincluding Netflix, YouTube and Amazon 
Prime‚Äîaccount for about 80% of Internet traffic in 2020 [Cisco 2020]. This section 
we will provide an overview of how popular video streaming services are imple-
mented in today‚Äôs Internet. We will see they are implemented using application-level 
protocols and servers that function in some ways like a cache.
2.6.1 Internet Video
In streaming stored video applications, the underlying medium is prerecorded video, 
such as a movie, a television show, a prerecorded sporting event, or a prerecorded 
user-generated video (such as those commonly seen on YouTube). These prere-
corded videos are placed on servers, and users send requests to the servers to view 
the videos on demand. Many Internet companies today provide streaming video, 
including, Netflix, YouTube (Google), Amazon, and TikTok.
But before launching into a discussion of video streaming, we should first get 
a quick feel for the video medium itself. A video is a sequence of images, typi-
cally being displayed at a constant rate, for example, at 24 or 30 images per second. 
An uncompressed, digitally encoded image consists of an array of pixels, with each 
pixel encoded into a number of bits to represent luminance and color. An important 
characteristic of video is that it can be compressed, thereby trading off video quality 
with bit rate. Today‚Äôs off-the-shelf compression algorithms can compress a video to 
essentially any bit rate desired. Of course, the higher the bit rate, the better the image 
quality and the better the overall user viewing experience.
From a networking perspective, perhaps the most salient characteristic of video 
is its high bit rate. Compressed Internet video typically ranges from 100 kbps for 
low-quality video to over 4 Mbps for streaming high-definition movies; 4K stream-
ing envisions a bitrate of more than 10 Mbps. This can translate to huge amount of 
traffic and storage, particularly for high-end video. For example, a single 2 Mbps 
Walking though 
distributed hash tables
VideoNote

video with a duration of 67 minutes will consume 1 gigabyte of storage and traffic. 
By far, the most important performance measure for streaming video is average end-
to-end throughput. In order to provide continuous playout, the network must provide 
an average throughput to the streaming application that is at least as large as the bit 
rate of the compressed video.
We can also use compression to create multiple versions of the same video, each 
at a different quality level. For example, we can use compression to create, say, three 
versions of the same video, at rates of 300 kbps, 1 Mbps, and 3 Mbps. Users can then 
decide which version they want to watch as a function of their current available band-
width. Users with high-speed Internet connections might choose the 3 Mbps version; 
users watching the video over 3G with a smartphone might choose the 300 kbps version.
2.6.2 HTTP Streaming and DASH
In HTTP streaming, the video is simply stored at an HTTP server as an ordinary 
file with a specific URL. When a user wants to see the video, the client establishes 
a TCP connection with the server and issues an HTTP GET request for that URL. 
The server then sends the video file, within an HTTP response message, as quickly 
as the underlying network protocols and traffic conditions will allow. On the client 
side, the bytes are collected in a client application buffer. Once the number of bytes 
in this buffer exceeds a predetermined threshold, the client application begins play-
back‚Äîspecifically, the streaming video application periodically grabs video frames 
from the client application buffer, decompresses the frames, and displays them on 
the user‚Äôs screen. Thus, the video streaming application is displaying video as it is 
receiving and buffering frames corresponding to latter parts of the video.
Although HTTP streaming, as described in the previous paragraph, has been 
extensively deployed in practice (for example, by YouTube since its inception), it has 
a major shortcoming: All clients receive the same encoding of the video, despite the 
large variations in the amount of bandwidth available to a client, both across different 
clients and also over time for the same client. This has led to the development of a new 
type of HTTP-based streaming, often referred to as Dynamic Adaptive Streaming 
over HTTP (DASH). In DASH, the video is encoded into several different versions, 
with each version having a different bit rate and, correspondingly, a different quality 
level. The client dynamically requests chunks of video segments of a few seconds in 
length. When the amount of available bandwidth is high, the client naturally selects 
chunks from a high-rate version; and when the available bandwidth is low, it naturally 
selects from a low-rate version. The client selects different chunks one at a time with 
HTTP GET request messages [Akhshabi 2011].
DASH allows clients with different Internet access rates to stream in video at 
different encoding rates. Clients with low-speed 3G connections can receive a low 
bit-rate (and low-quality) version, and clients with fiber connections can receive a 
high-quality version. DASH also allows a client to adapt to the available bandwidth 
if the available end-to-end bandwidth changes during the session. This feature is

particularly important for mobile users, who typically see their bandwidth availabil-
ity fluctuate as they move with respect to the base stations.
With DASH, each video version is stored in the HTTP server, each with a differ-
ent URL. The HTTP server also has a manifest file, which provides a URL for each 
version along with its bit rate. The client first requests the manifest file and learns 
about the various versions. The client then selects one chunk at a time by specifying a 
URL and a byte range in an HTTP GET request message for each chunk. While down-
loading chunks, the client also measures the received bandwidth and runs a rate deter-
mination algorithm to select the chunk to request next. Naturally, if the client has a lot 
of video buffered and if the measured receive bandwidth is high, it will choose a chunk 
from a high-bitrate version. And naturally if the client has little video buffered and the 
measured received bandwidth is low, it will choose a chunk from a low-bitrate version. 
DASH therefore allows the client to freely switch among different quality levels.
2.6.3 Content Distribution Networks
Today, many Internet video companies are distributing on-demand multi-Mbps 
streams to millions of users on a daily basis. YouTube, for example, with a library 
of hundreds of millions of videos, distributes hundreds of millions of video streams 
to users around the world every day. Streaming all this traffic to locations all over 
the world while providing continuous playout and high interactivity is clearly a chal-
lenging task.
For an Internet video company, perhaps the most straightforward approach to 
providing streaming video service is to build a single massive data center, store all 
of its videos in the data center, and stream the videos directly from the data center 
to clients worldwide. But there are three major problems with this approach. First, if 
the client is far from the data center, server-to-client packets will cross many com-
munication links and likely pass through many ISPs, with some of the ISPs possibly 
located on different continents. If one of these links provides a throughput that is less 
than the video consumption rate, the end-to-end throughput will also be below the 
consumption rate, resulting in annoying freezing delays for the user. (Recall from 
Chapter 1 that the end-to-end throughput of a stream is governed by the throughput 
at the bottleneck link.) The likelihood of this happening increases as the number of 
links in the end-to-end path increases. A second drawback is that a popular video will 
likely be sent many times over the same communication links. Not only does this 
waste network bandwidth, but the Internet video company itself will be paying its 
provider ISP (connected to the data center) for sending the same bytes into the Inter-
net over and over again. A third problem with this solution is that a single data center 
represents a single point of failure‚Äîif the data center or its links to the Internet goes 
down, it would not be able to distribute any video streams.
In order to meet the challenge of distributing massive amounts of video data 
to users distributed around the world, almost all major video-streaming companies 
make use of Content Distribution Networks (CDNs). A CDN manages servers in

multiple geographically distributed locations, stores copies of the videos (and other 
types of Web content, including documents, images, and audio) in its servers, and 
attempts to direct each user request to a CDN location that will provide the best user 
experience. The CDN may be a private CDN, that is, owned by the content provider 
itself; for example, Google‚Äôs CDN distributes YouTube videos and other types of 
content. The CDN may alternatively be a third-party CDN that distributes content 
on behalf of multiple content providers; Akamai, Limelight and Level-3 all operate 
third-party CDNs. A very readable overview of modern CDNs is [Leighton 2009; 
Nygren 2010].
CDNs typically adopt one of two different server placement philosophies 
[Huang 2008]:
‚Ä¢ Enter Deep. One philosophy, pioneered by Akamai, is to enter deep into the 
access networks of Internet Service Providers, by deploying server clusters in 
access ISPs all over the world. (Access networks are described in Section 1.3.) 
Akamai takes this approach with clusters in thousands of locations. The goal is 
to get close to end users, thereby improving user-perceived delay and throughput 
by decreasing the number of links and routers between the end user and the CDN 
server from which it receives content. Because of this highly distributed design, 
the task of maintaining and managing the clusters becomes challenging.
‚Ä¢ Bring Home. A second design philosophy, taken by Limelight and many 
other CDN companies, is to bring the ISPs home by building large clusters 
at a smaller number (for example, tens) of sites. Instead of getting inside the 
access ISPs, these CDNs typically place their clusters in Internet Exchange 
Points (IXPs) (see Section 1.3). Compared with the enter-deep design phi-
losophy, the bring-home design typically results in lower maintenance and 
management overhead, possibly at the expense of higher delay and lower 
throughput to end users.
Once its clusters are in place, the CDN replicates content across its clusters. The 
CDN may not want to place a copy of every video in each cluster, since some videos 
are rarely viewed or are only popular in some countries. In fact, many CDNs do not 
push videos to their clusters but instead use a simple pull strategy: If a client requests 
a video from a cluster that is not storing the video, then the cluster retrieves the 
video (from a central repository or from another cluster) and stores a copy locally 
while streaming the video to the client at the same time. Similar Web caching (see 
Section 2.2.5), when a cluster‚Äôs storage becomes full, it removes videos that are not 
frequently requested.
CDN Operation
Having identified the two major approaches toward deploying a CDN, let‚Äôs now dive 
down into the nuts and bolts of how a CDN operates. When a browser in a user‚Äôs

host is instructed to retrieve a specific video (identified by a URL), the CDN must 
intercept the request so that it can (1) determine a suitable CDN server cluster for that 
client at that time, and (2) redirect the client‚Äôs request to a server in that cluster. We‚Äôll 
shortly discuss how a CDN can determine a suitable cluster. But first let‚Äôs examine 
the mechanics behind intercepting and redirecting a request.
Most CDNs take advantage of DNS to intercept and redirect requests; an inter-
esting discussion of such a use of the DNS is [Vixie 2009]. Let‚Äôs consider a simple 
GOOGLE‚ÄôS NETWORK INFRASTRUCTURE
To support its vast array of services‚Äîincluding search, Gmail, calendar, YouTube 
video, maps, documents, and social networks‚ÄîGoogle has deployed an extensive 
private network and CDN infrastructure. Google‚Äôs CDN infrastructure has three tiers 
of server clusters:
‚Ä¢ 
 Nineteen ‚Äúmega data centers‚Äù in North America, Europe, and Asia [Google 
Locations 2020], with each data center having on the order of 100,000 servers. 
These mega data centers are responsible for serving dynamic (and often personal-
ized) content, including search results and Gmail messages.
‚Ä¢ 
 With about 90 clusters in IXPs scattered throughout the world, with each cluster 
consisting of hundreds of servers servers [Adhikari 2011a] [Google CDN 2020]. 
These clusters are responsible for serving static content, including YouTube videos.
‚Ä¢ 
 Many hundreds of ‚Äúenter-deep‚Äù clusters located within an access ISP. Here a cluster 
typically consists of tens of servers within a single rack. These enter-deep  servers 
perform TCP splitting (see Section 3.7) and serve static content [Chen 2011], 
including the static portions of Web pages that embody search results.
All of these data centers and cluster locations are networked together with 
Google‚Äôs own private network. When a user makes a search query, often the query 
is first sent over the local ISP to a nearby enter-deep cache, from where the static 
content is retrieved; while providing the static content to the client, the nearby cache 
also forwards the query over Google‚Äôs private network to one of the mega data cent-
ers, from where the personalized search results are retrieved. For a YouTube video, 
the video itself may come from one of the bring-home caches, whereas portions of 
the Web page surrounding the video may come from the nearby enter-deep cache, 
and the advertisements surrounding the video come from the data centers. In sum-
mary, except for the local ISPs, the Google cloud services are largely provided by a 
network infrastructure that is independent of the public Internet.
CASE STUDY

example to illustrate how the DNS is typically involved. Suppose a content provider, 
NetCinema, employs the third-party CDN company, KingCDN, to distribute its vid-
eos to its customers. On the NetCinema Web pages, each of its videos is assigned a 
URL that includes the string ‚Äúvideo‚Äù and a unique identifier for the video itself; for 
example, Transformers 7 might be assigned http://video.netcinema.com/6Y7B23V. 
Six steps then occur, as shown in Figure 2.25:
 1. The user visits the Web page at NetCinema.
 2. When the user clicks on the link http://video.netcinema.com/6Y7B23V, the 
user‚Äôs host sends a DNS query for video.netcinema.com.
 3. The user‚Äôs Local DNS Server (LDNS) relays the DNS query to an authoritative 
DNS server for NetCinema, which observes the string ‚Äúvideo‚Äù in the host-
name video.netcinema.com. To ‚Äúhand over‚Äù the DNS query to KingCDN, 
instead of returning an IP address, the NetCinema authoritative DNS server 
returns to the LDNS a hostname in the KingCDN‚Äôs domain, for example, 
a1105.kingcdn.com.
 4. From this point on, the DNS query enters into KingCDN‚Äôs private DNS infra-
structure. The user‚Äôs LDNS then sends a second query, now for a1105.kingcdn.
com, and KingCDN‚Äôs DNS system eventually returns the IP addresses of a 
KingCDN content server to the LDNS. It is thus here, within the KingCDN‚Äôs 
DNS system, that the CDN server from which the client will receive its content 
is specified.
Local
DNS server
NetCinema authoritative
 DNS server
www.NetCinema.com
KingCDN authoritative
server
KingCDN content
distribution server
2
5
6
3
1
4
Figure 2.25 ‚ô¶ DNS redirects a user‚Äôs request to a CDN server

5. The LDNS forwards the IP address of the content-serving CDN node to the 
user‚Äôs host.
 6. Once the client receives the IP address for a KingCDN content server, it estab-
lishes a direct TCP connection with the server at that IP address and issues an 
HTTP GET request for the video. If DASH is used, the server will first send to 
the client a manifest file with a list of URLs, one for each version of the video, 
and the client will dynamically select chunks from the different versions.
Cluster Selection Strategies
At the core of any CDN deployment is a cluster selection strategy, that is, a mecha-
nism for dynamically directing clients to a server cluster or a data center within the 
CDN. As we just saw, the CDN learns the IP address of the client‚Äôs LDNS server 
via the client‚Äôs DNS lookup. After learning this IP address, the CDN needs to select 
an appropriate cluster based on this IP address. CDNs generally employ proprietary 
cluster selection strategies. We now briefly survey a few approaches, each of which 
has its own advantages and disadvantages.
One simple strategy is to assign the client to the cluster that is geographically clos-
est. Using commercial geo-location databases (such as Quova [Quova 2020] and Max-
Mind [MaxMind 2020]), each LDNS IP address is mapped to a geographic location. 
When a DNS request is received from a particular LDNS, the CDN chooses the geo-
graphically closest cluster, that is, the cluster that is the fewest kilometers from the LDNS 
‚Äúas the bird flies.‚Äù Such a solution can work reasonably well for a large fraction of the cli-
ents [Agarwal 2009]. However, for some clients, the solution may perform poorly, since 
the geographically closest cluster may not be the closest cluster in terms of the length 
or number of hops of the network path. Furthermore, a problem inherent with all DNS-
based approaches is that some end-users are configured to use remotely located LDNSs 
[Shaikh 2001; Mao 2002], in which case the LDNS location may be far from the client‚Äôs 
location. Moreover, this simple strategy ignores the variation in delay and available band-
width over time of Internet paths, always assigning the same cluster to a particular client.
In order to determine the best cluster for a client based on the current traffic 
conditions, CDNs can instead perform periodic real-time measurements of delay 
and loss performance between their clusters and clients. For instance, a CDN can 
have each of its clusters periodically send probes (for example, ping messages or 
DNS queries) to all of the LDNSs around the world. One drawback of this approach 
is that many LDNSs are configured to not respond to such probes.
2.6.4 Case Studies: Netflix and YouTube
We conclude our discussion of streaming stored video by taking a look at two highly 
successful large-scale deployments: Netflix and YouTube. We‚Äôll see that each of 
these systems take a very different approach, yet employ many of the underlying 
principles discussed in this section.

Netflix
As of 2020, Netflix is the leading service provider for online movies and TV series in 
North America. As we discuss below, Netflix video distribution has two major compo-
nents: the Amazon cloud and its own private CDN infrastructure.
Netflix has a Web site that handles numerous functions, including user registra-
tion and login, billing, movie catalogue for browsing and searching, and a movie 
recommendation system. As shown in Figure 2.26, this Web site (and its associated 
backend databases) run entirely on Amazon servers in the Amazon cloud. Addition-
ally, the Amazon cloud handles the following critical functions:
‚Ä¢ Content ingestion. Before Netflix can distribute a movie to its customers, it must 
first ingest and process the movie. Netflix receives studio master versions of 
movies and uploads them to hosts in the Amazon cloud.
‚Ä¢ Content processing. The machines in the Amazon cloud create many different 
formats for each movie, suitable for a diverse array of client video players run-
ning on desktop computers, smartphones, and game consoles connected to televi-
sions. A different version is created for each of these formats and at multiple bit 
rates, allowing for adaptive streaming over HTTP using DASH.
‚Ä¢ Uploading versions to its CDN. Once all of the versions of a movie have been 
created, the hosts in the Amazon cloud upload the versions to its CDN.
Amazon Cloud
CDN server
CDN server
Upload
versions
to CDNs
CDN server
Client
Manifest 
Ô¨Åle
Video
chunks
(DASH)
Figure 2.26 ‚ô¶ Netflix video streaming platform

When Netflix first rolled out its video streaming service in 2007, it employed 
three third-party CDN companies to distribute its video content. Netflix has since 
created its own private CDN, from which it now streams all of its videos. To create 
its own CDN, Netflix has installed server racks both in IXPs and within residen-
tial ISPs themselves. Netflix currently has server racks in over 200 IXP locations; 
see [Bottger 2018] [Netflix Open Connect 2020] for a current list of IXPs housing 
Netflix racks. There are also hundreds of ISP locations housing Netflix racks; also 
see [Netflix Open Connect 2020], where Netflix provides to potential ISP partners 
instructions about installing a (free) Netflix rack for their networks. Each server in 
the rack has several 10 Gbps Ethernet ports and over 100 terabytes of storage. The 
number of servers in a rack varies: IXP installations often have tens of servers and 
contain the entire Netflix streaming video library, including multiple versions of the 
videos to support DASH. Netflix does not use pull-caching (Section 2.2.5) to popu-
late its CDN servers in the IXPs and ISPs. Instead, Netflix distributes by pushing the 
videos to its CDN servers during off-peak hours. For those locations that cannot hold 
the entire library, Netflix pushes only the most popular videos, which are determined 
on a day-to-day basis. The Netflix CDN design is described in some detail in the 
YouTube videos [Netflix Video 1] and [Netflix Video 2]; see also [Bottger 2018]. 
Having described the components of the Netflix architecture, let‚Äôs take a closer 
look at the interaction between the client and the various servers that are involved in 
movie delivery. As indicated earlier, the Web pages for browsing the Netflix video 
library are served from servers in the Amazon cloud. When a user selects a movie to 
play, the Netflix software, running in the Amazon cloud, first determines which of 
its CDN servers have copies of the movie. Among the servers that have the movie, 
the software then determines the ‚Äúbest‚Äù server for that client request. If the client is 
using a residential ISP that has a Netflix CDN server rack installed in that ISP, and 
this rack has a copy of the requested movie, then a server in this rack is typically 
selected. If not, a server at a nearby IXP is typically selected.
Once Netflix determines the CDN server that is to deliver the content, it sends 
the client the IP address of the specific server as well as a manifest file, which has 
the URLs for the different versions of the requested movie. The client and that CDN 
server then directly interact using a proprietary version of DASH. Specifically, 
as described in Section 2.6.2, the client uses the byte-range header in HTTP GET 
request messages, to request chunks from the different versions of the movie. Netflix 
uses chunks that are approximately four-seconds long [Adhikari 2012]. While the 
chunks are being downloaded, the client measures the received throughput and runs 
a rate-determination algorithm to determine the quality of the next chunk to request.
Netflix embodies many of the key principles discussed earlier in this section, 
including adaptive streaming and CDN distribution. However, because Netflix uses 
its own private CDN, which distributes only video (and not Web pages), Netflix has 
been able to simplify and tailor its CDN design. In particular, Netflix does not need to 
employ DNS redirect, as discussed in Section 2.6.3, to connect a particular client to a 
CDN server; instead, the Netflix software (running in the Amazon cloud) directly tells

the client to use a particular CDN server. Furthermore, the Netflix CDN uses push 
caching rather than pull caching (Section 2.2.5): content is pushed into the servers at 
scheduled times at off-peak hours, rather than dynamically during cache misses.
YouTube
With hundreds of hours of video uploaded to YouTube every minute and several 
billion video views per day, YouTube is indisputably the world‚Äôs largest video-
sharing site. YouTube began its service in April 2005 and was acquired by Google 
in November 2006. Although the Google/YouTube design and protocols are pro-
prietary, through several independent measurement efforts we can gain a basic 
understanding about how YouTube operates [Zink 2009; Torres 2011; Adhikari 
2011a]. As with Netflix, YouTube makes extensive use of CDN technology to dis-
tribute its videos [Torres 2011]. Similar to Netflix, Google uses its own private CDN 
to distribute YouTube videos, and has installed server clusters in many hundreds 
of different IXP and ISP locations. From these locations and directly from its huge 
data centers, Google distributes YouTube videos [Adhikari 2011a]. Unlike Netflix, 
however, Google uses pull caching, as described in Section 2.2.5, and DNS redirect, 
as described in Section 2.6.3. Most of the time, Google‚Äôs cluster-selection strategy 
directs the client to the cluster for which the RTT between client and cluster is the 
lowest; however, in order to balance the load across clusters, sometimes the client is 
directed (via DNS) to a more distant cluster [Torres 2011].
YouTube employs HTTP streaming, often making a small number of differ-
ent versions available for a video, each with a different bit rate and corresponding 
quality level. YouTube does not employ adaptive streaming (such as DASH), but 
instead requires the user to manually select a version. In order to save bandwidth 
and server resources that would be wasted by repositioning or early termination, 
YouTube uses the HTTP byte range request to limit the flow of transmitted data after 
a target amount of video is prefetched.
Several million videos are uploaded to YouTube every day. Not only are You-
Tube videos streamed from server to client over HTTP, but YouTube uploaders also 
upload their videos from client to server over HTTP. YouTube processes each video 
it receives, converting it to a YouTube video format and creating multiple versions 
at different bit rates. This processing takes place entirely within Google data centers. 
(See the case study on Google‚Äôs network infrastructure in Section 2.6.3.)
2.7 Socket Programming: Creating Network 
Applications
Now that we‚Äôve looked at a number of important network applications, let‚Äôs explore 
how network application programs are actually created. Recall from Section 2.1 that 
a typical network application consists of a pair of programs‚Äîa client program and

a server program‚Äîresiding in two different end systems. When these two programs 
are executed, a client process and a server process are created, and these processes 
communicate with each other by reading from, and writing to, sockets. When creat-
ing a network application, the developer‚Äôs main task is therefore to write the code for 
both the client and server programs.
There are two types of network applications. One type is an implementation 
whose operation is specified in a protocol standard, such as an RFC or some other 
standards document; such an application is sometimes referred to as ‚Äúopen,‚Äù since 
the rules specifying its operation are known to all. For such an implementation, the 
client and server programs must conform to the rules dictated by the RFC. For exam-
ple, the client program could be an implementation of the client side of the HTTP 
protocol, described in Section 2.2 and precisely defined in RFC 2616; similarly, 
the server program could be an implementation of the HTTP server protocol, also 
precisely defined in RFC 2616. If one developer writes code for the client program 
and another developer writes code for the server program, and both developers care-
fully follow the rules of the RFC, then the two programs will be able to interoper-
ate. Indeed, many of today‚Äôs network applications involve communication between 
client and server programs that have been created by independent developers‚Äîfor 
example, a Google Chrome browser communicating with an Apache Web server, or 
a BitTorrent client communicating with BitTorrent tracker.
The other type of network application is a proprietary network application. In 
this case, the client and server programs employ an application-layer protocol that has 
not been openly published in an RFC or elsewhere. A single developer (or develop-
ment team) creates both the client and server programs, and the developer has com-
plete control over what goes in the code. But because the code does not implement 
an open protocol, other independent developers will not be able to develop code that 
interoperates with the application.
In this section, we‚Äôll examine the key issues in developing a client-server appli-
cation, and we‚Äôll ‚Äúget our hands dirty‚Äù by looking at code that implements a very sim-
ple client-server application. During the development phase, one of the first decisions 
the developer must make is whether the application is to run over TCP or over UDP. 
Recall that TCP is connection oriented and provides a reliable byte-stream channel 
through which data flows between two end systems. UDP is connectionless and sends 
independent packets of data from one end system to the other, without any guarantees 
about delivery. Recall also that when a client or server program implements a proto-
col defined by an RFC, it should use the well-known port number associated with the 
protocol; conversely, when developing a proprietary application, the developer must 
be careful to avoid using such well-known port numbers. (Port numbers were briefly 
discussed in Section 2.1. They are covered in more detail in Chapter 3.)
We introduce UDP and TCP socket programming by way of a simple UDP 
application and a simple TCP application. We present the simple UDP and TCP 
applications in Python 3. We could have written the code in Java, C, or C++, but we 
chose Python mostly because Python clearly exposes the key socket concepts. With

Python there are fewer lines of code, and each line can be explained to the novice 
programmer without difficulty. But there‚Äôs no need to be frightened if you are not 
familiar with Python. You should be able to easily follow the code if you have expe-
rience programming in Java, C, or C++.
If you are interested in client-server programming with Java, you are encour-
aged to see the Companion Website for this textbook; in fact, you can find there 
all the examples in this section (and associated labs) in Java. For readers who are 
interested in client-server programming in C, there are several good references avail-
able [Donahoo 2001; Stevens 1997; Frost 1994]; our Python examples below have a 
similar look and feel to C.
2.7.1 Socket Programming with UDP
In this subsection, we‚Äôll write simple client-server programs that use UDP; in the 
following section, we‚Äôll write similar programs that use TCP.
Recall from Section 2.1 that processes running on different machines communi-
cate with each other by sending messages into sockets. We said that each process is 
analogous to a house and the process‚Äôs socket is analogous to a door. The application 
resides on one side of the door in the house; the transport-layer protocol resides on 
the other side of the door in the outside world. The application developer has control 
of everything on the application-layer side of the socket; however, it has little control 
of the transport-layer side.
Now let‚Äôs take a closer look at the interaction between two communicating pro-
cesses that use UDP sockets. Before the sending process can push a packet of data 
out the socket door, when using UDP, it must first attach a destination address to 
the packet. After the packet passes through the sender‚Äôs socket, the Internet will use 
this destination address to route the packet through the Internet to the socket in the 
receiving process. When the packet arrives at the receiving socket, the receiving 
process will retrieve the packet through the socket, and then inspect the packet‚Äôs 
contents and take appropriate action.
So you may be now wondering, what goes into the destination address that 
is attached to the packet? As you might expect, the destination host‚Äôs IP address 
is part of the destination address. By including the destination IP address in the 
packet, the routers in the Internet will be able to route the packet through the 
Internet to the destination host. But because a host may be running many net-
work application processes, each with one or more sockets, it is also necessary 
to identify the particular socket in the destination host. When a socket is created, 
an identifier, called a port number, is assigned to it. So, as you might expect, 
the packet‚Äôs destination address also includes the socket‚Äôs port number. In sum-
mary, the sending process attaches to the packet a destination address, which con-
sists of the destination host‚Äôs IP address and the destination socket‚Äôs port number.  
Moreover, as we shall soon see, the sender‚Äôs source address‚Äîconsisting of the

IP address of the source host and the port number of the source socket‚Äîare also 
attached to the packet. However, attaching the source address to the packet is typi-
cally not done by the UDP application code; instead it is automatically done by the 
underlying operating system.
We‚Äôll use the following simple client-server application to demonstrate socket 
programming for both UDP and TCP:
 1. The client reads a line of characters (data) from its keyboard and sends the data 
to the server.
 2. The server receives the data and converts the characters to uppercase.
 3. The server sends the modified data to the client.
 4. The client receives the modified data and displays the line on its screen.
Figure 2.27 highlights the main socket-related activity of the client and server that 
communicate over the UDP transport service.
Create  socket, port=x:
Server
serverSocket =
socket(AF_INET,SOCK_DGRAM)
(Running on serverIP)
Client
Read UDP segment from
serverSocket
Write reply to
specifying client address,
port number
serverSocket
Create datagram with serverIP
and port=x;
send datagram via
clientSocket
Create socket:
clientSocket =
socket(AF_INET,SOCK_DGRAM)
Read datagram from
clientSocket
Close
clientSocket
Figure 2.27 ‚ô¶ The client-server application using UDP

Now let‚Äôs get our hands dirty and take a look at the client-server program pair 
for a UDP implementation of this simple application. We also provide a detailed, 
line-by-line analysis after each program. We‚Äôll begin with the UDP client, which will 
send a simple application-level message to the server. In order for the server to be 
able to receive and reply to the client‚Äôs message, it must be ready and running‚Äîthat 
is, it must be running as a process before the client sends its message.
The client program is called UDPClient.py, and the server program is called 
UDPServer.py. In order to emphasize the key issues, we intentionally provide code 
that is minimal. ‚ÄúGood code‚Äù would certainly have a few more auxiliary lines, in 
particular for handling error cases. For this application, we have arbitrarily chosen 
12000 for the server port number.
UDPClient.py
Here is the code for the client side of the application:
from socket import *
serverName = ‚Äôhostname‚Äô
serverPort = 12000
clientSocket = socket(AF_INET, SOCK_DGRAM)
message = input(‚ÄôInput lowercase sentence:‚Äô)
clientSocket.sendto(message.encode(),(serverName, serverPort))
modifiedMessage, serverAddress = clientSocket.recvfrom(2048)
print(modifiedMessage.decode())
clientSocket.close()
Now let‚Äôs take a look at the various lines of code in UDPClient.py.
from socket import *
The socket module forms the basis of all network communications in Python. By 
including this line, we will be able to create sockets within our program.
serverName = ‚Äôhostname‚Äô
serverPort = 12000
The first line sets the variable serverName to the string ‚Äòhostname‚Äô. Here, we pro-
vide a string containing either the IP address of the server (e.g., ‚Äú128.138.32.126‚Äù) 
or the hostname of the server (e.g., ‚Äúcis.poly.edu‚Äù). If we use the hostname, then a 
DNS lookup will automatically be performed to get the IP address.) The second line 
sets the integer variable serverPort to 12000.
clientSocket = socket(AF_INET, SOCK_DGRAM)

This line creates the client‚Äôs socket, called clientSocket. The first param-
eter indicates the address family; in particular, AF_INET indicates that the 
underlying network is using IPv4. (Do not worry about this now‚Äîwe will dis-
cuss IPv4 in Chapter 4.) The second parameter indicates that the socket is of 
type SOCK_DGRAM, which means it is a UDP socket (rather than a TCP socket). 
Note that we are not specifying the port number of the client socket when we 
create it; we are instead letting the operating system do this for us. Now that the 
client process‚Äôs door has been created, we will want to create a message to send 
through the door.
message = input(‚ÄôInput lowercase sentence:‚Äô)
input() is a built-in function in Python. When this command is executed, the user 
at the client is prompted with the words ‚ÄúInput lowercase sentence:‚Äù The user then 
uses her keyboard to input a line, which is put into the variable message. Now that 
we have a socket and a message, we will want to send the message through the socket 
to the destination host.
clientSocket.sendto(message.encode(),¬†(serverName,¬†serverPort))
In the above line, we first convert the message from string type to byte type, as we 
need to send bytes into a socket; this is done with the encode() method. The  
method sendto() attaches the destination address (serverName, serverPort)  
to the message and sends the resulting packet into the process‚Äôs socket,  
clientSocket. (As mentioned earlier, the source address is also attached to 
the packet, although this is done automatically rather than explicitly by the code.)  
Sending a client-to-server message via a UDP socket is that simple! After sending 
the packet, the client waits to receive data from the server.
modifiedMessage, serverAddress = clientSocket.recvfrom(2048)
With the above line, when a packet arrives from the Internet at the client‚Äôs socket, the 
packet‚Äôs data is put into the variable modifiedMessage and the packet‚Äôs source 
address is put into the variable serverAddress. The variable serverAddress 
contains both the server‚Äôs IP address and the server‚Äôs port number. The program 
UDPClient doesn‚Äôt actually need this server address information, since it already 
knows the server address from the outset; but this line of Python provides the server 
address nevertheless. The method recvfrom also takes the buffer size 2048 as 
input. (This buffer size works for most purposes.)
print(modifiedMessage.decode())

This line prints out modifiedMessage on the user‚Äôs display, after converting the mes-
sage from bytes to string. It should be the original line that the user typed, but now 
capitalized.
clientSocket.close()
This line closes the socket. The process then terminates.
UDPServer.py
Let‚Äôs now take a look at the server side of the application:
from socket import *
serverPort = 12000
serverSocket = socket(AF_INET, SOCK_DGRAM)
serverSocket.bind((‚Äô‚Äô, serverPort))
print(‚ÄùThe server is ready to receive‚Äù)
while True:
    message, clientAddress = serverSocket.recvfrom(2048)
    modifiedMessage = message.decode().upper()
    serverSocket.sendto(modifiedMessage.encode(), 
clientAddress)
Note that the beginning of UDPServer is similar to UDPClient. It also imports the 
socket module, also sets the integer variable serverPort to 12000, and also  
creates a socket of type SOCK_DGRAM (a UDP socket). The first line of code that is 
significantly different from UDPClient is:
serverSocket.bind((‚Äô‚Äô, serverPort))
The above line binds (that is, assigns) the port number 12000 to the server‚Äôs socket. 
Thus, in UDPServer, the code (written by the application developer) is explicitly 
assigning a port number to the socket. In this manner, when anyone sends a packet to 
port 12000 at the IP address of the server, that packet will be directed to this socket. 
UDPServer then enters a while loop; the while loop will allow UDPServer to receive 
and process packets from clients indefinitely. In the while loop, UDPServer waits for 
a packet to arrive.
message, clientAddress = serverSocket.recvfrom(2048)
This line of code is similar to what we saw in UDPClient. When a packet arrives 
at the server‚Äôs socket, the packet‚Äôs data is put into the variable message and the

packet‚Äôs source address is put into the variable clientAddress. The variable 
 clientAddress contains both the client‚Äôs IP address and the client‚Äôs port number. 
Here, UDPServer will make use of this address information, as it provides a return 
address, similar to the return address with ordinary postal mail. With this source 
address information, the server now knows to where it should direct its reply.
modifiedMessage = message.decode().upper()
This line is the heart of our simple application. It takes the line sent by the client and, 
after converting the message to a string, uses the method upper() to capitalize it.
serverSocket.sendto(modifiedMessage.encode(), clientAddress)
This last line attaches the client‚Äôs address (IP address and port number) to the capital-
ized message (after converting the string to bytes), and sends the resulting packet into 
the server‚Äôs socket. (As mentioned earlier, the server address is also attached to the  
packet, although this is done automatically rather than explicitly by the code.) The 
Internet will then deliver the packet to this client address. After the server sends  
the packet, it remains in the while loop, waiting for another UDP packet to arrive 
(from any client running on any host).
To test the pair of programs, you run UDPClient.py on one host and UDPS-
erver.py on another host. Be sure to include the proper hostname or IP address of 
the server in UDPClient.py. Next, you execute UDPServer.py, the compiled server 
program, in the server host. This creates a process in the server that idles until it 
is contacted by some client. Then you execute UDPClient.py, the compiled client 
program, in the client. This creates a process in the client. Finally, to use the appli-
cation at the client, you type a sentence followed by a carriage return.
To develop your own UDP client-server application, you can begin by 
slightly modifying the client or server programs. For example, instead of convert-
ing all the letters to uppercase, the server could count the number of times the 
letter s appears and return this number. Or you can modify the client so that after 
receiving a capitalized sentence, the user can continue to send more sentences to 
the server.
2.7.2 Socket Programming with TCP
Unlike UDP, TCP is a connection-oriented protocol. This means that before the cli-
ent and server can start to send data to each other, they first need to handshake and 
establish a TCP connection. One end of the TCP connection is attached to the client 
socket and the other end is attached to a server socket. When creating the TCP con-
nection, we associate with it the client socket address (IP address and port number) 
and the server socket address (IP address and port number). With the TCP connec-
tion established, when one side wants to send data to the other side, it just drops the

data into the TCP connection via its socket. This is different from UDP, for which 
the server must attach a destination address to the packet before dropping it into the 
socket.
Now let‚Äôs take a closer look at the interaction of client and server programs 
in TCP. The client has the job of initiating contact with the server. In order for the 
server to be able to react to the client‚Äôs initial contact, the server has to be ready. This 
implies two things. First, as in the case of UDP, the TCP server must be running as 
a process before the client attempts to initiate contact. Second, the server program 
must have a special door‚Äîmore precisely, a special socket‚Äîthat welcomes some 
initial contact from a client process running on an arbitrary host. Using our house/
door analogy for a process/socket, we will sometimes refer to the client‚Äôs initial con-
tact as ‚Äúknocking on the welcoming door.‚Äù
With the server process running, the client process can initiate a TCP connection 
to the server. This is done in the client program by creating a TCP socket. When the 
client creates its TCP socket, it specifies the address of the welcoming socket in the 
server, namely, the IP address of the server host and the port number of the socket. 
After creating its socket, the client initiates a three-way handshake and establishes a 
TCP connection with the server. The three-way handshake, which takes place within 
the transport layer, is completely invisible to the client and server programs.
During the three-way handshake, the client process knocks on the welcom-
ing door of the server process. When the server ‚Äúhears‚Äù the knocking, it creates a 
new door‚Äîmore precisely, a new socket that is dedicated to that particular  client. 
In our example below, the welcoming door is a TCP socket object that we call 
 serverSocket; the newly created socket dedicated to the client making the con-
nection is called connectionSocket. Students who are encountering TCP sock-
ets for the first time sometimes confuse the welcoming socket (which is the initial 
point of contact for all clients wanting to communicate with the server), and each 
newly created server-side connection socket that is subsequently created for com-
municating with each client.
From the application‚Äôs perspective, the client‚Äôs socket and the server‚Äôs con-
nection socket are directly connected by a pipe. As shown in Figure 2.28, the cli-
ent process can send arbitrary bytes into its socket, and TCP guarantees that the 
server process will receive (through the connection socket) each byte in the order 
sent. TCP thus provides a reliable service between the client and server processes. 
Furthermore, just as people can go in and out the same door, the client process 
not only sends bytes into but also receives bytes from its socket; similarly, the 
server process not only receives bytes from but also sends bytes into its connec-
tion socket.
We use the same simple client-server application to demonstrate socket pro-
gramming with TCP: The client sends one line of data to the server, the server 
capitalizes the line and sends it back to the client. Figure 2.29 highlights the main 
socket-related activity of the client and server that communicate over the TCP trans-
port service.

TCPClient.py
Here is the code for the client side of the application:
from socket import *
serverName = ‚Äôservername‚Äô
serverPort = 12000
clientSocket = socket(AF_INET, SOCK_STREAM)
clientSocket.connect((serverName,serverPort))
sentence = input(‚ÄôInput lowercase sentence:‚Äô)
clientSocket.send(sentence.encode())
modifiedSentence = clientSocket.recv(1024)
print(‚ÄôFrom Server: ‚Äô, modifiedSentence.decode())¬†
clientSocket.close()
Let‚Äôs now take a look at the various lines in the code that differ significantly from the 
UDP implementation. The first such line is the creation of the client socket.
clientSocket = socket(AF_INET, SOCK_STREAM)
This line creates the client‚Äôs socket, called clientSocket. The first parameter 
again indicates that the underlying network is using IPv4. The second parameter 
Client process
Server process
Client
socket
Welcoming
socket
Three-way handshake
Connection
socket
bytes
bytes
Figure 2.28 ‚ô¶ The TCPServer process has two sockets

indicates that the socket is of type SOCK_STREAM, which means it is a TCP socket 
(rather than a UDP socket). Note that we are again not specifying the port number of 
the client socket when we create it; we are instead letting the operating system do this 
for us. Now the next line of code is very different from what we saw in UDPClient:
clientSocket.connect((serverName,serverPort))
Recall that before the client can send data to the server (or vice versa) using a TCP 
socket, a TCP connection must first be established between the client and server. The 
Close
connectionSocket
Write reply to
connectionSocket
Read request from
connectionSocket
Create  socket, port=x,
for incoming request:
Server
serverSocket =
socket()
Wait for incoming
connection request:
connectionSocket =
serverSocket.accept()
(Running on serverIP)
Client
TCP
connection setup
Create socket, connect
to serverIP, port=x:
clientSocket =
socket()
Read reply from
clientSocket
Send request using
clientSocket
Close
clientSocket
Figure 2.29 ‚ô¶ The client-server application using TCP

above line initiates the TCP connection between the client and server. The parameter 
of the connect() method is the address of the server side of the connection. After 
this line of code is executed, the three-way handshake is performed and a TCP con-
nection is established between the client and server.
sentence = input(‚ÄôInput lowercase sentence:‚Äô)
As with UDPClient, the above obtains a sentence from the user. The string  
sentence continues to gather characters until the user ends the line by typing a 
carriage return. The next line of code is also very different from UDPClient:
clientSocket.send(sentence.encode())
The above line sends the sentence through the client‚Äôs socket and into the TCP 
connection. Note that the program does not explicitly create a packet and attach the 
destination address to the packet, as was the case with UDP sockets. Instead the cli-
ent program simply drops the bytes in the string sentence into the TCP connec-
tion. The client then waits to receive bytes from the server.
modifiedSentence = clientSocket.recv(2048)
When characters arrive from the server, they get placed into the string  
modifiedSentence. Characters continue to accumulate in modifiedSen-
tence until the line ends with a carriage return character. After printing the capital-
ized sentence, we close the client‚Äôs socket:
clientSocket.close()
This last line closes the socket and, hence, closes the TCP connection between the 
client and the server. It causes TCP in the client to send a TCP message to TCP in 
the server (see Section 3.5).
TCPServer.py
Now let‚Äôs take a look at the server program.
from socket import *
serverPort = 12000
serverSocket = socket(AF_INET,SOCK_STREAM)
serverSocket.bind((‚Äô‚Äô,serverPort))
serverSocket.listen(1)
print(‚ÄôThe server is ready to receive‚Äô)

while True:
    connectionSocket, addr = serverSocket.accept()
    sentence = connectionSocket.recv(1024).decode()
    capitalizedSentence = sentence.upper()
    connectionSocket.send(capitalizedSentence.encode())¬†
    connectionSocket.close()
Let‚Äôs now take a look at the lines that differ significantly from UDPServer and TCP-
Client. As with TCPClient, the server creates a TCP socket with:
serverSocket=socket(AF_INET,SOCK_STREAM)
Similar to UDPServer, we associate the server port number, serverPort, with 
this socket:
serverSocket.bind((‚Äô‚Äô,serverPort))
But with TCP, serverSocket will be our welcoming socket. After establish-
ing this welcoming door, we will wait and listen for some client to knock on the 
door:
serverSocket.listen(1)
This line has the server listen for TCP connection requests from the client. The 
parameter specifies the maximum number of queued connections (at least 1).
connectionSocket, addr = serverSocket.accept()
When a client knocks on this door, the program invokes the accept() method for 
serverSocket, which creates a new socket in the server, called  connectionSocket, 
dedicated to this particular client. The client and server then complete the hand-
shaking, creating a TCP connection between the client‚Äôs clientSocket and the 
server‚Äôs connectionSocket. With the TCP connection established, the client 
and server can now send bytes to each other over the connection. With TCP, all bytes 
sent from one side are only guaranteed to arrive at the other side but also guaranteed 
to arrive in order. 
connectionSocket.close()
In this program, after sending the modified sentence to the client, we close the con-
nection socket. But since serverSocket remains open, another client can now 
knock on the door and send the server a sentence to modify.

This completes our discussion of socket programming in TCP. You are encour-
aged to run the two programs in two separate hosts, and also to modify them to 
achieve slightly different goals. You should compare the UDP program pair with the 
TCP program pair and see how they differ. You should also do many of the socket 
programming assignments described at the ends of Chapter 2, 4, and 9. Finally, we 
hope someday, after mastering these and more advanced socket programs, you will 
write your own popular network application, become very rich and famous, and 
remember the authors of this textbook!
2.8 Summary
In this chapter, we‚Äôve studied the conceptual and the implementation aspects of 
network applications. We‚Äôve learned about the ubiquitous client-server architec-
ture adopted by many Internet applications and seen its use in the HTTP, SMTP, 
and DNS protocols. We‚Äôve studied these important application-level protocols, 
and their corresponding associated applications (the Web, file transfer, e-mail, and 
DNS) in some detail. We‚Äôve learned about the P2P architecture and contrasted it 
with the client-server architecture. We‚Äôve also learned about streaming video, and 
how modern video distribution systems leverage CDNs. We‚Äôve examined how the 
socket API can be used to build network applications. We‚Äôve walked through the 
use of sockets for connection-oriented (TCP) and connectionless (UDP) end-to-end 
transport services. The first step in our journey down the layered network architec-
ture is now complete!
At the very beginning of this book, in Section 1.1, we gave a rather vague, bare-
bones definition of a protocol: ‚Äúthe format and the order of messages exchanged 
between two or more communicating entities, as well as the actions taken on the 
transmission and/or receipt of a message or other event.‚Äù The material in this chapter, 
and in particular our detailed study of the HTTP, SMTP, and DNS protocols, has 
now added considerable substance to this definition. Protocols are a key concept in 
networking; our study of application protocols has now given us the opportunity to 
develop a more intuitive feel for what protocols are all about.
In Section 2.1, we described the service models that TCP and UDP offer to 
applications that invoke them. We took an even closer look at these service models 
when we developed simple applications that run over TCP and UDP in Section 2.7. 
However, we have said little about how TCP and UDP provide these service models. 
For example, we know that TCP provides a reliable data service, but we haven‚Äôt said 
yet how it does so. In the next chapter, we‚Äôll take a careful look at not only the what, 
but also the how and why of transport protocols.
Equipped with knowledge about Internet application structure and application-
level protocols, we‚Äôre now ready to head further down the protocol stack and exam-
ine the transport layer in Chapter 3.

Homework Problems and Questions
Chapter 2 Review Questions
SECTION 2.1
 R1. List five nonproprietary Internet applications and the application-layer proto-
cols that they use.
 R2. What is the difference between network architecture and application architecture?
 R3. For a communication session between a pair of processes, which process is 
the client and which is the server?
 R4. For a P2P file-sharing application, do you agree with the statement, ‚ÄúThere is no 
notion of client and server sides of a communication session‚Äù? Why or why not?
 R5. What information is used by a process running on one host to identify a pro-
cess running on another host?
 R6. Suppose you wanted to do a transaction from a remote client to a server as 
fast as possible. Would you use UDP or TCP? Why?
 R7. Referring to Figure 2.4, we see that none of the applications listed in Figure 
2.4 requires both no data loss and timing. Can you conceive of an application 
that requires no data loss and that is also highly time-sensitive?
 R8. List the four broad classes of services that a transport protocol can provide. 
For each of the service classes, indicate if either UDP or TCP (or both) pro-
vides such a service.
 R9. Recall that TCP can be enhanced with TLS to provide process-to-process 
security services, including encryption. Does TLS operate at the transport 
layer or the application layer? If the application developer wants TCP to be 
enhanced with TLS, what does the developer have to do?
SECTIONS 2.2‚Äì2.5
 R10. What is meant by a handshaking protocol?
 R11. Why do HTTP, SMTP, and IMAP run on top of TCP rather than on UDP?
 R12. Consider an e-commerce site that wants to keep a purchase record for each of 
its customers. Describe how this can be done with cookies.
 R13. Describe how Web caching can reduce the delay in receiving a requested 
object. Will Web caching reduce the delay for all objects requested by a user 
or for only some of the objects? Why?
 R14. Telnet into a Web server and send a multiline request message. Include in 
the request message the If-modified-since: header line to force a 
response message with the 304 Not Modified status code.
 R15. List several popular messaging apps. Do they use the same protocols as SMS?

R16. Suppose Alice, with a Web-based e-mail account (such as Hotmail or Gmail), 
sends a message to Bob, who accesses his mail from his mail server using 
IMAP. Discuss how the message gets from Alice‚Äôs host to Bob‚Äôs host. Be 
sure to list the series of application-layer protocols that are used to move the 
message between the two hosts.
 R17. Print out the header of an e-mail message you have recently received. How 
many Received: header lines are there? Analyze each of the header lines 
in the message.
 R18. What is the HOL blocking issue in HTTP/1.1? How does HTTP/2 attempt to 
solve it? 
 R19. Is it possible for an organization‚Äôs Web server and mail server to have 
exactly the same alias for a hostname (for example, foo.com)? What would 
be the type for the RR that contains the hostname of the mail server?
 R20. Look over your received e-mails, and examine the header of a message sent 
from a user with a .edu e-mail address. Is it possible to determine from the 
header the IP address of the host from which the message was sent? Do the 
same for a message sent from a Gmail account.
SECTION 2.5
 R21. In BitTorrent, suppose Alice provides chunks to Bob throughout a 30-second 
interval. Will Bob necessarily return the favor and provide chunks to Alice in 
this same interval? Why or why not?
 R22. Consider a new peer Alice that joins BitTorrent without possessing any chunks. 
Without any chunks, she cannot become a top-four uploader for any of the other 
peers, since she has nothing to upload. How then will Alice get her first chunk?
 R23. What is an overlay network? Does it include routers? What are the edges in 
the overlay network?
SECTION 2.6
 R24. CDNs typically adopt one of two different server placement philosophies. 
Name and briefly describe them.
 R25. Besides network-related considerations such as delay, loss, and bandwidth 
performance, there are other important factors that go into designing a CDN 
server selection strategy. What are they?
SECTION 2.7
 R26. In Section 2.7, the UDP server described needed only one socket, whereas 
the TCP server needed two sockets. Why? If the TCP server were to support 
n simultaneous connections, each from a different client host, how many 
sockets would the TCP server need?

R27. For the client-server application over TCP described in Section 2.7, why 
must the server program be executed before the client program? For the 
client-server application over UDP, why may the client program be executed 
before the server program?
Problems
 P1. True or false?
a. A user requests a Web page that consists of some text and three images. 
For this page, the client will send one request message and receive four 
response messages.
b. Two distinct Web pages (for example, www.mit.edu/research 
.html and www.mit.edu/students.html) can be sent over the 
same persistent connection.
c. With nonpersistent connections between browser and origin server, it is 
possible for a single TCP segment to carry two distinct HTTP request 
messages.
d. The Date: header in the HTTP response message indicates when the 
object in the response was last modified.
e. HTTP response messages never have an empty message body.
 P2. SMS, iMessage, Wechat, and WhatsApp are all smartphone real-time mes-
saging systems. After doing some research on the Internet, for each of these 
systems write one paragraph about the protocols they use. Then write a para-
graph explaining how they differ.
 P3. Consider an HTTP client that wants to retrieve a Web document at a given 
URL. The IP address of the HTTP server is initially unknown. What transport 
and application-layer protocols besides HTTP are needed in this scenario?
 P4. Consider the following string of ASCII characters that were captured by 
Wireshark when the browser sent an HTTP GET message (i.e., this is the 
actual content of an HTTP GET message). The characters <cr><lf> are 
carriage return and line-feed characters (that is, the italized character string 
<cr> in the text below represents the single carriage-return character that was 
contained at that point in the HTTP header). Answer the following questions, 
indicating where in the HTTP GET message below you find the answer.
GET /cs453/index.html HTTP/1.1<cr><lf>Host: gai
a.cs.umass.edu<cr><lf>User-Agent: Mozilla/5.0 (
Windows;U; Windows NT 5.1; en-US; rv:1.7.2) Gec
ko/20040804 Netscape/7.2 (ax) <cr><lf>Accept:ex
t/xml, application/xml, application/xhtml+xml, text
/html;q=0.9, text/plain;q=0.8,image/png,*/*;q=0.5

<cr><lf>Accept-Language: en-us,en;q=0.5<cr><lf>Accept-
Encoding: zip,deflate<cr><lf>Accept-Charset: ISO
-8859-1,utf-8;q=0.7,*;q=0.7<cr><lf>Keep-Alive: 300<cr>
<lf>Connection:keep-alive<cr><lf><cr><lf>
a. What is the URL of the document requested by the browser?
b. What version of HTTP is the browser running?
c. Does the browser request a non-persistent or a persistent connection?
d. What is the IP address of the host on which the browser is running?
e. What type of browser initiates this message? Why is the browser type 
needed in an HTTP request message?
 P5. The text below shows the reply sent from the server in response to the HTTP 
GET message in the question above. Answer the following questions, indicat-
ing where in the message below you find the answer.
HTTP/1.1 200 OK<cr><lf>Date: Tue, 07 Mar 2008
12:39:45GMT<cr><lf>Server: Apache/2.0.52 (Fedora)
<cr><lf>Last-Modified: Sat, 10 Dec2005 18:27:46 
GMT<cr><lf>ETag: ‚Äù526c3-f22-a88a4c80‚Äù<cr><lf>Accept- 
Ranges: bytes<cr><lf>Content-Length: 3874<cr><lf> 
Keep-Alive: timeout=max=100<cr><lf>Connection:
Keep-Alive<cr><lf>Content-Type: text/html; charset= 
ISO-8859-1<cr><lf><cr><lf><!doctype html public ‚Äù- 
//w3c//dtd html 4.0transitional//en‚Äù><lf><html><lf> 
<head><lf> <meta http-equiv=‚ÄùContent-Type‚Äù  
content=‚Äùtext/html; charset=iso-8859-1‚Äù><lf> <meta
name=‚ÄùGENERATOR‚Äù content=‚ÄùMozilla/4.79 [en] (Windows NT
5.0; U) Netscape]‚Äù><lf> <title>CMPSCI 453 / 591 /  
NTU-ST550ASpring 2005 homepage</title><lf></head><lf> 
<much more document text following here (not shown)>
a. Was the server able to successfully find the document or not? What time 
was the document reply provided?
b. When was the document last modified?
c. How many bytes are there in the document being returned?
d. What are the first 5 bytes of the document being returned? Did the server 
agree to a persistent connection?
 P6. Obtain the HTTP/1.1 specification (RFC 2616). Answer the following  
questions:
a. Explain the mechanism used for signaling between the client and server 
to indicate that a persistent connection is being closed. Can the client, the 
server, or both signal the close of a connection?

b. What encryption services are provided by HTTP?
c. Can a client open three or more simultaneous connections with a given 
server?
d. Either a server or a client may close a transport connection between them 
if either one detects the connection has been idle for some time. Is it 
possible that one side starts closing a connection while the other side is 
transmitting data via this connection? Explain.
 P7. Suppose within your Web browser you click on a link to obtain a Web page. 
The IP address for the associated URL is not cached in your local host, so 
a DNS lookup is necessary to obtain the IP address. Suppose that n DNS 
servers are visited before your host receives the IP address from DNS; the 
successive visits incur an RTT of RTT1, . . . , RTTn. Further suppose that the 
Web page associated with the link contains exactly one object, consisting of 
a small amount of HTML text. Let RTT0 denote the RTT between the local 
host and the server containing the object. Assuming zero transmission time 
of the object, how much time elapses from when the client clicks on the link 
until the client receives the object?
 P8. Referring to Problem P7, suppose the HTML file references eight very small 
objects on the same server. Neglecting transmission times, how much time 
elapses with
a. Non-persistent HTTP with no parallel TCP connections?
b. Non-persistent HTTP with the browser configured for 6 parallel  
connections?
c. Persistent HTTP?
 P9. Consider Figure 2.12, for which there is an institutional network connected to 
the Internet. Suppose that the average object size is 1,000,000 bits and that the 
average request rate from the institution‚Äôs browsers to the origin servers is 16 
requests per second. Also suppose that the amount of time it takes from when 
the router on the Internet side of the access link forwards an HTTP request until 
it receives the response is three seconds on average (see Section 2.2.5). Model 
the total average response time as the sum of the average access delay (that 
is, the delay from Internet router to institution router) and the average Internet 
delay. For the average access delay, use ‚àÜ/(1 - ‚àÜb), where ‚àÜ is the average 
time required to send an object over the access link and b is the arrival rate of 
objects to the access link.
a. Find the total average response time.
b. Now suppose a cache is installed in the institutional LAN. Suppose the 
miss rate is 0.4. Find the total response time.
 P10. Consider a short, 10-meter link, over which a sender can transmit at a rate 
of 150 bits/sec in both directions. Suppose that packets containing data 
are¬†100,000 bits long, and packets containing only control (e.g., ACK or

handshaking) are 200 bits long. Assume that N parallel connections each 
get¬†1/N of the link bandwidth. Now consider the HTTP protocol, and suppose 
that each downloaded object is 100 Kbits long, and that the initial downloaded 
object contains 10 referenced objects from the same sender. Would parallel 
downloads via parallel instances of non-persistent HTTP make sense in this 
case? Now consider persistent HTTP. Do you expect significant gains over 
the non-persistent case? Justify and explain your answer.
 P11. Consider the scenario introduced in the previous problem. Now suppose that 
the link is shared by Bob with four other users. Bob uses parallel instances 
of non-persistent HTTP, and the other four users use non-persistent HTTP 
without parallel downloads.
a. Do Bob‚Äôs parallel connections help him get Web pages more quickly? 
Why or why not?
b. If all five users open five parallel instances of non-persistent HTTP, then 
would Bob‚Äôs parallel connections still be beneficial? Why or why not?
 P12. Write a simple TCP program for a server that accepts lines of input from a cli-
ent and prints the lines onto the server‚Äôs standard output. (You can do this by 
modifying the TCPServer.py program in the text.) Compile and execute your 
program. On any other machine that contains a Web browser, set the proxy 
server in the browser to the host that is running your server program; also con-
figure the port number appropriately. Your browser should now send its GET 
request messages to your server, and your server should display the messages 
on its standard output. Use this platform to determine whether your browser 
generates conditional GET messages for objects that are locally cached.
 P13. Consider sending over HTTP/2 a Web page that consists of  one video clip, 
and five images. Suppose that the video clip is transported as 2000 frames, 
and each image has three frames. 
a. If all the video frames are sent first without interleaving, how many 
‚Äúframe times‚Äù are needed until all five images are sent?
b. If frames are interleaved, how many frame times are needed until all five 
images are sent.
 P14. Consider the Web page in problem 13. Now HTTP/2 prioritization is 
employed. Suppose all the images are given priority over the video clip, and 
that the first image is given priority over the second image, the second image 
over the third image, and so on. How many frame times will be needed until 
the second image is sent?
 P15. What is the difference between MAIL FROM: in SMTP and From: in the 
mail message itself?
 P16. How does SMTP mark the end of a message body? How about HTTP? Can 
HTTP use the same method as SMTP to mark the end of a message body? 
Explain.

P17. Read RFC 5321 for SMTP. What does MTA stand for? Consider the follow-
ing received spam e-mail (modified from a real spam e-mail). Assuming only 
the originator of this spam e-mail is malicious and all other hosts are honest, 
identify the malacious host that has generated this spam e-mail.
From - Fri Nov 07 13:41:30 2008
Return-Path: <tennis5@pp33head.com>
Received: from barmail.cs.umass.edu (barmail.cs.umass.
edu
[128.119.240.3]) by cs.umass.edu (8.13.1/8.12.6) for
<hg@cs.umass.edu>; Fri, 7 Nov 2008 13:27:10 -0500
Received: from asusus-4b96 (localhost [127.0.0.1]) by
barmail.cs.umass.edu (Spam Firewall) for <hg@cs.umass.
edu>; Fri, 7
Nov 2008 13:27:07 -0500 (EST)
Received: from asusus-4b96 ([58.88.21.177]) by barmail.
cs.umass.edu
for <hg@cs.umass.edu>; Fri, 07 Nov 2008 13:27:07 -0500 
(EST)
Received: from [58.88.21.177] by inbnd55.exchangeddd.
com; Sat, 8
Nov 2008 01:27:07 +0700
From: ‚ÄùJonny‚Äù <tennis5@pp33head.com>
To: <hg@cs.umass.edu>
¬†
Subject: How to secure your savings
 P18. a. What is a whois database?
b. Use various whois databases on the Internet to obtain the names of two 
DNS servers. Indicate which whois databases you used.
c. Use nslookup on your local host to send DNS queries to three DNS 
servers: your local DNS server and the two DNS servers you found in 
part (b). Try querying for Type A, NS, and MX reports. Summarize your 
findings.
d. Use nslookup to find a Web server that has multiple IP addresses. Does 
the Web server of your institution (school or company) have multiple IP 
addresses?
e. Use the ARIN whois database to determine the IP address range used by 
your university.
f. Describe how an attacker can use whois databases and the nslookup tool 
to perform reconnaissance on an institution before launching an attack.
g. Discuss why whois databases should be publicly available.

P19. In this problem, we use the useful dig tool available on Unix and Linux hosts to 
explore the hierarchy of DNS servers. Recall that in Figure 2.19, a DNS server 
in the DNS hierarchy delegates a DNS query to a DNS server lower in the 
hierarchy, by sending back to the DNS client the name of that lower-level DNS 
server. First read the man page for dig, and then answer the following questions.
a. Starting with a root DNS server (from one of the root servers [a-m].
root-servers.net), initiate a sequence of queries for the IP address for your 
department‚Äôs Web server by using dig. Show the list of the names of DNS 
servers in the delegation chain in answering your query.
b. Repeat part (a) for several popular Web sites, such as google.com, yahoo 
.com, or amazon.com.
 P20. Suppose you can access the caches in the local DNS servers of your depart-
ment. Can you propose a way to roughly determine the Web servers (outside 
your department) that are most popular among the users in your department? 
Explain.
 P21. Suppose that your department has a local DNS server for all computers in the 
department. You are an ordinary user (i.e., not a network/system administra-
tor). Can you determine if an external Web site was likely accessed from a 
computer in your department a couple of seconds ago? Explain.
 P22. Consider distributing a file of F = 20 Gbits to N peers. The server has 
an upload rate of us = 30 Mbps, and each peer has a download rate of 
di = 2 Mbps and an upload rate of u. For N = 10, 100, and 1,000 and 
u = 300 Kbps, 700 Kbps, and 2 Mbps, prepare a chart giving the minimum 
distribution time for each of the combinations of N and u for both client-
server distribution and P2P distribution.
 P23. Consider distributing a file of F bits to N peers using a client-server archi-
tecture. Assume a fluid model where the server can simultaneously transmit 
to multiple peers, transmitting to each peer at different rates, as long as the 
combined rate does not exceed us.
a. Suppose that us/N ‚Ä¶ dmin. Specify a distribution scheme that has a distri-
bution time of NF/us.
b. Suppose that us/N √ö dmin. Specify a distribution scheme that has a distri-
bution time of F/dmin.
c. Conclude that the minimum distribution time is in general given by 
max5NF/us, F/dmin6.
 P24. Consider distributing a file of F bits to N peers using a P2P architecture. 
Assume a fluid model. For simplicity assume that dmin is very large, so that 
peer download bandwidth is never a bottleneck.
a. Suppose that us ‚Ä¶ (us + u1 + . . . + uN)/N. Specify a distribution 
scheme that has a distribution time of F/us.

b. Suppose that us √ö (us + u1 + . . . + uN)/N. Specify a distribution 
scheme that has a distribution time of NF/(us + u1 + . . . +  uN).
c. Conclude that the minimum distribution time is in general given by 
max5F/us, NF/(us + u1 + . . . + uN)6.
 P25. Consider an overlay network with N active peers, with each pair of peers hav-
ing an active TCP connection. Additionally, suppose that the TCP connec-
tions pass through a total of M routers. How many nodes and edges are there 
in the corresponding overlay network?
 P26. Suppose Bob joins a BitTorrent torrent, but he does not want to upload any 
data to any other peers (so called free-riding).
a. Bob claims that he can receive a complete copy of the file that is shared 
by the swarm. Is Bob‚Äôs claim possible? Why or why not?
b. Bob further claims that he can further make his ‚Äúfree-riding‚Äù more 
efficient by using a collection of multiple computers (with distinct IP 
addresses) in the computer lab in his department. How can he do that?
 P27. Consider a DASH system for which there are N video versions (at N different 
rates and qualities) and N audio versions (at N different rates and qualities). 
Suppose we want to allow the player to choose at any time any of the N video 
versions and any of the N audio versions.
a. If we create files so that the audio is mixed in with the video, so server 
sends only one media stream at given time, how many files will the server 
need to store (each a different URL)?
b. If the server instead sends the audio and video streams separately and has 
the client synchronize the streams, how many files will the server need to 
store?
 P28. Install and compile the Python programs TCPClient and UDPClient on one 
host and TCPServer and UDPServer on another host.
a. Suppose you run TCPClient before you run TCPServer. What happens? 
Why?
b. Suppose you run UDPClient before you run UDPServer. What happens? 
Why?
c. What happens if you use different port numbers for the client and server 
sides?
 P29. Suppose that in UDPClient.py, after we create the socket, we add the line:
clientSocket.bind((‚Äô‚Äô, 5432))
Will it become necessary to change UDPServer.py? What are the port num-
bers for the sockets in UDPClient and UDPServer? What were they before 
making this change?

P30. Can you configure your browser to open multiple simultaneous connections 
to a Web site? What are the advantages and disadvantages of having a large 
number of simultaneous TCP connections?
 P31. We have seen that Internet TCP sockets treat the data being sent as a byte 
stream but UDP sockets recognize message boundaries. What are one 
advantage and one disadvantage of byte-oriented API versus having the API 
explicitly recognize and preserve application-defined message boundaries?
 P32. What is the Apache Web server? How much does it cost? What functional-
ity does it currently have? You may want to look at Wikipedia to answer this 
question.
Socket Programming Assignments
The Companion Website includes six socket programming assignments. The first 
four assignments are summarized below. The fifth assignment makes use of the 
ICMP protocol and is summarized at the end of Chapter 5. It is highly recommended 
that students complete several, if not all, of these assignments. Students can find full 
details of these assignments, as well as important snippets of the Python code, at the 
Web site www.pearsonhighered.com/cs-resources.
Assignment 1: Web Server
In this assignment, you will develop a simple Web server in Python that is capable of 
processing only one request. Specifically, your Web server will (i) create a connection 
socket when contacted by a client (browser); (ii) receive the HTTP request from this 
connection; (iii) parse the request to determine the specific file being requested; (iv) get 
the requested file from the server‚Äôs file system; (v) create an HTTP response message 
consisting of the requested file preceded by header lines; and (vi) send the response 
over the TCP connection to the requesting browser. If a browser requests a file that is 
not present in your server, your server should return a ‚Äú404 Not Found‚Äù error message.
In the Companion Website, we provide the skeleton code for your server. Your 
job is to complete the code, run your server, and then test your server by sending 
requests from browsers running on different hosts. If you run your server on a host 
that already has a Web server running on it, then you should use a different port than 
port 80 for your Web server.
Assignment 2: UDP Pinger
In this programming assignment, you will write a client ping program in Python. 
Your client will send a simple ping message to a server, receive a corresponding 
pong message back from the server, and determine the delay between when the client

sent the ping message and received the pong message. This delay is called the Round 
Trip Time (RTT). The functionality provided by the client and server is similar to the 
functionality provided by standard ping program available in modern operating sys-
tems. However, standard ping programs use the Internet Control Message Protocol 
(ICMP) (which we will study in Chapter 5). Here we will create a nonstandard (but 
simple!) UDP-based ping program.
Your ping program is to send 10 ping messages to the target server over UDP. 
For each message, your client is to determine and print the RTT when the corre-
sponding pong message is returned. Because UDP is an unreliable protocol, a packet 
sent by the client or server may be lost. For this reason, the client cannot wait indefi-
nitely for a reply to a ping message. You should have the client wait up to one second 
for a reply from the server; if no reply is received, the client should assume that the 
packet was lost and print a message accordingly.
In this assignment, you will be given the complete code for the server (available 
in the Companion Website). Your job is to write the client code, which will be very 
similar to the server code. It is recommended that you first study carefully the server 
code. You can then write your client code, liberally cutting and pasting lines from 
the server code.
Assignment 3: Mail Client
The goal of this programming assignment is to create a simple mail client that sends 
e-mail to any recipient. Your client will need to establish a TCP connection with 
a mail server (e.g., a Google mail server), dialogue with the mail server using the 
SMTP protocol, send an e-mail message to a recipient (e.g., your friend) via the mail 
server, and finally close the TCP connection with the mail server.
For this assignment, the Companion Website provides the skeleton code for 
your client. Your job is to complete the code and test your client by sending e-mail 
to different user accounts. You may also try sending through different servers (for 
example, through a Google mail server and through your university mail server).
Assignment 4: Web Proxy
In this assignment, you will develop a Web proxy. When your proxy receives an 
HTTP request for an object from a browser, it generates a new HTTP request for 
the same object and sends it to the origin server. When the proxy receives the cor-
responding HTTP response with the object from the origin server, it creates a new 
HTTP response, including the object, and sends it to the client. 
For this assignment, the Companion Website provides the skeleton code for the 
proxy server. Your job is to complete the code, and then test it by having different 
browsers request Web objects via your proxy.

Wireshark Lab: HTTP
Having gotten our feet wet with the Wireshark packet sniffer in Lab 1, we‚Äôre now 
ready to use Wireshark to investigate protocols in operation. In this lab, we‚Äôll explore 
several aspects of the HTTP protocol: the basic GET/reply interaction, HTTP message 
formats, retrieving large HTML files, retrieving HTML files with embedded URLs, 
persistent and non-persistent connections, and HTTP authentication and security.
As is the case with all Wireshark labs, the full description of this lab is available 
at this book‚Äôs Web site, www.pearsonhighered.com/cs-resources.
Wireshark Lab: DNS
In this lab, we take a closer look at the client side of the DNS, the protocol that 
translates Internet hostnames to IP addresses. Recall from Section 2.5 that the cli-
ent‚Äôs role in the DNS is relatively simple‚Äîa client sends a query to its local DNS 
server and receives a response back. Much can go on under the covers, invisible to 
the DNS clients, as the hierarchical DNS servers communicate with each other to 
either recursively or iteratively resolve the client‚Äôs DNS query. From the DNS cli-
ent‚Äôs standpoint, however, the protocol is quite simple‚Äîa query is formulated to the 
local DNS server and a response is received from that server. We observe DNS in 
action in this lab.
As is the case with all Wireshark labs, the full description of this lab is available 
at this book‚Äôs Web site, www.pearsonhighered.com/cs-resources.

AN INTERVIEW WITH...
Tim Berners-Lee
Sir Tim Berners-Lee is known as the inventor of the World Wide 
Web. In 1989, while working as a fellow at CERN, he proposed 
an Internet-based distributed information management system includ-
ing the original version of the HTTP protocol. In the same year he 
successfully implemented his design on a client and server. He 
received the 2016 Turing award for ‚Äúinventing the World Wide 
Web, the first Web browser, and the fundamental protocols and 
algorithms allowing the Web to scale.‚Äù He is the Co-Founder of the 
World Wide Web Foundation, and currently is a Professorial Fellow 
of Computer Science at the University of Oxford and a professor at 
CSAIL at MIT.
You originally studied physics. How is networking similar to physics?
When you study physics, you imagine what rules of behavior on the very small scale could 
possibly give rise to the large-scale world as we see it. When you design a global system 
like the Web, you try to invent rules of behavior of Web pages and links and things that 
could in the large create a large-scale world as we would like it. One is analysis and the 
other synthesis, but they are very similar.
What influenced you to specialize in networking?
After my physics degree, the telecommunications research companies seemed to be the 
most interesting places. The microprocessor had just come out, and telecommunications  
was switching very fast from hardwired logic to microprocessor-based systems. It was  
very exciting.
What is the most challenging part of your job?
When two groups disagree strongly about something, but want in the end to achieve a com-
mon goal, finding exactly what they each mean and where the misunderstandings are can be 
very demanding. The chair of any working group knows that. However, this is what it takes 
to make progress toward consensus on a large scale.
Courtesy of Tim Berners-Lee

What people have inspired you professionally?
My parents, who were involved in the early days of computing, gave me a fascination with 
the whole subject. Mike Sendall and Peggie Rimmer, for whom I worked at various times 
at CERN are among the people who taught me and encouraged me. I later learned to admire 
the people, including Vanevar Bush, Doug Englebart, and Ted Nelson, who had had similar 
dreams in their time but had not had the benefit of the existence for PCs and the Internet to 
be able to realize it.



181
Residing between the application and network layers, the transport layer is a central 
piece of the layered network architecture. It has the critical role of providing com-
munication services directly to the application processes running on different hosts. 
The pedagogic approach we take in this chapter is to alternate between discussions of 
transport-layer principles and discussions of how these principles are implemented 
in existing protocols; as usual, particular emphasis will be given to Internet proto-
cols, in particular the TCP and UDP transport-layer protocols.
We‚Äôll begin by discussing the relationship between the transport and network 
layers. This sets the stage for examining the first critical function of the transport 
layer‚Äîextending the network layer‚Äôs delivery service between two end systems to 
a delivery service between two application-layer processes running on the end sys-
tems. We‚Äôll illustrate this function in our coverage of the Internet‚Äôs connectionless 
transport protocol, UDP.
We‚Äôll then return to principles and confront one of the most fundamental prob-
lems in computer networking‚Äîhow two entities can communicate reliably over a 
medium that may lose and corrupt data. Through a series of increasingly complicated 
(and realistic!) scenarios, we‚Äôll build up an array of techniques that transport proto-
cols use to solve this problem. We‚Äôll then show how these principles are embodied 
in TCP, the Internet‚Äôs connection-oriented transport protocol.
We‚Äôll next move on to a second fundamentally important problem in  
networking‚Äîcontrolling the transmission rate of transport-layer entities in order to 
avoid, or recover from, congestion within the network. We‚Äôll consider the causes 
and consequences of congestion, as well as commonly used congestion-control 
Transport 
Layer
3
CHAPTER
181

techniques. After obtaining a solid understanding of the issues behind congestion 
control, we‚Äôll study TCP‚Äôs approach to congestion control.
3.1 Introduction and Transport-Layer Services
In the previous two chapters, we touched on the role of the transport layer and the 
services that it provides. Let‚Äôs quickly review what we have already learned about 
the transport layer.
A transport-layer protocol provides for logical communication between 
application processes running on different hosts. By logical communication, we 
mean that from an application‚Äôs perspective, it is as if the hosts running the pro-
cesses were directly connected; in reality, the hosts may be on opposite sides of the 
planet, connected via numerous routers and a wide range of link types. Application 
processes use the logical communication provided by the transport layer to send 
messages to each other, free from the worry of the details of the physical infra-
structure used to carry these messages. Figure 3.1 illustrates the notion of logical 
communication.
As shown in Figure 3.1, transport-layer protocols are implemented in the end 
systems but not in network routers. On the sending side, the transport layer converts 
the application-layer messages it receives from a sending application process into 
transport-layer packets, known as transport-layer segments in Internet terminology. 
This is done by (possibly) breaking the application messages into smaller chunks 
and adding a transport-layer header to each chunk to create the transport-layer seg-
ment. The transport layer then passes the segment to the network layer at the send-
ing end system, where the segment is encapsulated within a network-layer packet (a 
datagram) and sent to the destination. It‚Äôs important to note that network routers act 
only on the network-layer fields of the datagram; that is, they do not examine the 
fields of the transport-layer segment encapsulated with the datagram. On the receiv-
ing side, the network layer extracts the transport-layer segment from the datagram 
and passes the segment up to the transport layer. The transport layer then processes 
the received segment, making the data in the segment available to the receiving 
application.
More than one transport-layer protocol may be available to network applications. 
For example, the Internet has two protocols‚ÄîTCP and UDP. Each of these protocols 
provides a different set of transport-layer services to the invoking application.
3.1.1 Relationship Between Transport and Network Layers
Recall that the transport layer lies just above the network layer in the protocol 
stack. Whereas a transport-layer protocol provides logical communication between

Network
Link
Physical
Application
Transport
Network
Link
Physical
Application
Transport
Network
Link
Physical
Network
Link
Physical
Network
Link
Physical
Network
Link
Physical
Network
Link
Physical
Content Provider Network
National or
Global ISP
Datacenter Network
Datacenter Network
Mobile Network
Enterprise Network
Home Network
Logical end-to-end transport
Local or
Regional ISP
Figure 3.1 ‚ô¶  The transport layer provides logical rather than physical  
communication between application processes

processes running on different hosts, a network-layer protocol provides logical- 
communication between hosts. This distinction is subtle but important. Let‚Äôs exam-
ine this distinction with the aid of a household analogy.
Consider two houses, one on the East Coast and the other on the West Coast, 
with each house being home to a dozen kids. The kids in the East Coast household 
are cousins of the kids in the West Coast household. The kids in the two households 
love to write to each other‚Äîeach kid writes each cousin every week, with each letter 
delivered by the traditional postal service in a separate envelope. Thus, each house-
hold sends 144 letters to the other household every week. (These kids would save a lot 
of money if they had e-mail!) In each of the households, there is one kid‚ÄîAnn in the 
West Coast house and Bill in the East Coast house‚Äîresponsible for mail collection  
and mail distribution. Each week Ann visits all her brothers and sisters, collects the 
mail, and gives the mail to a postal-service mail carrier, who makes daily visits to 
the house. When letters arrive at the West Coast house, Ann also has the job of dis-
tributing the mail to her brothers and sisters. Bill has a similar job on the East Coast.
In this example, the postal service provides logical communication between the 
two houses‚Äîthe postal service moves mail from house to house, not from person to 
person. On the other hand, Ann and Bill provide logical communication among the 
cousins‚ÄîAnn and Bill pick up mail from, and deliver mail to, their brothers and sis-
ters. Note that from the cousins‚Äô perspective, Ann and Bill are the mail service, even 
though Ann and Bill are only a part (the end-system part) of the end-to-end delivery 
process. This household example serves as a nice analogy for explaining how the 
transport layer relates to the network layer:
application messages = letters in envelopes
processes = cousins
hosts (also called end systems) = houses
transport-layer protocol = Ann and Bill
network-layer protocol = postal service (including mail carriers)
Continuing with this analogy, note that Ann and Bill do all their work within 
their respective homes; they are not involved, for example, in sorting mail in 
any intermediate mail center or in moving mail from one mail center to another.  
Similarly, transport-layer protocols live in the end systems. Within an end system, a 
transport protocol moves messages from application processes to the network edge 
(that is, the network layer) and vice versa, but it doesn‚Äôt have any say about how the 
messages are moved within the network core. In fact, as illustrated in Figure 3.1, 
intermediate routers neither act on, nor recognize, any information that the transport 
layer may have added to the application messages.
Continuing with our family saga, suppose now that when Ann and Bill go on 
vacation, another cousin pair‚Äîsay, Susan and Harvey‚Äîsubstitute for them and pro-
vide the household-internal collection and delivery of mail. Unfortunately for the 
two families, Susan and Harvey do not do the collection and delivery in exactly

the same way as Ann and Bill. Being younger kids, Susan and Harvey pick up and 
drop off the mail less frequently and occasionally lose letters (which are sometimes 
chewed up by the family dog). Thus, the cousin-pair Susan and Harvey do not pro-
vide the same set of services (that is, the same service model) as Ann and Bill. In 
an analogous manner, a computer network may make available multiple transport 
protocols, with each protocol offering a different service model to applications.
The possible services that Ann and Bill can provide are clearly constrained by 
the possible services that the postal service provides. For example, if the postal ser-
vice doesn‚Äôt provide a maximum bound on how long it can take to deliver mail 
between the two houses (for example, three days), then there is no way that Ann and 
Bill can guarantee a maximum delay for mail delivery between any of the cousin 
pairs. In a similar manner, the services that a transport protocol can provide are often 
constrained by the service model of the underlying network-layer protocol. If the 
network-layer protocol cannot provide delay or bandwidth guarantees for transport-
layer segments sent between hosts, then the transport-layer protocol cannot provide 
delay or bandwidth guarantees for application messages sent between processes.
Nevertheless, certain services can be offered by a transport protocol even when 
the underlying network protocol doesn‚Äôt offer the corresponding service at the net-
work layer. For example, as we‚Äôll see in this chapter, a transport protocol can offer 
reliable data transfer service to an application even when the underlying network 
protocol is unreliable, that is, even when the network protocol loses, garbles, or 
duplicates packets. As another example (which we‚Äôll explore in Chapter 8 when we 
discuss network security), a transport protocol can use encryption to guarantee that 
application messages are not read by intruders, even when the network layer cannot 
guarantee the confidentiality of transport-layer segments.
3.1.2 Overview of the Transport Layer in the Internet
Recall that the Internet makes two distinct transport-layer protocols available to the 
application layer. One of these protocols is UDP (User Datagram Protocol), which 
provides an unreliable, connectionless service to the invoking application. The sec-
ond of these protocols is TCP (Transmission Control Protocol), which provides a 
reliable, connection-oriented service to the invoking application. When designing a 
network application, the application developer must specify one of these two trans-
port protocols. As we saw in Section 2.7, the application developer selects between 
UDP and TCP when creating sockets.
To simplify terminology, we refer to the transport-layer packet as a segment. We 
mention, however, that the Internet literature (for example, the RFCs) also refers to the 
transport-layer packet for TCP as a segment but often refers to the packet for UDP as 
a datagram. However, this same Internet literature also uses the term datagram for the 
network-layer packet! For an introductory book on computer networking such as this, 
we believe that it is less confusing to refer to both TCP and UDP packets as segments, 
and reserve the term datagram for the network-layer packet.

Before proceeding with our brief introduction of UDP and TCP, it will be useful 
to say a few words about the Internet‚Äôs network layer. (We‚Äôll learn about the network 
layer in detail in Chapters 4 and 5.) The Internet‚Äôs network-layer protocol has a 
name‚ÄîIP, for Internet Protocol. IP provides logical communication between hosts. 
The IP service model is a best-effort delivery service. This means that IP makes 
its ‚Äúbest effort‚Äù to deliver segments between communicating hosts, but it makes no 
guarantees. In particular, it does not guarantee segment delivery, it does not guaran-
tee orderly delivery of segments, and it does not guarantee the integrity of the data 
in the segments. For these reasons, IP is said to be an unreliable service. We also 
mention here that every host has at least one network-layer address, a so-called IP 
address. We‚Äôll examine IP addressing in detail in Chapter 4; for this chapter we need 
only keep in mind that each host has an IP address.
Having taken a glimpse at the IP service model, let‚Äôs now summarize the service 
models provided by UDP and TCP. The most fundamental responsibility of UDP 
and TCP is to extend IP‚Äôs delivery service between two end systems to a delivery 
service between two processes running on the end systems. Extending host-to-host 
delivery to process-to-process delivery is called transport-layer multiplexing and  
demultiplexing. We‚Äôll discuss transport-layer multiplexing and demultiplexing in 
the next section. UDP and TCP also provide integrity checking by including error-
detection fields in their segments‚Äô headers. These two minimal transport-layer 
services‚Äîprocess-to-process data delivery and error checking‚Äîare the only two 
services that UDP provides! In particular, like IP, UDP is an unreliable service‚Äîit 
does not guarantee that data sent by one process will arrive intact (or at all!) to the 
destination process. UDP is discussed in detail in Section 3.3.
TCP, on the other hand, offers several additional services to applications. First 
and foremost, it provides reliable data transfer. Using flow control, sequence 
numbers, acknowledgments, and timers (techniques we‚Äôll explore in detail in this 
chapter), TCP ensures that data is delivered from sending process to receiving pro-
cess, correctly and in order. TCP thus converts IP‚Äôs unreliable service between end 
systems into a reliable data transport service between processes. TCP also provides 
congestion control. Congestion control is not so much a service provided to the 
invoking application as it is a service for the Internet as a whole, a service for the 
general good. Loosely speaking, TCP congestion control prevents any one TCP con-
nection from swamping the links and routers between communicating hosts with 
an excessive amount of traffic. TCP strives to give each connection traversing a 
congested link an equal share of the link bandwidth. This is done by regulating the 
rate at which the sending sides of TCP connections can send traffic into the network. 
UDP traffic, on the other hand, is unregulated. An application using UDP transport 
can send at any rate it pleases, for as long as it pleases.
A protocol that provides reliable data transfer and congestion control is neces-
sarily complex. We‚Äôll need several sections to cover the principles of reliable data 
transfer and congestion control, and additional sections to cover the TCP protocol 
itself. These topics are investigated in Sections 3.4 through 3.7. The approach taken

in this chapter is to alternate between basic principles and the TCP protocol. For 
example, we‚Äôll first discuss reliable data transfer in a general setting and then discuss 
how TCP specifically provides reliable data transfer. Similarly, we‚Äôll first discuss 
congestion control in a general setting and then discuss how TCP performs conges-
tion control. But before getting into all this good stuff, let‚Äôs first look at transport-
layer multiplexing and demultiplexing.
3.2 Multiplexing and Demultiplexing
In this section, we discuss transport-layer multiplexing and demultiplexing, that 
is, extending the host-to-host delivery service provided by the network layer to a  
process-to-process delivery service for applications running on the hosts. In order to 
keep the discussion concrete, we‚Äôll discuss this basic transport-layer service in the 
context of the Internet. We emphasize, however, that a multiplexing/demultiplexing 
service is needed for all computer networks.
At the destination host, the transport layer receives segments from the network 
layer just below. The transport layer has the responsibility of delivering the data in 
these segments to the appropriate application process running in the host. Let‚Äôs take 
a look at an example. Suppose you are sitting in front of your computer, and you are 
downloading Web pages while running one FTP session and two Telnet sessions. 
You therefore have four network application processes running‚Äîtwo Telnet pro-
cesses, one FTP process, and one HTTP process. When the transport layer in your 
computer receives data from the network layer below, it needs to direct the received 
data to one of these four processes. Let‚Äôs now examine how this is done.
First recall from Section 2.7 that a process (as part of a network application) 
can have one or more sockets, doors through which data passes from the network to 
the process and through which data passes from the process to the network. Thus, 
as shown in Figure 3.2, the transport layer in the receiving host does not actually 
deliver data directly to a process, but instead to an intermediary socket. Because at 
any given time there can be more than one socket in the receiving host, each socket 
has a unique identifier. The format of the identifier depends on whether the socket is 
a UDP or a TCP socket, as we‚Äôll discuss shortly.
Now let‚Äôs consider how a receiving host directs an incoming transport-layer 
segment to the appropriate socket. Each transport-layer segment has a set of fields in 
the segment for this purpose. At the receiving end, the transport layer examines these 
fields to identify the receiving socket and then directs the segment to that socket. 
This job of delivering the data in a transport-layer segment to the correct socket is 
called demultiplexing. The job of gathering data chunks at the source host from 
different sockets, encapsulating each data chunk with header information (that will 
later be used in demultiplexing) to create segments, and passing the segments to the 
network layer is called multiplexing. Note that the transport layer in the middle host

in Figure 3.2 must demultiplex segments arriving from the network layer below to 
either process P1 or P2 above; this is done by directing the arriving segment‚Äôs data 
to¬†the corresponding process‚Äôs socket. The transport layer in the middle host must 
also gather outgoing data from these sockets, form transport-layer segments, and 
pass these segments down to the network layer. Although we have introduced mul-
tiplexing and demultiplexing in the context of the Internet transport protocols, it‚Äôs 
important to realize that they are concerns whenever a single protocol at one layer (at 
the transport layer or elsewhere) is used by multiple protocols at the next higher layer.
To illustrate the demultiplexing job, recall the household analogy in the previous 
section. Each of the kids is identified by his or her name. When Bill receives a batch 
of mail from the mail carrier, he performs a demultiplexing operation by observing 
to whom the letters are addressed and then hand delivering the mail to his brothers 
and sisters. Ann performs a multiplexing operation when she collects letters from her 
brothers and sisters and gives the collected mail to the mail person.
Now that we understand the roles of transport-layer multiplexing and demulti-
plexing, let us examine how it is actually done in a host. From the discussion above, 
we know that transport-layer multiplexing requires (1) that sockets have unique 
identifiers, and (2) that each segment have special fields that indicate the socket to 
which the segment is to be delivered. These special fields, illustrated in Figure 3.3, 
are the source port number field and the destination port number field. (The UDP 
and TCP segments have other fields as well, as discussed in the subsequent sections 
of this chapter.) Each port number is a 16-bit number, ranging from 0 to 65535. 
The port numbers ranging from 0 to 1023 are called well-known port numbers  
and are restricted, which means that they are reserved for use by well-known 
Network
Key:
Process
Socket
Data link
Physical
Transport
Application
Network
Application
Data link
Physical
Transport
Network
Data link
Physical
Transport
P3
P2
P1
P4
Application
Figure 3.2 ‚ô¶ Transport-layer multiplexing and demultiplexing

application protocols such as HTTP (which uses port number 80) and FTP (which 
uses port number 21). The list of well-known port numbers is given in RFC 1700 
and is updated at http://www.iana.org [RFC 3232]. When we develop a new appli-
cation (such as the simple application developed in Section 2.7), we must assign the 
application a port number.
It should now be clear how the transport layer could implement the demultiplex-
ing service: Each socket in the host could be assigned a port number, and when 
a segment arrives at the host, the transport layer examines the destination port 
number in the segment and directs the segment to the corresponding socket. The 
segment‚Äôs data then passes through the socket into the attached process. As we‚Äôll 
see, this is basically how UDP does it. However, we‚Äôll also see that multiplexing/
demultiplexing in TCP is yet more subtle.
Connectionless Multiplexing and Demultiplexing
Recall from Section 2.7.1 that the Python program running in a host can create a 
UDP socket with the line
clientSocket = socket(AF_INET, SOCK_DGRAM)
When a UDP socket is created in this manner, the transport layer automatically 
assigns a port number to the socket. In particular, the transport layer assigns a port 
number in the range 1024 to 65535 that is currently not being used by any other UDP 
port in the host. Alternatively, we can add a line into our Python program after we 
create the socket to associate a specific port number (say, 19157) to this UDP socket 
via the socket bind() method:
clientSocket.bind((‚Äô‚Äô, 19157))
Source port #
32 bits
Dest. port #
Other header Ô¨Åelds
Application
data
(message)
Figure 3.3 ‚ô¶  Source and destination port-number fields in a transport-layer 
segment

If the application developer writing the code were implementing the server side of 
a ‚Äúwell-known protocol,‚Äù then the developer would have to assign the correspond-
ing well-known port number. Typically, the client side of the application lets the 
transport layer automatically (and transparently) assign the port number, whereas the 
server side of the application assigns a specific port number.
With port numbers assigned to UDP sockets, we can now precisely describe 
UDP multiplexing/demultiplexing. Suppose a process in Host A, with UDP port 
19157, wants to send a chunk of application data to a process with UDP port 46428 in 
Host B. The transport layer in Host A creates a transport-layer segment that includes 
the application data, the source port number (19157), the destination port number 
(46428), and two other values (which will be discussed later, but are unimportant for 
the current discussion). The transport layer then passes the resulting segment to the 
network layer. The network layer encapsulates the segment in an IP datagram and 
makes a best-effort attempt to deliver the segment to the receiving host. If the seg-
ment arrives at the receiving Host B, the transport layer at the receiving host exam-
ines the destination port number in the segment (46428) and delivers the segment 
to its socket identified by port 46428. Note that Host B could be running multiple 
processes, each with its own UDP socket and associated port number. As UDP seg-
ments arrive from the network, Host B directs (demultiplexes) each segment to the 
appropriate socket by examining the segment‚Äôs destination port number.
It is important to note that a UDP socket is fully identified by a two-tuple consist-
ing of a destination IP address and a destination port number. As a consequence, if 
two UDP segments have different source IP addresses and/or source port numbers, but 
have the same destination IP address and destination port number, then the two seg-
ments will be directed to the same destination process via the same destination socket.
You may be wondering now, what is the purpose of the source port number? 
As shown in Figure 3.4, in the A-to-B segment the source port number serves as 
part of a ‚Äúreturn address‚Äù‚Äîwhen B wants to send a segment back to A, the destina-
tion port in the B-to-A segment will take its value from the source port value of the 
A-to-B segment. (The complete return address is A‚Äôs IP address and the source port 
number.) As an example, recall the UDP server program studied in Section 2.7. In 
UDPServer.py, the server uses the recvfrom() method to extract the client-
side (source) port number from the segment it receives from the client; it then sends 
a new segment to the client, with the extracted source port number serving as the 
destination port number in this new segment.
Connection-Oriented Multiplexing and Demultiplexing
In order to understand TCP demultiplexing, we have to take a close look at TCP 
sockets and TCP connection establishment. One subtle difference between a 
TCP socket and a UDP socket is that a TCP socket is identified by a four-tuple: 
(source IP address, source port number, destination IP address, destination port 
number). Thus, when a TCP segment arrives from the network to a host, the host 
uses all four values to direct (demultiplex) the segment to the appropriate socket.

In particular, and in contrast with UDP, two arriving TCP segments with differ-
ent source IP addresses or source port numbers will (with the exception of a TCP 
segment carrying the original connection-establishment request) be directed to two 
different sockets. To gain further insight, let‚Äôs reconsider the TCP client-server pro-
gramming example in Section 2.7.2:
‚Ä¢ The TCP server application has a ‚Äúwelcoming socket,‚Äù that waits for connection-
establishment requests from TCP clients (see Figure 2.29) on port number 12000.
‚Ä¢ The TCP client creates a socket and sends a connection establishment request 
segment with the lines:
 clientSocket = socket(AF_INET, SOCK_STREAM)
 clientSocket.connect((serverName,12000))
‚Ä¢ A connection-establishment request is nothing more than a TCP segment with 
destination port number 12000 and a special connection-establishment bit set in 
the TCP header (discussed in Section 3.5). The segment also includes a source 
port number that was chosen by the client.
‚Ä¢ When the host operating system of the computer running the server process 
receives the incoming connection-request segment with destination port 12000, 
it locates the server process that is waiting to accept a connection on port number 
12000. The server process then creates a new socket:
 connectionSocket, addr = serverSocket.accept()
Host A
Client process
Socket
Server B
source port:
19157
dest. port:
46428
source port:
46428
dest. port:
19157
Figure 3.4 ‚ô¶ The inversion of source and destination port numbers

‚Ä¢ Also, the transport layer at the server notes the following four values in the con-
nection-request segment: (1) the source port number in the segment, (2) the IP 
address of the source host, (3) the destination port number in the segment, and 
(4) its own IP address. The newly created connection socket is identified by these 
four values; all subsequently arriving segments whose source port, source IP 
address, destination port, and destination IP address match these four values will 
be demultiplexed to this socket. With the TCP connection now in place, the client 
and server can now send data to each other.
The server host may support many simultaneous TCP connection sockets, with 
each socket attached to a process, and with each socket identified by its own four-
tuple. When a TCP segment arrives at the host, all four fields (source IP address, 
source port, destination IP address, destination port) are used to direct (demultiplex) 
the segment to the appropriate socket.
PORT SCANNING
We‚Äôve seen that a server process waits patiently on an open port for contact by a 
remote client. Some ports are reserved for well-known applications (e.g., Web, FTP, 
DNS, and SMTP servers); other ports are used by convention by popular applications 
(e.g., the Microsoft Windows SQL server listens for requests on UDP port 1434). Thus, 
if we determine that a port is open on a host, we may be able to map that port to a 
specific application running on the host. This is very useful for system administrators, 
who are often interested in knowing which network applications are running on the 
hosts in their networks. But attackers, in order to ‚Äúcase the joint,‚Äù also want to know 
which ports are open on target hosts. If a host is found to be running an application 
with a known security flaw (e.g., a SQL server listening on port 1434 was subject to 
a buffer overflow, allowing a remote user to execute arbitrary code on the vulnerable 
host, a flaw exploited by the Slammer worm [CERT 2003‚Äì04]), then that host is ripe 
for attack.
Determining which applications are listening on which ports is a relatively easy 
task. Indeed there are a number of public domain programs, called port scanners, 
that do just that. Perhaps the most widely used of these is nmap, freely available at 
http://nmap.org and included in most Linux distributions. For TCP, nmap sequentially 
scans ports, looking for ports that are accepting TCP connections. For UDP, nmap 
again sequentially scans ports, looking for UDP ports that respond to transmitted UDP 
segments. In both cases, nmap returns a list of open, closed, or unreachable ports. 
A host running nmap can attempt to scan any target host anywhere in the Internet. 
We‚Äôll revisit nmap in Section 3.5.6, when we discuss TCP connection management.
FOCUS ON SECURITY

The situation is illustrated in Figure 3.5, in which Host C initiates two HTTP 
sessions to server B, and Host A initiates one HTTP session to B. Hosts A and C 
and server B each have their own unique IP address‚ÄîA, C, and B, respectively. 
Host C assigns two different source port numbers (26145 and 7532) to its two HTTP 
connections. Because Host A is choosing source port numbers independently of C, 
it might also assign a source port of 26145 to its HTTP connection. But this is not 
a problem‚Äîserver B will still be able to correctly demultiplex the two connections 
having the same source port number, since the two connections have different source 
IP addresses.
Web Servers and TCP
Before closing this discussion, it‚Äôs instructive to say a few additional words about 
Web servers and how they use port numbers. Consider a host running a Web server, 
such as an Apache Web server, on port 80. When clients (for example, browsers) 
send segments to the server, all segments will have destination port 80. In particular, 
both the initial connection-establishment segments and the segments carrying HTTP 
source port:
7532
dest. port:
80
source IP:
C
dest. IP:
B
source port:
26145
dest. port:
80
source IP:
C
dest. IP:
B
source port:
26145
dest. port:
80
source IP:
A
dest. IP:
B
Per-connection
HTTP
processes
Transport-
layer
demultiplexing
Web 
server B
Web client
host C
Web client
host A
Figure 3.5 ‚ô¶  Two clients, using the same destination port number (80) to 
communicate with the same Web server application

request messages will have destination port 80. As we have just described, the server 
distinguishes the segments from the different clients using source IP addresses and 
source port numbers.
Figure 3.5 shows a Web server that spawns a new process for each connec-
tion. As shown in Figure 3.5, each of these processes has its own connection socket 
through which HTTP requests arrive and HTTP responses are sent. We mention, 
however, that there is not always a one-to-one correspondence between connection 
sockets and processes. In fact, today‚Äôs high-performing Web servers often use only 
one process, and create a new thread with a new connection socket for each new 
client connection. (A thread can be viewed as a lightweight subprocess.) If you did 
the first programming assignment in Chapter 2, you built a Web server that does just 
this. For such a server, at any given time there may be many connection sockets (with 
different identifiers) attached to the same process.
If the client and server are using persistent HTTP, then throughout the duration 
of the persistent connection the client and server exchange HTTP messages via the 
same server socket. However, if the client and server use non-persistent HTTP, then 
a new TCP connection is created and closed for every request/response, and hence 
a new socket is created and later closed for every request/response. This frequent 
creating and closing of sockets can severely impact the performance of a busy Web 
server (although a number of operating system tricks can be used to mitigate the 
problem). Readers interested in the operating system issues surrounding persistent 
and non-persistent HTTP are encouraged to see [Nielsen 1997; Nahum 2002].
Now that we‚Äôve discussed transport-layer multiplexing and demultiplexing, let‚Äôs 
move on and discuss one of the Internet‚Äôs transport protocols, UDP. In the next sec-
tion, we‚Äôll see that UDP adds little more to the network-layer protocol than a multi-
plexing/demultiplexing service.
3.3 Connectionless Transport: UDP
In this section, we‚Äôll take a close look at UDP, how it works, and what it does. 
We encourage you to refer back to Section 2.1, which includes an overview of the 
UDP service model, and to Section 2.7.1, which discusses socket programming using 
UDP.
To motivate our discussion about UDP, suppose you were interested in design-
ing a no-frills, bare-bones transport protocol. How might you go about doing this? 
You might first consider using a vacuous transport protocol. In particular, on the 
sending side, you might consider taking the messages from the application process 
and passing them directly to the network layer; and on the receiving side, you might 
consider taking the messages arriving from the network layer and passing them 
directly to the application process. But as we learned in the previous section, we have

to do a little more than nothing! At the very least, the transport layer has to provide a 
multiplexing/demultiplexing service in order to pass data between the network layer 
and the correct application-level process.
UDP, defined in [RFC 768], does just about as little as a transport protocol can do. 
Aside from the multiplexing/demultiplexing function and some light error checking, it 
adds nothing to IP. In fact, if the application developer chooses UDP instead of TCP, 
then the application is almost directly talking with IP. UDP takes messages from the 
application process, attaches source and destination port number fields for the multi-
plexing/demultiplexing service, adds two other small fields, and passes the resulting 
segment to the network layer. The network layer encapsulates the transport-layer seg-
ment into an IP datagram and then makes a best-effort attempt to deliver the segment 
to the receiving host. If the segment arrives at the receiving host, UDP uses the destina-
tion port number to deliver the segment‚Äôs data to the correct application process. Note 
that with UDP there is no handshaking between sending and receiving transport-layer 
entities before sending a segment. For this reason, UDP is said to be connectionless.
DNS is an example of an application-layer protocol that typically uses UDP. 
When the DNS application in a host wants to make a query, it constructs a DNS query 
message and passes the message to UDP. Without performing any handshaking with 
the UDP entity running on the destination end system, the host-side UDP adds header 
fields to the message and passes the resulting segment to the network layer. The net-
work layer encapsulates the UDP segment into a datagram and sends the datagram to 
a name server. The DNS application at the querying host then waits for a reply to its 
query. If it doesn‚Äôt receive a reply (possibly because the underlying network lost the 
query or the reply), it might try resending the query, try sending the query to another 
name server, or inform the invoking application that it can‚Äôt get a reply.
Now you might be wondering why an application developer would ever choose 
to build an application over UDP rather than over TCP. Isn‚Äôt TCP always preferable, 
since TCP provides a reliable data transfer service, while UDP does not? The answer 
is no, as some applications are better suited for UDP for the following reasons:
‚Ä¢ Finer application-level control over what data is sent, and when. Under UDP, as 
soon as an application process passes data to UDP, UDP will package the data 
inside a UDP segment and immediately pass the segment to the network layer. 
TCP, on the other hand, has a congestion-control mechanism that throttles the 
transport-layer TCP sender when one or more links between the source and des-
tination hosts become excessively congested. TCP will also continue to resend a 
segment until the receipt of the segment has been acknowledged by the destina-
tion, regardless of how long reliable delivery takes. Since real-time applications 
often require a minimum sending rate, do not want to overly delay segment trans-
mission, and can tolerate some data loss, TCP‚Äôs service model is not particularly 
well matched to these applications‚Äô needs. As discussed below, these applications 
can use UDP and implement, as part of the application, any additional functional-
ity that is needed beyond UDP‚Äôs no-frills segment-delivery service.

‚Ä¢ No connection establishment. As we‚Äôll discuss later, TCP uses a three-way hand-
shake before it starts to transfer data. UDP just blasts away without any formal 
preliminaries. Thus UDP does not introduce any delay to establish a connection. 
This is probably the principal reason why DNS runs over UDP rather than TCP‚Äî
DNS would be much slower if it ran over TCP. HTTP uses TCP rather than UDP, 
since reliability is critical for Web pages with text. But, as we briefly discussed 
in Section 2.2, the TCP connection-establishment delay in HTTP is an important 
contributor to the delays associated with downloading Web documents. Indeed, 
the QUIC protocol (Quick UDP Internet Connection, [IETF QUIC 2020]), used 
in Google‚Äôs Chrome browser, uses UDP as its underlying transport protocol and 
implements reliability in an application-layer protocol on top of UDP. We‚Äôll take 
a closer look at QUIC in Section 3.8.
‚Ä¢ No connection state. TCP maintains connection state in the end systems. This 
connection state includes receive and send buffers, congestion-control param-
eters, and sequence and acknowledgment number parameters. We will see in  
Section 3.5 that this state information is needed to implement TCP‚Äôs reliable data 
transfer service and to provide congestion control. UDP, on the other hand, does 
not maintain connection state and does not track any of these parameters. For this 
reason, a server devoted to a particular application can typically support many 
more active clients when the application runs over UDP rather than TCP.
‚Ä¢ Small packet header overhead. The TCP segment has 20 bytes of header over-
head in every segment, whereas UDP has only 8 bytes of overhead.
Figure 3.6 lists popular Internet applications and the transport protocols that 
they use. As we expect, e-mail, remote terminal access, and file transfer run over 
TCP‚Äîall these applications need the reliable data transfer service of TCP. We 
learned in Chapter 2 that early versions of HTTP ran over TCP but that more recent 
versions of HTTP run over UDP, providing their own error control and congestion 
control (among other services) at the application layer. Nevertheless, many important 
applications run over UDP rather than TCP. For example, UDP is used to carry network 
management (SNMP; see Section 5.7) data. UDP is preferred to TCP in this case, 
since network management applications must often run when the network is in a 
stressed state‚Äîprecisely when reliable, congestion-controlled data transfer is diffi-
cult to achieve. Also, as we mentioned earlier, DNS runs over UDP, thereby avoiding 
TCP‚Äôs connection-establishment delays.
As shown in Figure 3.6, both UDP and TCP are sometimes used today with 
multimedia applications, such as Internet phone, real-time video conferencing, and 
streaming of stored audio and video. We just mention now that all of these applica-
tions can tolerate a small amount of packet loss, so that reliable data transfer is not 
absolutely critical for the application‚Äôs success. Furthermore, real-time applications, 
like Internet phone and video conferencing, react very poorly to TCP‚Äôs congestion 
control. For these reasons, developers of multimedia applications may choose to run 
their applications over UDP instead of TCP. When packet loss rates are low, and

with some organizations blocking UDP traffic for security reasons (see Chapter 8), 
TCP becomes an increasingly attractive protocol for streaming media transport.
Although commonly done today, running multimedia applications over UDP 
needs to be done with care. As we mentioned above, UDP has no congestion control. 
But congestion control is needed to prevent the network from entering a congested 
state in which very little useful work is done. If everyone were to start streaming 
high-bit-rate video without using any congestion control, there would be so much 
packet overflow at routers that very few UDP packets would successfully traverse the 
source-to-destination path. Moreover, the high loss rates induced by the uncontrolled 
UDP senders would cause the TCP senders (which, as we‚Äôll see, do decrease their 
sending rates in the face of congestion) to dramatically decrease their rates. Thus, the 
lack of congestion control in UDP can result in high loss rates between a UDP sender 
and receiver, and the crowding out of TCP sessions. Many researchers have proposed 
new mechanisms to force all sources, including UDP sources, to perform adaptive 
congestion control [Mahdavi 1997; Floyd 2000; Kohler 2006: RFC 4340].
Before discussing the UDP segment structure, we mention that it is  possible 
for an application to have reliable data transfer when using UDP. This can be done 
if reliability is built into the application itself (for example, by adding acknowl-
edgment and retransmission mechanisms, such as those we‚Äôll study in the next 
section). We mentioned earlier that the QUIC protocol implements reliability 
in an application-layer protocol on top of UDP. But this is a nontrivial task that 
would keep an application developer busy debugging for a long time. Neverthe-
less,  building reliability directly into the application allows the application to ‚Äúhave 
Electronic mail
Remote terminal access
Secure remote terminal access
Web
File transfer
Remote Ô¨Åle server
Streaming multimedia
Internet telephony
Network management
Name translation
SMTP
Telnet
SSH
HTTP, HTTP/3
FTP
NFS
DASH
typically proprietary
SNMP
DNS
TCP
TCP
TCP
TCP (for HTTP), UDP (for HTTP/3) 
TCP
Typically UDP
TCP
UDP or TCP
Typically UDP
Typically UDP
Application
Application-Layer
Protocol
Underlying Transport
Protocol
Figure 3.6 ‚ô¶  Popular Internet applications and their underlying transport  
protocols

its cake and eat it too.‚Äù That is, application processes can communicate reliably 
without being subjected to the transmission-rate constraints imposed by TCP‚Äôs 
congestion-control mechanism.
3.3.1 UDP Segment Structure
The UDP segment structure, shown in Figure 3.7, is defined in RFC 768. The applica-
tion data occupies the data field of the UDP segment. For example, for DNS, the data 
field contains either a query message or a response message. For a streaming audio 
application, audio samples fill the data field. The UDP header has only four fields, 
each consisting of two bytes. As discussed in the previous section, the port numbers 
allow the destination host to pass the application data to the correct process run-
ning on the destination end system (that is, to perform the demultiplexing function).  
The length field specifies the number of bytes in the UDP segment (header plus 
data). An explicit length value is needed since the size of the data field may differ 
from one UDP segment to the next. The checksum is used by the receiving host to 
check whether errors have been introduced into the segment. In truth, the check-
sum is also calculated over a few of the fields in the IP header in addition to the 
UDP segment. But we ignore this detail in order to see the forest through the trees. 
We‚Äôll discuss the checksum calculation below. Basic principles of error detection are 
described in Section 6.2. The length field specifies the length of the UDP segment, 
including the header, in bytes.
3.3.2 UDP Checksum
The UDP checksum provides for error detection. That is, the checksum is used to 
determine whether bits within the UDP segment have been altered (for example, by 
noise in the links or while stored in a router) as it moved from source to destination. 
Source port #
32 bits
Dest. port #
Length
Checksum
Application
data
(message)
Figure 3.7 ‚ô¶ UDP segment structure

UDP at the sender side performs the 1s complement of the sum of all the 16-bit 
words in the segment, with any overflow encountered during the sum being wrapped 
around. This result is put in the checksum field of the UDP segment. Here we give 
a simple example of the checksum calculation. You can find details about efficient 
implementation of the calculation in RFC 1071 and performance over real data in 
[Stone 1998; Stone 2000]. As an example, suppose that we have the following three 
16-bit words:
0110011001100000
0101010101010101
1000111100001100
The sum of first two of these 16-bit words is
0110011001100000
0101010101010101
1011101110110101
Adding the third word to the above sum gives
1011101110110101
1000111100001100
0100101011000010
Note that this last addition had overflow, which was wrapped around. The 1s 
complement is obtained by converting all the 0s to 1s and converting all the 1s to 
0s. Thus, the 1s complement of the sum 0100101011000010 is 1011010100111101, 
which becomes the checksum. At the receiver, all four 16-bit words are added, 
including the checksum. If no errors are introduced into the packet, then clearly the 
sum at the receiver will be 1111111111111111. If one of the bits is a 0, then we know 
that errors have been introduced into the packet.
You may wonder why UDP provides a checksum in the first place, as many 
link-layer protocols (including the popular Ethernet protocol) also provide error 
checking. The reason is that there is no guarantee that all the links between source 
and destination provide error checking; that is, one of the links may use a link-layer 
protocol that does not provide error checking. Furthermore, even if segments are 
correctly transferred across a link, it‚Äôs possible that bit errors could be introduced 
when a segment is stored in a router‚Äôs memory. Given that neither link-by-link reli-
ability nor in-memory error detection is guaranteed, UDP must provide error detec-
tion at the transport layer, on an end-end basis, if the end-end data transfer service 
is to provide error detection. This is an example of the celebrated end-end principle 
in system design [Saltzer 1984], which states that since certain functionality (error 
detection, in this case) must be implemented on an end-end basis: ‚Äúfunctions placed

at the lower levels may be redundant or of little value when compared to the cost of 
providing them at the higher level.‚Äù
Because IP is supposed to run over just about any layer-2 protocol, it is useful 
for the transport layer to provide error checking as a safety measure. Although UDP 
provides error checking, it does not do anything to recover from an error. Some 
implementations of UDP simply discard the damaged segment; others pass the dam-
aged segment to the application with a warning.
That wraps up our discussion of UDP. We will soon see that TCP offers reli-
able data transfer to its applications as well as other services that UDP doesn‚Äôt offer. 
Naturally, TCP is also more complex than UDP. Before discussing TCP, however, 
it will be useful to step back and first discuss the underlying principles of reliable 
data transfer.
3.4 Principles of Reliable Data Transfer
In this section, we consider the problem of reliable data transfer in a general context. 
This is appropriate since the problem of implementing reliable data transfer occurs 
not only at the transport layer, but also at the link layer and the application layer 
as well. The general problem is thus of central importance to networking. Indeed, 
if one had to identify a ‚Äútop-ten‚Äù list of fundamentally important problems in all 
of networking, this would be a candidate to lead the list. In the next section, we‚Äôll 
examine TCP and show, in particular, that TCP exploits many of the principles that 
we are about to describe.
Figure 3.8 illustrates the framework for our study of reliable data transfer. The 
service abstraction provided to the upper-layer entities is that of a reliable channel 
through which data can be transferred. With a reliable channel, no transferred data 
bits are corrupted (flipped from 0 to 1, or vice versa) or lost, and all are delivered in 
the order in which they were sent. This is precisely the service model offered by TCP 
to the Internet applications that invoke it.
It is the responsibility of a reliable data transfer protocol to implement this 
service abstraction. This task is made difficult by the fact that the layer below the 
reliable data transfer protocol may be unreliable. For example, TCP is a reliable data 
transfer protocol that is implemented on top of an unreliable (IP) end-to-end network 
layer. More generally, the layer beneath the two reliably communicating end points 
might consist of a single physical link (as in the case of a link-level data transfer 
protocol) or a global internetwork (as in the case of a transport-level protocol). For 
our purposes, however, we can view this lower layer simply as an unreliable point-
to-point channel.
In this section, we will incrementally develop the sender and receiver sides of 
a reliable data transfer protocol, considering increasingly complex models of the 
underlying channel. For example, we‚Äôll consider what protocol mechanisms are

needed when the underlying channel can corrupt bits or lose entire packets. One 
assumption we‚Äôll adopt throughout our discussion here is that packets will be deliv-
ered in the order in which they were sent, with some packets possibly being lost; 
that is, the underlying channel will not reorder packets. Figure 3.8(b) illustrates the 
interfaces for our data transfer protocol. The sending side of the data transfer proto-
col will be invoked from above by a call to rdt_send(). It will pass the data to be 
delivered to the upper layer at the receiving side. (Here rdt stands for reliable data 
transfer protocol and _send indicates that the sending side of rdt is being called. 
The first step in developing any protocol is to choose a good name!) On the receiving 
side, rdt_rcv() will be called when a packet arrives from the receiving side of the 
channel. When the rdt protocol wants to deliver data to the upper layer, it will do so 
by calling deliver_data(). In the following, we use the terminology ‚Äúpacket‚Äù 
rather than transport-layer ‚Äúsegment.‚Äù Because the theory developed in this section 
Reliable channel
Unreliable channel
rdt_send()
udt_send()
Sending
process
Receiver
process
deliver_data()
Application
layer
Transport
layer
a. Provided service
Network
layer
Key:
Data
Packet
b. Service implementation
Reliable data
transfer protocol
(sending side)
Reliable data
transfer protocol
(receiving side)
rdt_rcv()
Figure 3.8 ‚ô¶  Reliable data transfer: Service model and service  
implementation

applies to computer networks in general and not just to the Internet transport layer, 
the generic term ‚Äúpacket‚Äù is perhaps more appropriate here.
In this section, we consider only the case of unidirectional data transfer, that is, 
data transfer from the sending to the receiving side. The case of reliable bidirectional 
(that is, full-duplex) data transfer is conceptually no more difficult but considerably 
more tedious to explain. Although we consider only unidirectional data transfer, it is 
important to note that the sending and receiving sides of our protocol will nonetheless 
need to transmit packets in both directions, as indicated in Figure 3.8. We will see 
shortly that, in addition to exchanging packets containing the data to be transferred, 
the sending and receiving sides of rdt will also need to exchange control packets 
back and forth. Both the send and receive sides of rdt send packets to the other side 
by a call to udt_send() (where udt stands for unreliable data transfer).
3.4.1 Building a Reliable Data Transfer Protocol
We now step through a series of protocols, each one becoming more complex, arriv-
ing at a flawless, reliable data transfer protocol.
Reliable Data Transfer over a Perfectly Reliable Channel: rdt1.0
We first consider the simplest case, in which the underlying channel is completely 
reliable. The protocol itself, which we‚Äôll call rdt1.0, is trivial. The finite-state 
machine (FSM) definitions for the rdt1.0 sender and receiver are shown in 
Figure 3.9. The FSM in Figure 3.9(a) defines the operation of the sender, while 
the FSM in Figure 3.9(b) defines the operation of the receiver. It is important to 
note that there are separate FSMs for the sender and for the receiver. The sender 
and receiver FSMs in Figure 3.9 each have just one state. The arrows in the FSM 
description indicate the transition of the protocol from one state to another. (Since 
each FSM in Figure 3.9 has just one state, a transition is necessarily from the one 
state back to itself; we‚Äôll see more complicated state diagrams shortly.) The event 
causing the transition is shown above the horizontal line labeling the transition, and 
the actions taken when the event occurs are shown below the horizontal line. When 
no action is taken on an event, or no event occurs and an action is taken, we‚Äôll use 
the symbol Œõ below or above the horizontal, respectively, to explicitly denote the 
lack of an action or event. The initial state of the FSM is indicated by the dashed 
arrow. Although the FSMs in Figure 3.9 have but one state, the FSMs we will see 
shortly have multiple states, so it will be important to identify the initial state of 
each FSM.
The sending side of rdt simply accepts data from the upper layer via the 
rdt_send(data) event, creates a packet containing the data (via the action 
make_pkt(data)) and sends the packet into the channel. In practice, the  
rdt_send(data) event would result from a procedure call (for example, to 
rdt_send()) by the upper-layer application.

On the receiving side, rdt receives a packet from the underlying channel via 
the rdt_rcv(packet) event, removes the data from the packet (via the action 
extract (packet, data)) and passes the data up to the upper layer (via 
the action deliver_data(data)). In practice, the rdt_rcv(packet) event 
would result from a procedure call (for example, to rdt_rcv()) from the lower-
layer protocol.
In this simple protocol, there is no difference between a unit of data and a packet. 
Also, all packet flow is from the sender to receiver; with a perfectly reliable chan-
nel there is no need for the receiver side to provide any feedback to the sender since 
nothing can go wrong! Note that we have also assumed that the receiver is able to 
receive data as fast as the sender happens to send data. Thus, there is no need for the 
receiver to ask the sender to slow down!
Reliable Data Transfer over a Channel with Bit Errors: rdt2.0
A more realistic model of the underlying channel is one in which bits in a packet may 
be corrupted. Such bit errors typically occur in the physical components of a network 
as a packet is transmitted, propagates, or is buffered. We‚Äôll continue to assume for 
the moment that all transmitted packets are received (although their bits may be cor-
rupted) in the order in which they were sent.
Before developing a protocol for reliably communicating over such a channel, 
first consider how people might deal with such a situation. Consider how you yourself 
Wait for
call from
above
a.  rdt1.0: sending side
rdt_send(data)
packet=make_pkt(data)
udt_send(packet)
Wait for
call from
below
b.  rdt1.0: receiving side
rdt_rcv(packet)
extract(packet,data)
deliver_data(data)
Figure 3.9 ‚ô¶ rdt1.0‚ÄîA protocol for a completely reliable channel

might dictate a long message over the phone. In a typical scenario, the message taker 
might say ‚ÄúOK‚Äù after each sentence has been heard, understood, and recorded. If the 
message taker hears a garbled sentence, you‚Äôre asked to repeat the garbled sentence. 
This message-dictation protocol uses both positive acknowledgments (‚ÄúOK‚Äù) and 
negative acknowledgments (‚ÄúPlease repeat that.‚Äù). These control messages allow 
the receiver to let the sender know what has been received correctly, and what has 
been received in error and thus requires repeating. In a computer network setting, 
reliable data transfer protocols based on such retransmission are known as ARQ 
(Automatic Repeat reQuest) protocols.
Fundamentally, three additional protocol capabilities are required in ARQ pro-
tocols to handle the presence of bit errors:
‚Ä¢ Error detection. First, a mechanism is needed to allow the receiver to detect when 
bit errors have occurred. Recall from the previous section that UDP uses the Inter-
net checksum field for exactly this purpose. In Chapter 6, we‚Äôll examine error-
detection and -correction techniques in greater detail; these techniques allow the 
receiver to detect and possibly correct packet bit errors. For now, we need only 
know that these techniques require that extra bits (beyond the bits of original data 
to be transferred) be sent from the sender to the receiver; these bits will be gath-
ered into the packet checksum field of the rdt2.0 data packet.
‚Ä¢ Receiver feedback. Since the sender and receiver are typically executing on dif-
ferent end systems, possibly separated by thousands of miles, the only way for 
the sender to learn of the receiver‚Äôs view of the world (in this case, whether or not 
a packet was received correctly) is for the receiver to provide explicit feedback 
to the sender. The positive (ACK) and negative (NAK) acknowledgment replies 
in the message-dictation scenario are examples of such feedback. Our rdt2.0 
protocol will similarly send ACK and NAK packets back from the receiver to 
the sender. In principle, these packets need only be one bit long; for example, a 0 
value could indicate a NAK and a value of 1 could indicate an ACK.
‚Ä¢ Retransmission. A packet that is received in error at the receiver will be retrans-
mitted by the sender.
Figure 3.10 shows the FSM representation of rdt2.0, a data transfer 
protocol employing error detection, positive acknowledgments, and negative 
acknowledgments.
The send side of rdt2.0 has two states. In the leftmost state, the send-side 
protocol is waiting for data to be passed down from the upper layer. When the 
rdt_send(data) event occurs, the sender will create a packet (sndpkt) con-
taining the data to be sent, along with a packet checksum (for example, as discussed 
in Section 3.3.2 for the case of a UDP segment), and then send the packet via the 
udt_send(sndpkt) operation. In the rightmost state, the sender protocol is wait-
ing for an ACK or a NAK packet from the receiver. If an ACK packet is received

(the notation rdt_rcv(rcvpkt) && isACK(rcvpkt) in Figure 3.10 cor-
responds to this event), the sender knows that the most recently transmitted packet 
has been received correctly and thus the protocol returns to the state of waiting for 
data from the upper layer. If a NAK is received, the protocol retransmits the last 
packet and waits for an ACK or NAK to be returned by the receiver in response to 
the retransmitted data packet. It is important to note that when the sender is in the 
wait-for-ACK-or-NAK state, it cannot get more data from the upper layer; that is, the 
rdt_send() event can not occur; that will happen only after the sender receives 
an ACK and leaves this state. Thus, the sender will not send a new piece of data until 
it is sure that the receiver has correctly received the current packet. Because of this 
behavior, protocols such as rdt2.0 are known as stop-and-wait protocols.
Wait for
call from
above
a.  rdt2.0: sending side
b.  rdt2.0: receiving side
rdt_rcv(rcvpkt) && corrupt(rcvpkt)
sndpkt=make_pkt(NAK)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && isNAK(rcvpkt)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && isACK(rcvpkt)
L
rdt_send(data)
sndpkt=make_pkt(data,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && notcorrupt(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
sndpkt=make_pkt(ACK)
udt_send(sndpkt)
Wait for
call from
below
Wait for
ACK or
NAK
Figure 3.10 ‚ô¶ rdt2.0‚ÄîA protocol for a channel with bit errors

The receiver-side FSM for rdt2.0 still has a single state. On packet arrival, 
the receiver replies with either an ACK or a NAK, depending on whether or not the 
received packet is corrupted. In Figure 3.10, the notation rdt_rcv(rcvpkt) && 
corrupt(rcvpkt) corresponds to the event in which a packet is received and is 
found to be in error.
Protocol rdt2.0 may look as if it works but, unfortunately, it has a fatal flaw. 
In particular, we haven‚Äôt accounted for the possibility that the ACK or NAK packet 
could be corrupted! (Before proceeding on, you should think about how this prob-
lem may be fixed.) Unfortunately, our slight oversight is not as innocuous as it may 
seem. Minimally, we will need to add checksum bits to ACK/NAK packets in order 
to detect such errors. The more difficult question is how the protocol should recover 
from errors in ACK or NAK packets. The difficulty here is that if an ACK or NAK 
is corrupted, the sender has no way of knowing whether or not the receiver has cor-
rectly received the last piece of transmitted data.
Consider three possibilities for handling corrupted ACKs or NAKs:
‚Ä¢ For the first possibility, consider what a human might do in the message-dictation 
scenario. If the speaker didn‚Äôt understand the ‚ÄúOK‚Äù or ‚ÄúPlease repeat that‚Äù reply 
from the receiver, the speaker would probably ask, ‚ÄúWhat did you say?‚Äù (thus 
introducing a new type of sender-to-receiver packet to our protocol). The receiver 
would then repeat the reply. But what if the speaker‚Äôs ‚ÄúWhat did you say?‚Äù is cor-
rupted? The receiver, having no idea whether the garbled sentence was part of the 
dictation or a request to repeat the last reply, would probably then respond with 
‚ÄúWhat did you say?‚Äù And then, of course, that response might be garbled. Clearly, 
we‚Äôre heading down a difficult path.
‚Ä¢ A second alternative is to add enough checksum bits to allow the sender not only 
to detect, but also to recover from, bit errors. This solves the immediate problem 
for a channel that can corrupt packets but not lose them.
‚Ä¢ A third approach is for the sender simply to resend the current data packet when 
it receives a garbled ACK or NAK packet. This approach, however, introduces 
duplicate packets into the sender-to-receiver channel. The fundamental diffi-
culty with duplicate packets is that the receiver doesn‚Äôt know whether the ACK 
or NAK it last sent was received correctly at the sender. Thus, it cannot know a 
priori whether an arriving packet contains new data or is a retransmission!
A simple solution to this new problem (and one adopted in almost all exist-
ing data transfer protocols, including TCP) is to add a new field to the data packet 
and have the sender number its data packets by putting a sequence number into 
this field. The receiver then need only check this sequence number to determine 
whether or not the received packet is a retransmission. For this simple case of a 
stop-and-wait protocol, a 1-bit sequence number will suffice, since it will allow the 
receiver to know whether the sender is resending the previously transmitted packet

(the sequence number of the received packet has the same sequence number as the 
most recently received packet) or a new packet (the sequence number changes, mov-
ing ‚Äúforward‚Äù in modulo-2 arithmetic). Since we are currently assuming a channel 
that does not lose packets, ACK and NAK packets do not themselves need to indicate 
the sequence number of the packet they are acknowledging. The sender knows that a 
received ACK or NAK packet (whether garbled or not) was generated in response to 
its most recently transmitted data packet.
Figures 3.11 and 3.12 show the FSM description for rdt2.1, our fixed version 
of rdt2.0. The rdt2.1 sender and receiver FSMs each now have twice as many 
states as before. This is because the protocol state must now reflect whether the 
packet currently being sent (by the sender) or expected (at the receiver) should have a 
sequence number of 0 or 1. Note that the actions in those states where a 0-numbered 
packet is being sent or expected are mirror images of those where a 1-numbered 
packet is being sent or expected; the only differences have to do with the handling 
of the sequence number.
Protocol rdt2.1 uses both positive and negative acknowledgments from the 
receiver to the sender. When an out-of-order packet is received, the receiver sends 
a positive acknowledgment for the packet it has received. When a corrupted packet 
Wait for
call 0 from
above
rdt_rcv(rcvpkt)&&
(corrupt(rcvpkt)||
isNAK(rcvpkt))
udt_send(sndpkt)
rdt_rcv(rcvpkt)&&
(corrupt(rcvpkt)||
isNAK(rcvpkt))
udt_send(sndpkt)
rdt_rcv(rcvpkt)
&& notcorrupt(rcvpkt)
&& isACK(rcvpkt)
rdt_rcv(rcvpkt)
&& notcorrupt(rcvpkt)
&& isACK(rcvpkt)
L
L
rdt_send(data)
sndpkt=make_pkt(0,data,checksum)
udt_send(sndpkt)
rdt_send(data)
sndpkt=make_pkt(1,data,checksum)
udt_send(sndpkt)
Wait for
ACK or
NAK 0
Wait for
ACK or
NAK 1
Wait for
call 1 from
above
Figure 3.11 ‚ô¶ rdt2.1 sender

is received, the receiver sends a negative acknowledgment. We can accomplish the 
same effect as a NAK if, instead of sending a NAK, we send an ACK for the last 
correctly received packet. A sender that receives two ACKs for the same packet (that 
is, receives duplicate ACKs) knows that the receiver did not correctly receive the 
packet following the packet that is being ACKed twice. Our NAK-free reliable data 
transfer protocol for a channel with bit errors is rdt2.2, shown in Figures 3.13 and 
3.14. One subtle change between rtdt2.1 and rdt2.2 is that the receiver must 
now include the sequence number of the packet being acknowledged by an ACK 
message (this is done by including the ACK, 0 or ACK, 1 argument in make_pkt() 
in the receiver FSM), and the sender must now check the sequence number of the 
packet being acknowledged by a received ACK message (this is done by including 
the 0 or 1 argument in isACK() in the sender FSM).
Reliable Data Transfer over a Lossy Channel with Bit Errors: rdt3.0
Suppose now that in addition to corrupting bits, the underlying channel can lose 
packets as well, a not-uncommon event in today‚Äôs computer networks (including 
the Internet). Two additional concerns must now be addressed by the protocol: how 
to detect packet loss and what to do when packet loss occurs. The use of check-
summing, sequence numbers, ACK packets, and retransmissions‚Äîthe techniques 
rdt_rcv(rcvpkt)&& notcorrupt
(rcvpkt)&& has_seq0(rcvpkt)
sndpkt=make_pkt(ACK,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && corrupt(rcvpkt)
sndpkt=make_pkt(NAK,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt)
 && corrupt(rcvpkt)
sndpkt=make_pkt(NAK,checksum)
udt_send(sndpkt)
sndpkt=make_pkt(ACK,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && notcorrupt(rcvpkt)
&& has_seq1(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
sndpkt=make_pkt(ACK,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt)&& notcorrupt(rcvpkt)
 && has_seq0(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
sndpkt=make_pkt(ACK,checksum)
udt_send(sndpkt)
Wait for
0 from
below
Wait for
1 from
below
rdt_rcv(rcvpkt)&& notcorrupt
(rcvpkt)&& has_seq1(rcvpkt)
Figure 3.12 ‚ô¶ rdt2.1 receiver

Wait for
call 0 from
above
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
isACK(rcvpkt,1))
udt_send(sndpkt)
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
isACK(rcvpkt,0))
udt_send(sndpkt)
rdt_rcv(rcvpkt)
&& notcorrupt(rcvpkt)
&& isACK(rcvpkt,0)
rdt_rcv(rcvpkt)
&& notcorrupt(rcvpkt)
&& isACK(rcvpkt,1)
rdt_send(data)
sndpkt=make_pkt(0,data,checksum)
udt_send(sndpkt)
rdt_send(data)
sndpkt=make_pkt(1,data,checksum)
udt_send(sndpkt)
Wait for
ACK 0
Wait for
ACK 1
L
L
Wait for
call 1 from
above
Figure 3.13 ‚ô¶ rdt2.2 sender
already developed in rdt2.2‚Äîwill allow us to answer the latter concern. Handling 
the first concern will require adding a new protocol mechanism.
There are many possible approaches toward dealing with packet loss (several 
more of which are explored in the exercises at the end of the chapter). Here, we‚Äôll 
put the burden of detecting and recovering from lost packets on the sender. Suppose 
that the sender transmits a data packet and either that packet, or the receiver‚Äôs ACK 
of that packet, gets lost. In either case, no reply is forthcoming at the sender from the 
receiver. If the sender is willing to wait long enough so that it is certain that a packet 
has been lost, it can simply retransmit the data packet. You should convince yourself 
that this protocol does indeed work.
But how long must the sender wait to be certain that something has been lost? 
The sender must clearly wait at least as long as a round-trip delay between the sender 
and receiver (which may include buffering at intermediate routers) plus whatever 
amount of time is needed to process a packet at the receiver. In many networks, this 
worst-case maximum delay is very difficult even to estimate, much less know with 
certainty. Moreover, the protocol should ideally recover from packet loss as soon as 
possible; waiting for a worst-case delay could mean a long wait until error recovery

is initiated. The approach thus adopted in practice is for the sender to judiciously 
choose a time value such that packet loss is likely, although not guaranteed, to have 
happened. If an ACK is not received within this time, the packet is retransmitted. 
Note that if a packet experiences a particularly large delay, the sender may retrans-
mit the packet even though neither the data packet nor its ACK have been lost. This 
introduces the possibility of duplicate data packets in the sender-to-receiver chan-
nel. Happily, protocol rdt2.2 already has enough functionality (that is, sequence 
numbers) to handle the case of duplicate packets.
From the sender‚Äôs viewpoint, retransmission is a panacea. The sender does not 
know whether a data packet was lost, an ACK was lost, or if the packet or ACK was 
simply overly delayed. In all cases, the action is the same: retransmit. Implement-
ing a time-based retransmission mechanism requires a countdown timer that can 
interrupt the sender after a given amount of time has expired. The sender will thus 
need to be able to (1) start the timer each time a packet (either a first-time packet or 
a retransmission) is sent, (2) respond to a timer interrupt (taking appropriate actions), 
and (3) stop the timer.
Figure 3.15 shows the sender FSM for rdt3.0, a protocol that reliably transfers 
data over a channel that can corrupt or lose packets; in the homework problems, you‚Äôll 
be asked to provide the receiver FSM for rdt3.0. Figure 3.16 shows how the pro-
tocol operates with no lost or delayed packets and how it handles lost data packets. In 
Figure 3.16, time moves forward from the top of the diagram toward the bottom of the 
Wait for
0 from
below
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
has_seq0(rcvpkt))
sndpkt=make_pkt(ACK,0,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
has_seq1(rcvpkt))
sndpkt=make_pkt(ACK,1,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && notcorrupt(rcvpkt)
&& has_seq1(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
sndpkt=make_pkt(ACK,1,checksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && notcorrupt(rcvpkt)
&& has_seq0(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
sndpkt=make_pkt(ACK,0,checksum)
udt_send(sndpkt)
Wait for
1 from
below
Figure 3.14 ‚ô¶ rdt2.2 receiver

diagram; note that a receive time for a packet is necessarily later than the send time for 
a packet as a result of transmission and propagation delays. In Figures 3.16(b)‚Äì(d), the 
send-side brackets indicate the times at which a timer is set and later times out. Sev-
eral of the more subtle aspects of this protocol are explored in the exercises at the end 
of this chapter. Because packet sequence numbers alternate between 0 and 1, protocol 
rdt3.0 is sometimes known as the alternating-bit protocol.
We have now assembled the key elements of a data transfer protocol. Check-
sums, sequence numbers, timers, and positive and negative acknowledgment packets 
each play a crucial and necessary role in the operation of the protocol. We now have 
a working reliable data transfer protocol!
3.4.2 Pipelined Reliable Data Transfer Protocols
Protocol rdt3.0 is a functionally correct protocol, but it is unlikely that anyone 
would be happy with its performance, particularly in today‚Äôs high-speed networks. 
At the heart of rdt3.0‚Äôs performance problem is the fact that it is a stop-and-wait 
protocol.
Wait for
call 0 from
above
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
isACK(rcvpkt,1))
timeout
udt_send(sndpkt)
start_timer
rdt_rcv(rcvpkt)
L
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
isACK(rcvpkt,0))
rdt_rcv(rcvpkt)
&& notcorrupt(rcvpkt)
&& isACK(rcvpkt,0)
stop_timer
rdt_rcv(rcvpkt)
&& notcorrupt(rcvpkt)
&& isACK(rcvpkt,1)
stop_timer
timeout
udt_send(sndpkt)
start_timer
rdt_send(data)
sndpkt=make_pkt(0,data,checksum)
udt_send(sndpkt)
start_timer
rdt_send(data)
sndpkt=make_pkt(1,data,checksum)
udt_send(sndpkt)
start_timer
Wait for
ACK 0
Wait for
ACK 1
L
L
Wait for
call 1 from
above
rdt_rcv(rcvpkt)
L
Figure 3.15 ‚ô¶ rdt3.0 sender

rcv pkt0
send ACK0
rcv pkt1
send ACK1
rcv pkt0
send ACK0
Sender
Receiver
a. Operation with no loss
pkt0
ACK0
pkt1
pkt0
ACK1
ACK0
(loss) X
b. Lost packet
rcv pkt0
send ACK0
rcv pkt1
send ACK1
c. Lost ACK
send pkt0
rcv ACK0
send pkt1
rcv ACK1
send pkt0
send pkt0
rcv ACK0
send pkt1
timeout
resend pkt1
rcv ACK1
send pkt0
rcv pkt0
send ACK0
rcv pkt1
(detect
duplicate)
send ACK1
send pkt0
rcv ACK0
send pkt1
rcv pkt0
send ACK0
timeout
resend pkt1
rcv pkt1
send ACK1
d. Premature timeout
rcv ACK1
send pkt0
rcv ACK1
do nothing
rcv pkt0
send ACK0
rcv pkt 1
(detect duplicate)
send ACK1
Sender
Receiver
Receiver
Sender
pkt0
ACK0
pkt1
ACK1
ACK1
ACK0
ACK1
ACK0
pkt1
pkt0
pkt0
pkt1
pkt1
pkt0
ACK1
ACK0
X (loss)
pkt1
rcv pkt0
send ACK0
send pkt0
rcv ACK0
send pkt1
timeout
resend pkt1
rcv ACK1
send pkt0
rcv pkt0
send ACK0
rcv pkt1
send ACK1
Sender
Receiver
pkt0
ACK0
pkt1
pkt0
ACK1
ACK0
Figure 3.16 ‚ô¶ Operation of rdt3.0, the alternating-bit protocol

To appreciate the performance impact of this stop-and-wait behavior, consider 
an idealized case of two hosts, one located on the West Coast of the United States 
and the other located on the East Coast, as shown in Figure 3.17. The speed-of-light 
round-trip propagation delay between these two end systems, RTT, is approximately 
30 milliseconds. Suppose that they are connected by a channel with a transmission 
rate, R, of 1 Gbps (109 bits per second). With a packet size, L, of 1,000 bytes (8,000 
bits) per packet, including both header fields and data, the time needed to actually 
transmit the packet into the 1 Gbps link is
dtrans = L
R =
8000 bits
109 bits/sec = 8 microseconds
Figure 3.18(a) shows that with our stop-and-wait protocol, if the sender begins 
sending the packet at t = 0, then at t = L/R = 8 microseconds, the last bit enters 
the channel at the sender side. The packet then makes its 15-msec cross-country jour-
ney, with the last bit of the packet emerging at the receiver at t = RTT/2 + L/R =
15.008 msec. Assuming for simplicity that ACK packets are extremely small (so that 
we can ignore their transmission time) and that the receiver can send an ACK as soon 
as the last bit of a data packet is received, the ACK emerges back at the sender at 
t = RTT + L/R = 30.008 msec. At this point, the sender can now transmit the next 
message. Thus, in 30.008 msec, the sender was sending for only 0.008 msec. If we 
define the utilization of the sender (or the channel) as the fraction of time the sender 
is actually busy sending bits into the channel, the analysis in Figure 3.18(a) shows 
that the stop-and-wait protocol has a rather dismal sender utilization, Usender, of
Usender =
L>R
RTT + L>R =
.008
30.008 = 0.00027
Data packets
Data packet
ACK packets
a. A stop-and-wait protocol in operation
b. A pipelined protocol in operation
Figure 3.17 ‚ô¶ Stop-and-wait versus pipelined protocol

First bit of Ô¨Årst packet
transmitted, t = 0
Last bit of Ô¨Årst packet
transmitted, t = L/R
First bit of Ô¨Årst packet
transmitted, t = 0
Last bit of Ô¨Årst packet
transmitted, t = L/R
ACK arrives, send next packet,
t = RTT + L/R
a. Stop-and-wait operation
Sender
Receiver
RTT
First bit of Ô¨Årst packet arrives
Last bit of Ô¨Årst packet arrives, send ACK
First bit of Ô¨Årst packet arrives
Last bit of Ô¨Årst packet arrives, send ACK
ACK arrives, send next packet,
t = RTT + L/R
b. Pipelined operation
Sender
Receiver
RTT
Last bit of 2nd packet arrives, send ACK
Last bit of 3rd packet arrives, send ACK
Figure 3.18 ‚ô¶ Stop-and-wait and pipelined sending

That is, the sender was busy only 2.7 hundredths of one percent of the time! 
Viewed another way, the sender was able to send only 1,000 bytes in 30.008 mil-
liseconds, an effective throughput of only 267 kbps‚Äîeven though a 1 Gbps link 
was available! Imagine the unhappy network manager who just paid a fortune for 
a gigabit capacity link but manages to get a throughput of only 267 kilobits per 
second! This is a graphic example of how network protocols can limit the capabili-
ties provided by the underlying network hardware. Also, we have neglected lower- 
layer protocol-processing times at the sender and receiver, as well as the process-
ing and queuing delays that would occur at any intermediate routers between the 
sender and receiver. Including these effects would serve only to further increase the  
delay and further accentuate the poor performance.
The solution to this particular performance problem is simple: Rather than oper-
ate in a stop-and-wait manner, the sender is allowed to send multiple packets with-
out waiting for acknowledgments, as illustrated in Figure 3.17(b). Figure 3.18(b) 
shows that if the sender is allowed to transmit three packets before having to wait for 
acknowledgments, the utilization of the sender is essentially tripled. Since the many 
in-transit sender-to-receiver packets can be visualized as filling a pipeline, this tech-
nique is known as pipelining. Pipelining has the following consequences for reliable 
data transfer protocols:
‚Ä¢ The range of sequence numbers must be increased, since each in-transit packet 
(not counting retransmissions) must have a unique sequence number and there 
may be multiple, in-transit, unacknowledged packets.
‚Ä¢ The sender and receiver sides of the protocols may have to buffer more than one 
packet. Minimally, the sender will have to buffer packets that have been transmit-
ted but not yet acknowledged. Buffering of correctly received packets may also 
be needed at the receiver, as discussed below.
‚Ä¢ The range of sequence numbers needed and the buffering requirements will 
depend on the manner in which a data transfer protocol responds to lost, cor-
rupted, and overly delayed packets. Two basic approaches toward pipelined error 
recovery can be identified: Go-Back-N and selective repeat.
3.4.3 Go-Back-N (GBN)
In a Go-Back-N (GBN) protocol, the sender is allowed to transmit multiple packets 
(when available) without waiting for an acknowledgment, but is constrained to have 
no more than some maximum allowable number, N, of unacknowledged packets in 
the pipeline. We describe the GBN protocol in some detail in this section. But before 
reading on, you are encouraged to play with the GBN animation (an awesome inter-
active animation) at the companion Web site.
Figure 3.19 shows the sender‚Äôs view of the range of sequence numbers in a GBN 
protocol. If we define base to be the sequence number of the oldest unacknowledged

packet and nextseqnum to be the smallest unused sequence number (that is, the 
sequence number of the next packet to be sent), then four intervals in the range of 
sequence numbers can be identified. Sequence numbers in the interval [0,base-1] 
correspond to packets that have already been transmitted and acknowledged. The inter-
val [base,nextseqnum-1] corresponds to packets that have been sent but not 
yet acknowledged. Sequence numbers in the interval [nextseqnum,base+N-1] 
can be used for packets that can be sent immediately, should data arrive from the 
upper layer. Finally, sequence numbers greater than or equal to base+N cannot 
be used until an unacknowledged packet currently in the pipeline (specifically, the 
packet with sequence number base) has been acknowledged.
As suggested by Figure 3.19, the range of permissible sequence numbers for 
transmitted but not yet acknowledged packets can be viewed as a window of size N 
over the range of sequence numbers. As the protocol operates, this window slides 
forward over the sequence number space. For this reason, N is often referred to as the 
window size and the GBN protocol itself as a sliding-window protocol. You might 
be wondering why we would even limit the number of outstanding, unacknowledged 
packets to a value of N in the first place. Why not allow an unlimited number of such 
packets? We‚Äôll see in Section 3.5 that flow control is one reason to impose a limit 
on the sender. We‚Äôll examine another reason to do so in Section 3.7, when we study 
TCP congestion control.
In practice, a packet‚Äôs sequence number is carried in a fixed-length field in the 
packet header. If k is the number of bits in the packet sequence number field, the 
range of sequence numbers is thus [0,2k - 1]. With a finite range of sequence num-
bers, all arithmetic involving sequence numbers must then be done using modulo 2k 
arithmetic. (That is, the sequence number space can be thought of as a ring of size 
2k, where sequence number 2k - 1 is immediately followed by sequence number 0.) 
Recall that rdt3.0 had a 1-bit sequence number and a range of sequence numbers 
of [0,1]. Several of the problems at the end of this chapter explore the consequences 
of a finite range of sequence numbers. We will see in Section 3.5 that TCP has a 
32-bit sequence number field, where TCP sequence numbers count bytes in the byte 
stream rather than packets.
Figures 3.20 and 3.21 give an extended FSM description of the sender and 
receiver sides of an ACK-based, NAK-free, GBN protocol. We refer to this FSM 
base
nextseqnum
Window size
N
Key:
Already
ACK‚Äôd
Sent, not
yet ACK‚Äôd
Usable,
not yet sent
Not usable
Figure 3.19 ‚ô¶ Sender‚Äôs view of sequence numbers in Go-Back-N

rdt_send(data)
if(nextseqnum<base+N){
   sndpkt[nextseqnum]=make_pkt(nextseqnum,data,checksum)
   udt_send(sndpkt[nextseqnum])
   if(base==nextseqnum)
      start_timer
   nextseqnum++
   }
else
   refuse_data(data)
L
rdt_rcv(rcvpkt) && notcorrupt(rcvpkt)
base=getacknum(rcvpkt)+1
If(base==nextseqnum)
   stop_timer
else
   start_timer
rdt_rcv(rcvpkt) && corrupt(rcvpkt)
L
base=1
nextseqnum=1
timeout
start_timer
udt_send(sndpkt[base])
udt_send(sndpkt[base+1])
...
udt_send(sndpkt[nextseqnum-1])
Wait
Figure 3.20 ‚ô¶ Extended FSM description of the GBN sender
rdt_rcv(rcvpkt)
  && notcorrupt(rcvpkt)
  && hasseqnum(rcvpkt,expectedseqnum)
extract(rcvpkt,data)
deliver_data(data)
sndpkt=make_pkt(expectedseqnum,ACK,checksum)
udt_send(sndpkt)
expectedseqnum++
L
expectedseqnum=1
sndpkt=make_pkt(0,ACK,checksum)
default
udt_send(sndpkt)
Wait
Figure 3.21 ‚ô¶ Extended FSM description of the GBN receiver

description as an extended FSM because we have added variables (similar to 
programming-language variables) for base and nextseqnum, and added opera-
tions on these variables and conditional actions involving these variables. Note that 
the extended FSM specification is now beginning to look somewhat like a program-
ming-language specification. [Bochman 1984] provides an excellent survey of addi-
tional extensions to FSM techniques as well as other programming-language-based 
techniques for specifying protocols.
The GBN sender must respond to three types of events:
‚Ä¢ Invocation from above. When rdt_send() is called from above, the sender 
first checks to see if the window is full, that is, whether there are N outstand-
ing, unacknowledged packets. If the window is not full, a packet is created and 
sent, and variables are appropriately updated. If the window is full, the sender 
simply returns the data back to the upper layer, an implicit indication that the 
window is full. The upper layer would presumably then have to try again later. 
In a real implementation, the sender would more likely have either buffered (but 
not immediately sent) this data, or would have a synchronization mechanism 
(for example, a semaphore or a flag) that would allow the upper layer to call  
rdt_send() only when the window is not full.
‚Ä¢ Receipt of an ACK. In our GBN protocol, an acknowledgment for a packet with 
sequence number n will be taken to be a cumulative acknowledgment, indicat-
ing that all packets with a sequence number up to and including n have been 
correctly received at the receiver. We‚Äôll come back to this issue shortly when we 
examine the receiver side of GBN.
‚Ä¢ A timeout event. The protocol‚Äôs name, ‚ÄúGo-Back-N,‚Äù is derived from the sender‚Äôs 
behavior in the presence of lost or overly delayed packets. As in the stop-and-wait 
protocol, a timer will again be used to recover from lost data or acknowledgment 
packets. If a timeout occurs, the sender resends all packets that have been previ-
ously sent but that have not yet been acknowledged. Our sender in Figure 3.20 
uses only a single timer, which can be thought of as a timer for the oldest trans-
mitted but not yet acknowledged packet. If an ACK is received but there are still 
additional transmitted but not yet acknowledged packets, the timer is restarted. If 
there are no outstanding, unacknowledged packets, the timer is stopped.
The receiver‚Äôs actions in GBN are also simple. If a packet with sequence number 
n is received correctly and is in order (that is, the data last delivered to the upper layer 
came from a packet with sequence number n - 1), the receiver sends an ACK for 
packet n and delivers the data portion of the packet to the upper layer. In all other 
cases, the receiver discards the packet and resends an ACK for the most recently 
received in-order packet. Note that since packets are delivered one at a time to the 
upper layer, if packet k has been received and delivered, then all packets with a

sequence number lower than k have also been delivered. Thus, the use of cumulative 
acknowledgments is a natural choice for GBN.
In our GBN protocol, the receiver discards out-of-order packets. Although 
it may seem silly and wasteful to discard a correctly received (but out-of-order) 
packet, there is some justification for doing so. Recall that the receiver must 
deliver data in order to the upper layer. Suppose now that packet n is expected, but 
packet n + 1 arrives. Because data must be delivered in order, the receiver could 
buffer (save) packet n + 1 and then deliver this packet to the upper layer after it 
had later received and delivered packet n. However, if packet n is lost, both it and 
packet n + 1 will eventually be retransmitted as a result of the GBN retransmis-
sion rule at the sender. Thus, the receiver can simply discard packet n + 1. The 
advantage of this approach is the simplicity of receiver buffering‚Äîthe receiver 
need not buffer any out-of-order packets. Thus, while the sender must maintain 
the upper and lower bounds of its window and the position of nextseqnum 
within this window, the only piece of information the receiver need maintain is 
the sequence number of the next in-order packet. This value is held in the variable 
expectedseqnum, shown in the receiver FSM in Figure 3.21. Of course, the 
disadvantage of throwing away a correctly received packet is that the subsequent 
retransmission of that packet might be lost or garbled and thus even more retrans-
missions would be required.
Figure 3.22 shows the operation of the GBN protocol for the case of a window 
size of four packets. Because of this window size limitation, the sender sends pack-
ets 0 through 3 but then must wait for one or more of these packets to be acknowl-
edged before proceeding. As each successive ACK (for example, ACK0 and ACK1) 
is received, the window slides forward and the sender can transmit one new packet 
(pkt4 and pkt5, respectively). On the receiver side, packet 2 is lost and thus packets 
3, 4, and 5 are found to be out of order and are discarded.
Before closing our discussion of GBN, it is worth noting that an implementa-
tion of this protocol in a protocol stack would likely have a structure similar to that 
of the extended FSM in Figure 3.20. The implementation would also likely be in 
the form of various procedures that implement the actions to be taken in response to 
the various events that can occur. In such event-based programming, the various 
procedures are called (invoked) either by other procedures in the protocol stack, or 
as the result of an interrupt. In the sender, these events would be (1) a call from the 
upper-layer entity to invoke rdt_send(), (2) a timer interrupt, and (3) a call from 
the lower layer to invoke rdt_rcv() when a packet arrives. The programming 
exercises at the end of this chapter will give you a chance to actually implement these 
routines in a simulated, but realistic, network setting.
We note here that the GBN protocol incorporates almost all of the techniques 
that we will encounter when we study the reliable data transfer components of TCP 
in Section 3.5. These techniques include the use of sequence numbers, cumulative 
acknowledgments, checksums, and a timeout/retransmit operation.

3.4.4 Selective Repeat (SR)
The GBN protocol allows the sender to potentially ‚Äúfill the pipeline‚Äù in Figure 3.17 
with packets, thus avoiding the channel utilization problems we noted with stop-
and-wait protocols. There are, however, scenarios in which GBN itself suffers from 
performance problems. In particular, when the window size and bandwidth-delay 
product are both large, many packets can be in the pipeline. A single packet error 
can thus cause GBN to retransmit a large number of packets, many unnecessarily. 
As the probability of channel errors increases, the pipeline can become filled with 
these unnecessary retransmissions. Imagine, in our message-dictation scenario, that 
Sender
Receiver
 send pkt0
 send pkt1
 send pkt2
send pkt3
  
(wait)
 rcv ACK0
send pkt4
 rcv ACK1
send pkt5
send pkt2
send pkt3
send pkt4
send pkt5
pkt2 timeout
rcv pkt0
send ACK0
rcv pkt1
send ACK1
rcv pkt3, discard
send ACK1
rcv pkt4, discard
send ACK1
rcv pkt5, discard
send ACK1
rcv pkt2, deliver
send ACK2
rcv pkt3, deliver
send ACK3
X
(loss)
Figure 3.22 ‚ô¶ Go-Back-N in operation

if every time a word was garbled, the surrounding 1,000 words (for example, a win-
dow size of 1,000 words) had to be repeated. The dictation would be slowed by all 
of the reiterated words.
As the name suggests, selective-repeat protocols avoid unnecessary retrans-
missions by having the sender retransmit only those packets that it suspects were 
received in error (that is, were lost or corrupted) at the receiver. This individual, 
as-needed, retransmission will require that the receiver individually acknowledge 
correctly received packets. A window size of N will again be used to limit the num-
ber of outstanding, unacknowledged packets in the pipeline. However, unlike GBN, 
the sender will have already received ACKs for some of the packets in the window. 
Figure 3.23 shows the SR sender‚Äôs view of the sequence number space. Figure 3.24 
details the various actions taken by the SR sender.
The SR receiver will acknowledge a correctly received packet whether or not it is 
in order. Out-of-order packets are buffered until any missing packets (that is, packets 
with lower sequence numbers) are received, at which point a batch of packets can be 
delivered in order to the upper layer. Figure 3.25 itemizes the various actions taken by 
the SR receiver. Figure 3.26 shows an example of SR operation in the presence of lost 
packets. Note that in Figure 3.26, the receiver initially buffers packets 3, 4, and 5, and 
delivers them together with packet 2 to the upper layer when packet 2 is finally received.
send_base
nextseqnum
Window size
N
Key:
Key:
Already
ACK‚Äôd
Sent, not
yet ACK‚Äôd
Usable,
not yet sent
Not usable
Out of order
(buffered) but
already ACK‚Äôd
Expected, not
yet received
Acceptable
(within
window)
Not usable
a. Sender view of sequence numbers
b. Receiver view of sequence numbers
rcv_base
Window size
N
Figure 3.23 ‚ô¶  Selective-repeat (SR) sender and receiver views  
of sequence-number space

It is important to note that in Step 2 in Figure 3.25, the receiver reacknowledges 
(rather than ignores) already received packets with certain sequence numbers below 
the current window base. You should convince yourself that this reacknowledgment 
is indeed needed. Given the sender and receiver sequence number spaces in Fig-
ure¬†3.23, for example, if there is no ACK for packet send_base propagating from 
1. Data received from above. When data is received from above, the SR sender
checks the next available sequence number for the packet. If the sequence
number is within the sender‚Äôs window, the data is packetized and sent; other-
wise it is either bufered or returned to the upper layer for later transmission, 
as in GBN.
2. Timeout. Timers are again used to protect against lost packets. However, each
packet must now have its own logical timer, since only a single packet will 
be transmitted on timeout. A single hardware timer can be used to mimic the
operation of multiple logical timers [Varghese 1997].
3. ACK received. If an ACK is received, the SR sender marks that packet as 
having been received, provided it is in the window. If the packet‚Äôs sequence
number is equal to send_base, the window base is moved forward to the 
unacknowledged packet with the smallest sequence number. If the window
moves and there are untransmitted packets with sequence numbers that now
fall within the window, these packets are transmitted.
Figure 3.24 ‚ô¶ SR sender events and actions
1. Packet with sequence number in [rcv_base, rcv_base+N-1]is cor-
rectly received. In this case, the received packet falls within the receiver‚Äôs win-
dow and a selective ACK packet is returned to the sender. If the packet was not
previously received, it is bufered. If this packet has a sequence number equal to
the base of the receive window (rcv_base in Figure 3.22), then this packet,
and any previously bufered and consecutively numbered (beginning with
rcv_base) packets are delivered to the upper layer. The receive window is
then moved forward by the number of packets delivered to the upper layer. As
an example, consider Figure 3.26. When a packet with a sequence number of
rcv_base=2 is received, it and packets 3, 4, and 5 can be delivered to the
upper layer.
2. Packet with sequence number in [rcv_base-N, rcv_base-1]is cor-
rectly received. In this case, an ACK must be generated, even though this is a
packet that the receiver has previously acknowledged.
3. Otherwise. Ignore the packet.
Figure 3.25 ‚ô¶ SR receiver events and actions

the receiver to the sender, the sender will eventually retransmit packet send_base, 
even though it is clear (to us, not the sender!) that the receiver has already received 
that packet. If the receiver were not to acknowledge this packet, the sender‚Äôs win-
dow would never move forward! This example illustrates an important aspect of 
SR protocols (and many other protocols as well). The sender and receiver will not 
always have an identical view of what has been received correctly and what has not. 
For SR protocols, this means that the sender and receiver windows will not always 
coincide.
pkt0 rcvd, delivered, ACK0 sent
0 1 2 3 4 5 6 7 8 9
pkt1 rcvd, delivered, ACK1 sent
0 1 2 3 4 5 6 7 8 9
pkt3 rcvd, bufered, ACK3 sent
0 1 2 3 4 5 6 7 8 9
pkt4 rcvd, bufered, ACK4 sent
0 1 2 3 4 5 6 7 8 9
pkt5 rcvd; bufered, ACK5 sent
0 1 2 3 4 5 6 7 8 9
pkt2 rcvd, pkt2,pkt3,pkt4,pkt5
delivered, ACK2 sent
0 1 2 3 4 5 6 7 8 9
pkt0 sent
0 1 2 3 4 5 6 7 8 9
pkt1 sent
0 1 2 3 4 5 6 7 8 9
pkt2 sent
0 1 2 3 4 5 6 7 8 9
pkt3 sent, window full
0 1 2 3 4 5 6 7 8 9
ACK0 rcvd, pkt4 sent
0 1 2 3 4 5 6 7 8 9
ACK1 rcvd, pkt5 sent
0 1 2 3 4 5 6 7 8 9
pkt2 TIMEOUT, pkt2
resent
0 1 2 3 4 5 6 7 8 9
ACK3 rcvd, nothing sent
0 1 2 3 4 5 6 7 8 9
X
(loss)
Sender
Receiver
Figure 3.26 ‚ô¶ SR operation

The lack of synchronization between sender and receiver windows has impor-
tant consequences when we are faced with the reality of a finite range of sequence 
numbers. Consider what could happen, for example, with a finite range of four packet 
sequence numbers, 0, 1, 2, 3, and a window size of three. Suppose packets 0 through 
2 are transmitted and correctly received and acknowledged at the receiver. At this 
point, the receiver‚Äôs window is over the fourth, fifth, and sixth packets, which have 
sequence numbers 3, 0, and 1, respectively. Now consider two scenarios. In the first 
scenario, shown in Figure 3.27(a), the ACKs for the first three packets are lost and 
the sender retransmits these packets. The receiver thus next receives a packet with 
sequence number 0‚Äîa copy of the first packet sent.
In the second scenario, shown in Figure 3.27(b), the ACKs for the first three 
packets are all delivered correctly. The sender thus moves its window forward and 
sends the fourth, fifth, and sixth packets, with sequence numbers 3, 0, and 1, respec-
tively. The packet with sequence number 3 is lost, but the packet with sequence 
number 0 arrives‚Äîa packet containing new data.
Now consider the receiver‚Äôs viewpoint in Figure 3.27, which has a figurative 
curtain between the sender and the receiver, since the receiver cannot ‚Äúsee‚Äù the 
actions taken by the sender. All the receiver observes is the sequence of messages 
it receives from the channel and sends into the channel. As far as it is concerned, 
the two scenarios in Figure 3.27 are identical. There is no way of distinguishing the 
retransmission of the first packet from an original transmission of the fifth packet. 
Clearly, a window size that is 1 less than the size of the sequence number space 
won‚Äôt work. But how small must the window size be? A problem at the end of the 
chapter asks you to show that the window size must be less than or equal to half the 
size of the sequence number space for SR protocols.
At the companion Web site, you will find an animation that illustrates the opera-
tion of the SR protocol. Try performing the same experiments that you did with the 
GBN animation. Do the results agree with what you expect?
This completes our discussion of reliable data transfer protocols. We‚Äôve covered 
a lot of ground and introduced numerous mechanisms that together provide for reli-
able data transfer. Table 3.1 summarizes these mechanisms. Now that we have seen 
all of these mechanisms in operation and can see the ‚Äúbig picture,‚Äù we encourage you 
to review this section again to see how these mechanisms were incrementally added 
to cover increasingly complex (and realistic) models of the channel connecting the 
sender and receiver, or to improve the performance of the protocols.
Let‚Äôs conclude our discussion of reliable data transfer protocols by consider-
ing one remaining assumption in our underlying channel model. Recall that we 
have assumed that packets cannot be reordered within the channel between the 
sender and receiver. This is generally a reasonable assumption when the sender and 
receiver are connected by a single physical wire. However, when the ‚Äúchannel‚Äù 
connecting the two is a network, packet reordering can occur. One manifestation of 
packet reordering is that old copies of a packet with a sequence or acknowledgment

pkt0
timeout
retransmit pkt0
0 1 2 3 0 1 2
pkt0
pkt1
pkt2
0 1 2 3 0 1 2
0 1 2 3 0 1 2
0 1 2 3 0 1 2
0 1 2 3 0 1 2
ACK0
ACK1
ACK2
x
0 1 2 3 0 1 2
0 1 2 3 0 1 2
Sender window
(after receipt)
a.
b.
Receiver window
(after receipt)
receive packet
with seq number 0
0 1 2 3 0 1 2
pkt0
pkt1
pkt2
pkt3
pkt0
0 1 2 3 0 1 2
0 1 2 3 0 1 2
0 1 2 3 0 1 2
0 1 2 3 0 1 2
ACK0
ACK1
ACK2
0 1 2 3 0 1 2
0 1 2 3 0 1 2
Sender window
(after receipt)
Receiver window
(after receipt)
receive packet
with seq number 0
0 1 2 3 0 1 2
x
x
x
Figure 3.27 ‚ô¶  SR receiver dilemma with too-large windows: A new packet 
or a retransmission?

number of x can appear, even though neither the sender‚Äôs nor the receiver‚Äôs win-
dow contains x. With packet reordering, the channel can be thought of as essen-
tially buffering packets and spontaneously emitting these packets at any point in 
the future. Because sequence numbers may be reused, some care must be taken to 
guard against such duplicate packets. The approach taken in practice is to ensure 
that a sequence number is not reused until the sender is ‚Äúsure‚Äù that any previously 
sent packets with sequence number x are no longer in the network. This is done 
by assuming that a packet cannot ‚Äúlive‚Äù in the network for longer than some fixed 
maximum amount of time. A maximum packet lifetime of approximately three 
minutes is assumed in the TCP extensions for high-speed networks [RFC 7323]. 
[Sunshine 1978] describes a method for using sequence numbers such that reorder-
ing problems can be completely avoided.
Table 3.1 ‚ô¶ Summary of reliable data transfer mechanisms and their use
Mechanism
Use, Comments
Checksum
Used to detect bit errors in a transmitted packet.
Timer
Used to timeout/retransmit a packet, possibly because the packet (or its ACK) 
was lost within the channel. Because timeouts can occur when a packet is delayed 
but not lost (premature timeout), or when a packet has been received by the 
receiver but the receiver-to-sender ACK has been lost, duplicate copies of a packet 
may be received by a receiver.
Sequence number
Used for sequential numbering of packets of data flowing from sender to receiver. 
Gaps in the sequence numbers of received packets allow the receiver to detect a 
lost packet. Packets with duplicate sequence numbers allow the receiver to detect 
duplicate copies of a packet.
Acknowledgment
Used by the receiver to tell the sender that a packet or set of packets has been 
received correctly. Acknowledgments will typically carry the sequence number of 
the packet or packets being acknowledged. Acknowledgments may be individual 
or cumulative, depending on the protocol.
Negative acknowledgment
Used by the receiver to tell the sender that a packet has not been received 
correctly. Negative acknowledgments will typically carry the sequence number  
of the packet that was not received correctly.
Window, pipelining
The sender may be restricted to sending only packets with sequence numbers that 
fall within a given range. By allowing multiple packets to be transmitted but not 
yet acknowledged, sender utilization can be increased over a stop-and-wait mode 
of operation. We‚Äôll see shortly that the window size may be set on the basis of 
the receiver‚Äôs ability to receive and buffer messages, or the level of congestion in 
the network, or both.

3.5 Connection-Oriented Transport: TCP
Now that we have covered the underlying principles of reliable data transfer, 
let‚Äôs turn to TCP‚Äîthe Internet‚Äôs transport-layer, connection-oriented, reliable 
transport protocol. In this section, we‚Äôll see that in order to provide reliable 
data transfer, TCP relies on many of the underlying principles discussed in 
the previous section, including error detection, retransmissions, cumulative 
acknowledgments, timers, and header fields for sequence and acknowledgment 
numbers. TCP is defined in RFC 793, RFC 1122, RFC 2018, RFC 5681, and 
RFC 7323.
3.5.1 The TCP Connection
TCP is said to be connection-oriented because before one application process can 
begin to send data to another, the two processes must first ‚Äúhandshake‚Äù with each 
other‚Äîthat is, they must send some preliminary segments to each other to establish 
the parameters of the ensuing data transfer. As part of TCP connection establish-
ment, both sides of the connection will initialize many TCP state variables (many of 
which will be discussed in this section and in Section 3.7) associated with the TCP 
connection.
The TCP ‚Äúconnection‚Äù is not an end-to-end TDM or FDM circuit as in a circuit-
switched network. Instead, the ‚Äúconnection‚Äù is a logical one, with common state 
residing only in the TCPs in the two communicating end systems. Recall that because 
the TCP protocol runs only in the end systems and not in the intermediate network 
elements (routers and link-layer switches), the intermediate network elements do 
not maintain TCP connection state. In fact, the intermediate routers are completely 
oblivious to TCP connections; they see datagrams, not connections.
A TCP connection provides a full-duplex service: If there is a TCP con-
nection between Process A on one host and Process B on another host, then 
application-layer data can flow from Process A to Process B at the same time 
as application-layer data flows from Process B to Process A. A TCP connec-
tion is also always point-to-point, that is, between a single sender and a single 
receiver. So-called ‚Äúmulticasting‚Äù (see the online supplementary materials for 
this text)‚Äîthe transfer of data from one sender to many receivers in a single 
send operation‚Äîis not possible with TCP. With TCP, two hosts are company 
and three are a crowd!
Let‚Äôs now take a look at how a TCP connection is established. Suppose a process 
running in one host wants to initiate a connection with another process in another 
host. Recall that the process that is initiating the connection is called the client  
process, while the other process is called the server process. The client application 
process first informs the client transport layer that it wants to establish a connection

to a process in the server. Recall from Section 2.7.2, a Python client program does 
this by issuing the command
clientSocket.connect((serverName,serverPort))
where serverName is the name of the server and serverPort identifies the 
process on the server. TCP in the client then proceeds to establish a TCP connec-
tion with TCP in the server. At the end of this section we discuss in some detail the 
connection-establishment procedure. For now it suffices to know that the client first 
sends a special TCP segment; the server responds with a second special TCP seg-
ment; and finally the client responds again with a third special segment. The first 
two segments carry no payload, that is, no application-layer data; the third of these 
segments may carry a payload. Because three segments are sent between the two 
hosts, this connection-establishment procedure is often referred to as a three-way 
handshake.
VINTON CERF, ROBERT KAHN, AND TCP/IP
In the early 1970s, packet-switched networks began to proliferate, with the 
ARPAnet‚Äîthe precursor of the Internet‚Äîbeing just one of many networks. Each of 
these networks had its own protocol. Two researchers, Vinton Cerf and Robert Kahn, 
recognized the importance of interconnecting these networks and invented a cross-
network protocol called TCP/IP, which stands for Transmission Control Protocol/
Internet Protocol. Although Cerf and Kahn began by seeing the protocol as a single 
entity, it was later split into its two parts, TCP and IP, which operated separately. 
Cerf and Kahn published a paper on TCP/IP in May 1974 in IEEE Transactions on 
Communications Technology [Cerf 1974].
The TCP/IP protocol, which is the bread and butter of today‚Äôs Internet, was 
devised before PCs, workstations, smartphones, and tablets, before the prolifera-
tion of Ethernet, cable, and DSL, WiFi, and other access network technologies, and 
before the Web, social media, and streaming video. Cerf and Kahn saw the need 
for a networking protocol that, on the one hand, provides broad support for yet-to-
be-defined applications and, on the other hand, allows arbitrary hosts and link-layer 
protocols to interoperate.
In 2004, Cerf and Kahn received the ACM‚Äôs Turing Award, considered the 
‚ÄúNobel Prize of Computing‚Äù for ‚Äúpioneering work on internetworking, including the 
design and implementation of the Internet‚Äôs basic communications protocols, TCP/IP, 
and for inspired leadership in networking.‚Äù
CASE HISTORY

Once a TCP connection is established, the two application processes can send 
data to each other. Let‚Äôs consider the sending of data from the client process to the 
server process. The client process passes a stream of data through the socket (the door 
of the process), as described in Section 2.7. Once the data passes through the door, 
the data is in the hands of TCP running in the client. As shown in Figure 3.28, TCP 
directs this data to the connection‚Äôs send buffer, which is one of the buffers that is 
set aside during the initial three-way handshake. From time to time, TCP will grab 
chunks of data from the send buffer and pass the data to the network layer. Interest-
ingly, the TCP specification [RFC 793] is very laid back about specifying when TCP 
should actually send buffered data, stating that TCP should ‚Äúsend that data in seg-
ments at its own convenience.‚Äù The maximum amount of data that can be grabbed 
and placed in a segment is limited by the maximum segment size (MSS). The MSS 
is typically set by first determining the length of the largest link-layer frame that 
can be sent by the local sending host (the so-called maximum transmission unit, 
MTU), and then setting the MSS to ensure that a TCP segment (when encapsulated 
in an IP datagram) plus the TCP/IP header length (typically 40 bytes) will fit into a 
single link-layer frame. Both Ethernet and PPP link-layer protocols have an MTU of 
1,500 bytes. Thus, a typical value of MSS is 1460 bytes. Approaches have also been 
proposed for discovering the path MTU‚Äîthe largest link-layer frame that can be sent 
on all links from source to destination [RFC 1191]‚Äîand setting the MSS based on 
the path MTU value. Note that the MSS is the maximum amount of application-layer 
data in the segment, not the maximum size of the TCP segment including headers. 
(This terminology is confusing, but we have to live with it, as it is well entrenched.)
TCP pairs each chunk of client data with a TCP header, thereby forming TCP 
segments. The segments are passed down to the network layer, where they are sepa-
rately encapsulated within network-layer IP datagrams. The IP datagrams are then 
sent into the network. When TCP receives a segment at the other end, the segment‚Äôs 
data is placed in the TCP connection‚Äôs receive buffer, as shown in Figure 3.28. The 
application reads the stream of data from this buffer. Each side of the connection has 
Process
writes data
Process
reads data
TCP
send
buffer
Socket
TCP
receive
buffer
Socket
Segment
Segment
Figure 3.28 ‚ô¶ TCP send and receive buffers

its own send buffer and its own receive buffer. (You can see the online flow-control 
interactive animation  at http://www.awl.com/kurose-ross, which provides an anima-
tion of the send and receive buffers.)
We see from this discussion that a TCP connection consists of buffers, variables, 
and a socket connection to a process in one host, and another set of buffers, vari-
ables, and a socket connection to a process in another host. As mentioned earlier, no 
buffers or variables are allocated to the connection in the network elements (routers, 
switches, and repeaters) between the hosts.
3.5.2 TCP Segment Structure
Having taken a brief look at the TCP connection, let‚Äôs examine the TCP segment 
structure. The TCP segment consists of header fields and a data field. The data field 
contains a chunk of application data. As mentioned above, the MSS limits the maxi-
mum size of a segment‚Äôs data field. When TCP sends a large file, such as an image as 
part of a Web page, it typically breaks the file into chunks of size MSS (except for the 
last chunk, which will often be less than the MSS). Interactive applications, however, 
often transmit data chunks that are smaller than the MSS; for example, with remote 
login applications such as Telnet and ssh, the data field in the TCP segment is often 
only one byte. Because the TCP header is typically 20 bytes (12 bytes more than the 
UDP header), segments sent by Telnet and ssh may be only 21 bytes in length.
Figure 3.29 shows the structure of the TCP segment. As with UDP, the header 
includes source and destination port numbers, which are used for multiplexing/
demultiplexing data from/to upper-layer applications. Also, as with UDP, the header 
includes a checksum field. A TCP segment header also contains the following fields:
‚Ä¢ The 32-bit sequence number field and the 32-bit acknowledgment number 
field are used by the TCP sender and receiver in implementing a reliable data 
transfer service, as discussed below.
‚Ä¢ The 16-bit receive window field is used for flow control. We will see shortly that 
it is used to indicate the number of bytes that a receiver is willing to accept.
‚Ä¢ The 4-bit header length field specifies the length of the TCP header in 32-bit 
words. The TCP header can be of variable length due to the TCP options field. 
(Typically, the options field is empty, so that the length of the typical TCP header 
is 20 bytes.)
‚Ä¢ The optional and variable-length options field is used when a sender and receiver 
negotiate the maximum segment size (MSS) or as a window scaling factor for use 
in high-speed networks. A time-stamping option is also defined. See RFC 854 
and RFC 1323 for additional details.
‚Ä¢ The flag field contains 6 bits. The ACK bit is used to indicate that the value 
carried in the acknowledgment field is valid; that is, the segment contains an 
acknowledgment for a segment that has been successfully received. The RST,

SYN, and FIN bits are used for connection setup and teardown, as we will discuss 
at the end of this section. The CWR and ECE bits are used in explicit congestion 
notification, as discussed in Section 3.7.2. Setting the PSH bit indicates that the 
receiver should pass the data to the upper layer immediately. Finally, the URG bit 
is used to indicate that there is data in this segment that the sending-side upper-
layer entity has marked as ‚Äúurgent.‚Äù The location of the last byte of this urgent 
data is indicated by the 16-bit urgent data pointer field. TCP must inform the 
receiving-side upper-layer entity when urgent data exists and pass it a pointer to 
the end of the urgent data. (In practice, the PSH, URG, and the urgent data pointer 
are not used. However, we mention these fields for completeness.)
Our experience as teachers is that our students sometimes find discussion of 
packet formats rather dry and perhaps a bit boring. For a fun and fanciful look at 
TCP header fields, particularly if you love Legos‚Ñ¢ as we do, see [Pomeranz 2010].
Sequence Numbers and Acknowledgment Numbers
Two of the most important fields in the TCP segment header are the sequence number 
field and the acknowledgment number field. These fields are a critical part of TCP‚Äôs 
reliable data transfer service. But before discussing how these fields are used to pro-
vide reliable data transfer, let us first explain what exactly TCP puts in these fields.
Source port #
Internet checksum
Header
length
Unused
URG
ECE
CWR
ACK
PSH
RST
SYN
FIN
32 bits
Dest port #
Receive window
Urgent data pointer
Sequence number
Acknowledgment number
Options
Data
Figure 3.29 ‚ô¶ TCP segment structure

TCP views data as an unstructured, but ordered, stream of bytes. TCP‚Äôs use of 
sequence numbers reflects this view in that sequence numbers are over the stream 
of transmitted bytes and not over the series of transmitted segments. The sequence 
number for a segment is therefore the byte-stream number of the first byte in the 
segment. Let‚Äôs look at an example. Suppose that a process in Host A wants to send a 
stream of data to a process in Host B over a TCP connection. The TCP in Host A will 
implicitly number each byte in the data stream. Suppose that the data stream consists 
of a file consisting of 500,000 bytes, that the MSS is 1,000 bytes, and that the first 
byte of the data stream is numbered 0. As shown in Figure 3.30, TCP constructs 500 
segments out of the data stream. The first segment gets assigned sequence number 
0, the second segment gets assigned sequence number 1,000, the third segment gets 
assigned sequence number 2,000, and so on. Each sequence number is inserted in the 
sequence number field in the header of the appropriate TCP segment.
Now let‚Äôs consider acknowledgment numbers. These are a little trickier than 
sequence numbers. Recall that TCP is full-duplex, so that Host A may be receiving 
data from Host B while it sends data to Host B (as part of the same TCP connection). 
Each of the segments that arrive from Host B has a sequence number for the data 
flowing from B to A. The acknowledgment number that Host A puts in its segment 
is the sequence number of the next byte Host A is expecting from Host B. It is good 
to look at a few examples to understand what is going on here. Suppose that Host A 
has received all bytes numbered 0 through 535 from B and suppose that it is about 
to send a segment to Host B. Host A is waiting for byte 536 and all the subsequent 
bytes in Host B‚Äôs data stream. So Host A puts 536 in the acknowledgment number 
field of the segment it sends to B.
As another example, suppose that Host A has received one segment from Host 
B containing bytes 0 through 535 and another segment containing bytes 900 through 
1,000. For some reason Host A has not yet received bytes 536 through 899. In this 
example, Host A is still waiting for byte 536 (and beyond) in order to re-create B‚Äôs 
data stream. Thus, A‚Äôs next segment to B will contain 536 in the acknowledgment 
number field. Because TCP only acknowledges bytes up to the first missing byte in 
the stream, TCP is said to provide cumulative acknowledgments.
0
1
1,000
1,999
499,999
File
Data for 1st segment
Data for 2nd segment
Figure 3.30 ‚ô¶ Dividing file data into TCP segments

This last example also brings up an important but subtle issue. Host A received 
the third segment (bytes 900 through 1,000) before receiving the second segment 
(bytes 536 through 899). Thus, the third segment arrived out of order. The sub-
tle issue is: What does a host do when it receives out-of-order segments in a TCP 
connection? Interestingly, the TCP RFCs do not impose any rules here and leave 
the decision up to the programmers implementing a TCP implementation. There 
are basically two choices: either (1) the receiver immediately discards out-of-order 
segments (which, as we discussed earlier, can simplify receiver design), or (2) the 
receiver keeps the out-of-order bytes and waits for the missing bytes to fill in the 
gaps. Clearly, the latter choice is more efficient in terms of network bandwidth, and 
is the approach taken in practice.
In Figure 3.30, we assumed that the initial sequence number was zero. In truth, 
both sides of a TCP connection randomly choose an initial sequence number. This 
is done to minimize the possibility that a segment that is still present in the network 
from an earlier, already-terminated connection between two hosts is mistaken for a 
valid segment in a later connection between these same two hosts (which also happen 
to be using the same port numbers as the old connection) [Sunshine 1978].
Telnet: A Case Study for Sequence and Acknowledgment Numbers
Telnet, defined in RFC 854, is a popular application-layer protocol used for remote 
login. It runs over TCP and is designed to work between any pair of hosts. Unlike the 
bulk data transfer applications discussed in Chapter 2, Telnet is an interactive appli-
cation. We discuss a Telnet example here, as it nicely illustrates TCP sequence and 
acknowledgment numbers. We note that many users now prefer to use the SSH proto-
col rather than Telnet, since data sent in a Telnet connection (including passwords!) 
are not encrypted, making Telnet vulnerable to eavesdropping attacks (as discussed 
in Section 8.7).
Suppose Host A initiates a Telnet session with Host B. Because Host A initiates 
the session, it is labeled the client, and Host B is labeled the server. Each character 
typed by the user (at the client) will be sent to the remote host; the remote host will 
send back a copy of each character, which will be displayed on the Telnet user‚Äôs 
screen. This ‚Äúecho back‚Äù is used to ensure that characters seen by the Telnet user 
have already been received and processed at the remote site. Each character thus 
traverses the network twice between the time the user hits the key and the time the 
character is displayed on the user‚Äôs monitor.
Now suppose the user types a single letter, ‚ÄòC,‚Äô and then grabs a coffee. Let‚Äôs 
examine the TCP segments that are sent between the client and server. As shown 
in Figure 3.31, we suppose the starting sequence numbers are 42 and 79 for the cli-
ent and server, respectively. Recall that the sequence number of a segment is the 
sequence number of the first byte in the data field. Thus, the first segment sent from 
the client will have sequence number 42; the first segment sent from the server will 
have sequence number 79. Recall that the acknowledgment number is the sequence

number of the next byte of data that the host is waiting for. After the TCP connec-
tion is established but before any data is sent, the client is waiting for byte 79 and the 
server is waiting for byte 42.
As shown in Figure 3.31, three segments are sent. The first segment is sent from 
the client to the server, containing the 1-byte ASCII representation of the letter ‚ÄòC‚Äô in 
its data field. This first segment also has 42 in its sequence number field, as we just 
described. Also, because the client has not yet received any data from the server, this 
first segment will have 79 in its acknowledgment number field.
The second segment is sent from the server to the client. It serves a dual purpose. 
First it provides an acknowledgment of the data the server has received. By putting 
43 in the acknowledgment field, the server is telling the client that it has successfully 
received everything up through byte 42 and is now waiting for bytes 43 onward. The 
second purpose of this segment is to echo back the letter ‚ÄòC.‚Äô Thus, the second seg-
ment has the ASCII representation of ‚ÄòC‚Äô in its data field. This second segment has 
the sequence number 79, the initial sequence number of the server-to-client data flow 
of this TCP connection, as this is the very first byte of data that the server is send-
ing. Note that the acknowledgment for client-to-server data is carried in a segment 
Time
Time
Host A
Host B
User types
'C'
Seq=42, ACK=79, data='C'
Seq=79, ACK=43, data='C'
Seq=43, ACK=80
Host ACKs
receipt of 'C',
echoes back 'C'
Host ACKs
receipt of
echoed 'C'
Figure 3.31 ‚ô¶  Sequence and acknowledgment numbers for a simple Telnet 
application over TCP

carrying server-to-client data; this acknowledgment is said to be piggybacked on the 
server-to-client data segment.
The third segment is sent from the client to the server. Its sole purpose is to 
acknowledge the data it has received from the server. (Recall that the second seg-
ment contained data‚Äîthe letter ‚ÄòC‚Äô‚Äîfrom the server to the client.) This segment 
has an empty data field (that is, the acknowledgment is not being piggybacked with 
any client-to-server data). The segment has 80 in the acknowledgment number field 
because the client has received the stream of bytes up through byte sequence number 
79 and it is now waiting for bytes 80 onward. You might think it odd that this seg-
ment also has a sequence number since the segment contains no data. But because 
TCP has a sequence number field, the segment needs to have some sequence number.
3.5.3 Round-Trip Time Estimation and Timeout
TCP, like our rdt protocol in Section 3.4, uses a timeout/retransmit mechanism to 
recover from lost segments. Although this is conceptually simple, many subtle issues 
arise when we implement a timeout/retransmit mechanism in an actual protocol such 
as TCP. Perhaps the most obvious question is the length of the timeout intervals. 
Clearly, the timeout should be larger than the connection‚Äôs round-trip time (RTT), 
that is, the time from when a segment is sent until it is acknowledged. Otherwise, 
unnecessary retransmissions would be sent. But how much larger? How should the 
RTT be estimated in the first place? Should a timer be associated with each and 
every unacknowledged segment? So many questions! Our discussion in this section 
is based on the TCP work in [Jacobson 1988] and the current IETF recommendations 
for managing TCP timers [RFC 6298].
Estimating the Round-Trip Time
Let‚Äôs begin our study of TCP timer management by considering how TCP estimates 
the round-trip time between sender and receiver. This is accomplished as follows. 
The sample RTT, denoted SampleRTT, for a segment is the amount of time between 
when the segment is sent (that is, passed to IP) and when an acknowledgment for 
the segment is received. Instead of measuring a SampleRTT for every transmitted 
segment, most TCP implementations take only one SampleRTT measurement at 
a time. That is, at any point in time, the SampleRTT is being estimated for only 
one of the transmitted but currently unacknowledged segments, leading to a new 
value of SampleRTT approximately once every RTT. Also, TCP never computes a  
SampleRTT for a segment that has been retransmitted; it only measures  
SampleRTT for segments that have been transmitted once [Karn 1987]. (A problem 
at the end of the chapter asks you to consider why.)
Obviously, the SampleRTT values will fluctuate from segment to segment due 
to congestion in the routers and to the varying load on the end systems. Because of 
this fluctuation, any given SampleRTT value may be atypical. In order to estimate

a typical RTT, it is therefore natural to take some sort of average of the SampleRTT 
values. TCP maintains an average, called EstimatedRTT, of the SampleRTT 
values. Upon obtaining a new SampleRTT, TCP updates EstimatedRTT accord-
ing to the following formula:
EstimatedRTT = (1 ‚Äì Œ±) # EstimatedRTT + Œ± # SampleRTT
The formula above is written in the form of a programming-language state-
ment‚Äîthe new value of EstimatedRTT is a weighted combination of the previous 
value of EstimatedRTT and the new value for SampleRTT. The recommended 
value of Œ± is Œ± = 0.125 (that is, 1/8) [RFC 6298], in which case the formula above 
becomes:
EstimatedRTT = 0.875 # EstimatedRTT + 0.125 # SampleRTT
Note that EstimatedRTT is a weighted average of the SampleRTT values. As 
discussed in a homework problem at the end of this chapter, this weighted average 
puts more weight on recent samples than on old samples. This is natural, as the more 
recent samples better reflect the current congestion in the network. In statistics, such 
an average is called an exponential weighted moving average (EWMA). The word 
‚Äúexponential‚Äù appears in EWMA because the weight of a given SampleRTT decays 
exponentially fast as the updates proceed. In the homework problems, you will be 
asked to derive the exponential term in EstimatedRTT.
Figure 3.32 shows the SampleRTT values and EstimatedRTT for a value 
of Œ± = 1/8 for a TCP connection between gaia.cs.umass.edu (in Amherst, 
Massachusetts) to fantasia.eurecom.fr (in the south of France). Clearly, 
the variations in the SampleRTT are smoothed out in the computation of the 
EstimatedRTT.
In addition to having an estimate of the RTT, it is also valuable to have a meas-
ure of the variability of the RTT. [RFC 6298] defines the RTT variation, DevRTT, 
as an estimate of how much SampleRTT typically deviates from EstimatedRTT:
DevRTT = (1 ‚Äì Œ≤) # DevRTT + Œ≤ # | SampleRTT ‚Äì EstimatedRTT |
Note that DevRTT is an EWMA of the difference between SampleRTT and 
EstimatedRTT. If the SampleRTT values have little fluctuation, then DevRTT 
will be small; on the other hand, if there is a lot of fluctuation, DevRTT will be large. 
The recommended value of Œ≤ is 0.25.
Setting and Managing the Retransmission Timeout Interval
Given values of EstimatedRTT and DevRTT, what value should be used for 
TCP‚Äôs timeout interval? Clearly, the interval should be greater than or equal to

EstimatedRTT, or unnecessary retransmissions would be sent. But the timeout 
interval should not be too much larger than EstimatedRTT; otherwise, when a 
segment is lost, TCP would not quickly retransmit the segment, leading to large  
data transfer delays. It is therefore desirable to set the timeout equal to the  
EstimatedRTT plus some margin. The margin should be large when there is a lot 
of fluctuation in the SampleRTT values; it should be small when there is little fluc-
tuation. The value of DevRTT should thus come into play here. All of these consid-
erations are taken into account in TCP‚Äôs method for determining the retransmission 
timeout interval:
TimeoutInterval = EstimatedRTT + 4 # DevRTT
An initial TimeoutInterval value of 1 second is recommended [RFC 
6298]. Also, when a timeout occurs, the value of TimeoutInterval is doubled 
to avoid a premature timeout occurring for a subsequent segment that will soon be 
acknowledged. However, as soon as a segment is received and EstimatedRTT is 
updated, the TimeoutInterval is again computed using the formula above.
TCP provides reliable data transfer by using positive acknowledgments and timers in much 
the same way that we studied in Section 3.4. TCP acknowledges data that has been 
received correctly, and it then retransmits segments when segments or their corresponding 
acknowledgments are thought to be lost or corrupted. Certain versions of TCP also have 
an implicit NAK mechanism‚Äîwith TCP‚Äôs fast retransmit mechanism, the receipt of three 
duplicate ACKs for a given segment serves as an implicit NAK for the following segment, 
triggering retransmission of that segment before timeout. TCP uses sequences of numbers to 
allow the receiver to identify lost or duplicate segments. Just as in the case of our reliable 
data transfer protocol, rdt3.0, TCP cannot itself tell for certain if a segment, or its ACK, is 
lost, corrupted, or overly delayed. At the sender, TCP‚Äôs response will be the same: retrans-
mit the segment in question.
TCP also uses pipelining, allowing the sender to have multiple transmitted but yet-to-
be-acknowledged segments outstanding at any given time. We saw earlier that pipelining 
can greatly improve a session‚Äôs throughput when the ratio of the segment size to round-
trip delay is small. The specific number of outstanding, unacknowledged segments that a 
sender can have is determined by TCP‚Äôs flow-control and congestion-control mechanisms. 
TCP flow control is discussed at the end of this section; TCP congestion control is discussed 
in Section 3.7. For the time being, we must simply be aware that the TCP sender uses 
pipelining.
PRINCIPLES IN PRACTICE

3.5.4 Reliable Data Transfer
Recall that the Internet‚Äôs network-layer service (IP service) is unreliable. IP does 
not guarantee datagram delivery, does not guarantee in-order delivery of datagrams, 
and does not guarantee the integrity of the data in the datagrams. With IP service, 
datagrams can overflow router buffers and never reach their destination, datagrams 
can arrive out of order, and bits in the datagram can get corrupted (flipped from 0 to 
1 and vice versa). Because transport-layer segments are carried across the network 
by IP datagrams, transport-layer segments can suffer from these problems as well.
TCP creates a reliable data transfer service on top of IP‚Äôs unreliable best-
effort service. TCP‚Äôs reliable data transfer service ensures that the data stream that 
a process reads out of its TCP receive buffer is uncorrupted, without gaps, with-
out duplication, and in sequence; that is, the byte stream is exactly the same byte 
stream that was sent by the end system on the other side of the connection. How TCP 
provides a reliable data transfer involves many of the principles that we studied in  
Section 3.4.
In our earlier development of reliable data transfer techniques, it was conceptu-
ally easiest to assume that an individual timer is associated with each transmitted 
but not yet acknowledged segment. While this is great in theory, timer management 
can require considerable overhead. Thus, the recommended TCP timer management 
RTT (milliseconds)
150
200
250
300
350
100
1
8
15
22
29
36
43
50
Time (seconds)
Sample RTT
57
64
71
78
85
92
99
106
Estimated RTT
Figure 3.32 ‚ô¶ RTT samples and RTT estimates

procedures [RFC 6298] use only a single retransmission timer, even if there are mul-
tiple transmitted but not yet acknowledged segments. The TCP protocol described in 
this section follows this single-timer recommendation.
We will discuss how TCP provides reliable data transfer in two incremental 
steps. We first present a highly simplified description of a TCP sender that uses only 
timeouts to recover from lost segments; we then present a more complete description 
that uses duplicate acknowledgments in addition to timeouts. In the ensuing discus-
sion, we suppose that data is being sent in only one direction, from Host A to Host B, 
and that Host A is sending a large file.
Figure 3.33 presents a highly simplified description of a TCP sender. We see 
that there are three major events related to data transmission and retransmission 
in the TCP sender: data received from application above; timer timeout; and ACK 
/* Assume sender is not constrained by TCP Ô¨Çow or congestion control, that data from above is less
than MSS in size, and that data transfer is in one direction only. */
NextSeqNum=InitialSeqNumber
SendBase=InitialSeqNumber
loop (forever) {
switch(event)
event: data received from application above
create TCP segment with sequence number NextSeqNum
if (timer currently not running)
start timer
pass segment to IP
NextSeqNum=NextSeqNum+length(data)
break;
event: timer timeout
retransmit not-yet-acknowledged segment with
smallest sequence number
start timer
break;
event: ACK received, with ACK Ô¨Åeld value of y
if (y > SendBase) {
SendBase=y
if (there are currently any not-yet-acknowledged segments)
start timer
}
break;
} /* end of loop forever */
Figure 3.33 ‚ô¶ Simplified TCP sender

receipt. Upon the occurrence of the first major event, TCP receives data from the 
application, encapsulates the data in a segment, and passes the segment to IP. Note 
that each segment includes a sequence number that is the byte-stream number of 
the first data byte in the segment, as described in Section 3.5.2. Also note that if the 
timer is already not running for some other segment, TCP starts the timer when the 
segment is passed to IP. (It is helpful to think of the timer as being associated with 
the oldest unacknowledged segment.) The expiration interval for this timer is the 
TimeoutInterval, which is calculated from EstimatedRTT and DevRTT, as 
described in Section 3.5.3.
The second major event is the timeout. TCP responds to the timeout event by 
retransmitting the segment that caused the timeout. TCP then restarts the timer.
The third major event that must be handled by the TCP sender is the arrival of 
an acknowledgment segment (ACK) from the receiver (more specifically, a segment 
containing a valid ACK field value). On the occurrence of this event, TCP compares 
the ACK value y with its variable SendBase. The TCP state variable SendBase 
is the sequence number of the oldest unacknowledged byte. (Thus SendBase‚Äì1 is 
the sequence number of the last byte that is known to have been received correctly 
and in order at the receiver.) As indicated earlier, TCP uses cumulative acknowl-
edgments, so that y acknowledges the receipt of all bytes before byte number y. If  
y > SendBase, then the ACK is acknowledging one or more previously unac-
knowledged segments. Thus the sender updates its SendBase variable; it also 
restarts the timer if there currently are any not-yet-acknowledged segments.
A Few Interesting Scenarios
We have just described a highly simplified version of how TCP provides reliable data 
transfer. But even this highly simplified version has many subtleties. To get a good 
feeling for how this protocol works, let‚Äôs now walk through a few simple scenarios. 
Figure 3.34 depicts the first scenario, in which Host A sends one segment to Host B. 
Suppose that this segment has sequence number 92 and contains 8 bytes of data. After 
sending this segment, Host A waits for a segment from B with acknowledgment num-
ber 100. Although the segment from A is received at B, the acknowledgment from B 
to A gets lost. In this case, the timeout event occurs, and Host A retransmits the same 
segment. Of course, when Host B receives the retransmission, it observes from the 
sequence number that the segment contains data that has already been received. Thus, 
TCP in Host B will discard the bytes in the retransmitted segment.
In a second scenario, shown in Figure 3.35, Host A sends two segments back to 
back. The first segment has sequence number 92 and 8 bytes of data, and the second 
segment has sequence number 100 and 20 bytes of data. Suppose that both segments 
arrive intact at B, and B sends two separate acknowledgments for each of these seg-
ments. The first of these acknowledgments has acknowledgment number 100; the 
second has acknowledgment number 120. Suppose now that neither of the acknowl-
edgments arrives at Host A before the timeout. When the timeout event occurs, Host

A resends the first segment with sequence number 92 and restarts the timer. As long 
as the ACK for the second segment arrives before the new timeout, the second seg-
ment will not be retransmitted.
In a third and final scenario, suppose Host A sends the two segments, exactly 
as in the second example. The acknowledgment of the first segment is lost in the 
network, but just before the timeout event, Host A receives an acknowledgment with 
acknowledgment number 120. Host A therefore knows that Host B has received  
everything up through byte 119; so Host A does not resend either of the two  
segments. This scenario is illustrated in Figure 3.36.
Doubling the Timeout Interval
We now discuss a few modifications that most TCP implementations employ. The 
first concerns the length of the timeout interval after a timer expiration. In this modi-
fication, whenever the timeout event occurs, TCP retransmits the not-yet-acknowl-
edged segment with the smallest sequence number, as described above. But each 
time TCP retransmits, it sets the next timeout interval to twice the previous value, 
Time
Time
Host A
Host B
Timeout
Seq=92, 8 bytes data
Seq=92, 8 bytes data
ACK=100
ACK=100
X
(loss)
Figure 3.34 ‚ô¶ Retransmission due to a lost acknowledgment

rather than deriving it from the last EstimatedRTT and DevRTT (as described 
in Section 3.5.3). For example, suppose TimeoutInterval associated with 
the oldest not yet acknowledged segment is .75 sec when the timer first expires. 
TCP will then retransmit this segment and set the new expiration time to 1.5 sec. If 
the timer expires again 1.5 sec later, TCP will again retransmit this segment, now 
setting the expiration time to 3.0 sec. Thus, the intervals grow exponentially after 
each retransmission. However, whenever the timer is started after either of the two 
other events (that is, data received from application above, and ACK received), the 
TimeoutInterval is derived from the most recent values of EstimatedRTT 
and DevRTT.
This modification provides a limited form of congestion control. (More com-
prehensive forms of TCP congestion control will be studied in Section 3.7.) The 
timer expiration is most likely caused by congestion in the network, that is, too many 
packets arriving at one (or more) router queues in the path between the source and 
destination, causing packets to be dropped and/or long queuing delays. In times of 
congestion, if the sources continue to retransmit packets persistently, the congestion 
Time
Time
Host A
Host B
seq=92 timeout interval
Seq=92, 8 bytes data
Seq=100, 20 bytes data
ACK=100
ACK=120
ACK=120
seq=92 timeout interval
Seq=92, 8 bytes data
Figure 3.35 ‚ô¶ Segment 100 not retransmitted

may get worse. Instead, TCP acts more politely, with each sender retransmitting after 
longer and longer intervals. We will see that a similar idea is used by Ethernet when 
we study CSMA/CD in Chapter 6.
Fast Retransmit
One of the problems with timeout-triggered retransmissions is that the timeout period 
can be relatively long. When a segment is lost, this long timeout period forces the 
sender to delay resending the lost packet, thereby increasing the end-to-end delay. 
Fortunately, the sender can often detect packet loss well before the timeout event 
occurs by noting so-called duplicate ACKs. A duplicate ACK is an ACK that reac-
knowledges a segment for which the sender has already received an earlier acknowl-
edgment. To understand the sender‚Äôs response to a duplicate ACK, we must look at 
why the receiver sends a duplicate ACK in the first place. Table 3.2 summarizes the 
TCP receiver‚Äôs ACK generation policy [RFC 5681]. When a TCP receiver receives 
Time
Time
Host A
Host B
Seq=92 timeout interval
Seq=92, 8 bytes data
Seq=100,  20 bytes data
ACK=100
ACK=120
X
(loss)
Figure 3.36 ‚ô¶  A cumulative acknowledgment avoids retransmission of the 
first segment

a segment with a sequence number that is larger than the next, expected, in-order 
sequence number, it detects a gap in the data stream‚Äîthat is, a missing segment. 
This gap could be the result of lost or reordered segments within the network. Since 
TCP does not use negative acknowledgments, the receiver cannot send an explicit 
negative acknowledgment back to the sender. Instead, it simply reacknowledges 
(that is, generates a duplicate ACK for) the last in-order byte of data it has received. 
(Note that Table 3.2 allows for the case that the receiver does not discard out-of-
order segments.)
Because a sender often sends a large number of segments back to back, if one 
segment is lost, there will likely be many back-to-back duplicate ACKs. If the TCP 
sender receives three duplicate ACKs for the same data, it takes this as an indication 
that the segment following the segment that has been ACKed three times has been 
lost. (In the homework problems, we consider the question of why the sender waits 
for three duplicate ACKs, rather than just a single duplicate ACK.) In the case that 
three duplicate ACKs are received, the TCP sender performs a fast retransmit [RFC 
5681], retransmitting the missing segment before that segment‚Äôs timer expires. This 
is shown in Figure 3.37, where the second segment is lost, then retransmitted before 
its timer expires. For TCP with fast retransmit, the following code snippet replaces 
the ACK received event in Figure 3.33:
event: ACK received, with ACK field value of y
            if (y > SendBase) {
            SendBase=y
            if (there are currently any not yet
                       acknowledged segments)
               start timer
               }
Table 3.2 ‚ô¶ TCP ACK Generation Recommendation [RFC 5681]
Event
TCP Receiver Action
Arrival of in-order segment with expected sequence number. All  
data up to expected sequence number already acknowledged.
Delayed ACK. Wait up to 500 msec for arrival of another in-order segment.  
If next in-order segment does not arrive in this interval, send an ACK.
Arrival of in-order segment with expected sequence number. One  
other in-order segment waiting for ACK transmission.
Immediately send single cumulative ACK, ACKing both in-order segments.
Arrival of out-of-order segment with higher-than-expected sequence 
number. Gap detected.
Immediately send duplicate ACK, indicating sequence number of next 
expected byte (which is the lower end of the gap).
Arrival of segment that partially or completely fills in gap in  
received data.
Immediately send ACK, provided that segment starts at the lower end  
of gap.

Host A
Host B
seq=100, 20 bytes of data
Timeout
Time
Time
X
seq=100, 20 bytes of data
seq=92, 8 bytes of data
seq=120, 15 bytes of data
seq=135, 6 bytes of data
seq=141, 16 bytes of data
ack=100
ack=100
ack=100
ack=100
Figure 3.37 ‚ô¶  Fast retransmit: retransmitting the missing segment before 
the segment‚Äôs timer expires
            else {/* a duplicate ACK for already ACKed
                   segment */
               increment number of duplicate ACKs
                   received for y
               if (number of duplicate ACKS received
                   for y==3)
                   /* TCP fast retransmit */
                   resend segment with sequence number y
               }
           break;
We noted earlier that many subtle issues arise when a timeout/retransmit mech-
anism is implemented in an actual protocol such as TCP. The procedures above, 
which have evolved as a result of more than 30 years of experience with TCP timers, 
should convince you that this is indeed the case!

Go-Back-N or Selective Repeat?
Let us close our study of TCP‚Äôs error-recovery mechanism by considering the fol-
lowing question: Is TCP a GBN or an SR protocol? Recall that TCP acknowledg-
ments are cumulative and correctly received but out-of-order segments are not 
individually ACKed by the receiver. Consequently, as shown in Figure 3.33 (see 
also Figure 3.19), the TCP sender need only maintain the smallest sequence number 
of a transmitted but unacknowledged byte (SendBase) and the sequence number 
of the next byte to be sent (NextSeqNum). In this sense, TCP looks a lot like a 
GBN-style protocol. But there are some striking differences between TCP and Go-
Back-N. Many TCP implementations will buffer correctly received but out-of-order 
segments [Stevens 1994]. Consider also what happens when the sender sends a 
sequence of segments 1, 2, . . . , N, and all of the segments arrive in order without 
error at the receiver. Further suppose that the acknowledgment for packet n 6 N 
gets lost, but the remaining N - 1 acknowledgments arrive at the sender before 
their respective timeouts. In this example, GBN would retransmit not only packet n, 
but also all of the subsequent packets n + 1, n + 2, . . . , N. TCP, on the other hand, 
would retransmit at most one segment, namely, segment n. Moreover, TCP would 
not even retransmit segment n if the acknowledgment for segment n + 1 arrived 
before the timeout for segment n.
A proposed modification to TCP, the so-called selective acknowledgment  
[RFC 2018], allows a TCP receiver to acknowledge out-of-order segments selectively 
rather than just cumulatively acknowledging the last correctly received, in-order  
segment. When combined with selective retransmission‚Äîskipping the retransmis-
sion of segments that have already been selectively acknowledged by the receiver‚Äî
TCP looks a lot like our generic SR protocol. Thus, TCP‚Äôs error-recovery mechanism 
is probably best categorized as a hybrid of GBN and SR protocols.
3.5.5 Flow Control
Recall that the hosts on each side of a TCP connection set aside a receive buffer 
for the connection. When the TCP connection receives bytes that are correct and in 
sequence, it places the data in the receive buffer. The associated application process 
will read data from this buffer, but not necessarily at the instant the data arrives. 
Indeed, the receiving application may be busy with some other task and may not even 
attempt to read the data until long after it has arrived. If the application is relatively 
slow at reading the data, the sender can very easily overflow the connection‚Äôs receive 
buffer by sending too much data too quickly.
TCP provides a flow-control service to its applications to eliminate the pos-
sibility of the sender overflowing the receiver‚Äôs buffer. Flow control is thus a speed-
matching service‚Äîmatching the rate at which the sender is sending against the rate 
at which the receiving application is reading. As noted earlier, a TCP sender can also 
be throttled due to congestion within the IP network; this form of sender control is

referred to as congestion control, a topic we will explore in detail in Sections 3.6 
and 3.7. Even though the actions taken by flow and congestion control are similar 
(the throttling of the sender), they are obviously taken for very different reasons. 
Unfortunately, many authors use the terms interchangeably, and the savvy reader 
would be wise to distinguish between them. Let‚Äôs now discuss how TCP provides its 
flow-control service. In order to see the forest for the trees, we suppose throughout 
this section that the TCP implementation is such that the TCP receiver discards out-
of-order segments.
TCP provides flow control by having the sender maintain a variable called 
the receive window. Informally, the receive window is used to give the sender an 
idea of how much free buffer space is available at the receiver. Because TCP is 
full-duplex, the sender at each side of the connection maintains a distinct receive 
window. Let‚Äôs investigate the receive window in the context of a file transfer. Sup-
pose that Host A is sending a large file to Host B over a TCP connection. Host B 
allocates a receive buffer to this connection; denote its size by RcvBuffer. From 
time to time, the application process in Host B reads from the buffer. Define the 
following variables:
‚Ä¢ LastByteRead: the number of the last byte in the data stream read from the 
buffer by the application process in B
‚Ä¢ LastByteRcvd: the number of the last byte in the data stream that has arrived 
from the network and has been placed in the receive buffer at B
Because TCP is not permitted to overflow the allocated buffer, we must have
LastByteRcvd ‚Äì LastByteRead ‚Ä¶ RcvBuffer
The receive window, denoted rwnd is set to the amount of spare room in the buffer:
rwnd = RcvBuffer ‚Äì [LastByteRcvd ‚Äì LastByteRead]
Because the spare room changes with time, rwnd is dynamic. The variable rwnd is 
illustrated in Figure 3.38.
How does the connection use the variable rwnd to provide the flow-control 
service? Host B tells Host A how much spare room it has in the connection buffer 
by placing its current value of rwnd in the receive window field of every segment it 
sends to A. Initially, Host B sets rwnd = RcvBuffer. Note that to pull this off, 
Host B must keep track of several connection-specific variables.
Host A in turn keeps track of two variables, LastByteSent and Last-
ByteAcked, which have obvious meanings. Note that the difference between these 
two variables, LastByteSent ‚Äì LastByteAcked, is the amount of unac-
knowledged data that A has sent into the connection. By keeping the amount of 
unacknowledged data less than the value of rwnd, Host A is assured that it is not

overflowing the receive buffer at Host B. Thus, Host A makes sure throughout the 
connection‚Äôs life that
LastByteSent ‚Äì LastByteAcked ‚Ä¶ rwnd
There is one minor technical problem with this scheme. To see this, suppose Host 
B‚Äôs receive buffer becomes full so that rwnd = 0. After advertising rwnd = 0 to 
Host A, also suppose that B has nothing to send to A. Now consider what happens. 
As the application process at B empties the buffer, TCP does not send new seg-
ments with new rwnd values to Host A; indeed, TCP sends a segment to Host A 
only if it has data to send or if it has an acknowledgment to send. Therefore, Host 
A is never informed that some space has opened up in Host B‚Äôs receive buffer‚Äî
Host A is blocked and can transmit no more data! To solve this problem, the TCP 
specification requires Host A to continue to send segments with one data byte when 
B‚Äôs receive window is zero. These segments will be acknowledged by the receiver. 
Eventually the buffer will begin to empty and the acknowledgments will contain a 
nonzero rwnd value.
The online site at for this book provides an interactive animation that illustrates 
the operation of the TCP receive window.
Having described TCP‚Äôs flow-control service, we briefly mention here that UDP 
does not provide flow control and consequently, segments may be lost at the receiver 
due to buffer overflow. For example, consider sending a series of UDP segments 
from a process on Host A to a process on Host B. For a typical UDP implementation, 
UDP will append the segments in a finite-sized buffer that ‚Äúprecedes‚Äù the corre-
sponding socket (that is, the door to the process). The process reads one entire seg-
ment at a time from the buffer. If the process does not read the segments fast enough 
from the buffer, the buffer will overflow and segments will get dropped.
Application
process 
Data
from IP
TCP data
in buffer
rwnd
RcvBufer
Spare room
Figure 3.38 ‚ô¶  The receive window (rwnd) and the receive buffer 
(RcvBuffer)

3.5.6 TCP Connection Management
In this subsection, we take a closer look at how a TCP connection is established and 
torn down. Although this topic may not seem particularly thrilling, it is important 
because TCP connection establishment can significantly add to perceived delays (for 
example, when surfing the Web). Furthermore, many of the most common network 
attacks‚Äîincluding the incredibly popular SYN flood attack (see sidebar on the SYN 
flood attack)‚Äîexploit vulnerabilities in TCP connection management. Let‚Äôs first 
take a look at how a TCP connection is established. Suppose a process running in 
one host (client) wants to initiate a connection with another process in another host 
(server). The client application process first informs the client TCP that it wants to 
establish a connection to a process in the server. The TCP in the client then proceeds 
to establish a TCP connection with the TCP in the server in the following manner:
‚Ä¢ Step 1. The client-side TCP first sends a special TCP segment to the server-side 
TCP. This special segment contains no application-layer data. But one of the flag 
bits in the segment‚Äôs header (see Figure 3.29), the SYN bit, is set to 1. For this 
reason, this special segment is referred to as a SYN segment. In addition, the cli-
ent randomly chooses an initial sequence number (client_isn) and puts this 
number in the sequence number field of the initial TCP SYN segment. This seg-
ment is encapsulated within an IP datagram and sent to the server. There has been 
considerable interest in properly randomizing the choice of the client_isn in 
order to avoid certain security attacks [CERT 2001‚Äì09; RFC 4987].
‚Ä¢ Step 2. Once the IP datagram containing the TCP SYN segment arrives at the 
server host (assuming it does arrive!), the server extracts the TCP SYN segment 
from the datagram, allocates the TCP buffers and variables to the connection, 
and sends a connection-granted segment to the client TCP. (We‚Äôll see in Chapter 
8 that the allocation of these buffers and variables before completing the third 
step of the three-way handshake makes TCP vulnerable to a denial-of-service 
attack known as SYN flooding.) This connection-granted segment also contains 
no application-layer data. However, it does contain three important pieces of 
information in the segment header. First, the SYN bit is set to 1. Second, the 
acknowledgment field of the TCP segment header is set to client_isn+1. 
Finally, the server chooses its own initial sequence number (server_isn) and 
puts this value in the sequence number field of the TCP segment header. This 
connection-granted segment is saying, in effect, ‚ÄúI received your SYN packet to 
start a connection with your initial sequence number, client_isn. I agree to 
establish this connection. My own initial sequence number is server_isn.‚Äù 
The connection-granted segment is referred to as a SYNACK segment.
‚Ä¢ Step 3. Upon receiving the SYNACK segment, the client also allocates buffers 
and variables to the connection. The client host then sends the server yet another 
segment; this last segment acknowledges the server‚Äôs connection-granted segment 
(the client does so by putting the value server_isn+1 in the acknowledgment

field of the TCP segment header). The SYN bit is set to zero, since the connection 
is established. This third stage of the three-way handshake may carry client-to-
server data in the segment payload.
Once these three steps have been completed, the client and server hosts can send 
segments containing data to each other. In each of these future segments, the SYN 
bit will be set to zero. Note that in order to establish the connection, three packets 
are sent between the two hosts, as illustrated in Figure 3.39. For this reason, this 
connection-establishment procedure is often referred to as a three-way handshake. 
Several aspects of the TCP three-way handshake are explored in the homework prob-
lems (Why are initial sequence numbers needed? Why is a three-way handshake, 
as opposed to a two-way handshake, needed?). It‚Äôs interesting to note that a rock 
climber and a belayer (who is stationed below the rock climber and whose job it is 
to handle the climber‚Äôs safety rope) use a three-way-handshake communication pro-
tocol that is identical to TCP‚Äôs to ensure that both sides are ready before the climber 
begins ascent.
All good things must come to an end, and the same is true with a TCP connec-
tion. Either of the two processes participating in a TCP connection can end the con-
nection. When a connection ends, the ‚Äúresources‚Äù (that is, the buffers and variables) 
Time
Time
Client host
Connection
request
Connection
granted
Server host
SYN=1, seq=client_isn
SYN=1, seq=server_isn,
ack=client_isn+1
SYN=0, seq=client_isn+1,
ack=server_isn+1
ACK
Figure 3.39 ‚ô¶ TCP three-way handshake: segment exchange

in the hosts are deallocated. As an example, suppose the client decides to close the 
connection, as shown in Figure 3.40. The client application process issues a close 
command. This causes the client TCP to send a special TCP segment to the server 
process. This special segment has a flag bit in the segment‚Äôs header, the FIN bit (see 
Figure 3.29), set to 1. When the server receives this segment, it sends the client an 
acknowledgment segment in return. The server then sends its own shutdown segment, 
which has the FIN bit set to 1. Finally, the client acknowledges the server‚Äôs shutdown 
segment. At this point, all the resources in the two hosts are now deallocated.
During the life of a TCP connection, the TCP protocol running in each host 
makes transitions through various TCP states. Figure 3.41 illustrates a typical 
sequence of TCP states that are visited by the client TCP. The client TCP begins 
in the CLOSED state. The application on the client side initiates a new TCP con-
nection (by creating a Socket object in our Python examples from Chapter 2). This 
causes TCP in the client to send a SYN segment to TCP in the server. After hav-
ing sent the SYN segment, the client TCP enters the SYN_SENT state. While in 
the SYN_SENT state, the client TCP waits for a segment from the server TCP that 
includes an acknowledgment for the client‚Äôs previous segment and has the SYN bit 
Time
Time
Client
Close
Close
Server
FIN
ACK
ACK
FIN
Closed
Timed wait
Figure 3.40 ‚ô¶ Closing a TCP connection

set to 1. Having received such a segment, the client TCP enters the ESTABLISHED 
state. While in the ESTABLISHED state, the TCP client can send and receive TCP 
segments containing payload (that is, application-generated) data.
Suppose that the client application decides it wants to close the connection. (Note 
that the server could also choose to close the connection.) This causes the client TCP 
to send a TCP segment with the FIN bit set to 1 and to enter the FIN_WAIT_1 state. 
While in the FIN_WAIT_1 state, the client TCP waits for a TCP segment from the 
server with an acknowledgment. When it receives this segment, the client TCP enters 
the FIN_WAIT_2 state. While in the FIN_WAIT_2 state, the client waits for another 
segment from the server with the FIN bit set to 1; after receiving this segment, the client 
TCP acknowledges the server‚Äôs segment and enters the TIME_WAIT state. The TIME_
WAIT state lets the TCP client resend the final acknowledgment in case the ACK is 
lost. The time spent in the TIME_WAIT state is implementation-dependent, but typical 
values are 30 seconds, 1 minute, and 2 minutes. After the wait, the connection formally 
closes and all resources on the client side (including port numbers) are released.
Figure 3.42 illustrates the series of states typically visited by the server-side 
TCP, assuming the client begins connection teardown. The transitions are self-
explanatory. In these two state-transition diagrams, we have only shown how a TCP 
connection is normally established and shut down. We have not described what hap-
pens in certain pathological scenarios, for example, when both sides of a connection 
want to initiate or shut down at the same time. If you are interested in learning about 
CLOSED
SYN_SENT
ESTABLISHED
FIN_WAIT_1
FIN_WAIT_2
TIME_WAIT
Send SYN
Send FIN
Receive ACK, 
send nothing
Wait 30 seconds
Receive FIN, 
send ACK
Receive SYN & ACK, 
send ACK
Client application
initiates a TCP connection
Client application
initiates close connection
Figure 3.41 ‚ô¶ A typical sequence of TCP states visited by a client TCP

this and other advanced issues concerning TCP, you are encouraged to see Stevens‚Äô 
comprehensive book [Stevens 1994].
Our discussion above has assumed that both the client and server are prepared to 
communicate, that is, that the server is listening on the port to which the client sends 
its SYN segment. Let‚Äôs consider what happens when a host receives a TCP segment 
whose port numbers or source IP address do not match with any of the ongoing sock-
ets in the host. For example, suppose a host receives a TCP SYN packet with desti-
nation port 80, but the host is not accepting connections on port 80 (that is, it is not 
running a Web server on port 80). Then the host will send a special reset segment to 
the source. This TCP segment has the RST flag bit (see Section 3.5.2) set to 1. Thus, 
when a host sends a reset segment, it is telling the source ‚ÄúI don‚Äôt have a socket for 
that segment. Please do not resend the segment.‚Äù When a host receives a UDP packet 
whose destination port number doesn‚Äôt match with an ongoing UDP socket, the host 
sends a special ICMP datagram, as discussed in Chapter 5.
Now that we have a good understanding of TCP connection management, let‚Äôs 
revisit the nmap port-scanning tool and examine more closely how it works. To explore 
a specific TCP port, say port 6789, on a target host, nmap will send a TCP SYN seg-
ment with destination port 6789 to that host. There are three possible outcomes:
‚Ä¢ The source host receives a TCP SYNACK segment from the target host. Since this 
means that an application is running with TCP port 6789 on the target post, nmap 
returns ‚Äúopen.‚Äù
CLOSED
LISTEN
SYN_RCVD
ESTABLISHED
CLOSE_WAIT
LAST_ACK
Receive FIN,
send ACK
Receive ACK, 
send nothing
Send FIN
Receive SYN 
send SYN & ACK
Server application
creates a listen socket
Receive ACK, 
send nothing
Figure 3.42 ‚ô¶  A typical sequence of TCP states visited by a server-side TCP

THE SYN FLOOD ATTACK
We‚Äôve seen in our discussion of TCP‚Äôs three-way handshake that a server allocates 
and initializes connection variables and buffers in response to a received SYN. The 
server then sends a SYNACK in response, and awaits an ACK segment from the cli-
ent. If the client does not send an ACK to complete the third step of this 3-way hand-
shake, eventually (often after a minute or more) the server will terminate the half-open 
connection and reclaim the allocated resources.
This TCP connection management protocol sets the stage for a classic Denial of 
Service (DoS) attack known as the SYN flood attack. In this attack, the attacker(s) send 
a large number of TCP SYN segments, without completing the third handshake step. With 
this deluge of SYN segments, the server‚Äôs connection resources become exhausted as 
they are allocated (but never used!) for half-open connections; legitimate clients are then 
denied service. Such SYN flooding attacks were among the first documented DoS attacks 
[CERT SYN 1996]. Fortunately, an effective defense known as SYN cookies [RFC 
4987] are now deployed in most major operating systems. SYN cookies work as follows:
‚Ä¢ 
 When the server receives a SYN segment, it does not know if the segment is 
coming from a legitimate user or is part of a SYN flood attack. So, instead of 
creating a half-open TCP connection for this SYN, the server creates an initial 
TCP sequence number that is a complicated function (hash function) of source 
and destination IP addresses and port numbers of the SYN segment, as well as 
a secret number only known to the server. This carefully crafted initial sequence 
number is the so-called ‚Äúcookie.‚Äù The server then sends the client a SYNACK 
packet with this special initial sequence number. Importantly, the server does not 
remember the cookie or any other state information corresponding to the SYN.
‚Ä¢ 
 A legitimate client will return an ACK segment. When the server receives this 
ACK, it must verify that the ACK corresponds to some SYN sent earlier. But how 
is this done if the server maintains no memory about SYN segments? As you may 
have guessed, it is done with the cookie. Recall that for a legitimate ACK, the 
value in the acknowledgment field is equal to the initial sequence number in the 
SYNACK (the cookie value in this case) plus one (see Figure 3.39). The server 
can then run the same hash function using the source and destination IP address 
and port numbers in the SYNACK (which are the same as in the original SYN) 
and the secret number. If the result of the function plus one is the same as the 
acknowledgment (cookie) value in the client‚Äôs SYNACK, the server concludes that 
the ACK corresponds to an earlier SYN segment and is hence valid. The server 
then creates a fully open connection along with a socket.
‚Ä¢ 
 On the other hand, if the client does not return an ACK segment, then the origi-
nal SYN has done no harm at the server, since the server hasn‚Äôt yet allocated 
any resources in response to the original bogus SYN.
FOCUS ON SECURITY

‚Ä¢ The source host receives a TCP RST segment from the target host. This means 
that the SYN segment reached the target host, but the target host is not running 
an application with TCP port 6789. But the attacker at least knows that the seg-
ments destined to the host at port 6789 are not blocked by any firewall on the path 
between source and target hosts. (Firewalls are discussed in Chapter 8.)
‚Ä¢ The source receives nothing. This likely means that the SYN segment was blocked 
by an intervening firewall and never reached the target host.
Nmap is a powerful tool that can ‚Äúcase the joint‚Äù not only for open TCP ports, 
but also for open UDP ports, for firewalls and their configurations, and even for 
the¬†versions of applications and operating systems. Most of this is done by manip-
ulating TCP connection-management segments. You can download nmap from 
www.nmap.org.
This completes our introduction to error control and flow control in TCP. In 
Section 3.7, we‚Äôll return to TCP and look at TCP congestion control in some depth. 
Before doing so, however, we first step back and examine congestion-control issues 
in a broader context.
3.6 Principles of Congestion Control
In the previous sections, we examined both the general principles and specific TCP 
mechanisms used to provide for a reliable data transfer service in the face of packet 
loss. We mentioned earlier that, in practice, such loss typically results from the over-
flowing of router buffers as the network becomes congested. Packet retransmission 
thus treats a symptom of network congestion (the loss of a specific transport-layer 
segment) but does not treat the cause of network congestion‚Äîtoo many sources 
attempting to send data at too high a rate. To treat the cause of network congestion, 
mechanisms are needed to throttle senders in the face of network congestion.
In this section, we consider the problem of congestion control in a general con-
text, seeking to understand why congestion is a bad thing, how network congestion 
is manifested in the performance received by upper-layer applications, and various 
approaches that can be taken to avoid, or react to, network congestion. This more 
general study of congestion control is appropriate since, as with reliable data trans-
fer, it is high on our ‚Äútop-ten‚Äù list of fundamentally important problems in network-
ing. The following section contains a detailed study of TCP‚Äôs congestion-control 
algorithm.
3.6.1 The Causes and the Costs of Congestion
Let‚Äôs begin our general study of congestion control by examining three increas-
ingly complex scenarios in which congestion occurs. In each case, we‚Äôll look at why

congestion occurs in the first place and at the cost of congestion (in terms of resources 
not fully utilized and poor performance received by the end systems). We‚Äôll not (yet) 
focus on how to react to, or avoid, congestion but rather focus on the simpler issue of 
understanding what happens as hosts increase their transmission rate and the network 
becomes congested.
Scenario 1: Two Senders, a Router with Infinite Buffers
We begin by considering perhaps the simplest congestion scenario possible: Two 
hosts (A and B) each have a connection that shares a single hop between source and 
destination, as shown in Figure 3.43.
Let‚Äôs assume that the application in Host A is sending data into the connection 
(for example, passing data to the transport-level protocol via a socket) at an average 
rate of lin bytes/sec. These data are original in the sense that each unit of data is sent 
into the socket only once. The underlying transport-level protocol is a simple one. 
Data is encapsulated and sent; no error recovery (e.g., retransmission), flow control, 
or congestion control is performed. Ignoring the additional overhead due to adding 
transport- and lower-layer header information, the rate at which Host A offers traffic 
to the router in this first scenario is thus lin bytes/sec. Host B operates in a similar 
manner, and we assume for simplicity that it too is sending at a rate of lin bytes/sec. 
Packets from Hosts A and B pass through a router and over a shared outgoing link 
of capacity R. The router has buffers that allow it to store incoming packets when 
the packet-arrival rate exceeds the outgoing link‚Äôs capacity. In this first scenario, we 
assume that the router has an infinite amount of buffer space.
Figure 3.44 plots the performance of Host A‚Äôs connection under this first sce-
nario. The left graph plots the per-connection throughput (number of bytes per 
Host B
Unlimited shared
output link buffers
lin: original data
Host A
Host D
Host C
lout 
Figure 3.43 ‚ô¶  Congestion scenario 1: Two connections sharing a single 
hop with infinite buffers

second at the receiver) as a function of the connection-sending rate. For a sending 
rate between 0 and R/2, the throughput at the receiver equals the sender‚Äôs sending 
rate‚Äîeverything sent by the sender is received at the receiver with a finite delay. 
When the sending rate is above R/2, however, the throughput is only R/2. This upper 
limit on throughput is a consequence of the sharing of link capacity between two 
connections. The link simply cannot deliver packets to a receiver at a steady-state 
rate that exceeds R/2. No matter how high Hosts A and B set their sending rates, they 
will each never see a throughput higher than R/2.
Achieving a per-connection throughput of R/2 might actually appear to be a good 
thing, because the link is fully utilized in delivering packets to their destinations. The 
right-hand graph in Figure 3.44, however, shows the consequence of operating near link 
capacity. As the sending rate approaches R/2 (from the left), the average delay becomes 
larger and larger. When the sending rate exceeds R/2, the average number of queued 
packets in the router is unbounded, and the average delay between source and destina-
tion becomes infinite (assuming that the connections operate at these sending rates for 
an infinite period of time and there is an infinite amount of buffering available). Thus, 
while operating at an aggregate throughput of near R may be ideal from a throughput 
standpoint, it is far from ideal from a delay standpoint. Even in this (extremely) ideal-
ized scenario, we‚Äôve already found one cost of a congested network‚Äîlarge queuing 
delays are experienced as the packet-arrival rate nears the link capacity.
Scenario 2: Two Senders and a Router with Finite Buffers
Let‚Äôs now slightly modify scenario 1 in the following two ways (see Figure 3.45). 
First, the amount of router buffering is assumed to be finite. A consequence of this 
real-world assumption is that packets will be dropped when arriving to an already-
full buffer. Second, we assume that each connection is reliable. If a packet containing 
R/2
R/2
Delay
R/2
lin
lin
lout
a.
b.
Figure 3.44 ‚ô¶  Congestion scenario 1: Throughput and delay as a function 
of host sending rate

a transport-level segment is dropped at the router, the sender will eventually retrans-
mit it. Because packets can be retransmitted, we must now be more careful with our 
use of the term sending rate. Specifically, let us again denote the rate at which the 
application sends original data into the socket by lin bytes/sec. The rate at which the 
transport layer sends segments (containing original data and retransmitted data) into 
the network will be denoted l‚Ä≤in bytes/sec. l‚Ä≤in is sometimes referred to as the offered 
load to the network.
The performance realized under scenario 2 will now depend strongly on how 
retransmission is performed. First, consider the unrealistic case that Host A is able 
to somehow (magically!) determine whether or not a buffer is free in the router and 
thus sends a packet only when a buffer is free. In this case, no loss would occur, lin 
would be equal to l‚Ä≤in, and the throughput of the connection would be equal to lin. 
This case is shown in Figure 3.46(a). From a throughput standpoint, performance 
is ideal‚Äîeverything that is sent is received. Note that the average host sending rate 
cannot exceed R/2 under this scenario, since packet loss is assumed never to occur.
Consider next the slightly more realistic case that the sender retransmits only 
when a packet is known for certain to be lost. (Again, this assumption is a bit of 
a stretch. However, it is possible that the sending host might set its timeout large 
enough to be virtually assured that a packet that has not been acknowledged has been 
lost.) In this case, the performance might look something like that shown in Fig-
ure¬†3.46(b). To appreciate what is happening here, consider the case that the offered 
load, l‚Ä≤in (the rate of original data transmission plus retransmissions), equals R/2. 
According to Figure 3.46(b), at this value of the offered load, the rate at which data 
Finite shared output
link buffers
Host B
Host A
Host D
Host C
lout 
lin: original data
l‚Äôin: original data, plus
retransmitted data
Figure 3.45 ‚ô¶  Scenario 2: Two hosts (with retransmissions) and a router 
with finite buffers

are delivered to the receiver application is R/3. Thus, out of the 0.5R units of data 
transmitted, 0.333R bytes/sec (on average) are original data and 0.166R bytes/sec (on 
average) are retransmitted data. We see here another cost of a congested network‚Äî
the sender must perform retransmissions in order to compensate for dropped (lost) 
packets due to buffer overflow.
Finally, let us consider the case that the sender may time out prematurely and 
retransmit a packet that has been delayed in the queue but not yet lost. In this case, 
both the original data packet and the retransmission may reach the receiver. Of 
course, the receiver needs but one copy of this packet and will discard the retrans-
mission. In this case, the work done by the router in forwarding the retransmitted 
copy of the original packet was wasted, as the receiver will have already received 
the original copy of this packet. The router would have better used the link trans-
mission capacity to send a different packet instead. Here then is yet another cost of 
a congested network‚Äîunneeded retransmissions by the sender in the face of large 
delays may cause a router to use its link bandwidth to forward unneeded copies of a 
packet. Figure 3.46 (c) shows the throughput versus offered load when each packet 
is assumed to be forwarded (on average) twice by the router. Since each packet is 
forwarded twice, the throughput will have an asymptotic value of R/4 as the offered 
load approaches R/2.
Scenario 3: Four Senders, Routers with Finite Buffers, and  
Multihop Paths
In our final congestion scenario, four hosts transmit packets, each over overlap-
ping two-hop paths, as shown in Figure 3.47. We again assume that each host uses 
a timeout/retransmission mechanism to implement a reliable data transfer service, 
that all hosts have the same value of lin, and that all router links have capacity  
R bytes/sec.
R/2
R/2
R/2
lout
a.
b.
R/2
lout
R/3
R/2
R/2
lout
R/4
c.
l‚Äôin
l‚Äôin
l‚Äôin
Figure 3.46 ‚ô¶ Scenario 2 performance with finite buffers

Let‚Äôs consider the connection from Host A to Host C, passing through routers 
R1 and R2. The A‚ÄìC connection shares router R1 with the D‚ÄìB connection and 
shares router R2 with the B‚ÄìD connection. For extremely small values of lin, buffer 
overflows are rare (as in congestion scenarios 1 and 2), and the throughput approxi-
mately equals the offered load. For slightly larger values of lin, the corresponding 
throughput is also larger, since more original data is being transmitted into the net-
work and delivered to the destination, and overflows are still rare. Thus, for small 
values of lin, an increase in lin results in an increase in lout.
Having considered the case of extremely low traffic, let‚Äôs next examine the case 
that lin (and hence l‚Ä≤in) is extremely large. Consider router R2. The A‚ÄìC traffic 
arriving to router R2 (which arrives at R2 after being forwarded from R1) can have 
an arrival rate at R2 that is at most R, the capacity of the link from R1 to R2, regard-
less of the value of lin. If l‚Ä≤in is extremely large for all connections (including the  
Host B
Host A
R1
R4
R2
R3
Host C
Host D
Finite shared output
link buffers
lin: original data
l‚Äôin: original
data, plus
retransmitted
data
lout
Figure 3.47 ‚ô¶ Four senders, routers with finite buffers, and multihop paths

B‚ÄìD connection), then the arrival rate of B‚ÄìD traffic at R2 can be much larger than 
that of the A‚ÄìC traffic. Because the A‚ÄìC and B‚ÄìD traffic must compete at router 
R2 for the limited amount of buffer space, the amount of A‚ÄìC traffic that success-
fully gets through R2 (that is, is not lost due to buffer overflow) becomes smaller 
and smaller as the offered load from B‚ÄìD gets larger and larger. In the limit, as the 
offered load approaches infinity, an empty buffer at R2 is immediately filled by a 
B‚ÄìD packet, and the throughput of the A‚ÄìC connection at R2 goes to zero. This, in 
turn, implies that the A‚ÄìC end-to-end throughput goes to zero in the limit of heavy 
traffic. These considerations give rise to the offered load versus throughput tradeoff 
shown in Figure 3.48.
The reason for the eventual decrease in throughput with increasing offered 
load is evident when one considers the amount of wasted work done by the net-
work. In the high-traffic scenario outlined above, whenever a packet is dropped at 
a second-hop router, the work done by the first-hop router in forwarding a packet 
to the second-hop router ends up being ‚Äúwasted.‚Äù The network would have been 
equally well off (more accurately, equally bad off) if the first router had simply 
discarded that packet and remained idle. More to the point, the transmission capac-
ity used at the first router to forward the packet to the second router could have 
been much more profitably used to transmit a different packet. (For example, when 
selecting a packet for transmission, it might be better for a router to give priority 
to packets that have already traversed some number of upstream routers.) So here 
we see yet another cost of dropping a packet due to congestion‚Äîwhen a packet 
is dropped along a path, the transmission capacity that was used at each of the 
upstream links to forward that packet to the point at which it is dropped ends up 
having been wasted.
R/2
lout
l‚Äôin
Figure 3.48 ‚ô¶  Scenario 3 performance with finite buffers and multihop 
paths

3.6.2 Approaches to Congestion Control
In Section 3.7, we‚Äôll examine TCP‚Äôs specific approach to congestion control in great 
detail. Here, we identify the two broad approaches to congestion control that are 
taken in practice and discuss specific network architectures and congestion-control 
protocols embodying these approaches.
At the highest level, we can distinguish among congestion-control approaches 
by whether the network layer provides explicit assistance to the transport layer for 
congestion-control purposes:
‚Ä¢ End-to-end congestion control. In an end-to-end approach to congestion control, 
the network layer provides no explicit support to the transport layer for conges-
tion-control purposes. Even the presence of network congestion must be inferred 
by the end systems based only on observed network behavior (for example, packet 
loss and delay). We‚Äôll see shortly in Section 3.7.1 that TCP takes this end-to-end 
approach toward congestion control, since the IP layer is not required to provide 
feedback to hosts regarding network congestion. TCP segment loss (as indicated 
by a timeout or the receipt of three duplicate acknowledgments) is taken as an 
indication of network congestion, and TCP decreases its window size accord-
ingly. We‚Äôll also see a more recent proposal for TCP congestion control that 
uses increasing round-trip segment delay as an indicator of increased network  
congestion
‚Ä¢ Network-assisted congestion control. With network-assisted congestion control, 
routers provide explicit feedback to the sender and/or receiver regarding the con-
gestion state of the network. This feedback may be as simple as a single bit indi-
cating congestion at a link‚Äîan approach taken in the early IBM SNA [Schwartz 
1982], DEC DECnet [Jain 1989; Ramakrishnan 1990] architectures, and ATM 
[Black 1995] network architectures. More sophisticated feedback is also possible. 
For example, in ATM Available Bite Rate (ABR) congestion control, a router 
informs the sender of the maximum host sending rate it (the router) can support 
on an outgoing link. As noted above, the Internet-default versions of IP and TCP 
adopt an end-to-end approach towards congestion control. We‚Äôll see, however, 
in Section 3.7.2 that, more recently, IP and TCP may also optionally implement 
network-assisted congestion control.
For network-assisted congestion control, congestion information is typically 
fed back from the network to the sender in one of two ways, as shown in Fig-
ure¬†3.49. Direct feedback may be sent from a network router to the sender. This 
form of notification typically takes the form of a choke packet (essentially say-
ing, ‚ÄúI‚Äôm congested!‚Äù). The second and more common form of notification occurs 
when a router marks/updates a field in a packet flowing from sender to receiver to 
indicate congestion. Upon receipt of a marked packet, the receiver then notifies 
the sender of the congestion indication. This latter form of notification takes a full 
round-trip time.

3.7 TCP Congestion Control
In this section, we return to our study of TCP. As we learned in Section 3.5, TCP provides 
a reliable transport service between two processes running on different hosts. Another key 
component of TCP is its congestion-control mechanism. As indicated in the previous sec-
tion, what we might refer to as ‚ÄúClassic‚Äù  TCP‚Äîthe version of TCP standardized in [RFC 
2581] and most recently [RFC 5681]‚Äîuses end-to-end congestion control rather than 
network-assisted congestion control, since the IP layer provides no explicit feedback to 
the end systems regarding network congestion. We‚Äôll first cover this ‚Äúclassic‚Äù version of 
TCP in depth in Section 7.3.1. In Section 7.3.2, we‚Äôll then look at newer flavors of TCP 
that use an explicit congestion indication provided by the network layer, or differ a bit 
from ‚Äúclassic‚Äù TCP in any of several different ways. We‚Äôll then cover the challenge of 
providing fairness among transport layer flows that must share a congested link.
3.7.1 Classic TCP Congestion Control
The approach taken by TCP is to have each sender limit the rate at which it sends traf-
fic into its connection as a function of perceived network congestion. If a TCP sender 
perceives that there is little congestion on the path between itself and the destination, 
then the TCP sender increases its send rate; if the sender perceives that there is conges-
tion along the path, then the sender reduces its send rate. But this approach raises three 
questions. First, how does a TCP sender limit the rate at which it sends traffic into its 
connection? Second, how does a TCP sender perceive that there is congestion on the 
path between itself and the destination? And third, what algorithm should the sender 
use to change its send rate as a function of perceived end-to-end congestion?
Host A
Network feedback via receiver
Direct network
feedback
Host B
Figure 3.49 ‚ô¶  Two feedback pathways for network-indicated congestion 
information

Let‚Äôs first examine how a TCP sender limits the rate at which it sends traffic into 
its connection. In Section 3.5, we saw that each side of a TCP connection consists of 
a receive buffer, a send buffer, and several variables (LastByteRead, rwnd, and 
so on). The TCP congestion-control mechanism operating at the sender keeps track 
of an additional variable, the congestion window. The congestion window, denoted 
cwnd, imposes a constraint on the rate at which a TCP sender can send traffic into 
the network. Specifically, the amount of unacknowledged data at a sender may not 
exceed the minimum of cwnd and rwnd, that is:
LastByteSent ‚Äì LastByteAcked ‚Ä¶ min{cwnd, rwnd}
In order to focus on congestion control (as opposed to flow control), let us henceforth 
assume that the TCP receive buffer is so large that the receive-window constraint can 
be ignored; thus, the amount of unacknowledged data at the sender is solely limited 
by cwnd. We will also assume that the sender always has data to send, that is, that 
all segments in the congestion window are sent.
The constraint above limits the amount of unacknowledged data at the sender and 
therefore indirectly limits the sender‚Äôs send rate. To see this, consider a connection for 
which loss and packet transmission delays are negligible. Then, roughly, at the begin-
ning of every RTT, the constraint permits the sender to send cwnd bytes of data into the 
connection; at the end of the RTT the sender receives acknowledgments for the data. 
Thus the sender‚Äôs send rate is roughly cwnd/RTT bytes/sec. By adjusting the value of 
cwnd, the sender can therefore adjust the rate at which it sends data into its connection.
Let‚Äôs next consider how a TCP sender perceives that there is congestion on the 
path between itself and the destination. Let us define a ‚Äúloss event‚Äù at a TCP sender 
as the occurrence of either a timeout or the receipt of three duplicate ACKs from the 
receiver. (Recall our discussion in Section 3.5.4 of the timeout event in Figure 3.33 
and the subsequent modification to include fast retransmit on receipt of three dupli-
cate ACKs.) When there is excessive congestion, then one (or more) router buffers 
along the path overflows, causing a datagram (containing a TCP segment) to be 
dropped. The dropped datagram, in turn, results in a loss event at the sender‚Äîeither 
a timeout or the receipt of three duplicate ACKs‚Äîwhich is taken by the sender to be 
an indication of congestion on the sender-to-receiver path.
Having considered how congestion is detected, let‚Äôs next consider the more opti-
mistic case when the network is congestion-free, that is, when a loss event doesn‚Äôt 
occur. In this case, acknowledgments for previously unacknowledged segments 
will be received at the TCP sender. As we‚Äôll see, TCP will take the arrival of these 
acknowledgments as an indication that all is well‚Äîthat segments being transmitted 
into the network are being successfully delivered to the destination‚Äîand will use 
acknowledgments to increase its congestion window size (and hence its transmis-
sion rate). Note that if acknowledgments arrive at a relatively slow rate (e.g., if the 
end-end path has high delay or contains a low-bandwidth link), then the congestion 
window will be increased at a relatively slow rate. On the other hand, if acknowl-
edgments arrive at a high rate, then the congestion window will be increased more

quickly. Because TCP uses acknowledgments to trigger (or clock) its increase in 
congestion window size, TCP is said to be self-clocking.
Given the mechanism of adjusting the value of cwnd to control the sending rate, 
the critical question remains: How should a TCP sender determine the rate at which it 
should send? If TCP senders collectively send too fast, they can congest the network, 
leading to the type of congestion collapse that we saw in Figure 3.48. Indeed, the ver-
sion of TCP that we‚Äôll study shortly was developed in response to observed Internet 
congestion collapse [Jacobson 1988] under earlier versions of TCP. However, if TCP 
senders are too cautious and send too slowly, they could under utilize the bandwidth 
in the network; that is, the TCP senders could send at a higher rate without congest-
ing the network. How then do the TCP senders determine their sending rates such 
that they don‚Äôt congest the network but at the same time make use of all the avail-
able bandwidth? Are TCP senders explicitly coordinated, or is there a distributed 
approach in which the TCP senders can set their sending rates based only on local 
information? TCP answers these questions using the following guiding principles:
‚Ä¢ A lost segment implies congestion, and hence, the TCP sender‚Äôs rate should be 
decreased when a segment is lost. Recall from our discussion in Section 3.5.4, 
that a timeout event or the receipt of four acknowledgments for a given segment 
(one original ACK and then three duplicate ACKs) is interpreted as an implicit 
‚Äúloss event‚Äù indication of the segment following the quadruply ACKed segment, 
triggering a retransmission of the lost segment. From a congestion-control stand-
point, the question is how the TCP sender should decrease its congestion window 
size, and hence its sending rate, in response to this inferred loss event.
‚Ä¢ An acknowledged segment indicates that the network is delivering the sender‚Äôs 
segments to the receiver, and hence, the sender‚Äôs rate can be increased when an 
ACK arrives for a previously unacknowledged segment. The arrival of acknowl-
edgments is taken as an implicit indication that all is well‚Äîsegments are being 
successfully delivered from sender to receiver, and the network is thus not con-
gested. The congestion window size can thus be increased.
‚Ä¢ Bandwidth probing. Given ACKs indicating a congestion-free source-to-destina-
tion path and loss events indicating a congested path, TCP‚Äôs strategy for adjusting 
its transmission rate is to increase its rate in response to arriving ACKs until a loss 
event occurs, at which point, the transmission rate is decreased. The TCP sender 
thus increases its transmission rate to probe for the rate that at which congestion 
onset begins, backs off from that rate, and then to begins probing again to see 
if the congestion onset rate has changed. The TCP sender‚Äôs behavior is perhaps 
analogous to the child who requests (and gets) more and more goodies until finally 
he/she is finally told ‚ÄúNo!‚Äù, backs off a bit, but then begins making requests again 
shortly afterward. Note that there is no explicit signaling of congestion state by 
the network‚ÄîACKs and loss events serve as implicit signals‚Äîand that each TCP 
sender acts on local information asynchronously from other TCP senders.
Given this overview of TCP congestion control, we‚Äôre now in a position to consider the 
details of the celebrated TCP congestion-control algorithm, which was first described

in [Jacobson 1988] and is standardized in [RFC 5681]. The algorithm has three major 
components: (1) slow start, (2) congestion avoidance, and (3) fast recovery. Slow start 
and congestion avoidance are mandatory components of TCP, differing in how they 
increase the size of cwnd in response to received ACKs. We‚Äôll see shortly that slow 
start increases the size of cwnd more rapidly (despite its name!) than congestion avoid-
ance. Fast recovery is recommended, but not required, for TCP senders.
Slow Start
When a TCP connection begins, the value of cwnd is typically initialized to a small 
value of 1 MSS [RFC 3390], resulting in an initial sending rate of roughly MSS/
RTT. For example, if MSS = 500 bytes and RTT = 200 msec, the resulting initial  
sending rate is only about 20 kbps. Since the available bandwidth to the TCP sender 
may be much larger than MSS/RTT, the TCP sender would like to find the amount of 
available bandwidth quickly. Thus, in the slow-start state, the value of cwnd begins 
at 1 MSS and increases by 1 MSS every time a transmitted segment is first acknowl-
edged. In the example of Figure 3.50, TCP sends the first segment into the network 
Host A
Host B
one segment
two segments
four segments
RTT
Time
Time
Figure 3.50 ‚ô¶ TCP slow start

and waits for an acknowledgment. When this acknowledgment arrives, the TCP 
sender increases the congestion window by one MSS and sends out two maximum-
sized segments. These segments are then acknowledged, with the sender increasing 
the congestion window by 1 MSS for each of the acknowledged segments, giving a 
congestion window of 4 MSS, and so on. This process results in a doubling of the 
sending rate every RTT. Thus, the TCP send rate starts slow but grows exponentially 
during the slow start phase.
But when should this exponential growth end? Slow start provides several 
answers to this question. First, if there is a loss event (i.e., congestion) indicated 
by a timeout, the TCP sender sets the value of cwnd to 1 and begins the slow start 
process anew. It also sets the value of a second state variable, ssthresh (short-
hand for ‚Äúslow start threshold‚Äù) to cwnd/2‚Äîhalf of the value of the congestion 
window value when congestion was detected. The second way in which slow start 
may end is directly tied to the value of ssthresh. Since ssthresh is half the 
value of cwnd when congestion was last detected, it might be a bit reckless to keep 
doubling¬†cwnd when it reaches or surpasses the value of ssthresh. Thus, when 
the value of cwnd equals ssthresh, slow start ends and TCP transitions into con-
gestion avoidance mode. As we‚Äôll see, TCP increases cwnd more cautiously when 
in congestion-avoidance mode. The final way in which slow start can end is if three 
duplicate ACKs are detected, in which case TCP performs a fast retransmit (see Sec-
tion 3.5.4) and enters the fast recovery state, as discussed below. TCP‚Äôs behavior in 
slow start is summarized in the FSM description of TCP congestion control in Figure 
3.51. The slow-start algorithm traces it roots to [Jacobson 1988]; an approach similar 
to slow start was also proposed independently in [Jain 1986].
Congestion Avoidance
On entry to the congestion-avoidance state, the value of cwnd is approximately half 
its value when congestion was last encountered‚Äîcongestion could be just around 
the corner! Thus, rather than doubling the value of cwnd every RTT, TCP adopts a 
more conservative approach and increases the value of cwnd by just a single MSS 
every RTT [RFC 5681]. This can be accomplished in several ways. A common 
approach is for the TCP sender to increase cwnd by MSS bytes (MSS/cwnd) when-
ever a new acknowledgment arrives. For example, if MSS is 1,460 bytes and cwnd 
is 14,600¬†bytes, then 10 segments are being sent within an RTT. Each arriving ACK 
(assuming one ACK per segment) increases the congestion window size by 1/10 
MSS, and thus, the value of the congestion window will have increased by one MSS 
after ACKs when all 10 segments have been received.
But when should congestion avoidance‚Äôs linear increase (of 1 MSS per RTT) 
end? TCP‚Äôs congestion-avoidance algorithm behaves the same when a timeout occurs 
as in the case of slow start: The value of cwnd is set to 1 MSS, and the value of 
ssthresh is updated to half the value of cwnd when the loss event occurred. Recall, 
however, that a loss event also can be triggered by a triple duplicate ACK event.

In this case, the network is continuing to deliver some segments from sender to receiver 
(as indicated by the receipt of duplicate ACKs). So TCP‚Äôs behavior to this type of loss 
event should be less drastic than with a timeout-indicated loss: TCP halves the value 
of cwnd (adding in 3 MSS for good measure to account for the triple duplicate ACKs 
received) and records the value of ssthresh to be half the value of cwnd when the 
triple duplicate ACKs were received. The fast-recovery state is then entered.
Fast Recovery
In fast recovery, the value of cwnd is increased by 1 MSS for every duplicate 
ACK received for the missing segment that caused TCP to enter the fast-recovery 
state. Eventually, when an ACK arrives for the missing segment, TCP enters the  
Slow
start
duplicate ACK
dupACKcount++
duplicate ACK
dupACKcount++
timeout
ssthresh=cwnd/2
cwnd=1 MSS
dupACKcount=0
cwnd=1 MSS
ssthresh=64 KB
dupACKcount=0
timeout
ssthresh=cwnd/2
cwnd=1
dupACKcount=0
timeout
ssthresh=cwnd/2
cwnd=1 MSS
dupACKcount=0
cwnd $ssthresh
Congestion
avoidance
Fast
recovery
new ACK
cwnd=cwnd+MSS ‚Ä¢(MSS/cwnd)
dupACKcount=0
transmit new segment(s), as allowed
new ACK
cwnd=cwnd+MSS
dupACKcount=0
transmit new segment(s), as allowed
retransmit missing segment
retransmit missing segment
dupACKcount==3
ssthresh=cwnd/2
cwnd=ssthresh+3‚Ä¢MSS
retransmit missing segment
duplicate ACK
cwnd=cwnd+MSS
transmit new segment(s), as allowed
dupACKcount==3
ssthresh=cwnd/2
cwnd=ssthresh+3‚Ä¢MSS
retransmit missing segment
retransmit missing segment
new ACK
cwnd=ssthresh
dupACKcount=0
L
L
Figure 3.51 ‚ô¶ FSM description of TCP congestion control
Examining the behavior 
of TCP
VideoNote

congestion-avoidance state after deflating cwnd. If a timeout event occurs, fast 
recovery transitions to the slow-start state after performing the same actions as in 
slow start and congestion avoidance: The value of cwnd is set to 1 MSS, and the 
value of ssthresh is set to half the value of cwnd when the loss event occurred.
TCP SPLITTING: OPTIMIZING THE PERFORMANCE OF CLOUD SERVICES
For cloud services such as search, e-mail, and social networks, it is desirable to provide a 
high-level of responsiveness, ideally giving users the illusion that the services are running 
within their own end systems (including their smartphones). This can be a major challenge, 
as users are often located far away from the data centers responsible for serving the 
dynamic content associated with the cloud services. Indeed, if the end system is far from 
a data center, then the RTT will be large, potentially leading to poor response time perfor-
mance due to TCP slow start.
As a case study, consider the delay in receiving a response for a search query. 
Typically, the server requires three TCP windows during slow start to deliver the response 
[Pathak 2010]. Thus the time from when an end system initiates a TCP connection until the 
time when it receives the last packet of the response is roughly 4 # RTT (one RTT to set up 
the TCP connection plus three RTTs for the three windows of data) plus the processing time 
in the data center. These RTT delays can lead to a noticeable delay in returning search 
results for a significant fraction of queries. Moreover, there can be significant packet loss 
in access networks, leading to TCP retransmissions and even larger delays.
One way to mitigate this problem and improve user-perceived performance is to  
(1) deploy front-end servers closer to the users, and (2) utilize TCP splitting by break-
ing the TCP connection at the front-end server. With TCP splitting, the client establishes 
a TCP connection to the nearby front-end, and the front-end maintains a persistent TCP 
connection to the data center with a very large TCP congestion window [Tariq 2008, 
Pathak 2010, Chen 2011]. With this approach, the response time roughly becomes 
4 # RTTFE + RTTBE + processing time, where RTTFE is the round-trip time between client and 
front-end server, and RTTBE is the round-trip time between the front-end server and the data 
center (back-end server). If the front-end server is close to client, then this response time 
approximately becomes RTT plus processing time, since RTTFE is negligibly small and RTTBE 
is approximately RTT. In summary, TCP splitting can reduce the networking delay roughly 
from 4 # RTT to RTT, significantly improving user-perceived performance, particularly for 
users who are far from the nearest data center. TCP splitting also helps reduce TCP  
retransmission delays caused by losses in access networks. Google and Akamai have 
made extensive use of their CDN servers in access networks (recall our discussion in 
Section 2.6) to perform TCP splitting for the cloud services they support [Chen 2011].
PRINCIPLES IN PRACTICE

Fast recovery is a recommended, but not required, component of TCP [RFC 
5681]. It is interesting that an early version of TCP, known as TCP Tahoe, uncon-
ditionally cut its congestion window to 1 MSS and entered the slow-start phase after 
either a timeout-indicated or triple-duplicate-ACK-indicated loss event. The newer 
version of TCP, TCP Reno, incorporated fast recovery.
Figure 3.52 illustrates the evolution of TCP‚Äôs congestion window for both Reno 
and Tahoe. In this figure, the threshold is initially equal to 8 MSS. For the first 
eight transmission rounds, Tahoe and Reno take identical actions. The congestion 
window climbs exponentially fast during slow start and hits the threshold at the fourth 
round of transmission. The congestion window then climbs linearly until a triple 
duplicate- ACK event occurs, just after transmission round 8. Note that the congestion 
window is 12 # MSS when this loss event occurs. The value of ssthresh is then set 
to 0.5 # cwnd = 6 # MSS. Under TCP Reno, the congestion window is set to cwnd =  
9 # MSS and then grows linearly. Under TCP Tahoe, the congestion window is set to 
1 MSS and grows exponentially until it reaches the value of ssthresh, at which 
point it grows linearly.
Figure 3.51 presents the complete FSM description of TCP‚Äôs congestion-control 
algorithms‚Äîslow start, congestion avoidance, and fast recovery. The figure also 
indicates where transmission of new segments or retransmitted segments can occur. 
Although it is important to distinguish between TCP error control/retransmission and 
TCP congestion control, it‚Äôs also important to appreciate how these two aspects of 
TCP are inextricably linked.
TCP Congestion Control: Retrospective
Having delved into the details of slow start, congestion avoidance, and fast recovery,  
it‚Äôs worthwhile to now step back and view the forest from the trees. Ignoring the 
0
1
0
2
3
4
5
6
7
8
Transmission round
TCP Tahoe
ssthresh
ssthresh
Congestion window
(in segments)
9
10 11 12 13 14 15
2
4
6
8
10
12
14
16
TCP Reno
Figure 3.52 ‚ô¶ Evolution of TCP‚Äôs congestion window (Tahoe and Reno)

initial slow-start period when a connection begins and assuming that losses are indi-
cated by triple duplicate ACKs rather than timeouts, TCP‚Äôs congestion control con-
sists of linear (additive) increase in cwnd of 1 MSS per RTT and then a halving 
(multiplicative decrease) of cwnd on a triple duplicate-ACK event. For this reason, 
TCP congestion control is often referred to as an additive-increase, multiplicative-
decrease (AIMD) form of congestion control. AIMD congestion control gives rise 
to the ‚Äúsaw tooth‚Äù behavior shown in Figure 3.53, which also nicely illustrates our 
earlier intuition of TCP ‚Äúprobing‚Äù for bandwidth‚ÄîTCP linearly increases its conges-
tion window size (and hence its transmission rate) until a triple duplicate-ACK event 
occurs. It then decreases its congestion window size by a factor of two but then again 
begins increasing it linearly, probing to see if there is additional available bandwidth.
TCP‚Äôs AIMD algorithm was developed based on a tremendous amount of 
engineering insight and experimentation with congestion control in operational 
networks. Ten years after TCP‚Äôs development, theoretical analyses showed that 
TCP‚Äôs congestion-control algorithm serves as a distributed asynchronous-optimization 
algorithm that results in several important aspects of user and network performance 
being simultaneously optimized [Kelly 1998]. A rich theory of congestion control 
has since been developed [Srikant 2012].
TCP Cubic
Given TCP Reno‚Äôs additive-increase, multiplicative-decrease approach to conges-
tion control, one might naturally wonder whether this is the best way to ‚Äúprobe‚Äù for a 
packet sending rate that is just below the threshold of triggering packet loss. Indeed, 
cutting the sending rate in half (or even worse, cutting the sending rate to one packet 
per RTT as in an earlier version of TCP known as TCP Tahoe) and then increasing 
rather slowly over time may be overly cautious. If the state of the  congested link 
24 K
16 K
8 K
Time
Congestion window
Figure 3.53 ‚ô¶ Additive-increase, multiplicative-decrease congestion control

where packet loss occurred hasn‚Äôt changed much, then perhaps it‚Äôs better to more 
quickly ramp up the sending rate to get close to the pre-loss sending rate and only 
then probe cautiously for bandwidth. This insight lies at the heart of a flavor of TCP 
known as TCP CUBIC [Ha 2008, RFC 8312].
TCP CUBIC differs only slightly from TCP Reno. Once again, the congestion 
window is increased only on ACK receipt, and the slow start and fast recovery phases 
remain the same. CUBIC only changes the congestion avoidance phase, as follows:
‚Ä¢ Let Wmax be size of TCP‚Äôs congestion control window when loss was last detected, 
and let K be the future point in time when TCP CUBIC‚Äôs window size will again reach 
Wmax, assuming no losses. Several tunable CUBIC parameters determine the value K, 
that is, how quickly the protocol‚Äôs congestion window size would reach Wmax.
‚Ä¢ CUBIC increases the congestion window as a function of cube of the distance 
between the current time, t, and K. Thus, when t is further away from K, the 
congestion window size increases are much larger than when t is close to K. That 
is, CUBIC quickly ramps up TCP‚Äôs sending rate to get close to the pre-loss rate, 
Wmax, and only then probes cautiously for bandwidth as it approaches Wmax.
‚Ä¢ When t is greater than K, the cubic rule implies that CUBIC‚Äôs congestion window 
increases are small when t is still close to K (which is good if the congestion 
level of the link causing loss hasn‚Äôt changed much) but then increases rapidly as 
t exceeds K (which allows CUBIC to more quickly find a new operating point if 
the congestion level of the link that caused loss has changed significantly).
Under these rules, the idealized performance of TCP Reno and TCP CUBIC are 
compared in Figure 3.54, adapted from [Huston 2017]. We see the slow start phase 
Time
TCP
sending
rate
TCP Reno
TCP CUBIC
Wmax
t0
t1
t2
t3
t4
Key:
Figure 3.54 ‚ô¶  TCP congestion avoidance sending rates: TCP Reno and 
TCP CUBIC

ending at t0. Then, when congestion loss occurs at t1, t2, and t3, CUBIC more quickly 
ramps up close to Wmax (thereby enjoying more overall throughput than TCP Reno). 
We can see graphically how TCP CUBIC attempts to maintain the flow for as long 
as possible just below the (unknown to the sender) congestion threshold. Note that 
at t3, the congestion level has presumably decreased appreciably, allowing both TCP 
Reno and TCP CUBIC to achieve sending rates higher than Wmax.
TCP CUBIC has recently gained wide deployment. While measurements 
taken around 2000 on popular Web servers showed that nearly all were running 
some  version of TCP Reno [Padhye 2001], more recent measurements of the 5000 
most popular Web servers shows that nearly 50% are running a version of TCP 
CUBIC [Yang 2014], which is also the default version of TCP used in the Linux 
operating system.
Macroscopic Description of TCP Reno Throughput
Given the saw-toothed behavior of TCP Reno, it‚Äôs natural to consider what the 
average throughput (that is, the average rate) of a long-lived TCP Reno connec-
tion might be. In this analysis, we‚Äôll ignore the slow-start phases that occur after 
timeout events. (These phases are typically very short, since the sender grows out 
of the phase exponentially fast.) During a particular round-trip interval, the rate at 
which TCP sends data is a function of the congestion window and the current RTT. 
When the window size is w bytes and the current round-trip time is RTT seconds, 
then TCP‚Äôs transmission rate is roughly w/RTT. TCP then probes for additional 
bandwidth by increasing w by 1 MSS each RTT until a loss event occurs. Denote by 
W the value of w when a loss event occurs. Assuming that RTT and W are approxi-
mately constant over the duration of the connection, the TCP transmission rate 
ranges from W/(2 ¬∑ RTT) to W/RTT.
These assumptions lead to a highly simplified macroscopic model for the steady-
state behavior of TCP. The network drops a packet from the connection when the rate 
increases to W/RTT; the rate is then cut in half and then increases by MSS/RTT every 
RTT until it again reaches W/RTT. This process repeats itself over and over again. 
Because TCP‚Äôs throughput (that is, rate) increases linearly between the two extreme 
values, we have
average throughput of a connection = 0.75 # W
RTT
Using this highly idealized model for the steady-state dynamics of TCP, we 
can also derive an interesting expression that relates a connection‚Äôs loss rate to its 
available bandwidth [Mathis 1997]. This derivation is outlined in the homework 
problems. A more sophisticated model that has been found empirically to agree with 
measured data is [Padhye 2000].

3.7.2  Network-Assisted Explicit Congestion Notification 
and Delayed-based Congestion Control
Since the initial standardization of slow start and congestion avoidance in the late 
1980‚Äôs [RFC 1122], TCP has implemented the form of end-end congestion control 
that we studied in Section 3.7.1: a TCP sender receives no explicit congestion indica-
tions from the network layer, and instead infers congestion through observed packet 
loss. More recently, extensions to both IP and TCP [RFC 3168] have been proposed, 
implemented, and deployed that allow the network to explicitly signal congestion to 
a TCP sender and receiver. In addition, a number of variations of TCP congestion 
control protocols have been proposed that infer congestion using measured packet 
delay. We‚Äôll take a look at both network-assisted and delay-based congestion control 
in this section.
Explicit Congestion Notification
Explicit Congestion Notification [RFC 3168] is the form of network-assisted con-
gestion control performed within the Internet. As shown in Figure 3.55, both TCP 
and IP are involved. At the network layer, two bits (with four possible values, 
 overall) in the Type of Service field of the IP datagram header (which we‚Äôll discuss 
in Section 4.3) are used for ECN.
One setting of the ECN bits is used by a router to indicate that it (the router) is 
experiencing congestion. This congestion indication is then carried in the marked 
IP datagram to the destination host, which then informs the sending host, as shown 
in Figure 3.55. RFC 3168 does not provide a definition of when a router is con-
gested; that decision is a configuration choice made possible by the router vendor, 
and decided by the network operator. However, the intuition is that the congestion 
indication bit can be set to signal the onset of congestion to the send before loss actu-
ally occurs. A second setting of the ECN bits is used by the sending host to inform 
routers that the sender and receiver are ECN-capable, and thus capable of taking 
action in response to ECN-indicated network congestion.
As shown in Figure 3.55, when the TCP in the receiving host receives an ECN 
congestion indication via a received datagram, the TCP in the receiving host informs 
the TCP in the sending host of the congestion indication by setting the ECE (Explicit 
Congestion Notification Echo) bit (see Figure 3.29) in a receiver-to-sender TCP 
ACK segment. The TCP sender, in turn, reacts to an ACK with a congestion indica-
tion by halving the congestion window, as it would react to a lost segment using fast 
retransmit, and sets the CWR (Congestion Window Reduced) bit in the header of the 
next transmitted TCP sender-to-receiver segment.
Other transport-layer protocols besides TCP may also make use of network-
layer-signaled ECN. The Datagram Congestion Control Protocol (DCCP) [RFC 
4340] provides a low-overhead, congestion-controlled UDP-like unreliable service 
that utilizes ECN. DCTCP (Data Center TCP) [Alizadeh 2010, RFC 8257] and

DCQCN (Data Center Quantized Congestion Notification) [Zhu 2015] designed 
 specifically for data center networks, also makes use of ECN. Recent Internet meas-
urements show increasing deployment of ECN capabilities in popular servers as well 
as in routers along paths to those servers [K√ºhlewind 2013].
Delay-based Congestion Control
Recall from our ECN discussion above that a congested router can set the congestion 
indication bit to signal congestion onset to senders before full buffers cause  packets 
to be dropped at that router. This allows senders to decrease their sending rates 
 earlier, hopefully before packet loss, thus avoiding costly packet loss and retrans-
mission. A second congestion-avoidance approach takes a delay-based approach to 
also proactively detect congestion onset before packet loss occurs.
In TCP Vegas [Brakmo 1995], the sender measures the RTT of the source-to-
destination path for all acknowledged packets. Let RTTmin be the minimum of these 
measurements at a sender; this occurs when the path is uncongested and packets 
experience minimal queueing delay. If TCP Vegas‚Äô congestion window size is cwnd, 
then the uncongested throughput rate would be cwnd/RTTmin. The intuition behind 
TCP Vegas is that if the actual sender-measured throughput is close to this value, the 
TCP sending rate can be increased since (by definition and by measurement) the path 
is not yet congested. However, if the actual sender-measured throughput is signifi-
cantly less than the uncongested throughput rate, the path is congested and the Vegas 
TCP sender will decrease its sending rate. Details can be found in [Brakmo 1995].
ECN Echo = 1
Host A
Host B
ECN = 11
ECN Echo bit set in 
receiver-to-sender
TCP ACK segment
ECN bits set in IP
datagram header
at congested router
Figure 3.55 ‚ô¶  Explicit Congestion Notification: network-assisted  
congestion control

TCP Vegas operates under the intuition that TCP senders should ‚ÄúKeep the pipe 
just full, but no fuller‚Äù [Kleinrock 2018]. ‚ÄúKeeping the pipe full‚Äù means that links 
(in particular the bottleneck link that is limiting a connection‚Äôs throughput) are kept 
busy transmitting, doing useful work; ‚Äúbut no fuller‚Äù means that there is nothing to 
gain (except increased delay!) if large queues are allowed to build up while the pipe 
is kept full.
The BBR congestion control protocol [Cardwell 2017] builds on ideas in TCP 
Vegas, and incorporates mechanisms that allows it compete fairly (see Section 3.7.3) 
with TCP non-BBR senders. [Cardwell 2017] reports that in 2016, Google began 
using BBR for all TCP traffic on its private B4 network [Jain 2013] that intercon-
nects Google data centers, replacing CUBIC. It is also being deployed on Google and 
YouTube Web servers. Other delay-based TCP congestion control protocols include 
TIMELY for data center networks [Mittal 2015], and Compound TCP (CTPC) [Tan 
2006] and FAST [Wei 2006] for high-speed and long distance networks.
3.7.3 Fairness
Consider K TCP connections, each with a different end-to-end path, but all pass-
ing through a bottleneck link with transmission rate R bps. (By bottleneck link, we 
mean that for each connection, all the other links along the connection‚Äôs path are not 
congested and have abundant transmission capacity as compared with the transmis-
sion capacity of the bottleneck link.) Suppose each connection is transferring a large 
file and there is no UDP traffic passing through the bottleneck link. A congestion-
control mechanism is said to be fair if the average transmission rate of each connec-
tion is approximately R/K; that is, each connection gets an equal share of the link  
bandwidth.
Is TCP‚Äôs AIMD algorithm fair, particularly given that different TCP connec-
tions may start at different times and thus may have different window sizes at a given 
point in time? [Chiu 1989] provides an elegant and intuitive explanation of why TCP 
congestion control converges to provide an equal share of a bottleneck link‚Äôs band-
width among competing TCP connections.
Let‚Äôs consider the simple case of two TCP connections sharing a single link 
with transmission rate R, as shown in Figure 3.55. Assume that the two connections 
have the same MSS and RTT (so that if they have the same congestion window size, 
then they have the same throughput), that they have a large amount of data to send, 
and that no other TCP connections or UDP datagrams traverse this shared link. Also, 
ignore the slow-start phase of TCP and assume the TCP connections are operating in 
CA mode (AIMD) at all times.
Figure 3.56 plots the throughput realized by the two TCP connections. If TCP is 
to share the link bandwidth equally between the two connections, then the realized 
throughput should fall along the 45-degree arrow (equal bandwidth share) emanat-
ing from the origin. Ideally, the sum of the two throughputs should equal R. (Cer-
tainly, each connection receiving an equal, but zero, share of the link capacity is not

a desirable situation!) So the goal should be to have the achieved throughputs fall 
somewhere near the intersection of the equal bandwidth share line and the full band-
width utilization line in Figure 3.56.
Suppose that the TCP window sizes are such that at a given point in time, con-
nections 1 and 2 realize throughputs indicated by point A in Figure 3.56. Because the 
amount of link bandwidth jointly consumed by the two connections is less than R, no 
loss will occur, and both connections will increase their window by 1 MSS per RTT 
as a result of TCP‚Äôs congestion-avoidance algorithm. Thus, the joint throughput of 
the two connections proceeds along a 45-degree line (equal increase for both connec-
tions) starting from point A. Eventually, the link bandwidth jointly consumed by the 
two connections will be greater than R, and eventually packet loss will occur. Sup-
pose that connections 1 and 2 experience packet loss when they realize throughputs 
indicated by point B. Connections 1 and 2 then decrease their windows by a factor of 
two. The resulting throughputs realized are thus at point C, halfway along a vector 
starting at B and ending at the origin. Because the joint bandwidth use is less than R 
at point C, the two connections again increase their throughputs along a 45-degree 
line starting from C. Eventually, loss will again occur, for example, at point D, and 
the two connections again decrease their window sizes by a factor of two, and so on. 
You should convince yourself that the bandwidth realized by the two connections 
eventually fluctuates along the equal bandwidth share line. You should also convince 
yourself that the two connections will converge to this behavior regardless of where 
they are in the two-dimensional space! Although a number of idealized assumptions 
lie behind this scenario, it still provides an intuitive feel for why TCP results in an 
equal sharing of bandwidth among connections.
In our idealized scenario, we assumed that only TCP connections traverse the 
bottleneck link, that the connections have the same RTT value, and that only a 
 single TCP connection is associated with a host-destination pair. In practice, these 
 conditions are typically not met, and client-server applications can thus obtain very 
unequal portions of link bandwidth. In particular, it has been shown that when 
TCP connection 2
TCP connection 1
Bottleneck
router capacity R
Figure 3.56 ‚ô¶ Two TCP connections sharing a single bottleneck link

multiple connections share a common bottleneck, those sessions with a smaller RTT 
are able to grab the available bandwidth at that link more quickly as it becomes free 
(that is, open their congestion windows faster) and thus will enjoy higher throughput 
than those connections with larger RTTs [Lakshman 1997].
Fairness and UDP
We have just seen how TCP congestion control regulates an application‚Äôs trans-
mission rate via the congestion window mechanism. Many multimedia applications, 
such as Internet phone and video conferencing, often do not run over TCP for this 
very reason‚Äîthey do not want their transmission rate throttled, even if the network 
is very congested. Instead, these applications prefer to run over UDP, which does 
not have built-in congestion control. When running over UDP, applications can 
pump their audio and video into the network at a constant rate and occasionally lose 
packets, rather than reduce their rates to ‚Äúfair‚Äù levels at times of congestion and not 
lose any packets. From the perspective of TCP, the multimedia applications running 
over UDP are not being fair‚Äîthey do not cooperate with the other connections nor 
adjust their transmission rates appropriately. Because TCP congestion control will 
decrease its transmission rate in the face of increasing congestion (loss), while UDP 
sources need not, it is possible for UDP sources to crowd out TCP traffic. A number 
R
R
Equal
bandwidth
share
Connection 1 throughput
Connection 2 throughput
D
B
C
A
Full bandwidth
utilization line
Figure 3.57 ‚ô¶ Throughput realized by TCP connections 1 and 2

of congestion-control mechanisms have been proposed for the Internet that prevent 
UDP traffic from bringing the Internet‚Äôs throughput to a grinding halt [Floyd 1999; 
Floyd 2000; Kohler 2006; RFC 4340].
Fairness and Parallel TCP Connections
But even if we could force UDP traffic to behave fairly, the fairness problem would 
still not be completely solved. This is because there is nothing to stop a TCP-based 
application from using multiple parallel connections. For example, Web browsers 
often use multiple parallel TCP connections to transfer the multiple objects within 
a Web page. (The exact number of multiple connections is configurable in most 
browsers.) When an application uses multiple parallel connections, it gets a larger 
fraction of the bandwidth in a congested link. As an example, consider a link of rate 
R supporting nine ongoing client-server applications, with each of the applications 
using one TCP connection. If a new application comes along and also uses one TCP 
connection, then each application gets approximately the same transmission rate of 
R/10. But if this new application instead uses 11 parallel TCP connections, then the 
new application gets an unfair allocation of more than R/2. Because Web traffic is so 
pervasive in the Internet, multiple parallel connections are not uncommon.
3.8 Evolution of Transport-Layer Functionality
Our discussion of specific Internet transport protocols in this chapter has focused on 
UDP and TCP‚Äîthe two ‚Äúwork horses‚Äù of the Internet transport layer. However, as 
we‚Äôve seen, three decades of experience with these two protocols has identified cir-
cumstances in which neither is ideally suited, and so the design and implementation 
of transport layer functionality has continued to evolve.
We‚Äôve seen a rich evolution in the use of TCP over the past decade. In 
 Sections 3.7.1 and 3.7.2, we learned that in addition to ‚Äúclassic‚Äù versions of TCP 
such as TCP Tahoe and Reno, there are now several newer versions of TCP that have 
been developed, implemented, deployed, and are in significant use today. These 
include TCP CUBIC, DCTCP, CTCP, BBR, and more. Indeed, measurements in 
[Yang 2014] indicate that CUBIC (and its predecessor, BIC [Xu 2004]) and CTCP 
are more widely deployed on Web servers than classic TCP Reno; we also saw that 
BBR is being deployed in Google‚Äôs internal B4 network, as well as on many of 
Google‚Äôs public-facing servers.
And there are many (many!) more versions of TCP! There are versions of TCP 
specifically designed for use over wireless links, over high-bandwidth paths with 
large RTTs, for paths with packet re-ordering, and for short paths strictly within data 
centers. There are versions of TCP that implement different priorities among TCP

connections competing for bandwidth at a bottleneck link, and for TCP connections 
whose segments are being sent over different source-destination paths in parallel. 
There are also variations of TCP that deal with packet acknowledgment and TCP 
session establishment/closure differently than we studied in Section 3.5.6. Indeed, 
it‚Äôs probably not even correct anymore to refer to ‚Äúthe‚Äù TCP protocol; perhaps the 
only common features of these protocols is that they use the TCP segment format that 
we studied in Figure 3.29, and that they should compete ‚Äúfairly‚Äù amongst themselves 
in the face of network congestion! For a survey of the many flavors of TCP, see 
 [Afanasyev 2010] and [Narayan 2018].
QUIC: Quick UDP Internet Connections
If the transport services needed by an application don‚Äôt quite fit either the UDP 
or TCP service models‚Äîperhaps an application needs more services than those 
provided by UDP but does not want all of the particular functionality that comes 
with TCP, or may want different services than those provided by TCP‚Äîapplica-
tion designers can always ‚Äúroll their own‚Äù protocol at the application layer. This 
is the approach taken in the QUIC (Quick UDP Internet Connections) protocol 
[Langley 2017, QUIC 2020]. Specifically, QUIC is a new application-layer pro-
tocol designed from the ground up to improve the performance of transport-layer 
services for secure HTTP. QUIC has already been widely deployed, although is 
still in the process of being standardized as an Internet RFC [QUIC 2020]. Google 
has deployed QUIC on many of its public-facing Web servers, in its mobile video 
streaming YouTube app, in its Chrome browser, and in Android‚Äôs Google Search 
app. With more than 7% of Internet traffic today now being QUIC [Langley 2017], 
we‚Äôll want to take a closer look. Our study of QUIC will also serve as a nice culmi-
nation of our study of the transport layer, as QUIC uses many of the approaches for 
reliable data transfer, congestion control, and connection management that we‚Äôve 
studied in this chapter.
As shown in Figure 3.58, QUIC is an application-layer protocol, using UDP as 
its underlying transport-layer protocol, and is designed to interface above specifi-
cally to a simplified but evolved version of HTTP/2. In the near future, HTTP/3 
will natively incorporate QUIC [HTTP/3 2020]. Some of QUIC‚Äôs major features 
include:
‚Ä¢ Connection-Oriented and Secure. Like TCP, QUIC is a connection-oriented 
protocol between two endpoints. This requires a handshake between endpoints 
to¬† set up the QUIC connection state. Two pieces of connection state are the 
source and destination connection ID. All QUIC packets are encrypted, and as 
suggested in Figure 3.58, QUIC combines the handshakes needed to establish 
connection state with those needed for authentication and encryption (transport 
layer security topics that we‚Äôll study in Chapter 8), thus providing faster estab-
lishment than the protocol stack in Figure 3.58(a), where multiple RTTs are

required to first establish a TCP connection, and then establish a TLS connection 
over the TCP connection.
‚Ä¢ Streams. QUIC allows several different application-level ‚Äústreams‚Äù to be mul-
tiplexed through a single QUIC connection, and once a QUIC connection is 
established, new streams can be quickly added. A stream is an abstraction for the 
reliable, in-order bi-directional delivery of data between two QUIC endpoints. In  
the context of HTTP/3, there would be a different stream  for each object in a Web 
page. Each connection has a connection ID, and each stream within a connection 
has a stream ID; both of these IDs are contained in a QUIC packet header (along 
with other header information). Data from multiple streams may be contained 
within a single QUIC segment, which is carried over UDP. The Stream Control 
Transmission Protocol (SCTP) [RFC 4960, RFC 3286] is an earlier reliable, mes-
sage-oriented protocol that pioneered the notion of multiplexing multiple appli-
cation-level ‚Äústreams‚Äù through a single SCTP connection. We‚Äôll see in Chapter 7 
that SCTP is used in  control plane protocols in 4G/5G cellular wireless networks.
‚Ä¢ Reliable, TCP-friendly congestion-controlled data transfer. As illustrated 
in Figure 3.59(b), QUIC provides reliable data transfer to each QUIC stream 
separately. Figure 3.59(a) shows the case of HTTP/1.1 sending multiple HTTP 
requests, all over a single TCP connection. Since TCP provides reliable, in-order 
byte delivery, this means that the multiple HTTP requests must be delivered in-
order at the destination HTTP server. Thus, if bytes from one HTTP request are 
lost, the remaining HTTP requests can not be delivered until those lost bytes are 
retransmitted and correctly received by TCP at the HTTP server‚Äîthe so-called 
HOL blocking problem that we encountered earlier in Section 2.2.5. Since QUIC 
provides a reliable in-order delivery on a per-stream basis, a lost UDP segment 
only impacts those streams whose data was carried in that segment; HTTP mes-
sages in other streams can continue to be received and delivered to the applica-
tion. QUIC provides reliable data transfer using acknowledgment mechanisms 
similar to TCP‚Äôs, as specified in [RFC 5681].
IP
TCP
TLS
HTTP/2
IP
UDP
QUIC
HTTP/2 (slimmed)
Network
Transport
Application
HTTP/3
a.
b.
Figure 3.58 ‚ô¶  (a) traditional secure HTTP protocol stack, and the 
(b) secure QUIC-based HTTP/3 protocol stack

QUIC‚Äôs congestion control is based on TCP NewReno [RFC 6582], a slight 
modification to the TCP Reno protocol that we studied in Section 3.7.1. QUIC‚Äôs 
Draft specification [QUIC-recovery 2020] notes ‚ÄúReaders familiar with TCP‚Äôs 
loss detection and congestion control will find algorithms here that parallel well-
known TCP ones.‚Äù Since we‚Äôve carefully studied TCP‚Äôs congestion control in 
Section 3.7.1, we‚Äôd be right at home reading the details of QUIC‚Äôs draft specifica-
tion of its congestion control algorithm!
In closing, it‚Äôs worth highlighting again that QUIC is an application-layer 
 protocol providing reliable, congestion-controlled data transfer between two 
 endpoints. The authors of QUIC [Langley 2017] stress that this means that changes 
can be made to QUIC at ‚Äúapplication-update timescales,‚Äù that is, much faster than 
TCP or UDP update timescales.
3.9 Summary
We began this chapter by studying the services that a transport-layer protocol can 
provide to network applications. At one extreme, the transport-layer protocol can be 
very simple and offer a no-frills service to applications, providing only a multiplexing/
demultiplexing function for communicating processes. The Internet‚Äôs UDP  protocol 
TLS encryption 
TCP RDT
TCP CC
TCP RDT
TCP CC
TLS encryption 
HTTP
request
HTTP
request
HTTP
request
QUIC congestion control 
QUIC
encryption
QUIC
encryption
QUIC
encryption
QUIC RDT
QUIC RDT
QUIC RDT
HTTP
request
HTTP
request
HTTP
request
QUIC congestion control 
QUIC
encryption
QUIC
encryption
QUIC
encryption
QUIC RDT
QUIC RDT
QUIC RDT
UDP
UDP
Transport
Application
a.  HTTP 1.1
b.  HTTP/3
HTTP
request
HTTP
request
HTTP
request
HTTP
request
HTTP
request
HTTP
request
Figure 3.59 ‚ô¶  (a) HTTP/1.1: a single-connection client and server using application-level TLS 
encryption over TCP‚Äôs reliable data transfer (RDT) and congestion control (CC) 
(b)¬†HTTP/3: a  multi-stream client and server using QUIC‚Äôs encryption, reliable 
data transfer and congestion  control over UDP‚Äôs unreliable datagram service

is an example of such a no-frills transport-layer protocol. At the other extreme, a 
 transport-layer  protocol can provide a variety of guarantees to applications, such as 
reliable delivery of data, delay guarantees, and bandwidth guarantees. Nevertheless, 
the services that a transport protocol can provide are often constrained by the service 
model of the underlying network-layer protocol. If the network-layer protocol cannot 
provide delay or bandwidth guarantees to transport-layer segments, then the transport-
layer protocol cannot provide delay or bandwidth guarantees for the messages sent 
between processes.
We learned in Section 3.4 that a transport-layer protocol can provide reliable 
data transfer even if the underlying network layer is unreliable. We saw that provid-
ing reliable data transfer has many subtle points, but that the task can be accom-
plished by carefully combining acknowledgments, timers, retransmissions, and 
sequence numbers.
Although we covered reliable data transfer in this chapter, we should keep 
in mind that reliable data transfer can be provided by link-, network-, transport-, 
or application-layer protocols. Any of the upper four layers of the protocol 
stack can implement acknowledgments, timers, retransmissions, and sequence 
numbers and provide reliable data transfer to the layer above. In fact, over 
the years, engineers and computer scientists have independently designed and 
implemented link-, network-, transport-, and application-layer protocols that 
provide reliable data transfer (although many of these protocols have quietly 
disappeared).
In Section 3.5, we took a close look at TCP, the Internet‚Äôs connection-oriented 
and reliable transport-layer protocol. We learned that TCP is complex, involving con-
nection management, flow control, and round-trip time estimation, as well as reli-
able data transfer. In fact, TCP is actually more complex than our description‚Äîwe 
intentionally did not discuss a variety of TCP patches, fixes, and improvements that 
are widely implemented in various versions of TCP. All of this complexity, however, 
is hidden from the network application. If a client on one host wants to send data 
reliably to a server on another host, it simply opens a TCP socket to the server and 
pumps data into that socket. The client-server application is blissfully unaware of 
TCP‚Äôs complexity.
In Section 3.6, we examined congestion control from a broad perspective, and 
in Section 3.7, we showed how TCP implements congestion control. We learned that 
congestion control is imperative for the well-being of the network. Without conges-
tion control, a network can easily become gridlocked, with little or no data being 
transported end-to-end. In Section 3.7, we learned that classic TCP implements an 
end-to-end congestion-control mechanism that additively increases its transmission 
rate when the TCP connection‚Äôs path is judged to be congestion-free, and multiplica-
tively decreases its transmission rate when loss occurs. This mechanism also strives 
to give each TCP connection passing through a congested link an equal share of the 
link bandwidth. We also studied several newer variations of TCP congestion control

that try to determine TCP‚Äôs sending rate rate more quickly than classic TCP, use a 
delay-based approach or explicit congestion notification from the network (rather 
than a loss-based approach) to determine TCP‚Äôs sending rate. We also examined in 
some depth the impact of TCP connection establishment and slow start on latency. 
We observed that in many important scenarios, connection establishment and slow 
start significantly contribute to end-to-end delay. We emphasize once more that 
while TCP congestion control has evolved over the years, it remains an area of 
intensive research and will likely continue to evolve in the upcoming years. To 
wrap up this chapter, in Section 3.8, we studied recent developments in implement-
ing many of the transport layer‚Äôs functions‚Äîreliable data transfer, congestion con-
trol, connection establishment, and more‚Äîin the application layer using the QUIC 
protocol.
In Chapter 1, we said that a computer network can be partitioned into the 
 ‚Äúnetwork edge‚Äù and the ‚Äúnetwork core.‚Äù The network edge covers everything 
that happens in the end systems. Having now covered the application layer and 
the t ransport layer, our discussion of the network edge is complete. It is time to 
explore the network core! This journey begins in the next two chapters, where 
we‚Äôll study the network layer, and continues into Chapter 6, where we‚Äôll study the 
link layer.
Homework Problems and Questions
Chapter 3 Review Questions
SECTIONS 3.1‚Äì3.3 
 R1. Suppose the network layer provides the following service. The network 
layer in the source host accepts a segment of maximum size 1,200 bytes and 
a destination host address from the transport layer. The network layer then 
guarantees to deliver the segment to the transport layer at the destination 
host. Suppose many network application processes can be running at the 
destination host.
a. Design the simplest possible transport-layer protocol that will get applica-
tion data to the desired process at the destination host. Assume the operat-
ing system in the destination host has assigned a 4-byte port number to 
each running application process.
b. Modify this protocol so that it provides a ‚Äúreturn address‚Äù to the destina-
tion process.
c. In your protocols, does the transport layer ‚Äúhave to do anything‚Äù in the 
core of the computer network?

lives in its own house, each house has a unique address, and each person 
in a given house has a unique name. Suppose this planet has a mail service 
that delivers letters from source house to destination house. The mail service 
requires that (1) the letter be in an envelope, and that (2) the address of the 
destination house (and nothing more) be clearly written on the envelope. Sup-
pose each family has a delegate family member who collects and distributes 
letters for the other family members. The letters do not necessarily provide 
any indication of the recipients of the letters.
a. Using the solution to Problem R1 above as inspiration, describe a protocol 
that the delegates can use to deliver letters from a sending family member 
to a receiving family member.
b. In your protocol, does the mail service ever have to open the envelope and 
examine the letter in order to provide its service?
 R3. Consider a TCP connection between Host A and Host B. Suppose that the 
TCP segments traveling from Host A to Host B have source port number x 
and destination port number y. What are the source and destination port num-
bers for the segments traveling from Host B to Host A?
 R4. Describe why an application developer might choose to run an application 
over UDP rather than TCP.
 R5. Why is it that voice and video traffic is often sent over TCP rather than UDP 
in today‚Äôs Internet? (Hint: The answer we are looking for has nothing to do 
with TCP‚Äôs congestion-control mechanism.)
 R6. Is it possible for an application to enjoy reliable data transfer even when the 
application runs over UDP? If so, how?
 R7. Suppose a process in Host C has a UDP socket with port number 6789. 
Suppose both Host A and Host B each send a UDP segment to Host C with 
destination port number 6789. Will both of these segments be directed to the 
same socket at Host C? If so, how will the process at Host C know that these 
two segments originated from two different hosts?
 R8. Suppose that a Web server runs in Host C on port 80. Suppose this Web 
server uses persistent connections, and is currently receiving requests from 
two different Hosts, A and B. Are all of the requests being sent through the 
same socket at Host C? If they are being passed through different sockets, do 
both of the sockets have port 80? Discuss and explain.
SECTION 3.4
 R9. In our rdt protocols, why did we need to introduce sequence numbers?
 R10. In our rdt protocols, why did we need to introduce timers?
HOMEWORK PROBLEMS AND QUESTIONS     285

R11. Suppose that the roundtrip delay between sender and receiver is constant and 
known to the sender. Would a timer still be necessary in protocol rdt 3.0, 
assuming that packets can be lost? Explain.
 R12. Visit the Go-Back-N interactive animation at the companion Web site.
a. Have the source send five packets, and then pause the animation before 
any of the five packets reach the destination. Then kill the first packet and 
resume the animation. Describe what happens.
b. Repeat the experiment, but now let the first packet reach the destination 
and kill the first acknowledgment. Describe again what happens.
c. Finally, try sending six packets. What happens?
 R13. Repeat R12, but now with the Selective Repeat interactive animation. How 
are Selective Repeat and Go-Back-N different?
SECTION 3.5
 R14. True or false?
a. Host A is sending Host B a large file over a TCP connection. Assume Host 
B has no data to send Host A. Host B will not send acknowledgments to 
Host A because Host B cannot piggyback the acknowledgments on data.
b. The size of the TCP rwnd never changes throughout the duration of the 
connection.
c. Suppose Host A is sending Host B a large file over a TCP connection. The 
number of unacknowledged bytes that A sends cannot exceed the size of 
the receive buffer.
d. Suppose Host A is sending a large file to Host B over a TCP connection. 
If the sequence number for a segment of this connection is m, then the 
sequence number for the subsequent segment will necessarily be m + 1.
e. The TCP segment has a field in its header for rwnd.
f. Suppose that the last SampleRTT in a TCP connection is equal to 1 sec. 
The current value of TimeoutInterval for the connection will neces-
sarily be √ö 1 sec.
g. Suppose Host A sends one segment with sequence number 38 and 4 
bytes of data over a TCP connection to Host B. In this same segment, the 
acknowledgment number is necessarily 42.
 R15. Suppose Host A sends two TCP segments back to back to Host B over a 
TCP connection. The first segment has sequence number 90; the second has 
sequence number 110.
a. How much data is in the first segment?
b. Suppose that the first segment is lost but the second segment arrives at 
B. In the acknowledgment that Host B sends to Host A, what will be the 
acknowledgment number?

R16. Consider the Telnet example discussed in Section 3.5. A few seconds after 
the user types the letter ‚ÄòC,‚Äô the user types the letter ‚ÄòR.‚Äô After typing the let-
ter ‚ÄòR,‚Äô how many segments are sent, and what is put in the sequence number 
and acknowledgment fields of the segments?
SECTION 3.7
 R17. Suppose two TCP connections are present over some bottleneck link of rate R 
bps. Both connections have a huge file to send (in the same direction over the 
bottleneck link). The transmissions of the files start at the same time. What 
transmission rate would TCP like to give to each of the connections?
 R18. True or false? Consider congestion control in TCP. When the timer expires at 
the sender, the value of ssthresh is set to one half of its previous value.
 R19. In the discussion of TCP splitting in the sidebar in Section 3.7, it was 
claimed that the response time with TCP splitting is approximately 
4 # RTTFE + RTTBE + processing time. Justify this claim.
Problems
 P1. Suppose Client A initiates a Telnet session with Server S. At about the same 
time, Client B also initiates a Telnet session with Server S. Provide possible 
source and destination port numbers for
a. The segments sent from A to S.
b. The segments sent from B to S.
c. The segments sent from S to A.
d. The segments sent from S to B.
e. If A and B are different hosts, is it possible that the source port number in 
the segments from A to S is the same as that from B to S?
f. How about if they are the same host?
 P2. Consider Figure 3.5. What are the source and destination port values in the 
segments flowing from the server back to the clients‚Äô processes? What are 
the IP addresses in the network-layer datagrams carrying the transport-layer 
segments?
 P3. UDP and TCP use 1s complement for their checksums. Suppose you have 
the following three 8-bit bytes: 01010011, 01100110, 01110100. What is the 
1s complement of the sum of these 8-bit bytes? (Note that although UDP and 
TCP use 16-bit words in computing the checksum, for this problem you are 
being asked to consider 8-bit sums.) Show all work. Why is it that UDP takes 
the 1s complement of the sum; that is, why not just use the sum? With the 1s 
complement scheme, how does the receiver detect errors? Is it possible that a 
1-bit error will go undetected? How about a 2-bit error?

P4. a.  Suppose you have the following 2 bytes: 01011100 and 01100101. What 
is the 1s complement of the sum of these 2 bytes?
b. Suppose you have the following 2 bytes: 11011010 and 01100101. What 
is the 1s complement of the sum of these 2 bytes?
c. For the bytes in part (a), give an example where one bit is flipped in each 
of the 2 bytes and yet the 1s complement doesn‚Äôt change.
 P5. Suppose that the UDP receiver computes the Internet checksum for the 
received UDP segment and finds that it matches the value carried in the 
checksum field. Can the receiver be absolutely certain that no bit errors have 
occurred? Explain.
 P6. Consider our motivation for correcting protocol rdt2.1. Show that the 
receiver, shown in Figure 3.60, when operating with the sender shown in 
Figure 3.11, can lead the sender and receiver to enter into a deadlock state, 
where each is waiting for an event that will never occur.
 P7. In protocol rdt3.0, the ACK packets flowing from the receiver to the 
sender do not have sequence numbers (although they do have an ACK field 
that contains the sequence number of the packet they are acknowledging). 
Why is it that our ACK packets do not require sequence numbers?
Wait for
0 from
below
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
has_seq0(rcvpkt)))
compute chksum
make_pkt(sndpkt,NAK,chksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) &&
(corrupt(rcvpkt)||
has_seq1(rcvpkt)))
compute chksum
make_pkt(sndpkt,NAK,chksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && notcorrupt(rcvpkt)
&& has_seq1(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
compute chksum
make_pkt(sendpkt,ACK,chksum)
udt_send(sndpkt)
rdt_rcv(rcvpkt) && notcorrupt(rcvpkt)
&& has_seq0(rcvpkt)
extract(rcvpkt,data)
deliver_data(data)
compute chksum
make_pkt(sendpkt,ACK,chksum)
udt_send(sndpkt)
Wait for
1 from
below
Figure 3.60 ‚ô¶ An incorrect receiver for protocol rdt 2.1

P8. Draw the FSM for the receiver side of protocol rdt3.0.
 P9. Give a trace of the operation of protocol rdt3.0 when data packets and 
acknowledgment packets are garbled. Your trace should be similar to that 
used in Figure 3.16.
 P10. Consider a channel that can lose packets but has a maximum delay that is 
known. Modify protocol rdt2.1 to include sender timeout and retransmit. 
Informally argue why your protocol can communicate correctly over this 
channel.
 P11. Consider the rdt2.2 receiver in Figure 3.14, and the creation of a new 
packet in the self-transition (i.e., the transition from the state back to 
itself) in the Wait-for-0-from-below and the Wait-for-1-from-below states: 
sndpkt=make_pkt(ACK,1,checksum) and sndpkt=make_
pkt(ACK,0,checksum). Would the protocol work correctly if this action 
were removed from the self-transition in the Wait-for-1-from-below state? 
Justify your answer. What if this event were removed from the self-transition 
in the Wait-for-0-from-below state? [Hint: In this latter case, consider what 
would happen if the first sender-to-receiver packet were corrupted.]
 P12. The sender side of rdt3.0 simply ignores (that is, takes no action on)  
all received packets that are either in error or have the wrong value in the 
acknum field of an acknowledgment packet. Suppose that in such circum-
stances, rdt3.0 were simply to retransmit the current data packet. Would 
the protocol still work? (Hint: Consider what would happen if there were 
only bit errors; there are no packet losses but premature timeouts can occur. 
Consider how many times the nth packet is sent, in the limit as n approaches 
infinity.)
 P13. Consider the rdt 3.0 protocol. Draw a diagram showing that if the 
network connection between the sender and receiver can reorder messages 
(that is, that two messages propagating in the medium between the sender 
and receiver can be reordered), then the alternating-bit protocol will not 
work correctly (make sure you clearly identify the sense in which it will 
not work correctly). Your diagram should have the sender on the left and 
the receiver on the right, with the time axis running down the page, show-
ing data (D) and acknowledgment (A) message exchange. Make sure you 
indicate the sequence number associated with any data or acknowledgment 
segment.
 P14. Consider a reliable data transfer protocol that uses only negative acknowledg-
ments. Suppose the sender sends data only infrequently. Would a NAK-only 
protocol be preferable to a protocol that uses ACKs? Why? Now suppose the 
sender has a lot of data to send and the end-to-end connection experiences 
few losses. In this second case, would a NAK-only protocol be preferable to 
a protocol that uses ACKs? Why?

P15. Consider the cross-country example shown in Figure 3.17. How big would 
the window size have to be for the channel utilization to be greater than 
98¬†percent? Suppose that the size of a packet is 1,500 bytes, including both 
header fields and data.
 P16. Suppose an application uses rdt 3.0 as its transport layer protocol. As the 
stop-and-wait protocol has very low channel utilization (shown in the cross-
country example), the designers of this application let the receiver keep send-
ing back a number (more than two) of alternating ACK 0 and ACK 1 even if 
the corresponding data have not arrived at the receiver. Would this applica-
tion design increase the channel utilization? Why? Are there any potential 
problems with this approach? Explain.
 P17. Consider two network entities, A and B, which are connected by a perfect 
bi-directional channel (i.e., any message sent will be received correctly; the 
channel will not corrupt, lose, or re-order packets). A and B are to deliver 
data messages to each other in an alternating manner: First, A must deliver 
a message to B, then B must deliver a message to A, then A must deliver a 
message to B and so on. If an entity is in a state where it should not attempt 
to deliver a message to the other side, and there is an event like rdt_
send(data) call from above that attempts to pass data down for transmis-
sion to the other side, this call from above can simply be ignored with a call 
to rdt_unable_to_send(data), which informs the higher layer that it 
is currently not able to send data. [Note: This simplifying assumption is made 
so you don‚Äôt have to worry about buffering data.]
 
 Draw a FSM specification for this protocol (one FSM for A, and one FSM 
for B!). Note that you do not have to worry about a reliability mechanism 
here; the main point of this question is to create a FSM specification that 
reflects the synchronized behavior of the two entities. You should use the 
following events and actions that have the same meaning as protocol rdt1.0 in 
Figure 3.9: rdt_send(data), packet = make_pkt(data), udt_
send(packet), rdt_rcv(packet), extract (packet,data), 
deliver_data(data). Make sure your protocol reflects the strict alter-
nation of sending between A and B. Also, make sure to indicate the initial 
states for A and B in your FSM descriptions.
 P18. In the generic SR protocol that we studied in Section 3.4.4, the sender 
transmits a message as soon as it is available (if it is in the window) without 
waiting for an acknowledgment. Suppose now that we want an SR protocol 
that sends messages two at a time. That is, the sender will send a pair of mes-
sages and will send the next pair of messages only when it knows that both 
messages in the first pair have been received correctly.
 
 Suppose that the channel may lose messages but will not corrupt or reorder 
messages. Design an error-control protocol for the unidirectional reliable

transfer of messages. Give an FSM description of the sender and receiver. 
Describe the format of the packets sent between sender and receiver, and vice 
versa. If you use any procedure calls other than those in Section 3.4  
(for example, udt_send(), start_timer(), rdt_rcv(), and so on), 
clearly state their actions. Give an example (a timeline trace of sender and 
receiver) showing how your protocol recovers from a lost packet.
 P19. Consider a scenario in which Host A wants to simultaneously send packets 
to Hosts B and C. A is connected to B and C via a broadcast channel‚Äîa 
packet sent by A is carried by the channel to both B and C. Suppose that 
the broadcast channel connecting A, B, and C can independently lose and 
corrupt packets (and so, for example, a packet sent from A might be cor-
rectly received by B, but not by C). Design a stop-and-wait-like error-control 
protocol for reliably transferring packets from A to B and C, such that A will 
not get new data from the upper layer until it knows that both B and C have 
correctly received the current packet. Give FSM descriptions of A and C. 
(Hint: The FSM for B should be essentially the same as for C.) Also, give a 
description of the packet format(s) used.
 P20. Consider a scenario in which Host A and Host B want to send messages to 
Host C. Hosts A and C are connected by a channel that can lose and corrupt 
(but not reorder) messages. Hosts B and C are connected by another channel 
(independent of the channel connecting A and C) with the same properties. 
The transport layer at Host C should alternate in delivering messages from  
A and B to the layer above (that is, it should first deliver the data from a packet 
from A, then the data from a packet from B, and so on). Design a stop-and-
wait-like error-control protocol for reliably transferring packets from A and 
B to C, with alternating delivery at C as described above. Give FSM descrip-
tions of A and C. (Hint: The FSM for B should be essentially the same as  
for A.) Also, give a description of the packet format(s) used.
 P21. Suppose we have two network entities, A and B. B has a supply of data mes-
sages that will be sent to A according to the following conventions. When A 
gets a request from the layer above to get the next data (D) message from B, 
A must send a request (R) message to B on the A-to-B channel. Only when B 
receives an R message can it send a data (D) message back to A on the B-to-
A channel. A should deliver exactly one copy of each D message to the layer 
above. R messages can be lost (but not corrupted) in the A-to-B channel; D 
messages, once sent, are always delivered correctly. The delay along both 
channels is unknown and variable.
 
 Design (give an FSM description of) a protocol that incorporates the appro-
priate mechanisms to compensate for the loss-prone A-to-B channel and 
implements message passing to the layer above at entity A, as discussed 
above. Use only those mechanisms that are absolutely necessary.

P22. Consider the GBN protocol with a sender window size of 4 and a sequence 
number range of 1,024. Suppose that at time t, the next in-order packet 
that the receiver is expecting has a sequence number of k. Assume that the 
medium does not reorder messages. Answer the following questions:
a. What are the possible sets of sequence numbers inside the sender‚Äôs  
window at time t? Justify your answer.
b. What are all possible values of the ACK field in all possible messages 
currently propagating back to the sender at time t? Justify your answer.
 P23. Consider the GBN and SR protocols. Suppose the sequence number space  
is of size k. What is the largest allowable sender window that will avoid  
the occurrence of problems such as that in Figure 3.27 for each of these 
protocols?
 P24. Answer true or false to the following questions and briefly justify your 
answer:
a. With the SR protocol, it is possible for the sender to receive an ACK for a 
packet that falls outside of its current window.
b. With GBN, it is possible for the sender to receive an ACK for a packet 
that falls outside of its current window.
c. The alternating-bit protocol is the same as the SR protocol with a sender 
and receiver window size of 1.
d. The alternating-bit protocol is the same as the GBN protocol with a sender 
and receiver window size of 1.
 P25. We have said that an application may choose UDP for a transport protocol 
because UDP offers finer application control (than TCP) of what data is sent 
in a segment and when.
Why does an application have more control of what data is sent in a segment?
Why does an application have more control on when the segment is sent?
 P26. Consider transferring an enormous file of L bytes from Host A to Host B. 
Assume an MSS of 536 bytes.
a. What is the maximum value of L such that TCP sequence numbers are not 
exhausted? Recall that the TCP sequence number field has 4 bytes.
b. For the L you obtain in (a), find how long it takes to transmit the file. 
Assume that a total of 66 bytes of transport, network, and data-link header 
are added to each segment before the resulting packet is sent out over a 
155 Mbps link. Ignore flow control and congestion control so A can pump 
out the segments back to back and continuously.
 P27. Host A and B are communicating over a TCP connection, and Host B has 
already received from A all bytes up through byte 126. Suppose Host A  
then sends two segments to Host B back-to-back. The first and second

segments contain 80 and 40 bytes of data, respectively. In the first segment, 
the sequence number is 127, the source port number is 302, and the des-
tination port number is 80. Host B sends an acknowledgment whenever it 
receives a segment from Host A.
a. In the second segment sent from Host A to B, what are the sequence num-
ber, source port number, and destination port number?
b. If the first segment arrives before the second segment, in the acknowledg-
ment of the first arriving segment, what is the acknowledgment number, 
the source port number, and the destination port number?
c. If the second segment arrives before the first segment, in the acknowledg-
ment of the first arriving segment, what is the acknowledgment number?
d. Suppose the two segments sent by A arrive in order at B. The first 
acknowledgment is lost and the second acknowledgment arrives after the 
first timeout interval. Draw a timing diagram, showing these segments 
and all other segments and acknowledgments sent. (Assume there is no 
additional packet loss.) For each segment in your figure, provide the 
sequence number and the number of bytes of data; for each acknowledg-
ment that you add, provide the acknowledgment number.
 P28. Host A and B are directly connected with a 100 Mbps link. There is one TCP 
connection between the two hosts, and Host A is sending to Host B an enor-
mous file over this connection. Host A can send its application data into its 
TCP socket at a rate as high as 120 Mbps but Host B can read out of its TCP 
receive buffer at a maximum rate of 50 Mbps. Describe the effect of TCP 
flow control.
 P29. SYN cookies were discussed in Section 3.5.6.
a. Why is it necessary for the server to use a special initial sequence number 
in the SYNACK?
b. Suppose an attacker knows that a target host uses SYN cookies. Can the 
attacker create half-open or fully open connections by simply sending an 
ACK packet to the target? Why or why not?
c. Suppose an attacker collects a large amount of initial sequence numbers sent 
by the server. Can the attacker cause the server to create many fully open 
connections by sending ACKs with those initial sequence numbers? Why?
 P30. Consider the network shown in Scenario 2 in Section 3.6.1. Suppose both 
sending hosts A and B have some fixed timeout values.
a. Argue that increasing the size of the finite buffer of the router might pos-
sibly decrease the throughput (lout).
b. Now suppose both hosts dynamically adjust their timeout values (like 
what TCP does) based on the buffering delay at the router. Would increas-
ing the buffer size help to increase the throughput? Why?

P31. Suppose that the five measured SampleRTT values (see Section 3.5.3) 
are 106 ms, 120 ms, 140 ms, 90 ms, and 115 ms. Compute the Estimat-
edRTT after each of these SampleRTT values is obtained, using a value of 
Œ± = 0.125 and assuming that the value of EstimatedRTT was 100 ms 
just before the first of these five samples were obtained. Compute also the 
DevRTT after each sample is obtained, assuming a value of Œ≤ = 0.25 and 
assuming the value of DevRTT was 5 ms just before the first of these five 
samples was obtained. Last, compute the TCP TimeoutInterval after 
each of these samples is obtained.
 P32. Consider the TCP procedure for estimating RTT. Suppose that Œ± = 0.1. Let 
SampleRTT1 be the most recent sample RTT, let SampleRTT2 be the next 
most recent sample RTT, and so on.
a. For a given TCP connection, suppose four acknowledgments have  
been returned with corresponding sample RTTs: SampleRTT4,  
SampleRTT3, SampleRTT2, and SampleRTT1. Express  
EstimatedRTT in terms of the four sample RTTs.
b. Generalize your formula for n sample RTTs.
c. For the formula in part (b) let n approach infinity. Comment on why this 
averaging procedure is called an exponential moving average.
 P33. In Section 3.5.3, we discussed TCP‚Äôs estimation of RTT. Why do you think 
TCP avoids measuring the SampleRTT for retransmitted segments?
 P34. What is the relationship between the variable SendBase in Section 3.5.4 
and the variable LastByteRcvd in Section 3.5.5?
 P35. What is the relationship between the variable LastByteRcvd in  
Section 3.5.5 and the variable y in Section 3.5.4?
 P36. In Section 3.5.4, we saw that TCP waits until it has received three dupli-
cate ACKs before performing a fast retransmit. Why do you think the TCP 
designers chose not to perform a fast retransmit after the first duplicate ACK 
for a segment is received?
 P37. Compare GBN, SR, and TCP (no delayed ACK). Assume that the timeout 
values for all three protocols are sufficiently long such that five consecutive 
data segments and their corresponding ACKs can be received (if not lost in 
the channel) by the receiving host (Host B) and the sending host (Host A) 
respectively. Suppose Host A sends five data segments to Host B, and the 
second segment (sent from A) is lost. In the end, all five data segments have 
been correctly received by Host B.
a. How many segments has Host A sent in total and how many ACKs has 
Host B sent in total? What are their sequence numbers? Answer this  
question for all three protocols.

then which protocol successfully delivers all five data segments in short-
est time interval?
 P38. In our description of TCP in Figure 3.53, the value of the threshold,  
ssthresh, is set as ssthresh=cwnd/2 in several places and 
ssthresh value is referred to as being set to half the window size when a 
loss event occurred. Must the rate at which the sender is sending when the 
loss event occurred be approximately equal to cwnd segments per RTT? 
Explain your answer. If your answer is no, can you suggest a different  
manner in which ssthresh should be set?
 P39. Consider Figure 3.46(b). If l‚Ä≤in increases beyond R/2, can lout increase 
beyond R/3? Explain. Now consider Figure 3.46(c). If l‚Ä≤in increases beyond 
R/2, can lout increase beyond R/4 under the assumption that a packet will be 
forwarded twice on average from the router to the receiver? Explain.
 P40. Consider Figure 3.61. Assuming TCP Reno is the protocol experiencing the 
behavior shown above, answer the following questions. In all cases, you 
should provide a short discussion justifying your answer.
a. Identify the intervals of time when TCP slow start is operating.
b. Identify the intervals of time when TCP congestion avoidance is operating.
c. After the 16th transmission round, is segment loss detected by a triple 
duplicate ACK or by a timeout?
d. After the 22nd transmission round, is segment loss detected by a triple 
duplicate ACK or by a timeout?
0
0
2
4
6
8
10 12
Transmission round
14 16 18 20 22 24 26
5
10
15
20
25
Congestion window size (segments)
30
35
40
45
Figure 3.61 ‚ô¶ TCP window size as a function of time
PROBLEMS     295
Examining the behavior 
of TCP
VideoNote

e. What is the initial value of ssthresh at the first transmission round?
f. What is the value of ssthresh at the 18th transmission round?
g. What is the value of ssthresh at the 24th transmission round?
h. During what transmission round is the 70th segment sent?
i. Assuming a packet loss is detected after the 26th round by the receipt of 
a triple duplicate ACK, what will be the values of the congestion window 
size and of ssthresh?
j. Suppose TCP Tahoe is used (instead of TCP Reno), and assume that triple 
duplicate ACKs are received at the 16th round. What are the ssthresh 
and the congestion window size at the 19th round?
k. Again suppose TCP Tahoe is used, and there is a timeout event at  
22nd round. How many packets have been sent out from 17th round till 
22nd round, inclusive?
 P41. Refer to Figure 3.55, which illustrates the convergence of TCP‚Äôs AIMD 
algorithm. Suppose that instead of a multiplicative decrease, TCP decreased 
the window size by a constant amount. Would the resulting AIAD algorithm 
converge to an equal share algorithm? Justify your answer using a diagram 
similar to Figure 3.55.
 P42. In Section 3.5.4, we discussed the doubling of the timeout interval after a 
timeout event. This mechanism is a form of congestion control. Why does 
TCP need a window-based congestion-control mechanism (as studied in  
Section 3.7) in addition to this doubling-timeout-interval mechanism?
 P43. Host A is sending an enormous file to Host B over a TCP connection. Over 
this connection there is never any packet loss and the timers never expire. 
Denote the transmission rate of the link connecting Host A to the Internet by 
R bps. Suppose that the process in Host A is capable of sending data into its 
TCP socket at a rate S bps, where S = 10 # R. Further suppose that the TCP 
receive buffer is large enough to hold the entire file, and the send buffer can 
hold only one percent of the file. What would prevent the process in Host 
A from continuously passing data to its TCP socket at rate S bps? TCP flow 
control? TCP congestion control? Or something else? Elaborate.
 P44. Consider sending a large file from a host to another over a TCP connection 
that has no loss.
a. Suppose TCP uses AIMD for its congestion control without slow start. 
Assuming cwnd increases by 1 MSS every time a batch of ACKs is 
received and assuming approximately constant round-trip times, how long 
does it take for cwnd increase from 6 MSS to 12 MSS (assuming no loss 
events)?
b. What is the average throughput (in terms of MSS and RTT) for this con-
nection up through time = 6 RTT?

tion loss next occurs drops to 0.75*Wmax (unbeknownst to the TCP senders, 
of course). Show the evolution of both TCP Reno and TCP CUBIC for two 
more rounds each (Hint: note that the times at which TCP Reno and TCP 
CUBIC react to congestion loss may not be the same anymore).
 P46. Consider Figure 3.54 again. Suppose that at t3, the sending rate at which conges-
tion loss next occurs increases to 1.5*Wmax. Show the evolution of both TCP 
Reno and TCP CUBIC for at two more rounds each (Hint: see the hint in P45).
 P47. Recall the macroscopic description of TCP throughput. In the period of time 
from when the connection‚Äôs rate varies from W/(2 ? RTT) to W/RTT, only one 
packet is lost (at the very end of the period).
a. Show that the loss rate (fraction of packets lost) is equal to
L = loss rate =
1
3
8 W2 + 3
4 W
b. Use the result above to show that if a connection has loss rate L, then its 
average rate is approximately given by
‚âà 1.22 # MSS
RTT 2L
 P48. Consider that only a single TCP (Reno) connection uses one 10 Mbps link 
which does not buffer any data. Suppose that this link is the only congested 
link between the sending and receiving hosts. Assume that the TCP sender 
has a huge file to send to the receiver, and the receiver‚Äôs receive buffer 
is much larger than the congestion window. We also make the following 
assumptions: each TCP segment size is 1,500 bytes; the two-way propagation 
delay of this connection is 150 msec; and this TCP connection is always in 
congestion avoidance phase, that is, ignore slow start.
a. What is the maximum window size (in segments) that this TCP connec-
tion can achieve?
b. What is the average window size (in segments) and average throughput 
(in bps) of this TCP connection?
c. How long would it take for this TCP connection to reach its maximum 
window again after recovering from a packet loss?
 P49. Consider the scenario described in the previous problem. Suppose that the 
10¬†Mbps link can buffer a finite number of segments. Argue that in order for 
the link to always be busy sending data, we would like to choose a buffer size 
that is at least the product of the link speed C and the two-way propagation 
delay between the sender and the receiver.
PROBLEMS     297

P50. Repeat Problem 46, but replacing the 10 Mbps link with a 10 Gbps link. Note 
that in your answer to part c, you will realize that it takes a very long time for 
the congestion window size to reach its maximum window size after recover-
ing from a packet loss. Sketch a solution to solve this problem.
 P51. Let T (measured by RTT) denote the time interval that a TCP connection 
takes to increase its congestion window size from W/2 to W, where W is the 
maximum congestion window size. Argue that T is a function of TCP‚Äôs  
average throughput.
 P52. Consider a simplified TCP‚Äôs AIMD algorithm where the congestion window 
size is measured in number of segments, not in bytes. In additive increase, the 
congestion window size increases by one segment in each RTT. In multipli-
cative decrease, the congestion window size decreases by half (if the result 
is not an integer, round down to the nearest integer). Suppose that two TCP 
connections, C1 and C2, share a single congested link of speed 30 segments 
per second. Assume that both C1 and C2 are in the congestion avoidance 
phase. Connection C1‚Äôs RTT is 50 msec and connection C2‚Äôs RTT is 100 msec. 
Assume that when the data rate in the link exceeds the link‚Äôs speed, all  
TCP connections experience data segment loss.
a. If both C1 and C2 at time t0 have a congestion window of 10 segments, 
what are their congestion window sizes after 1000 msec?
b. In the long run, will these two connections get the same share of the band-
width of the congested link? Explain.
 P53. Consider the network described in the previous problem. Now suppose that 
the two TCP connections, C1 and C2, have the same RTT of 100 msec.  
Suppose that at time t0, C1‚Äôs congestion window size is 15 segments but C2‚Äôs 
congestion window size is 10 segments.
a. What are their congestion window sizes after 2200 msec?
b. In the long run, will these two connections get about the same share of the 
bandwidth of the congested link?
c. We say that two connections are synchronized, if both connections reach 
their maximum window sizes at the same time and reach their minimum 
window sizes at the same time. In the long run, will these two connec-
tions get synchronized eventually? If so, what are their maximum window 
sizes?
d. Will this synchronization help to improve the utilization of the shared 
link? Why? Sketch some idea to break this synchronization.
 P54. Consider a modification to TCP‚Äôs congestion control algorithm. Instead of 
additive increase, we can use multiplicative increase. A TCP sender increases 
its window size by a small positive constant a (0 6 a 6 1) whenever it 
receives a valid ACK. Find the functional relationship between loss rate L

regardless of TCP‚Äôs average throughput, a TCP connection always spends the 
same amount of time to increase its congestion window size from W/2 to W.
 P55. In our discussion of TCP futures in Section 3.7, we noted that to achieve a 
throughput of 10 Gbps, TCP could only tolerate a segment loss probability of 
2 # 10-10 (or equivalently, one loss event for every 5,000,000,000 segments). 
Show the derivation for the values of 2 # 10-10 (1 out of 5,000,000) for the 
RTT and MSS values given in Section 3.7. If TCP needed to support a  
100 Gbps connection, what would the tolerable loss be?
 P56. In our discussion of TCP congestion control in Section 3.7, we implicitly 
assumed that the TCP sender always had data to send. Consider now the case 
that the TCP sender sends a large amount of data and then goes idle (since it 
has no more data to send) at t1. TCP remains idle for a relatively long period 
of time and then wants to send more data at t2. What are the advantages and 
disadvantages of having TCP use the cwnd and ssthresh values from t1 
when starting to send data at t2? What alternative would you recommend? 
Why?
 P57. In this problem, we investigate whether either UDP or TCP provides a degree 
of end-point authentication.
a. Consider a server that receives a request within a UDP packet and 
responds to that request within a UDP packet (for example, as done by a 
DNS server). If a client with IP address X spoofs its address with address 
Y, where will the server send its response?
b. Suppose a server receives a SYN with IP source address Y, and after 
responding with a SYNACK, receives an ACK with IP source address Y 
with the correct acknowledgment number. Assuming the server chooses a 
random initial sequence number and there is no ‚Äúman-in-the-middle,‚Äù can 
the server be certain that the client is indeed at Y (and not at some other 
address X that is spoofing Y)?
 P58. In this problem, we consider the delay introduced by the TCP slow-start 
phase. Consider a client and a Web server directly connected by one link of 
rate R. Suppose the client wants to retrieve an object whose size is exactly 
equal to 15 S, where S is the maximum segment size (MSS). Denote the 
round-trip time between client and server as RTT (assumed to be constant). 
Ignoring protocol headers, determine the time to retrieve the object (includ-
ing TCP connection establishment) when
a. 4 S/R 7 S/R + RTT 7 2S/R
b. S/R + RTT 7 4 S/R
c. S/R 7 RTT.
PROBLEMS     299

Programming Assignments
Implementing a Reliable Transport Protocol
In this laboratory programming assignment, you will be writing the sending and 
receiving transport-level code for implementing a simple reliable data transfer pro-
tocol. There are two versions of this lab, the alternating-bit-protocol version and the 
GBN version. This lab should be fun‚Äîyour implementation will differ very little 
from what would be required in a real-world situation.
Since you probably don‚Äôt have standalone machines (with an OS that you can 
modify), your code will have to execute in a simulated hardware/software environ-
ment. However, the programming interface provided to your routines‚Äîthe code that 
would call your entities from above and from below‚Äîis very close to what is done 
in an actual UNIX environment. (Indeed, the software interfaces described in this 
programming assignment are much more realistic than the infinite loop senders and 
receivers that many texts describe.) Stopping and starting timers are also simulated, 
and timer interrupts will cause your timer handling routine to be activated.
The full lab assignment, as well as code you will need to compile with your own 
code, are available at this book‚Äôs Web site: www.pearsonhighered.com/cs-resources.
Wireshark Lab: Exploring TCP
In this lab, you‚Äôll use your Web browser to access a file from a Web server. As in earlier 
Wireshark labs, you‚Äôll use Wireshark to capture the packets arriving at your computer. 
Unlike earlier labs, you‚Äôll also be able to download a Wireshark-readable packet trace 
from the Web server from which you downloaded the file. In this server trace, you‚Äôll 
find the packets that were generated by your own access of the Web server. You‚Äôll ana-
lyze the client- and server-side traces to explore aspects of TCP. In particular, you‚Äôll 
evaluate the performance of the TCP connection between your computer and the Web 
server. You‚Äôll trace TCP‚Äôs window behavior, and infer packet loss, retransmission, 
flow control and congestion control behavior, and estimated roundtrip time.
As is the case with all Wireshark labs, the full description of this lab is available 
at this book‚Äôs Web site, www.pearsonhighered.com/cs-resources.
Wireshark Lab: Exploring UDP
In this short lab, you‚Äôll do a packet capture and analysis of your favorite application 
that uses UDP (for example, DNS or a multimedia application such as Skype). As we 
learned in Section 3.3, UDP is a simple, no-frills transport protocol. In this lab, you‚Äôll 
investigate the header fields in the UDP segment as well as the checksum calculation.
As is the case with all Wireshark labs, the full description of this lab is available 
at this book‚Äôs Web site, www.pearsonhighered.com/cs-resources.

Please describe one or two of the most exciting projects you have worked on during your 
career. What were the biggest challenges?
School teaches us lots of ways to find answers. In every interesting problem I‚Äôve worked 
on, the challenge has been finding the right question. When Mike Karels and I started look-
ing at TCP congestion, we spent months staring at protocol and packet traces asking ‚ÄúWhy 
is it failing?‚Äù. One day in Mike‚Äôs office, one of us said ‚ÄúThe reason I can‚Äôt figure out why 
it fails is because I don‚Äôt understand how it ever worked to begin with.‚Äù That turned out to 
be the right question and it forced us to figure out the ‚Äúack clocking‚Äù that makes TCP work. 
After that, the rest was easy.
More generally, where do you see the future of networking and the Internet?
For most people, the Web is the Internet. Networking geeks smile politely since we know 
the Web is an application running over the Internet but what if they‚Äôre right? The Internet 
is about enabling conversations between pairs of hosts. The Web is about distributed infor-
mation production and consumption. ‚ÄúInformation propagation‚Äù is a very general view of 
communication of which ‚Äúpairwise conversation‚Äù is a tiny subset. We need to move into the 
larger tent. Networking today deals with broadcast media (radios, PONs, etc.) by pretending 
it‚Äôs a point-to-point wire. That‚Äôs massively inefficient. Terabits-per-second of data are being 
exchanged all over the World via thumb drives or smart phones but we don‚Äôt know how to 
treat that as ‚Äúnetworking‚Äù. ISPs are busily setting up caches and CDNs to scalably distribute 
video and audio. Caching is a necessary part of the solution but there‚Äôs no part of today‚Äôs 
networking‚Äîfrom Information, Queuing or Traffic Theory down to the Internet protocol 
Van Jacobson works at Google and was previously a Research 
Fellow at PARC. Prior to that, he was co-founder and Chief Scientist 
of Packet Design. Before that, he was Chief Scientist at Cisco. 
Before joining Cisco, he was head of the Network Research 
Group at Lawrence Berkeley National Laboratory and taught at UC 
Berkeley and Stanford. Van received the ACM SIGCOMM Award 
in 2001 for outstanding lifetime contribution to the field of commu-
nication networks and the IEEE Kobayashi Award in 2002 for ‚Äúcon-
tributing to the understanding of network congestion and developing 
congestion control mechanisms that enabled the successful scaling 
of the Internet‚Äù. He was elected to the U.S. National Academy of 
Engineering in 2004.
Van Jacobson
AN INTERVIEW WITH...
Courtesy of Van Jacobson

specs‚Äîthat tells us how to engineer and deploy it. I think and hope that over the next few 
years, networking will evolve to embrace the much larger vision of communication that 
underlies the Web.
What people inspired you professionally?
When I was in grad school, Richard Feynman visited and gave a colloquium. He talked 
about a piece of Quantum theory that I‚Äôd been struggling with all semester and his explana-
tion was so simple and lucid that what had been incomprehensible gibberish to me became 
obvious and inevitable. That ability to see and convey the simplicity that underlies our  
complex world seems to me a rare and wonderful gift.
What are your recommendations for students who want careers in computer science and 
networking?
It‚Äôs a wonderful field‚Äîcomputers and networking have probably had more impact on society 
than any invention since the book. Networking is fundamentally about connecting stuff, and 
studying it helps you make intellectual connections: Ant foraging & Bee dances demonstrate 
protocol design better than RFCs, traffic jams or people leaving a packed stadium are the 
essence of congestion, and students finding flights back to school in a post-Thanksgiving  
blizzard are the core of dynamic routing. If you‚Äôre interested in lots of stuff and want to 
have an impact, it‚Äôs hard to imagine a better field.

303
We learned in the previous chapter that the transport layer provides various forms 
of process-to-process communication by relying on the network layer‚Äôs host-to-host 
communication service. We also learned that the transport layer does so without any 
knowledge about how the network layer actually implements this service. So perhaps 
you‚Äôre now wondering, what‚Äôs under the hood of the host-to-host communication 
service, what makes it tick?
In this chapter and the next, we‚Äôll learn exactly how the network layer can pro-
vide its host-to-host communication service. We‚Äôll see that unlike the transport and 
application layers, there is a piece of the network layer in each and every host and 
router in the network. Because of this, network-layer protocols are among the most 
challenging (and therefore among the most interesting!) in the protocol stack.
Since the network layer is arguably the most complex layer in the protocol 
stack, we‚Äôll have a lot of ground to cover here. Indeed, there is so much to cover 
that we cover the network layer in two chapters. We‚Äôll see that the network layer 
can be decomposed into two interacting parts, the data plane and the control plane. 
In Chapter 4, we‚Äôll first cover the data plane functions of the network layer‚Äîthe 
per-router functions in the network layer that determine how a datagram (that is, a 
network-layer packet) arriving on one of a router‚Äôs input links is forwarded to one 
of that router‚Äôs output links. We‚Äôll cover both traditional IP forwarding (where for-
warding is based on a datagram‚Äôs destination address) and generalized forwarding 
(where forwarding and other functions may be performed using values in several 
different fields in the datagram‚Äôs header). We‚Äôll study the IPv4 and IPv6 protocols 
and addressing in detail. In Chapter 5, we‚Äôll cover the control plane functions of 
the network layer‚Äîthe network-wide logic that controls how a datagram is routed 
The Network 
Layer: Data 
Plane
4
CHAPTER

among routers along an end-to-end path from the source host to the destination host. 
We‚Äôll cover routing algorithms, as well as routing protocols, such as OSPF and BGP, 
that are in widespread use in today‚Äôs Internet. Traditionally, these control-plane rout-
ing protocols and data-plane forwarding functions have been implemented together, 
monolithically, within a router. Software-defined networking (SDN) explicitly sepa-
rates the data plane and control plane by implementing these control plane functions 
as a separate service, typically in a remote ‚Äúcontroller.‚Äù We‚Äôll also cover SDN con-
trollers in Chapter 5.
This distinction between data-plane and control-plane functions in the network 
layer is an important concept to keep in mind as you learn about the network layer ‚Äî
it will help structure your thinking about the network layer and reflects a modern 
view of the network layer‚Äôs role in computer networking.
4.1 Overview of Network Layer
Figure 4.1 shows a simple network with two hosts, H1 and H2, and several routers on 
the path between H1 and H2. Let‚Äôs suppose that H1 is sending information to H2, and 
consider the role of the network layer in these hosts and in the intervening routers. The 
network layer in H1 takes segments from the transport layer in H1, encapsulates each 
segment into a datagram, and then sends the datagrams to its nearby router, R1. At the 
receiving host, H2, the network layer receives the datagrams from its nearby router 
R2, extracts the transport-layer segments, and delivers the segments up to the transport 
layer at H2. The primary data-plane role of each router is to forward datagrams from 
its input links to its output links; the primary role of the network control plane is to 
coordinate these local, per-router forwarding actions so that datagrams are ultimately 
transferred end-to-end, along paths of routers between source and destination hosts. 
Note that the routers in Figure 4.1 are shown with a truncated protocol stack, that is, 
with no upper layers above the network layer, because routers do not run application-  
and transport-layer protocols such as those we examined in Chapters 2 and 3.
4.1.1  Forwarding and Routing: The Data and  
Control Planes
The primary role of the network layer is deceptively simple‚Äîto move packets from 
a sending host to a receiving host. To do so, two important network-layer functions 
can be identified:
‚Ä¢ Forwarding. When a packet arrives at a router‚Äôs input link, the router must move 
the packet to the appropriate output link. For example, a packet arriving from 
Host H1 to Router R1 in Figure 4.1 must be forwarded to the next router on 
a path to H2. As we will see, forwarding is but one function (albeit the most

Link
Physical
Network
Enterprise Network
Link
Physical
Application
Transport
Network
End System H2
Router R1
Router R2
Link
Physical
Application
Transport
Network
End System H1
Link
Physical
Network
Link
Physical
Network
Link
Physical
Network
Link
Physical
Network
Figure 4.1 ‚ô¶ The network layer

common and important one!) implemented in the data plane. In the more general 
case, which we‚Äôll cover in Section 4.4, a packet might also be blocked from exit-
ing a router (for example, if the packet originated at a known malicious sending 
host, or if the packet were destined to a forbidden destination host), or might be 
duplicated and sent over multiple outgoing links.
‚Ä¢ Routing. The network layer must determine the route or path taken by packets as 
they flow from a sender to a receiver. The algorithms that calculate these paths 
are referred to as routing algorithms. A routing algorithm would determine, for 
example, the path along which packets flow from H1 to H2 in Figure 4.1. Routing 
is implemented in the control plane of the network layer.
The terms forwarding and routing are often used interchangeably by authors dis-
cussing the network layer. We‚Äôll use these terms much more precisely in this book.  
Forwarding refers to the router-local action of transferring a packet from an input 
link interface to the appropriate output link interface. Forwarding takes place at very 
short timescales (typically a few nanoseconds), and thus is typically implemented in 
hardware. Routing refers to the network-wide process that determines the end-to-end 
paths that packets take from source to destination. Routing takes place on much longer 
timescales (typically seconds), and as we will see is often implemented in software. 
Using our driving analogy, consider the trip from Pennsylvania to Florida undertaken 
by our traveler back in Section 1.3.1. During this trip, our driver passes through many 
interchanges en route to Florida. We can think of forwarding as the process of getting 
through a single interchange: A car enters the interchange from one road and deter-
mines which road it should take to leave the interchange. We can think of routing as 
the process of planning the trip from Pennsylvania to Florida: Before embarking on 
the trip, the driver has consulted a map and chosen one of many paths possible, with 
each path consisting of a series of road segments connected at interchanges.
A key element in every network router is its forwarding table. A router forwards 
a packet by examining the value of one or more fields in the arriving packet‚Äôs header, 
and then using these header values to index into its forwarding table. The value stored 
in the forwarding table entry for those values indicates the outgoing link interface at 
that router to which that packet is to be forwarded. For example, in Figure 4.2, a packet 
with header field value of 0110 arrives to a router. The router indexes into its forward-
ing table and determines that the output link interface for this packet is interface 2. 
The router then internally forwards the packet to interface 2. In Section 4.2, we‚Äôll look 
inside a router and examine the forwarding function in much greater detail. Forward-
ing is the key function performed by the data-plane functionality of the network layer.
Control Plane: The Traditional Approach 
But now you are undoubtedly wondering how a router‚Äôs forwarding tables are con-
figured in the first place. This is a crucial issue, one that exposes the important inter-
play between forwarding (in data plane) and routing (in control plane). As shown

in Figure 4.2, the routing algorithm determines the contents of the routers‚Äô forward-
ing tables. In this example, a routing algorithm runs in each and every router and 
both forwarding and routing functions are contained within a router. As we‚Äôll see in 
Sections 5.3 and 5.4, the routing algorithm function in one router communicates with 
the routing algorithm function in other routers to compute the values for its forward-
ing table. How is this communication performed? By exchanging routing messages 
containing routing information according to a routing protocol! We‚Äôll cover routing 
algorithms and protocols in Sections 5.2 through 5.4.
The distinct and different purposes of the forwarding and routing functions can 
be further illustrated by considering the hypothetical (and unrealistic, but technically 
feasible) case of a network in which all forwarding tables are configured directly by 
human network operators physically present at the routers. In this case, no routing 
protocols would be required! Of course, the human operators would need to interact 
with each other to ensure that the forwarding tables were configured in such a way 
that packets reached their intended destinations. It‚Äôs also likely that human configu-
ration would be more error-prone and much slower to respond to changes in the net-
work topology than a routing protocol. We‚Äôre thus fortunate that all networks have 
both a forwarding and a routing function!
0110
Local forwarding
table
header
0100
0110
0111
1001
3
2
2
1
output
Control plane
Data plane
Routing
Algorithm
Values in arriving
packet‚Äôs header
1
2
3
Figure 4.2 ‚ô¶ Routing algorithms determine values in forward tables

Control Plane: The SDN Approach 
The approach to implementing routing functionality shown in Figure 4.2‚Äîwith each 
router having a routing component that communicates with the routing component of 
other routers‚Äîhas been the traditional approach adopted by routing vendors in their 
products, at least until recently. Our observation that humans could manually configure 
forwarding tables does suggest, however, that there may be other ways for control-
plane functionality to determine the contents of the data-plane forwarding tables.
Figure 4.3 shows an alternative approach in which a physically separate, remote 
controller computes and distributes the forwarding tables to be used by each and 
every router.  Note that the data plane components of Figures 4.2 and 4.3 are identi-
cal. In Figure 4.3; however, control-plane routing functionality is separated from the 
0110
Local forwarding
table
header
0100
0110
0111
1001
3
2
2
1
output
Remote Controller
Values in arriving
packet‚Äôs header
1
2
3
Control plane
Data plane
Figure 4.3 ‚ô¶  A remote controller determines and distributes values in 
 forwarding tables

physical router‚Äîthe routing device performs forwarding only, while the remote con-
troller computes and distributes forwarding tables. The remote controller might be 
implemented in a remote data center with high reliability and redundancy, and might 
be managed by the ISP or some third party. How might the routers and the remote 
controller communicate? By exchanging messages containing forwarding tables and 
other pieces of routing information. The control-plane approach shown in Figure 4.3 
is at the heart of software-defined networking (SDN), where the network is ‚Äúsoft-
ware-defined‚Äù because the controller that computes forwarding tables and interacts 
with routers is implemented in software. Increasingly, these software implementa-
tions are also open, that is, similar to Linux OS code, the code is publically available, 
allowing ISPs (and networking researchers and students!) to innovate and propose 
changes to the software that controls network-layer functionality. We will cover the 
SDN control plane in Section 5.5.
4.1.2 Network Service Model
Before delving into the network layer‚Äôs data plane, let‚Äôs wrap up our introduction 
by taking the broader view and consider the different types of service that might be 
offered by the network layer. When the transport layer at a sending host transmits a 
packet into the network (that is, passes it down to the network layer at the sending 
host), can the transport layer rely on the network layer to deliver the packet to the 
destination? When multiple packets are sent, will they be delivered to the transport 
layer in the receiving host in the order in which they were sent? Will the amount 
of time between the sending of two sequential packet transmissions be the same 
as the amount of time between their reception? Will the network provide any feed-
back about congestion in the network? The answers to these questions and others 
are determined by the service model provided by the network layer. The network 
service model defines the characteristics of end-to-end delivery of packets between 
sending and receiving hosts.
Let‚Äôs now consider some possible services that the network layer could provide. 
These services could include:
‚Ä¢ Guaranteed delivery. This service guarantees that a packet sent by a source host 
will eventually arrive at the destination host.
‚Ä¢ Guaranteed delivery with bounded delay. This service not only guarantees 
delivery of the packet, but delivery within a specified host-to-host delay bound  
(for example, within 100 msec).
‚Ä¢ In-order packet delivery. This service guarantees that packets arrive at the desti-
nation in the order that they were sent.
‚Ä¢ Guaranteed minimal bandwidth. This network-layer service emulates the behav-
ior of a transmission link of a specified bit rate (for example, 1 Mbps) between 
sending and receiving hosts. As long as the sending host transmits bits (as part

of packets) at a rate below the specified bit rate, then all packets are eventually 
delivered to the destination host.
‚Ä¢ Security. The network layer could encrypt all datagrams at the source and decrypt them 
at the destination, thereby providing confidentiality to all transport-layer segments.
This is only a partial list of services that a network layer could provide‚Äîthere are 
countless variations possible.
The Internet‚Äôs network layer provides a single service, known as best-effort 
service. With best-effort service, packets are neither guaranteed to be received in the 
order in which they were sent, nor is their eventual delivery even guaranteed. There 
is no guarantee on the end-to-end delay nor is there a minimal bandwidth guaran-
tee. It might appear that best-effort service is a euphemism for no service at all‚Äîa 
network that delivered no packets to the destination would satisfy the definition of  
best-effort delivery service! Other network architectures have defined and imple-
mented service models that go beyond the Internet‚Äôs best-effort service. For example, 
the ATM network architecture [Black 1995] provides for guaranteed in-order delay, 
bounded delay, and guaranteed minimal bandwidth. There have also been proposed 
service model extensions to the Internet architecture; for example, the Intserv archi-
tecture [RFC 1633] aims to provide end-end delay guarantees and congestion-free 
communication. Interestingly, in spite of these well-developed alternatives, the 
Internet‚Äôs basic best-effort service model combined with adequate bandwidth provi-
sioning and bandwidth-adaptive application-level protocols such as the DASH pro-
tocol we encountered in Section 2.6.2 have arguably proven to be more than ‚Äúgood 
enough‚Äù to enable an amazing range of applications, including streaming video ser-
vices such as Netflix and video-over-IP, real-time conferencing applications such as 
Skype and Facetime.
An Overview of Chapter 4
Having now provided an overview of the network layer, we‚Äôll cover the data-plane 
component of the network layer in the following sections in this chapter. In Section 4.2, 
we‚Äôll dive down into the internal hardware operations of a router, including input 
and output packet processing, the router‚Äôs internal switching mechanism, and packet 
queueing and scheduling. In Section 4.3, we‚Äôll take a look at traditional IP forwarding, 
in which packets are forwarded to output ports based on their destination IP addresses. 
We‚Äôll encounter IP addressing, the celebrated IPv4 and IPv6 protocols and more. In 
Section 4.4, we‚Äôll cover more generalized forwarding, where packets may be for-
warded to output ports based on a large number of header values (i.e., not only based 
on destination IP address). Packets may be blocked or duplicated at the router, or 
may have certain header field values rewritten‚Äîall under software control. This more 
generalized form of packet forwarding is a key component of a modern network data 
plane, including the data plane in software-defined networks (SDN). In Section 4.5, 
we‚Äôll learn about ‚Äúmiddleboxes‚Äù that can perform functions in addition to forwarding.

We mention here in passing that the terms forwarding and switching are often 
used interchangeably by computer-networking researchers and practitioners; we‚Äôll 
use both terms interchangeably in this textbook as well. While we‚Äôre on the topic 
of terminology, it‚Äôs also worth mentioning two other terms that are often used inter-
changeably, but that we will use more carefully. We‚Äôll reserve the term packet switch 
to mean a general packet-switching device that transfers a packet from input link 
interface to output link interface, according to values in a packet‚Äôs header fields. 
Some packet switches, called link-layer switches (examined in Chapter 6), base 
their forwarding decision on values in the fields of the link-layer frame; switches are 
thus referred to as link-layer (layer 2) devices. Other packet switches, called routers, 
base their forwarding decision on header field values in the network-layer datagram. 
Routers are thus network-layer (layer 3) devices. (To fully appreciate this important 
distinction, you might want to review Section 1.5.2, where we discuss network-layer 
datagrams and link-layer frames and their relationship.) Since our focus in this chap-
ter is on the network layer, we‚Äôll mostly use the term router in place of packet switch.
4.2 What‚Äôs Inside a Router?
Now that we‚Äôve overviewed the data and control planes within the network layer, the 
important distinction between forwarding and routing, and the services and functions of 
the network layer, let‚Äôs turn our attention to its forwarding function‚Äîthe actual transfer 
of packets from a router‚Äôs incoming links to the appropriate outgoing links at that router.
A high-level view of a generic router architecture is shown in Figure 4.4. Four 
router components can be identified:
Input port
Output port
Input port
Output port
Routing
processor
Routing, management
control plane (software)
Forwarding
data plane (hardware)
Switch
fabric
Figure 4.4 ‚ô¶ Router architecture

‚Ä¢ Input ports. An input port performs several key functions. It performs the physi-
cal layer function of terminating an incoming physical link at a router; this is 
shown in the leftmost box of an input port and the rightmost box of an output 
port in Figure 4.4. An input port also performs link-layer functions needed to 
interoperate with the link layer at the other side of the incoming link; this is 
represented by the middle boxes in the input and output ports. Perhaps most cru-
cially, a lookup function is also performed at the input port; this will occur in the 
rightmost box of the input port. It is here that the forwarding table is consulted 
to determine the router output port to which an arriving packet will be forwarded 
via the switching fabric. Control packets (for example, packets carrying routing 
protocol information) are forwarded from an input port to the routing processor. 
Note that the term ‚Äúport‚Äù here‚Äîreferring to the physical input and output router 
interfaces‚Äîis distinctly different from the software ports associated with network 
applications and sockets discussed in Chapters 2 and 3. In practice, the number of 
ports supported by a router can range from a relatively small number in enterprise 
routers, to hundreds of 10 Gbps ports in a router at an ISP‚Äôs edge, where the num-
ber of incoming lines tends to be the greatest. The Juniper MX2020, edge router, 
for example, supports up to 800 100 Gbps Ethernet ports, with an overall router 
system capacity of 800 Tbps [Juniper MX 2020 2020].
‚Ä¢ Switching fabric. The switching fabric connects the router‚Äôs input ports to its 
output ports. This switching fabric is completely contained within the router‚Äîa 
network inside of a network router!
‚Ä¢ Output ports. An output port stores packets received from the switching fabric 
and transmits these packets on the outgoing link by performing the necessary 
link-layer and physical-layer functions. When a link is bidirectional (that is, car-
ries traffic in both directions), an output port will typically be paired with the 
input port for that link on the same line card.
‚Ä¢ Routing processor. The routing processor performs control-plane functions. In tra-
ditional routers, it executes the routing protocols (which we‚Äôll study in Sections 5.3 
and 5.4), maintains routing tables and attached link state information, and com-
putes the forwarding table for the router. In SDN routers, the routing processor is 
responsible for communicating with the remote controller in order to (among other 
activities) receive forwarding table entries computed by the remote controller, and 
install these entries in the router‚Äôs input ports. The routing processor also performs 
the network management functions that we‚Äôll study in Section 5.7.
A router‚Äôs input ports, output ports, and switching fabric are almost always 
implemented in hardware, as shown in Figure 4.4. To appreciate why a hardware 
implementation is needed, consider that with a 100 Gbps input link and a 64-byte 
IP datagram, the input port has only 5.12 ns to process the datagram before another 
datagram may arrive. If N ports are combined on a line card (as is often done in 
practice), the datagram-processing pipeline must operate N times faster‚Äîfar too

fast for software implementation. Forwarding hardware can be implemented either 
using a router vendor‚Äôs own hardware designs, or constructed using purchased  
merchant-silicon chips (for example, as sold by companies such as Intel and Broadcom).
While the data plane operates at the nanosecond time scale, a router‚Äôs control 
functions‚Äîexecuting the routing protocols, responding to attached links that go up 
or down, communicating with the remote controller (in the SDN case) and perform-
ing management functions‚Äîoperate at the millisecond or second timescale. These 
control plane functions are thus usually implemented in software and execute on the 
routing processor (typically a traditional CPU).
Before delving into the details of router internals, let‚Äôs return to our analogy 
from the beginning of this chapter, where packet forwarding was compared to cars 
entering and leaving an interchange. Let‚Äôs suppose that the interchange is a rounda-
bout, and that as a car enters the roundabout, a bit of processing is required. Let‚Äôs 
consider what information is required for this processing:
‚Ä¢ Destination-based forwarding. Suppose the car stops at an entry station and indi-
cates its final destination (not at the local roundabout, but the ultimate destination 
of its journey). An attendant at the entry station looks up the final destination, 
determines the roundabout exit that leads to that final destination, and tells the 
driver which roundabout exit to take.
‚Ä¢ Generalized forwarding. The attendant could also determine the car‚Äôs exit ramp on 
the basis of many other factors besides the destination. For example, the selected 
exit ramp might depend on the car‚Äôs origin, for example the state that issued the 
car‚Äôs license plate. Cars from a certain set of states might be directed to use one exit 
ramp (that leads to the destination via a slow road), while cars from other states 
might be directed to use a different exit ramp (that leads to the destination via super-
highway). The same decision might be made based on the model, make and year 
of the car. Or a car not deemed roadworthy might be blocked and not be allowed to 
pass through the roundabout. In the case of generalized forwarding, any number of 
factors may contribute to the attendant‚Äôs choice of the exit ramp for a given car.
Once the car enters the roundabout (which may be filled with other cars entering 
from other input roads and heading to other roundabout exits), it eventually leaves at 
the prescribed roundabout exit ramp, where it may encounter other cars leaving the 
roundabout at that exit.
We can easily recognize the principal router components in Figure 4.4 in this 
analogy‚Äîthe entry road and entry station correspond to the input port (with a lookup 
function to determine to local outgoing port); the roundabout corresponds to the 
switch fabric; and the roundabout exit road corresponds to the output port. With this 
analogy, it‚Äôs instructive to consider where bottlenecks might occur. What happens if 
cars arrive blazingly fast (for example, the roundabout is in Germany or Italy!) but 
the station attendant is slow? How fast must the attendant work to ensure there‚Äôs no 
backup on an entry road? Even with a blazingly fast attendant, what happens if cars

traverse the roundabout slowly‚Äîcan backups still occur? And what happens if most 
of the cars entering at all of the roundabout‚Äôs entrance ramps all want to leave the 
roundabout at the same exit ramp‚Äîcan backups occur at the exit ramp or elsewhere? 
How should the roundabout operate if we want to assign priorities to different cars, 
or block certain cars from entering the roundabout in the first place? These are all 
analogous to critical questions faced by router and switch designers.
In the following subsections, we‚Äôll look at router functions in more detail. [Turner 
1988; McKeown 1997a; Partridge 1998; Iyer 2008; Serpanos 2011; Zilberman 2019] 
provide a discussion of specific router architectures. For concreteness and simplicity, 
we‚Äôll initially assume in this section that forwarding decisions are based only on the 
packet‚Äôs destination address, rather than on a generalized set of packet header fields. We 
will cover the case of more generalized packet forwarding in Section 4.4.
4.2.1 Input Port Processing and Destination-Based Forwarding
A more detailed view of input processing is shown in Figure 4.5. As just discussed, 
the input port‚Äôs line-termination function and link-layer processing implement the 
physical and link layers for that individual input link. The lookup performed in the 
input port is central to the router‚Äôs operation‚Äîit is here that the router uses the for-
warding table to look up the output port to which an arriving packet will be forwarded 
via the switching fabric. The forwarding table is either computed and updated by the 
routing processor (using a routing protocol to interact with the routing processors in 
other network routers) or is received from a remote SDN controller. The forwarding 
table is copied from the routing processor to the line cards over a separate bus (e.g., 
a PCI bus) indicated by the dashed line from the routing processor to the input line 
cards in Figure 4.4. With such a shadow copy at each line card, forwarding decisions 
can be made locally, at each input port, without invoking the centralized routing pro-
cessor on a per-packet basis and thus avoiding a centralized processing bottleneck.
Let‚Äôs now consider the ‚Äúsimplest‚Äù case that the output port to which an incoming 
packet is to be switched is based on the packet‚Äôs destination address. In the case of 
32-bit IP addresses, a brute-force implementation of the forwarding table would have 
one entry for every possible destination address. Since there are more than 4 billion 
possible addresses, this option is totally out of the question.
Line
termination
Data link
processing
(protocol,
decapsulation)
Lookup, fowarding,
queuing
Switch
fabric
Figure 4.5 ‚ô¶ Input port processing

As an example of how this issue of scale can be handled, let‚Äôs suppose that our 
router has four links, numbered 0 through 3, and that packets are to be forwarded to 
the link interfaces as follows:
 
Destination Address Range 
Link Interface
 11001000 00010111 00010000 00000000 
 
through 
0
 11001000 00010111 00010111 11111111 
 11001000 00010111 00011000 00000000 
 
through 
1
 11001000 00010111 00011000 11111111 
 11001000 00010111 00011001 00000000 
 
through 
2
 11001000 00010111 00011111 11111111 
 
Otherwise 
3
Clearly, for this example, it is not necessary to have 4 billion entries in the router‚Äôs 
forwarding table. We could, for example, have the following forwarding table with 
just four entries:
 
Prefix 
Link Interface
 
11001000 00010111 00010 
0
 
11001000 00010111 00011000 
1
 
11001000 00010111 00011 
2
 
Otherwise 
3
With this style of forwarding table, the router matches a prefix of the packet‚Äôs des-
tination address with the entries in the table; if there‚Äôs a match, the router forwards 
the packet to a link associated with the match. For example, suppose the packet‚Äôs 
destination address is 11001000 00010111 00010110 10100001; because 
the 21-bit prefix of this address matches the first entry in the table, the router forwards 
the packet to link interface 0. If a prefix doesn‚Äôt match any of the first three entries, 
then the router forwards the packet to the default interface 3. Although this sounds 
simple enough, there‚Äôs a very important subtlety here. You may have noticed that it is 
possible for a destination address to match more than one entry. For example, the first 
24 bits of the address 11001000 00010111 00011000 10101010 match the 
second entry in the table, and the first 21 bits of the address match the third entry in the 
table. When there are multiple matches, the router uses the longest prefix matching 
rule; that is, it finds the longest matching entry in the table and forwards the packet to 
the link interface associated with the longest prefix match. We‚Äôll see exactly why this

longest prefix-matching rule is used when we study Internet addressing in more detail 
in Section 4.3.
Given the existence of a forwarding table, lookup is conceptually simple‚Äî 
hardware logic just searches through the forwarding table looking for the longest 
prefix match. But at Gigabit transmission rates, this lookup must be performed in 
nanoseconds (recall our earlier example of a 10 Gbps link and a 64-byte IP data-
gram). Thus, not only must lookup be performed in hardware, but techniques beyond 
a simple linear search through a large table are needed; surveys of fast lookup algo-
rithms can be found in [Gupta 2001, Ruiz-Sanchez 2001]. Special attention must 
also be paid to memory access times, resulting in designs with embedded on-chip 
DRAM and faster SRAM (used as a DRAM cache) memories. In practice, Ternary 
Content Addressable Memories (TCAMs) are also often used for lookup [Yu 2004]. 
With a TCAM, a 32-bit IP address is presented to the memory, which returns the 
content of the forwarding table entry for that address in essentially constant time. 
The Cisco Catalyst 6500 and 7600 Series routers and switches can hold upwards of 
a million TCAM forwarding table entries [Cisco TCAM 2014].
Once a packet‚Äôs output port has been determined via the lookup, the packet 
can be sent into the switching fabric. In some designs, a packet may be temporarily 
blocked from entering the switching fabric if packets from other input ports are cur-
rently using the fabric. A blocked packet will be queued at the input port and then 
scheduled to cross the fabric at a later point in time. We‚Äôll take a closer look at the 
blocking, queuing, and scheduling of packets (at both input ports and output ports) 
shortly. Although ‚Äúlookup‚Äù is arguably the most important action in input port pro-
cessing, many other actions must be taken: (1) physical- and link-layer processing 
must occur, as discussed previously; (2) the packet‚Äôs version number, checksum and 
time-to-live field‚Äîall of which we‚Äôll study in Section 4.3‚Äîmust be checked and the 
latter two fields rewritten; and (3) counters used for network management (such as 
the number of IP datagrams received) must be updated.
Let‚Äôs close our discussion of input port processing by noting that the input port 
steps of looking up a destination IP address (‚Äúmatch‚Äù) and then sending the packet 
into the switching fabric to the specified output port (‚Äúaction‚Äù) is a specific case of a 
more general ‚Äúmatch plus action‚Äù abstraction that is performed in many networked 
devices, not just routers. In link-layer switches (covered in Chapter 6), link-layer 
destination addresses are looked up and several actions may be taken in addition to 
sending the frame into the switching fabric towards the output port. In firewalls (cov-
ered in Chapter 8)‚Äîdevices that filter out selected incoming packets‚Äîan incoming 
packet whose header matches a given criteria (e.g., a combination of source/destina-
tion IP addresses and transport-layer port numbers) may be dropped (action). In a 
network address translator (NAT, covered in Section 4.3), an incoming packet whose 
transport-layer port number matches a given value will have its port number rewrit-
ten before forwarding (action). Indeed, the ‚Äúmatch plus action‚Äù abstraction [Bosshart 
2013] is both powerful and prevalent in network devices today, and is central to the 
notion of generalized forwarding that we‚Äôll study in Section 4.4.

4.2.2 Switching
The switching fabric is at the very heart of a router, as it is through this fabric that 
the packets are actually switched (that is, forwarded) from an input port to an output 
port. Switching can be accomplished in a number of ways, as shown in Figure 4.6:
‚Ä¢ Switching via memory. The simplest, earliest routers were traditional computers, 
with switching between input and output ports being done under direct control of 
the CPU (routing processor). Input and output ports functioned as traditional I/O 
devices in a traditional operating system. An input port with an arriving packet 
first signaled the routing processor via an interrupt. The packet was then copied 
from the input port into processor memory. The routing processor then extracted 
the destination address from the header, looked up the appropriate output port 
in the forwarding table, and copied the packet to the output port‚Äôs buffers. In 
this scenario, if the memory bandwidth is such that a maximum of B packets per 
second can be written into, or read from, memory, then the overall forwarding 
throughput (the total rate at which packets are transferred from input ports to out-
put ports) must be less than B/2. Note also that two packets cannot be forwarded 
Memory
A
B
C
X
Y
Z
Memory
Key:
Input port
Output port
A
X
Y
Z
B
C
Interconnection Network
A
B
C
X
Y
Z
Bus
Figure 4.6 ‚ô¶ Three switching techniques

at the same time, even if they have different destination ports, since only one 
memory read/write can be done at a time over the shared system bus.
 
Some modern routers switch via memory. A major difference from early routers, 
however, is that the lookup of the destination address and the storing of the packet 
into the appropriate memory location are performed by processing on the input line 
cards. In some ways, routers that switch via memory look very much like shared-
memory multiprocessors, with the processing on a line card switching (writing) 
packets into the memory of the appropriate output port. Cisco‚Äôs Catalyst 8500 
series switches [Cisco 8500 2020] internally switches packets via a shared memory.
‚Ä¢ Switching via a bus. In this approach, an input port transfers a packet directly to the 
output port over a shared bus, without intervention by the routing processor. This is 
typically done by having the input port pre-pend a switch-internal label (header) to 
the packet indicating the local output port to which this packet is being transferred 
and transmitting the packet onto the bus. All output ports receive the packet, but 
only the port that matches the label will keep the packet. The label is then removed 
at the output port, as this label is only used within the switch to cross the bus. If mul-
tiple packets arrive to the router at the same time, each at a different input port, all 
but one must wait since only one packet can cross the bus at a time. Because every 
packet must cross the single bus, the switching speed of the router is limited to the 
bus speed; in our roundabout analogy, this is as if the roundabout could only contain 
one car at a time. Nonetheless, switching via a bus is often sufficient for routers that 
operate in small local area and enterprise networks. The Cisco 6500 router [Cisco 
6500 2020] internally switches packets over a 32-Gbps-backplane bus.
‚Ä¢ Switching via an interconnection network. One way to overcome the bandwidth 
limitation of a single, shared bus is to use a more sophisticated interconnection net-
work, such as those that have been used in the past to interconnect processors in a 
multiprocessor computer architecture. A crossbar switch is an interconnection net-
work consisting of 2N buses that connect N input ports to N output ports, as shown 
in Figure 4.6. Each vertical bus intersects each horizontal bus at a crosspoint, 
which can be opened or closed at any time by the switch fabric controller (whose 
logic is part of the switching fabric itself). When a packet arrives from port A and 
needs to be forwarded to port Y, the switch controller closes the crosspoint at the 
intersection of busses A and Y, and port A then sends the packet onto its bus, which 
is picked up (only) by bus Y. Note that a packet from port B can be forwarded to 
port X at the same time, since the A-to-Y and B-to-X packets use different input 
and output busses. Thus, unlike the previous two switching approaches, cross-
bar switches are capable of forwarding multiple packets in parallel. A crossbar 
switch is non-blocking‚Äîa packet being forwarded to an output port will not be 
blocked from reaching that output port as long as no other packet is currently being 
forwarded to that output port. However, if two packets from two different input 
ports are destined to that same output port, then one will have to wait at the input, 
since only one packet can be sent over any given bus at a time. Cisco 12000 series

switches [Cisco 12000 2020] use a crossbar switching network; the Cisco 7600 
series can be configured to use either a bus or crossbar switch [Cisco 7600 2020].
 
More sophisticated interconnection networks use multiple stages of switching 
elements to allow packets from different input ports to proceed towards the same 
output port at the same time through the multi-stage switching fabric. See [Tobagi 
1990] for a survey of switch architectures. The Cisco CRS employs a three-stage 
non-blocking switching strategy. A router‚Äôs switching capacity can also be scaled 
by running multiple switching fabrics in parallel. In this approach, input ports 
and output ports are connected to N switching fabrics that operate in parallel. An 
input port breaks a packet into K smaller chunks, and sends (‚Äúsprays‚Äù) the chunks 
through K of these N switching fabrics to the selected output port, which reas-
sembles the K chunks back into the original packet.
4.2.3 Output Port Processing
Output port processing, shown in Figure 4.7, takes packets that have been stored 
in the output port‚Äôs memory and transmits them over the output link. This includes 
selecting (i.e., scheduling) and de-queueing packets for transmission, and perform-
ing the needed link-layer and physical-layer transmission functions.
4.2.4 Where Does Queuing Occur?
If we consider input and output port functionality and the configurations shown  
in Figure 4.6, it‚Äôs clear that packet queues may form at both the input ports and the 
output ports, just as we identified cases where cars may wait at the inputs and out-
puts of the traffic intersection in our roundabout analogy. The location and extent of 
queueing (either at the input port queues or the output port queues) will depend on 
the traffic load, the relative speed of the switching fabric, and the line speed. Let‚Äôs 
now consider these queues in a bit more detail, since as these queues grow large, the 
router‚Äôs memory can eventually be exhausted and packet loss will occur when no 
memory is available to store arriving packets. Recall that in our earlier  discussions, 
we said that packets were ‚Äúlost within the network‚Äù or ‚Äúdropped at a router.‚Äù It is here, 
at these queues within a router, where such packets are actually dropped and lost.
Line
termination
Data link
processing
(protocol,
encapsulation)
Queuing (buffer
management)
Switch
fabric
Figure 4.7 ‚ô¶ Output port processing

Suppose that the input and output line speeds (transmission rates) all have an 
identical transmission rate of Rline packets per second, and that there are N input ports 
and N output ports. To further simplify the discussion, let‚Äôs assume that all packets 
have the same fixed length, and that packets arrive to input ports in a synchronous 
manner. That is, the time to send a packet on any link is equal to the time to receive a 
packet on any link, and during such an interval of time, either zero or one packets can 
arrive on an input link. Define the switching fabric transfer rate Rswitch as the rate at 
which packets can be moved from input port to output port. If Rswitch is N times faster 
than Rline, then only negligible queuing will occur at the input ports. This is because 
even in the worst case, where all N input lines are receiving packets, and all packets 
are to be forwarded to the same output port, each batch of N packets (one packet per 
input port) can be cleared through the switch fabric before the next batch arrives.
Input Queueing
But what happens if the switch fabric is not fast enough (relative to the input line 
speeds) to transfer all arriving packets through the fabric without delay? In this case, 
packet queuing can also occur at the input ports, as packets must join input port 
queues to wait their turn to be transferred through the switching fabric to the output 
port. To illustrate an important consequence of this queuing, consider a crossbar 
switching fabric and suppose that (1) all link speeds are identical, (2) that one packet 
can be transferred from any one input port to a given output port in the same amount 
of time it takes for a packet to be received on an input link, and (3) packets are moved 
from a given input queue to their desired output queue in an FCFS manner. Multiple 
packets can be transferred in parallel, as long as their output ports are different. How-
ever, if two packets at the front of two input queues are destined for the same output 
queue, then one of the packets will be blocked and must wait at the input queue‚Äîthe 
switching fabric can transfer only one packet to a given output port at a time.
Figure 4.8 shows an example in which two packets (darkly shaded) at the front 
of their input queues are destined for the same upper-right output port. Suppose that 
the switch fabric chooses to transfer the packet from the front of the upper-left queue. 
In this case, the darkly shaded packet in the lower-left queue must wait. But not only 
must this darkly shaded packet wait, so too must the lightly shaded packet that is 
queued behind that packet in the lower-left queue, even though there is no conten-
tion for the middle-right output port (the destination for the lightly shaded packet). 
This phenomenon is known as head-of-the-line (HOL) blocking in an input-queued 
switch‚Äîa queued packet in an input queue must wait for transfer through the fabric 
(even though its output port is free) because it is blocked by another packet at the 
head of the line. [Karol 1987] shows that due to HOL blocking, the input queue will 
grow to unbounded length (informally, this is equivalent to saying that significant 
packet loss will occur) under certain assumptions as soon as the packet arrival rate 
on the input links reaches only 58 percent of their capacity. A number of solutions to 
HOL blocking are discussed in [McKeown 1997].

Output Queueing
Let‚Äôs next consider whether queueing can occur at a switch‚Äôs output ports. Suppose 
that Rswitch is again N times faster than Rline and that packets arriving at each of the N 
input ports are destined to the same output port. In this case, in the time it takes to send a  
single packet onto the outgoing link, N new packets will arrive at this output port 
(one from each of the N input ports). Since the output port can transmit only a single 
packet in a unit of time (the packet transmission time), the N arriving packets will 
have to queue (wait) for transmission over the outgoing link. Then N more packets 
can possibly arrive in the time it takes to transmit just one of the N packets that had 
just previously been queued. And so on. Thus, packet queues can form at the output 
ports even when the switching fabric is N times faster than the port line speeds. 
Eventually, the number of queued packets can grow large enough to exhaust avail-
able memory at the output port.
Switch
fabric
Output port contention at time t ‚Äî
one dark packet can be transferred
Light blue packet experiences HOL blocking
Switch
fabric
Key:
destined for upper output 
port
destined for middle output 
port
destined for lower output 
port
Figure 4.8 ‚ô¶ HOL blocking at and input-queued switch

When there is not enough memory to buffer an incoming packet, a decision must 
be made to either drop the arriving packet (a policy known as drop-tail) or remove 
one or more already-queued packets to make room for the newly arrived packet. In 
some cases, it may be advantageous to drop (or mark the header of) a packet before 
the buffer is full in order to provide a congestion signal to the sender. This mark-
ing could be done using the Explicit Congestion Notification bits that we studied in 
Section 3.7.2. A number of proactive packet-dropping and -marking policies (which 
collectively have become known as active queue management (AQM) algorithms) 
have been proposed and analyzed [Labrador 1999, Hollot 2002]. One of the most 
widely studied and implemented AQM algorithms is the Random Early Detection 
(RED) algorithm [Christiansen 2001]. More recent AQM policies include PIE (the 
Proportional Integral controller Enhanced [RFC 8033]), and CoDel [Nichols 2012].
Output port queuing is illustrated in Figure 4.9. At time t, a packet has arrived 
at each of the incoming input ports, each destined for the uppermost outgoing port. 
Assuming identical line speeds and a switch operating at three times the line speed, one 
time unit later (that is, in the time needed to receive or send a packet), all three original 
packets have been transferred to the outgoing port and are queued awaiting transmis-
sion. In the next time unit, one of these three packets will have been transmitted over the 
outgoing link. In our example, two new packets have arrived at the incoming side of the 
Switch
fabric
Output port contention at time t
One packet time later
Switch
fabric
Figure 4.9 ‚ô¶ Output port queueing

switch; one of these packets is destined for this uppermost output port. A consequence 
of such queuing is that a packet scheduler at the output port must choose one packet, 
among those queued, for transmission‚Äîa topic we‚Äôll cover in the following section.
How Much Buffering Is ‚ÄúEnough?‚Äù
Our study above has shown how a packet queue forms when bursts of packets arrive 
at a router‚Äôs input or (more likely) output port, and the packet arrival rate temporarily 
exceeds the rate at which packets can be forwarded. The longer the amount of time 
that this mismatch persists, the longer the queue will grow, until eventually a port‚Äôs 
buffers become full and packets are dropped. One natural question is how much 
buffering should be provisioned at a port. It turns out the answer to this question is 
much more complicated than one might imagine and can teach us quite a bit about 
the subtle interaction among congestion-aware senders at the network‚Äôs edge and the 
network core!
For many years, the rule of thumb [RFC 3439] for buffer sizing was that the 
amount of buffering (B) should be equal to an average round-trip time (RTT, say 
250¬†msec) times the link capacity (C). Thus, a 10-Gbps link with an RTT of 250¬†msec 
would need an amount of buffering equal to B = RTT # C = 2.5 Gbits of buff-
ers. This result was based on an analysis of the queueing dynamics of a relatively 
small number of TCP flows [Villamizar 1994]. More recent theoretical and experi-
mental efforts [Appenzeller 2004], however, suggest that when a large number of 
independent TCP flows (N) pass through a link, the amount of buffering needed is 
B = RTT # C> 2N. In core networks, where a large number of TCP flows typi-
cally pass through large backbone router links, the value of N can be large, with 
the decrease in needed buffer size becoming quite significant. [Appenzeller 2004; 
Wischik 2005; Beheshti 2008] provide very readable discussions of the buffer-sizing 
problem from a theoretical, implementation, and operational standpoint.
It‚Äôs temping to think that more buffering must be better‚Äîlarger buffers would 
allow a router to absorb larger fluctuations in the packet arrival rate, thereby decreas-
ing the router‚Äôs packet loss rate. But larger buffers also mean potentially longer 
queueing delays. For gamers and for interactive teleconferencing users, tens of mil-
liseconds count. Increasing the amount of per-hop buffer by a factor of 10 to decrease 
packet loss could increase the end-end delay by a factor of 10! Increased RTTs also 
make TCP senders less responsive and slower to respond to incipient congestion and/
or packet loss. These delay-based considerations show that buffering is a double-
edged sword‚Äîbuffering can be used to absorb short-term statistical fluctuations in 
traffic but can also lead to increased delay and the attendant concerns. Buffering is 
a bit like salt‚Äîjust the right amount of salt makes food better, but too much makes 
it inedible!
In the discussion above, we‚Äôve implicitly assumed that many independent send-
ers are competing for bandwidth and buffers at a congested link. While this is prob-
ably an excellent assumption for routers within the network core, at the network edge

this may not hold. Figure 4.10(a) shows a home router sending TCP segments to a 
remote game server. Following [Nichols 2012], suppose that it takes 20 ms to trans-
mit a packet (containing a gamer‚Äôs TCP segment), that there are negligible queueing 
delays elsewhere on the path to the game server, and that the RTT is 200 ms. As 
shown in Figure 4.10(b), suppose that at time t = 0, a burst of 25 packets arrives to 
the queue. One of these queued packets is then transmitted once every 20 ms, so that 
at t = 200 msec, the first ACK arrives, just as the 21st packet is being transmitted. 
This ACK arrival causes the TCP sender to send another packet, which is queued at 
the outgoing link of the home router. At t = 220, the next ACK arrives, and another 
TCP segment is released by the gamer and is queued, as the 22nd packet is being 
transmitted, and so on. You should convince yourself that in this scenario, ACK 
clocking results in a new packet arriving at the queue every time a queued packet 
is sent, resulting in queue size at the home router‚Äôs outgoing link that is always five 
packets! That is, the end-end-pipe is full (delivering packets to the destination at the 
path bottleneck rate of one packet every 20 ms), but the amount of queueing delay is 
constant and persistent. As a result, the gamer is unhappy with the delay, and the par-
ent (who even knows wireshark!) is confused because he or she doesn‚Äôt understand 
why delays are persistent and excessively long, even when there is no other traffic 
on the home network.
This scenario above of long delay due to persistent buffering is known as buff-
erbloat and illustrates that not only is throughput important, but also minimal delay 
is important as well [Kleinrock 2018], and that the interaction among senders at the 
network edge and queues within the network can indeed be complex and subtle. The 
DOCSIS 3.1 standard for cable networks that we will study in Chapter 6, recently 
added a specific AQM mechanism [RFC 8033, RFC 8034] to combat bufferbloat, 
while preserving bulk throughput performance.
250 ms RTT
Time (ms)
Queue length
0
200
25
5
a.
b.
Home Network
Internet
Figure 4.10 ‚ô¶ Bufferbloat: persistent queues

4.2.5 Packet Scheduling
Let‚Äôs now return to the question of determining the order in which queued packets are 
transmitted over an outgoing link. Since you yourself have undoubtedly had to wait in 
long lines on many occasions and observed how waiting customers are served, you‚Äôre 
no doubt familiar with many of the queueing disciplines commonly used in routers. 
There is first-come-first-served (FCFS, also known as first-in-first-out, FIFO). The 
British are famous for patient and orderly FCFS queueing at bus stops and in the mar-
ketplace (‚ÄúOh, are you queueing?‚Äù). Other countries operate on a priority basis, with 
one class of waiting customers given priority service over other waiting customers. 
There is also round-robin queueing, where customers are again divided into classes 
(as in priority queueing) but each class of customer is given service in turn.
First-in-First-Out (FIFO)
Figure 4.11 shows the queuing model abstraction for the FIFO link-scheduling dis-
cipline. Packets arriving at the link output queue wait for transmission if the link is 
currently busy transmitting another packet. If there is not sufficient buffering space 
to hold the arriving packet, the queue‚Äôs packet-discarding policy then determines 
whether the packet will be dropped (lost) or whether other packets will be removed 
from the queue to make space for the arriving packet, as discussed above. In our  
discussion below, we‚Äôll ignore packet discard. When a packet is completely transmit-
ted over the outgoing link (that is, receives service) it is removed from the queue.
The FIFO (also known as first-come-first-served, or FCFS) scheduling discipline 
selects packets for link transmission in the same order in which they arrived 
at the output link queue. We‚Äôre all familiar with FIFO queuing from service centers, 
where arriving customers join the back of the single waiting line, remain in order, and 
are then served when they reach the front of the line. Figure 4.12 shows the FIFO queue 
in operation. Packet arrivals are indicated by numbered arrows above the upper time-
line, with the number indicating the order in which the packet arrived. Individual packet 
departures are shown below the lower timeline. The time that a packet spends in service 
(being transmitted) is indicated by the shaded rectangle between the two timelines. In 
Arrivals
Departures
Queue
(waiting area)
Link
(server)
Figure 4.11 ‚ô¶ FIFO queueing abstraction

our examples here, let‚Äôs assume that each packet takes three units of time to be transmit-
ted. Under the FIFO discipline, packets leave in the same order in which they arrived. 
Note that after the departure of packet 4, the link remains idle (since packets 1 through 
4 have been transmitted and removed from the queue) until the arrival of packet 5.
Priority Queuing
Under priority queuing, packets arriving at the output link are classified into prior-
ity classes upon arrival at the queue, as shown in Figure 4.13. In practice, a network 
operator may configure a queue so that packets carrying network management infor-
mation (for example, as indicated by the source or destination TCP/UDP port num-
ber) receive priority over user traffic; additionally, real-time voice-over-IP packets 
might receive priority over non-real-time traffic such e-mail packets. Each priority 
class typically has its own queue. When choosing a packet to transmit, the priority 
Time
Arrivals
Departures
Packet
in service
Time
1
1
2
3
4
5
2
3
1
t = 0
t = 2
t = 4
t = 6
t = 8
t = 10
t = 12
t = 14
2
3
4
5
4
5
Figure 4.12 ‚ô¶ The FIFO queue in operation
Arrivals
Departures
Low-priority queue
(waiting area)
Classify
High-priority queue
(waiting area)
Link
(server)
Figure 4.13 ‚ô¶ The priority queueing model

NET NEUTRALITY
We‚Äôve seen that packet scheduling mechanisms (e.g., priority traffic scheduling disciplines 
such a strict priority, and WFQ) can be used to provide different levels of service to differ-
ent ‚Äúclasses‚Äù of traffic. The definition of what precisely constitutes a ‚Äúclass‚Äù of traffic is up 
to an ISP to decide, but could be potentially based on any set of fields in the IP datagram 
header. For example, the port field in the IP datagram header could be used to classify 
datagrams according to the ‚Äúwell-know service‚Äù associated with that port: SNMP network 
management datagram (port 161) might be assigned to a higher priority class than an 
IMAP e-mail protocol (ports 143, or 993) datagram and therefore receive better service. 
An ISP could also potentially use a datagram‚Äôs source IP address to provide priority to 
datagrams being sent by certain companies (who have presumably paid the ISP for this 
privilege) over datagrams being sent from other companies (who have not paid); an ISP 
PRINCIPLES IN PRACTICE
queuing discipline will transmit a packet from the highest priority class that has a 
nonempty queue (that is, has packets waiting for transmission). The choice among 
packets in the same priority class is typically done in a FIFO manner.
Figure 4.14 illustrates the operation of a priority queue with two priority classes. 
Packets 1, 3, and 4 belong to the high-priority class, and packets 2 and 5 belong to 
the low-priority class. Packet 1 arrives and, finding the link idle, begins transmission. 
During the transmission of packet 1, packets 2 and 3 arrive and are queued in the low- 
and high-priority queues, respectively. After the transmission of packet 1, packet 3  
(a¬† high-priority packet) is selected for transmission over packet 2 (which, even 
though it arrived earlier, is a low-priority packet). At the end of the transmission of 
packet 3, packet 2 then begins transmission. Packet 4 (a high-priority packet) arrives 
during the transmission of packet 2 (a low-priority packet). Under a non-preemptive 
priority queuing discipline, the transmission of a packet is not interrupted once it 
Arrivals
Departures
Packet
in service
1
1
2
3
4
5
2
3
1
4
5
Time
Time
t = 0
t = 2
t = 4
t = 6
t = 8
t = 10
t = 12
t = 14
2
3
4
5
Figure 4.14 ‚ô¶ The priority queue in operation

could even block traffic with a source IP address in a given company, or country. There 
are many mechanisms that would allow an ISP to provide different levels of service to dif-
ferent classes of traffic. The real question is what policies and laws determine what an ISP 
can actually do. Of course, these laws will vary by country; see [Smithsonian 2017] for a 
brief survey. Here, we‚Äôll briefly consider US policy on what has come to be known as ‚Äúnet 
neutrality.‚Äù
The term ‚Äúnet neutrality‚Äù doesn‚Äôt have a precise decision, but the March 2015 
Order on Protecting and Promoting an Open Internet [FCC 2015] by the US Federal 
Communications Commission provides three ‚Äúclear, bright line‚Äù rules that are now often 
associated with net neutrality:
‚Ä¢ 
‚ÄúNo Blocking.¬†.¬†.¬†.¬†A person engaged in the provision of broadband Internet access 
service,¬†.¬†.¬†.¬†shall not block lawful content, applications, services, or non-harmful 
devices, subject to reasonable network management.‚Äù
‚Ä¢ 
‚ÄúNo Throttling.¬†.¬†.¬†.¬†A person engaged in the provision of broadband Internet 
access service,¬†.¬†.¬†.¬†shall not impair or degrade lawful Internet traffic on the basis of 
Internet content, application, or service, or use of a non-harmful device, subject to rea-
sonable network management.‚Äù
‚Ä¢ 
‚ÄúNo Paid Prioritization.¬†.¬†.¬†.¬†A person engaged in the provision of broadband 
Internet access service,¬†.¬†.¬†.¬†shall not engage in paid prioritization. ‚ÄúPaid prioritization‚Äù 
refers to the management of a broadband provider‚Äôs network to directly or indirectly 
favor some traffic over other traffic, including through use of techniques such as traffic 
shaping, prioritization, resource reservation, or other forms of preferential traffic man-
agement,¬†.¬†.¬†.‚Äù
Quite interestingly, before the Order, ISP behaviors violating the first two of these rules 
had been observed [Faulhaber 2012]. In 2005, an ISP in North Carolina agreed to stop 
its practice of blocking its customers from using Vonage, a voice-over-IP service that com-
peted with its own telephone service. In 2007, Comcast was judged to be interfering with 
BitTorrent P2P traffic by internally creating and sending TCP RST packets to BitTorrent send-
ers and receivers, which caused them to close their BitTorrent connection [FCC 2008].
Both sides of the net neutrality debate have been argued strenuously, mostly focused 
on the extent to which net neutrality provides benefits to customers, while at the same 
time promoting innovation. See [Peha 2006, Faulhaber 2012, Economides 2017, 
Madhyastha 2017].
The 2015 FCC Order on Protecting and Promoting an Open Internet, which banned 
ISPs from blocking, throttling, or providing paid prioritizing, was superseded by the 2017 
FCC Restoring Internet Freedom Order, [FCC 2017] which rolled back these prohibitions 
and focused instead on ISP transparency. With so much interest and so many changes,  
it‚Äôs probably safe to say we aren‚Äôt close to having seen the final chapter written on net 
neutrality in the United States, or elsewhere.

has begun. In this case, packet 4 queues for transmission and begins being transmit-
ted after the transmission of packet 2 is completed.
Round Robin and Weighted Fair Queuing (WFQ)
Under the round robin queuing discipline, packets are sorted into classes as with 
priority queuing. However, rather than there being a strict service priority among 
classes, a round robin scheduler alternates service among the classes. In the simplest 
form of round robin scheduling, a class 1 packet is transmitted, followed by a class 
2 packet, followed by a class 1 packet, followed by a class 2 packet, and so on. A 
so-called work-conserving queuing discipline will never allow the link to remain 
idle whenever there are packets (of any class) queued for transmission. A work-
conserving round robin discipline that looks for a packet of a given class but finds 
none will immediately check the next class in the round robin sequence.
Figure 4.15 illustrates the operation of a two-class round robin queue. In this 
example, packets 1, 2, and 4 belong to class 1, and packets 3 and 5 belong to the 
second class. Packet 1 begins transmission immediately upon arrival at the output 
queue. Packets 2 and 3 arrive during the transmission of packet 1 and thus queue for 
transmission. After the transmission of packet 1, the link scheduler looks for a class 2 
packet and thus transmits packet 3. After the transmission of packet 3, the scheduler 
looks for a class 1 packet and thus transmits packet 2. After the transmission of packet 2, 
packet 4 is the only queued packet; it is thus transmitted immediately after packet 2.
A generalized form of round robin queuing that has been widely implemented 
in routers is the so-called weighted fair queuing (WFQ) discipline [Demers 1990; 
Parekh 1993. WFQ is illustrated in Figure 4.16. Here, arriving packets are classified 
and queued in the appropriate per-class waiting area. As in round robin scheduling, 
a WFQ scheduler will serve classes in a circular manner‚Äîfirst serving class 1, then 
serving class 2, then serving class 3, and then (assuming there are three classes) 
repeating the service pattern. WFQ is also a work-conserving queuing discipline and 
Arrivals
Packet
in service
1
1
2
3
4
5
2
3
1
2
3
4
5
4
5
Departures
Time
Time
t = 0
t = 2
t = 4
t = 6
t = 8
t = 10
t = 12
t = 14
Figure 4.15 ‚ô¶ The two-class robin queue in operation

thus will immediately move on to the next class in the service sequence when it finds 
an empty class queue.
WFQ differs from round robin in that each class may receive a differential amount 
of service in any interval of time. Specifically, each class, i, is assigned a weight, wi. 
Under WFQ, during any interval of time during which there are class i packets to send, 
class i will then be guaranteed to receive a fraction of service equal to wi>(gwj), where 
the sum in the denominator is taken over all classes that also have packets queued for 
transmission. In the worst case, even if all classes have queued packets, class i will still 
be guaranteed to receive a fraction wi >(gwj) of the bandwidth, where in this worst 
case the sum in the denominator is over all classes. Thus, for a link with transmission 
rate R, class i will always achieve a throughput of at least R # wi >(gwj). Our descrip-
tion of WFQ has been idealized, as we have not considered the fact that packets are 
discrete and a packet‚Äôs transmission will not be interrupted to begin transmission of 
another packet; [Demers 1990; Parekh 1993] discuss this packetization issue.
4.3 The Internet Protocol (IP): IPv4, Addressing, 
IPv6, and More
Our study of the network layer thus far in Chapter 4‚Äîthe notion of the data and con-
trol plane component of the network layer, our distinction between forwarding and 
routing, the identification of various network service models, and our look inside a 
router‚Äîhave often been without reference to any specific computer network archi-
tecture or protocol. In this section, we‚Äôll focus on key aspects of the network layer on 
today‚Äôs Internet and the celebrated Internet Protocol (IP).
There are two versions of IP in use today. We‚Äôll first examine the widely 
deployed IP protocol version 4, which is usually referred to simply as IPv4 [RFC 
791] in Section 4.3.1. We‚Äôll examine IP version 6 [RFC 2460; RFC 4291], which has 
Classify
Arrivals
Departures
w1
w2
w3
Link
Figure 4.16 ‚ô¶ Weighted fair queueing

been proposed to replace IPv4, in Section 4.3.4. In between, we‚Äôll primarily cover 
Internet addressing‚Äîa topic that might seem rather dry and detail-oriented but we‚Äôll 
see is crucial to understanding how the Internet‚Äôs network layer works. To master IP 
addressing is to master the Internet‚Äôs network layer itself!
4.3.1 IPv4 Datagram Format
Recall that the Internet‚Äôs network-layer packet is referred to as a datagram. We begin 
our study of IP with an overview of the syntax and semantics of the IPv4 datagram. 
You might be thinking that nothing could be drier than the syntax and semantics of a 
packet‚Äôs bits. Nevertheless, the datagram plays a central role in the Internet‚Äîevery 
networking student and professional needs to see it, absorb it, and master it. (And 
just to see that protocol headers can indeed be fun to study, check out [Pomeranz 
2010]). The IPv4 datagram format is shown in Figure 4.17. The key fields in the IPv4 
datagram are the following:
‚Ä¢ Version number. These 4 bits specify the IP protocol version of the datagram. 
By looking at the version number, the router can determine how to interpret the 
remainder of the IP datagram. Different versions of IP use different datagram 
formats. The datagram format for IPv4 is shown in Figure 4.17. The datagram 
format for the new version of IP (IPv6) is discussed in Section 4.3.4.
‚Ä¢ Header length. Because an IPv4 datagram can contain a variable number of 
options (which are included in the IPv4 datagram header), these 4 bits are needed 
Version
Type of service
Header
length
Upper-layer
protocol
16-bit IdentiÔ¨Åer
Time-to-live
13-bit Fragmentation offset
Flags
Datagram length (bytes)
Header checksum
32 bits
32-bit Source IP address
32-bit Destination IP address
Options (if any)
Data
Figure 4.17 ‚ô¶ IPv4 datagram format

to determine where in the IP datagram the payload (for example, the transport-
layer segment being encapsulated in this datagram) actually begins. Most IP data-
grams do not contain options, so the typical IP datagram has a 20-byte header.
‚Ä¢ Type of service. The type of service (TOS) bits were included in the IPv4 header 
to allow different types of IP datagrams to be distinguished from each other. For 
example, it might be useful to distinguish real-time datagrams (such as those 
used by an IP telephony application) from non-real-time traffic (e.g., FTP). The 
 specific level of service to be provided is a policy issue determined and config-
ured by the network administrator for that router. We also learned in Section 3.7.2 
that two of the TOS bits are used for Explicit Congestion  Notification.
‚Ä¢ Datagram length. This is the total length of the IP datagram (header plus data), meas-
ured in bytes. Since this field is 16 bits long, the theoretical maximum size of the IP 
datagram is 65,535 bytes. However, datagrams are rarely larger than 1,500 bytes, which 
allows an IP datagram to fit in the payload field of a maximally sized Ethernet frame.
‚Ä¢ Identifier, flags, fragmentation offset. These three fields have to do with so-called 
IP fragmentation, when a large IP datagram is broken into several smaller IP data-
grams which are then forwarded independently to the destination, where they are 
reassembled before their payload data (see below) is passed up to the transport layer 
at the destination host. Interestingly, the new version of IP, IPv6, does not allow for 
fragmentation. We‚Äôll not cover fragmentation here; but readers can find a detailed 
discussion online, among the ‚Äúretired‚Äù material from earlier versions of this book.
‚Ä¢ Time-to-live. The time-to-live (TTL) field is included to ensure that datagrams 
do not circulate forever (due to, for example, a long-lived routing loop) in the 
network. This field is decremented by one each time the datagram is processed by 
a router. If the TTL field reaches 0, a router must drop that datagram.
‚Ä¢ Protocol. This field is typically used only when an IP datagram reaches its final 
destination. The value of this field indicates the specific transport-layer protocol 
to which the data portion of this IP datagram should be passed. For example, a 
value of 6 indicates that the data portion is passed to TCP, while a value of 17 indi-
cates that the data is passed to UDP. For a list of all possible values, see [IANA 
Protocol Numbers 2016]. Note that the protocol number in the IP datagram has 
a role that is analogous to the role of the port number field in the transport-layer 
segment. The protocol number is the glue that binds the network and transport 
layers together, whereas the port number is the glue that binds the transport and 
application layers together. We‚Äôll see in Chapter 6 that the link-layer frame also 
has a special field that binds the link layer to the network layer.
‚Ä¢ Header checksum. The header checksum aids a router in detecting bit errors in a 
received IP datagram. The header checksum is computed by treating each 2¬†bytes 
in the header as a number and summing these numbers using 1s complement arith-
metic. As discussed in Section 3.3, the 1s complement of this sum, known as 
the Internet checksum, is stored in the checksum field. A router computes the 
header checksum for each received IP datagram and detects an error condition if

the checksum carried in the datagram header does not equal the computed check-
sum. Routers typically discard datagrams for which an error has been detected. 
Note that the checksum must be recomputed and stored again at each router, since 
the TTL field, and possibly the options field as well, will change. An interesting 
discussion of fast algorithms for computing the Internet checksum is [RFC 1071]. 
A question often asked at this point is, why does TCP/IP perform error checking at 
both the transport and network layers? There are several reasons for this repetition. 
First, note that only the IP header is checksummed at the IP layer, while the TCP/
UDP checksum is computed over the entire TCP/UDP segment. Second, TCP/
UDP and IP do not necessarily both have to belong to the same protocol stack. 
TCP can, in principle, run over a different network-layer protocol (for example, 
ATM) [Black 1995]) and IP can carry data that will not be passed to TCP/UDP.
‚Ä¢ Source and destination IP addresses. When a source creates a datagram, it inserts 
its IP address into the source IP address field and inserts the address of the ulti-
mate destination into the destination IP address field. Often the source host deter-
mines the destination address via a DNS lookup, as discussed in Chapter 2. We‚Äôll 
discuss IP addressing in detail in Section 4.3.2.
‚Ä¢ Options. The options fields allow an IP header to be extended. Header options 
were meant to be used rarely‚Äîhence the decision to save overhead by not includ-
ing the information in options fields in every datagram header. However, the 
mere existence of options does complicate matters‚Äîsince datagram headers can 
be of variable length, one cannot determine a priori where the data field will start. 
Also, since some datagrams may require options processing and others may not, 
the amount of time needed to process an IP datagram at a router can vary greatly. 
These considerations become particularly important for IP processing in high-
performance routers and hosts. For these reasons and others, IP options were not 
included in the IPv6 header, as discussed in Section 4.3.4.
‚Ä¢ Data (payload). Finally, we come to the last and most important field‚Äîthe raison 
d‚Äôetre for the datagram in the first place! In most circumstances, the data field of 
the IP datagram contains the transport-layer segment (TCP or UDP) to be deliv-
ered to the destination. However, the data field can carry other types of data, such 
as ICMP messages (discussed in Section 5.6).
Note that an IP datagram has a total of 20 bytes of header (assuming no options). 
If the datagram carries a TCP segment, then each datagram carries a total of 
40 bytes of header (20 bytes of IP header plus 20 bytes of TCP header) along with 
the application-layer message.
4.3.2 IPv4 Addressing
We now turn our attention to IPv4 addressing. Although you may be thinking that 
addressing must be a straightforward topic, hopefully by the end of this section you‚Äôll 
be convinced that Internet addressing is not only a juicy, subtle, and interesting topic

but also one that is of central importance to the Internet. An excellent treatment of 
IPv4 addressing can be found in the first chapter in [Stewart 1999].
Before discussing IP addressing, however, we‚Äôll need to say a few words about 
how hosts and routers are connected into the Internet. A host typically has only a 
single link into the network; when IP in the host wants to send a datagram, it does 
so over this link. The boundary between the host and the physical link is called 
an interface. Now consider a router and its interfaces. Because a router‚Äôs job is to 
receive a datagram on one link and forward the datagram on some other link, a router 
necessarily has two or more links to which it is connected. The boundary between the 
router and any one of its links is also called an interface. A router thus has multiple 
interfaces, one for each of its links. Because every host and router is capable of send-
ing and receiving IP datagrams, IP requires each host and router interface to have 
its own IP address. Thus, an IP address is technically associated with an interface, 
rather than with the host or router containing that interface.
Each IP address is 32 bits long (equivalently, 4 bytes), and there are thus a total 
of 232 (or approximately 4 billion) possible IP addresses. These addresses are typi-
cally written in so-called dotted-decimal notation, in which each byte of the address 
is written in its decimal form and is separated by a period (dot) from other bytes in 
the address. For example, consider the IP address 193.32.216.9. The 193 is the deci-
mal equivalent of the first 8 bits of the address; the 32 is the decimal equivalent of 
the second 8 bits of the address, and so on. Thus, the address 193.32.216.9 in binary 
notation is
11000001 00100000 11011000 00001001
Each interface on every host and router in the global Internet must have an IP address 
that is globally unique (except for interfaces behind NATs, as discussed in Section 4.3.3). 
These addresses cannot be chosen in a willy-nilly manner, however. A portion of 
an interface‚Äôs IP address will be determined by the subnet to which it is connected.
Figure 4.18 provides an example of IP addressing and interfaces. In this figure, 
one router (with three interfaces) is used to interconnect seven hosts. Take a close 
look at the IP addresses assigned to the host and router interfaces, as there are sev-
eral things to notice. The three hosts in the upper-left portion of Figure 4.18, and 
the router interface to which they are connected, all have an IP address of the form 
223.1.1.xxx. That is, they all have the same leftmost 24 bits in their IP address. These 
four interfaces are also interconnected to each other by a network that contains no 
routers. This network could be interconnected by an Ethernet LAN, in which case 
the interfaces would be interconnected by an Ethernet switch (as we‚Äôll discuss in 
Chapter 6), or by a wireless access point (as we‚Äôll discuss in Chapter 7). We‚Äôll repre-
sent this routerless network connecting these hosts as a cloud for now, and dive into 
the internals of such networks in Chapters 6 and 7.
In IP terms, this network interconnecting three host interfaces and one router 
interface forms a subnet [RFC 950]. (A subnet is also called an IP network or simply

a network in the Internet literature.) IP addressing assigns an address to this subnet: 
223.1.1.0/24, where the /24 (‚Äúslash-24‚Äù) notation, sometimes known as a subnet 
mask, indicates that the leftmost 24 bits of the 32-bit quantity define the subnet 
address. The 223.1.1.0/24 subnet thus consists of the three host interfaces (223.1.1.1, 
223.1.1.2, and 223.1.1.3) and one router interface (223.1.1.4). Any additional hosts 
attached to the 223.1.1.0/24 subnet would be required to have an address of the form 
223.1.1.xxx. There are two additional subnets shown in Figure 4.18: the 223.1.2.0/24 
network and the 223.1.3.0/24 subnet. Figure 4.19 illustrates the three IP subnets pre-
sent in Figure 4.18.
The IP definition of a subnet is not restricted to Ethernet segments that connect 
multiple hosts to a router interface. To get some insight here, consider Figure 4.20, 
which shows three routers that are interconnected with each other by point-to-point 
links. Each router has three interfaces, one for each point-to-point link and one for 
the broadcast link that directly connects the router to a pair of hosts. What subnets 
are present here? Three subnets, 223.1.1.0/24, 223.1.2.0/24, and 223.1.3.0/24, are 
similar to the subnets we encountered in Figure 4.18. But note that there are three 
additional subnets in this example as well: one subnet, 223.1.9.0/24, for the inter-
faces that connect routers R1 and R2; another subnet, 223.1.8.0/24, for the interfaces 
that connect routers R2 and R3; and a third subnet, 223.1.7.0/24, for the interfaces 
that connect routers R3 and R1. For a general interconnected system of routers and 
hosts, we can use the following recipe to define the subnets in the system:
223.1.1.1
223.1.2.1
223.1.2.2
223.1.1.2
223.1.1.4
223.1.2.9
223.1.3.27
223.1.1.3
223.1.3.1
223.1.3.2
Figure 4.18 ‚ô¶ Interface addresses and subnets

To determine the subnets, detach each interface from its host or router, creating 
islands of isolated networks, with interfaces terminating the end points of the 
isolated networks. Each of these isolated networks is called a subnet.
If we apply this procedure to the interconnected system in Figure 4.20, we get six 
islands or subnets.
From the discussion above, it‚Äôs clear that an organization (such as a company or 
academic institution) with multiple Ethernet segments and point-to-point links will 
have multiple subnets, with all of the devices on a given subnet having the same subnet 
address. In principle, the different subnets could have quite different subnet addresses. 
In practice, however, their subnet addresses often have much in common. To understand 
why, let‚Äôs next turn our attention to how addressing is handled in the global Internet.
The Internet‚Äôs address assignment strategy is known as Classless Interdomain 
Routing (CIDR‚Äîpronounced cider) [RFC 4632]. CIDR generalizes the notion of 
subnet addressing. As with subnet addressing, the 32-bit IP address is divided into 
two parts and again has the dotted-decimal form a.b.c.d/x, where x indicates the 
number of bits in the first part of the address.
The x most significant bits of an address of the form a.b.c.d/x constitute the 
network portion of the IP address, and are often referred to as the prefix (or network 
prefix) of the address. An organization is typically assigned a block of contiguous 
addresses, that is, a range of addresses with a common prefix (see the Principles in 
Practice feature). In this case, the IP addresses of devices within the organization 
will share the common prefix. When we cover the Internet‚Äôs BGP routing protocol in 
223.1.1.0/24
223.1.2.0/24
223.1.3.0/24
Figure 4.19 ‚ô¶ Subnet addresses

Section 5.4, we‚Äôll see that only these x leading prefix bits are considered by routers 
outside the organization‚Äôs network. That is, when a router outside the organization 
forwards a datagram whose destination address is inside the organization, only the 
leading x bits of the address need be considered. This considerably reduces the size 
of the forwarding table in these routers, since a single entry of the form a.b.c.d/x will 
be sufficient to forward packets to any destination within the organization.
The remaining 32-x bits of an address can be thought of as distinguishing among the 
devices within the organization, all of which have the same network prefix. These are 
the bits that will be considered when forwarding packets at routers within the organiza-
tion. These lower-order bits may (or may not) have an additional subnetting structure, 
such as that discussed above. For example, suppose the first 21 bits of the CIDRized 
address a.b.c.d/21 specify the organization‚Äôs network prefix and are common to the IP 
addresses of all devices in that organization. The remaining 11 bits then identify the 
specific hosts in the organization. The organization‚Äôs internal structure might be such 
that these 11 rightmost bits are used for subnetting within the organization, as discussed 
above. For example, a.b.c.d/24 might refer to a specific subnet within the organization.
Before CIDR was adopted, the network portions of an IP address were constrained 
to be 8, 16, or 24 bits in length, an addressing scheme known as classful addressing, 
223.1.8.1
223.1.8.0
223.1.9.1
223.1.7.1
223.1.2.6
223.1.2.1
223.1.2.2
223.1.3.1
223.1.3.2
223.1.1.3
223.1.7.0
223.1.9.2
223.1.3.27
223.1.1.1
223.1.1.4
R1
R2
R3
Figure 4.20 ‚ô¶ Three routers interconnecting six subnets

since subnets with 8-, 16-, and 24-bit subnet addresses were known as class A, B, and 
C networks, respectively. The requirement that the subnet portion of an IP address be 
exactly 1, 2, or 3 bytes long turned out to be problematic for supporting the rapidly 
growing number of organizations with small and medium-sized subnets. A class C 
(/24) subnet could accommodate only up to 28 2 2 5 254 hosts (two of the 28 5 256 
addresses are reserved for special use)‚Äîtoo small for many organizations. However, a 
class¬†B (/16) subnet, which supports up to 65,634 hosts, was too large. Under classful 
addressing, an organization with, say, 2,000 hosts was typically allocated a class B 
(/16) subnet address. This led to a rapid depletion of the class B address space and 
poor utilization of the assigned address space. For example, the organization that 
used a class B address for its 2,000 hosts was allocated enough of the address space 
for up to 65,534 interfaces‚Äîleaving more than 63,000 addresses that could not be 
used by other organizations.
This example of an ISP that connects eight organizations to the Internet nicely illustrates how 
carefully allocated CIDRized addresses facilitate routing. Suppose, as shown in Figure 4.21, 
that the ISP (which we‚Äôll call Fly-By-Night-ISP) advertises to the outside world that it should 
be sent any datagrams whose first 20 address bits match 200.23.16.0/20. The rest of 
the world need not know that within the address block 200.23.16.0/20 there are in fact 
eight other organizations, each with its own subnets. This ability to use a single prefix to 
advertise multiple networks is often referred to as address aggregation (also route 
aggregation or route summarization).
Address aggregation works extremely well when addresses are allocated in blocks 
to ISPs and then from ISPs to client organizations. But what happens when addresses 
are not allocated in such a hierarchical manner? What would happen, for example, if 
Fly-By-Night-ISP acquires ISPs-R-Us and then has Organization 1 connect to the Internet 
through its subsidiary ISPs-R-Us? As shown in Figure 4.21, the subsidiary ISPs-R-Us owns 
the address block 199.31.0.0/16, but Organization 1‚Äôs IP addresses are unfortunately 
outside of this address block. What should be done here? Certainly, Organization 1 could 
renumber all of its routers and hosts to have addresses within the ISPs-R-Us address block. 
But this is a costly solution, and Organization 1 might well be reassigned to another 
subsidiary in the future. The solution typically adopted is for Organization 1 to keep its 
IP addresses in 200.23.18.0/23. In this case, as shown in Figure 4.22, Fly-By-Night-ISP 
continues to advertise the address block 200.23.16.0/20 and ISPs-R-Us continues to 
advertise 199.31.0.0/16. However, ISPs-R-Us now also advertises the block of addresses 
for Organization 1, 200.23.18.0/23. When other routers in the larger Internet see the 
address blocks 200.23.16.0/20 (from Fly-By-Night-ISP) and 200.23.18.0/23 (from ISPs-
R-Us) and want to route to an address in the block 200.23.18.0/23, they will use longest 
prefix matching (see Section 4.2.1), and route toward ISPs-R-Us, as it advertises the long-
est (i.e., most-specific) address prefix that matches the destination address.
PRINCIPLES IN PRACTICE

Organization 0
200.23.16.0/23
Organization 1
Fly-By-Night-ISP
‚ÄúSend me anything
  with addresses
  beginning
  200.23.16.0/20‚Äù
ISPs-R-Us
200.23.18.0/23
Organization 2
200.23.20.0/23
Organization 7
200.23.30.0/23
Internet
‚ÄúSend me anything
  with addresses
  beginning
  199.31.0.0/16‚Äù
Figure 4.21 ‚ô¶ Hierarchical addressing and route aggregation
Organization 0
200.23.16.0/23
Organization 2
Fly-By-Night-ISP
‚ÄúSend me anything
  with addresses
  beginning
  200.23.16.0/20‚Äù
ISPs-R-Us
200.23.20.0/23
Organization 7
200.23.30.0/23
Organization 1
200.23.18.0/23
Internet
‚ÄúSend me anything
  with addresses
  beginning
  199.31.0.0/16 or
  200.23.18.0/23‚Äù
Figure 4.22 ‚ô¶ ISPs-R-Us has a more specific route to Organization 1

We would be remiss if we did not mention yet another type of IP address, the IP 
broadcast address 255.255.255.255. When a host sends a datagram with destination 
address 255.255.255.255, the message is delivered to all hosts on the same subnet. 
Routers optionally forward the message into neighboring subnets as well (although 
they usually don‚Äôt).
Having now studied IP addressing in detail, we need to know how hosts and 
subnets get their addresses in the first place. Let‚Äôs begin by looking at how an 
organization gets a block of addresses for its devices, and then look at how a 
device (such as a host) is assigned an address from within the organization‚Äôs block 
of addresses.
Obtaining a Block of Addresses
In order to obtain a block of IP addresses for use within an organization‚Äôs subnet, 
a network administrator might first contact its ISP, which would provide addresses 
from a larger block of addresses that had already been allocated to the ISP. For 
example, the ISP may itself have been allocated the address block 200.23.16.0/20. 
The ISP, in turn, could divide its address block into eight equal-sized contiguous 
address blocks and give one of these address blocks out to each of up to eight organi-
zations that are supported by this ISP, as shown below. (We have underlined the 
subnet part of these addresses for your convenience.)
ISP‚Äôs block:     200.23.16.0/20     11001000 00010111 00010000 00000000
Organization 0   200.23.16.0/23     11001000 00010111 00010000 00000000
Organization 1   200.23.18.0/23     11001000 00010111 00010010 00000000
Organization 2   200.23.20.0/23     11001000 00010111 00010100 00000000
¬†¬†¬†¬†‚Ä¶¬†¬† ‚Ä¶¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†¬†                 ¬†¬†¬†‚Ä¶
Organization 7   200.23.30.0/23     11001000 00010111 00011110 00000000
While obtaining a set of addresses from an ISP is one way to get a block of 
addresses, it is not the only way. Clearly, there must also be a way for the ISP itself 
to get a block of addresses. Is there a global authority that has ultimate responsibility 
for managing the IP address space and allocating address blocks to ISPs and other 
organizations? Indeed there is! IP addresses are managed under the authority of the 
Internet Corporation for Assigned Names and Numbers (ICANN) [ICANN 2020], 
based on guidelines set forth in [RFC 7020]. The role of the nonprofit ICANN organ-
ization is not only to allocate IP addresses, but also to manage the DNS root servers. 
It also has the very contentious job of assigning domain names and resolving domain 
name disputes. The ICANN allocates addresses to regional Internet registries (for 
example, ARIN, RIPE, APNIC, and LACNIC, which together form the Address

Supporting Organization of ICANN [ASO-ICANN 2020]), and handle the alloca-
tion/management of addresses within their regions.
Obtaining a Host Address: The Dynamic Host Configuration Protocol
Once an organization has obtained a block of addresses, it can assign individual 
IP addresses to the host and router interfaces in its organization. A system admin-
istrator will typically manually configure the IP addresses into the router (often 
remotely, with a network management tool). Host addresses can also be config-
ured manually, but typically this is done using the Dynamic Host Configuration 
Protocol (DHCP) [RFC 2131]. DHCP allows a host to obtain (be allocated) an 
IP address automatically. A network administrator can configure DHCP so that a 
given host receives the same IP address each time it connects to the network, or a 
host may be assigned a temporary IP address that will be different each time the 
host connects to the network. In addition to host IP address assignment, DHCP also 
allows a host to learn additional information, such as its subnet mask, the address 
of its first-hop router (often called the default gateway), and the address of its local 
DNS server.
Because of DHCP‚Äôs ability to automate the network-related aspects of connect-
ing a host into a network, it is often referred to as a plug-and-play or zeroconf 
(zero-configuration) protocol. This capability makes it very attractive to the network 
administrator who would otherwise have to perform these tasks manually! DHCP 
is also enjoying widespread use in residential Internet access networks, enterprise 
networks, and in wireless LANs, where hosts join and leave the network frequently. 
Consider, for example, the student who carries a laptop from a dormitory room to 
a library to a classroom. It is likely that in each location, the student will be con-
necting into a new subnet and hence will need a new IP address at each location. 
DHCP is ideally suited to this situation, as there are many users coming and going, 
and addresses are needed for only a limited amount of time. The value of DHCP‚Äôs 
plug-and-play capability is clear, since it‚Äôs unimaginable that a system administrator 
would be able to reconfigure laptops at each location, and few students (except those 
taking a computer networking class!) would have the expertise to configure their 
laptops manually.
DHCP is a client-server protocol. A client is typically a newly arriving host 
wanting to obtain network configuration information, including an IP address for 
itself. In the simplest case, each subnet (in the addressing sense of Figure 4.20) will 
have a DHCP server. If no server is present on the subnet, a DHCP relay agent (typi-
cally a router) that knows the address of a DHCP server for that network is needed. 
Figure 4.23 shows a DHCP server attached to subnet 223.1.2/24, with the router 
serving as the relay agent for arriving clients attached to subnets 223.1.1/24 and 
223.1.3/24. In our discussion below, we‚Äôll assume that a DHCP server is available 
on the subnet.

For a newly arriving host, the DHCP protocol is a four-step process, as shown in 
Figure 4.24 for the network setting shown in Figure 4.23. In this figure, yiaddr (as 
in ‚Äúyour Internet address‚Äù) indicates the address being allocated to the newly arriving 
client. The four steps are:
‚Ä¢ DHCP server discovery. The first task of a newly arriving host is to find a DHCP 
server with which to interact. This is done using a DHCP discover message, 
which a client sends within a UDP packet to port 67. The UDP packet is encap-
sulated in an IP datagram. But to whom should this datagram be sent? The host 
doesn‚Äôt even know the IP address of the network to which it is attaching, much 
less the address of a DHCP server for this network. Given this, the DHCP client 
creates an IP datagram containing its DHCP discover message along with the 
broadcast destination IP address of 255.255.255.255 and a ‚Äúthis host‚Äù source IP 
address of 0.0.0.0. The DHCP client passes the IP datagram to the link layer, 
which then broadcasts this frame to all nodes attached to the subnet (we will cover 
the details of link-layer broadcasting in Section 6.4).
‚Ä¢ DHCP server offer(s). A DHCP server receiving a DHCP discover message 
responds to the client with a DHCP offer message that is broadcast to all 
223.1.1.1
223.1.1.2
223.1.1.4
223.1.2.9
223.1.3.27
223.1.1.3
223.1.3.1
223.1.3.2
223.1.2.1
223.1.2.5
223.1.2.2
Arriving
DHCP
client
DHCP
server
Figure 4.23 ‚ô¶ DHCP client and server

DHCP server:
223.1.2.5
Arriving client
DHCP discover
Time
Time
src: 0.0.0.0, 68
dest: 255.255.255.255,67
DHCPDISCOVER
yiaddr: 0.0.0.0
transaction ID: 654
src: 223.1.2.5, 67
dest: 255.255.255.255,68
DHCPOFFER
yiaddrr: 223.1.2.4
transaction ID: 654
DHCP server ID: 223.1.2.5
Lifetime: 3600 secs
DHCP offer
src: 223.1.2.5, 67
dest: 255.255.255.255,68
DHCPACK
yiaddrr: 223.1.2.4
transaction ID: 655
DHCP server ID: 223.1.2.5
Lifetime: 3600 secs
DHCP ACK
src: 0.0.0.0, 68
dest: 255.255.255.255, 67
DHCPREQUEST
yiaddrr: 223.1.2.4
transaction ID: 655
DHCP server ID: 223.1.2.5
Lifetime: 3600 secs
DHCP request
Figure 4.24 ‚ô¶ DHCP client-server interaction
nodes on the subnet, again using the IP broadcast address of 255.255.255.255. 
(You might want to think about why this server reply must also be broadcast). 
Since several DHCP servers can be present on the subnet, the client may find 
itself in the enviable position of being able to choose from among several 
offers. Each server offer message contains the transaction ID of the received 
discover message, the proposed IP address for the client, the network mask, 
and an IP address lease time‚Äîthe amount of time for which the IP address 
will be valid. It is common for the server to set the lease time to several hours 
or days [Droms 2002].

‚Ä¢ DHCP request. The newly arriving client will choose from among one or more 
server offers and respond to its selected offer with a DHCP request message, 
echoing back the configuration parameters.
‚Ä¢ DHCP ACK. The server responds to the DHCP request message with a DHCP 
ACK message, confirming the requested parameters.
Once the client receives the DHCP ACK, the interaction is complete and the 
client can use the DHCP-allocated IP address for the lease duration. Since a client 
may want to use its address beyond the lease‚Äôs expiration, DHCP also provides a 
mechanism that allows a client to renew its lease on an IP address.
From a mobility aspect, DHCP does have one very significant shortcoming. 
Since a new IP address is obtained from DHCP each time a node connects to a 
new subnet, a TCP connection to a remote application cannot be maintained as a 
mobile node moves between subnets. In Chapter 7, we will learn how mobile cel-
lular networks allow a host to retain its IP address and ongoing TCP connections as 
it moves between base stations in a provider‚Äôs cellular network. Additional details 
about DHCP can be found in [Droms 2002] and [dhc 2020]. An open source refer-
ence implementation of DHCP is available from the Internet Systems Consortium 
[ISC 2020].
4.3.3 Network Address Translation (NAT)
Given our discussion about Internet addresses and the IPv4 datagram format, 
we‚Äôre now well aware that every IP-capable device needs an IP address. With the 
 proliferation of small office, home office (SOHO) subnets, this would seem to imply 
that whenever a SOHO wants to install a LAN to connect multiple machines, a range 
of addresses would need to be allocated by the ISP to cover all of the SOHO‚Äôs IP 
devices (including phones, tablets, gaming devices, IP TVs, printers and more). 
If the subnet grew bigger, a larger block of addresses would have to be allocated. 
But what if the ISP had already allocated the contiguous portions of the SOHO 
 network‚Äôs current address range? And what typical homeowner wants (or should 
need) to know how to manage IP addresses in the first place? Fortunately, there 
is a simpler approach to address allocation that has found increasingly widespread 
use in such scenarios: network address translation (NAT) [RFC 2663; RFC 3022; 
Huston 2004, Zhang 2007; Huston 2017].
Figure 4.25 shows the operation of a NAT-enabled router. The NAT-enabled 
router, residing in the home, has an interface that is part of the home network on 
the right of Figure 4.25. Addressing within the home network is exactly as we 
have seen above‚Äîall four interfaces in the home network have the same subnet 
address of 10.0.0.0/24. The address space 10.0.0.0/8 is one of three portions of 
the IP address space that is reserved in [RFC 1918] for a private network or a 
realm with private addresses, such as the home network in Figure 4.25. A realm 
with private addresses refers to a network whose addresses only have meaning to

devices within that network. To see why this is important, consider the fact that 
there are hundreds of thousands of home networks, many using the same address 
space, 10.0.0.0/24. Devices within a given home network can send packets to each 
other using 10.0.0.0/24 addressing. However, packets forwarded beyond the home 
network into the larger global Internet clearly cannot use these addresses (as either 
a source or a destination address) because there are hundreds of thousands of net-
works using this block of addresses. That is, the 10.0.0.0/24 addresses can only 
have meaning within the given home network. But if private addresses only have 
meaning within a given network, how is addressing handled when packets are sent 
to or received from the global Internet, where addresses are necessarily unique? The 
answer lies in understanding NAT.
The NAT-enabled router does not look like a router to the outside world. Instead 
the NAT router behaves to the outside world as a single device with a single IP 
address. In Figure 4.25, all traffic leaving the home router for the larger Internet has 
a source IP address of 138.76.29.7, and all traffic entering the home router must have a 
destination address of 138.76.29.7. In essence, the NAT-enabled router is hiding 
the details of the home network from the outside world. (As an aside, you might 
wonder where the home network computers get their addresses and where the router 
gets its single IP address. Often, the answer is the same‚ÄîDHCP! The router gets its 
address from the ISP‚Äôs DHCP server, and the router runs a DHCP server to provide 
addresses to computers within the NAT-DHCP-router-controlled home network‚Äôs 
address space.)
3
2
10.0.0.1
138.76.29.7
10.0.0.4
10.0.0.2
10.0.0.3
NAT translation table
WAN side
138.76.29.7, 5001
LAN side
10.0.0.1, 3345
. . .
. . .
S = 138.76.29.7, 5001
D = 128.119.40.186, 80 
1
4
S = 128.119.40.186, 80
D = 138.76.29.7, 5001
S = 128.119.40.186, 80
D = 10.0.0.1, 3345 
S = 10.0.0.1, 3345
D = 128.119.40.186, 80
Figure 4.25 ‚ô¶ Network address translation

If all datagrams arriving at the NAT router from the WAN have the same desti-
nation IP address (specifically, that of the WAN-side interface of the NAT router), 
then how does the router know the internal host to which it should forward a given 
datagram? The trick is to use a NAT translation table at the NAT router, and to 
include port numbers as well as IP addresses in the table entries.
Consider the example in Figure 4.25. Suppose a user sitting in a home net-
work behind host 10.0.0.1 requests a Web page on some Web server (port 80) 
with IP address 128.119.40.186. The host 10.0.0.1 assigns the (arbitrary) source 
port number 3345 and sends the datagram into the LAN. The NAT router receives 
the datagram, generates a new source port number 5001 for the datagram, replaces 
the source IP address with its WAN-side IP address 138.76.29.7, and replaces the 
original source port number 3345 with the new source port number 5001. When 
generating a new source port number, the NAT router can select any source port 
number that is not currently in the NAT translation table. (Note that because a port 
number field is 16 bits long, the NAT protocol can support over 60,000 simul-
taneous connections with a single WAN-side IP address for the router!) NAT 
in the router also adds an entry to its NAT translation table. The Web server, 
blissfully unaware that the arriving datagram containing the HTTP request has 
been manipulated by the NAT router, responds with a datagram whose destination 
address is the IP address of the NAT router, and whose destination port number is 
5001. When this datagram arrives at the NAT router, the router indexes the NAT 
translation table using the destination IP address and destination port number to 
obtain the appropriate IP address (10.0.0.1) and destination port number (3345) 
for the browser in the home network. The router then rewrites the datagram‚Äôs 
destination address and destination port number, and forwards the datagram into 
the home network.
NAT has enjoyed widespread deployment in recent years. But NAT is 
not without detractors. First, one might argue that, port numbers are meant to 
be used for addressing processes, not for addressing hosts. This violation can 
indeed cause problems for servers running on the home network, since, as we 
have seen in Chapter 2, server processes wait for incoming requests at well-
known port numbers and peers in a P2P protocol need to accept incoming con-
nections when acting as servers. How can one peer connect to another peer that 
is behind a NAT server, and has a DHCP-provided NAT address? Technical 
solutions to these problems include NAT traversal tools [RFC 5389] [RFC 
5389, RFC 5128, Ford 2005].
More ‚Äúphilosophical‚Äù arguments have also been raised against NAT by 
architectural purists. Here, the concern is that routers are meant to be layer 3 
(i.e., network-layer) devices, and should process packets only up to the net-
work layer. NAT violates this principle that hosts should be talking directly 
with each other, without interfering nodes modifying IP addresses, much less 
port numbers. We‚Äôll return to this debate later in Section 4.5, when we cover 
middleboxes.

4.3.4 IPv6
In the early 1990s, the Internet Engineering Task Force began an effort to develop a 
successor to the IPv4 protocol. A prime motivation for this effort was the realization 
that the 32-bit IPv4 address space was beginning to be used up, with new subnets 
INSPECTING DATAGRAMS: FIREWALLS AND INTRUSION DETECTION SYSTEMS
Suppose you are assigned the task of administering a home, departmental, university, or 
corporate network. Attackers, knowing the IP address range of your network, can easily 
send IP datagrams to addresses in your range. These datagrams can do all kinds of  
devious things, including mapping your network with ping sweeps and port scans, 
crashing vulnerable hosts with malformed packets, scanning for open TCP/UDP ports on 
servers in your network, and infecting hosts by including malware in the packets. As the 
network administrator, what are you going to do about all those bad guys out there, each 
capable of sending malicious packets into your network? Two popular defense mechanisms  
to malicious packet attacks are firewalls and intrusion detection systems (IDSs).
As a network administrator, you may first try installing a firewall between your 
network and the Internet. (Most access routers today have firewall capability.) 
Firewalls inspect the datagram and segment header fields, denying suspicious data-
grams entry into the internal network. For example, a firewall may be configured to 
block all ICMP echo request packets (see Section 5.6), thereby preventing an attack-
er from doing a traditional port scan across your IP address range. Firewalls can 
also block packets based on source and destination IP addresses and port numbers. 
Additionally, firewalls can be configured to track TCP connections, granting entry 
only to datagrams that belong to approved connections.
Additional protection can be provided with an IDS. An IDS, typically situated at 
the network boundary, performs ‚Äúdeep packet inspection,‚Äù examining not only head-
er fields but also the payloads in the datagram (including application-layer data). 
An IDS has a database of packet signatures that are known to be part of attacks. 
This database is automatically updated as new attacks are discovered. As packets 
pass through the IDS, the IDS attempts to match header fields and payloads to the 
signatures in its signature database. If such a match is found, an alert is created. An 
intrusion prevention system (IPS) is similar to an IDS, except that it actually blocks 
packets in addition to creating alerts. We‚Äôll explore firewalls and IDSs in more detail 
in Section 4.5 and in again Chapter 8.
Can firewalls and IDSs fully shield your network from all attacks? The answer is 
clearly no, as attackers continually find new attacks for which signatures are not yet 
available. But firewalls and traditional signature-based IDSs are useful in protecting 
your network from known attacks.
FOCUS ON SECURITY

and IP nodes being attached to the Internet (and being allocated unique IP addresses) 
at a breathtaking rate. To respond to this need for a large IP address space, a new 
IP protocol, IPv6, was developed. The designers of IPv6 also took this opportunity 
to tweak and augment other aspects of IPv4, based on the accumulated operational 
experience with IPv4.
The point in time when IPv4 addresses would be completely allocated (and 
hence no new networks could attach to the Internet) was the subject of considerable 
debate. The estimates of the two leaders of the IETF‚Äôs Address Lifetime Expec-
tations working group were that addresses would become exhausted in 2008 and 
2018, respectively [Solensky 1996]. In February 2011, IANA allocated out the last 
remaining pool of unassigned IPv4 addresses to a regional registry. While these reg-
istries still have available IPv4 addresses within their pool, once these addresses are 
exhausted, there are no more available address blocks that can be allocated from a 
central pool [Huston 2011a]. A recent survey of IPv4 address-space exhaustion, and 
the steps taken to prolong the life of the address space is [Richter 2015]; a recent 
analysis of IPv4 address use is [Huston 2019].
Although the mid-1990s estimates of IPv4 address depletion suggested that a 
considerable amount of time might be left until the IPv4 address space was exhausted, 
it was realized that considerable time would be needed to deploy a new technology 
on such an extensive scale, and so the process to develop IP version 6 (IPv6) [RFC 
2460] was begun [RFC 1752]. (An often-asked question is what happened to IPv5? 
It was initially envisioned that the ST-2 protocol would become IPv5, but ST-2 was 
later dropped.) An excellent source of information about IPv6 is [Huitema 1998].
IPv6 Datagram Format
The format of the IPv6 datagram is shown in Figure 4.26. The most important 
changes introduced in IPv6 are evident in the datagram format:
‚Ä¢ Expanded addressing capabilities. IPv6 increases the size of the IP address from 
32 to 128 bits. This ensures that the world won‚Äôt run out of IP addresses. Now, 
every grain of sand on the planet can be IP-addressable. In addition to unicast and 
multicast addresses, IPv6 has introduced a new type of address, called an anycast 
address, that allows a datagram to be delivered to any one of a group of hosts. 
(This feature could be used, for example, to send an HTTP GET to the nearest of 
a number of mirror sites that contain a given document.)
‚Ä¢ A streamlined 40-byte header. As discussed below, a number of IPv4 fields have 
been dropped or made optional. The resulting 40-byte fixed-length header allows 
for faster processing of the IP datagram by a router. A new encoding of options 
allows for more flexible options processing.
‚Ä¢ Flow labeling. IPv6 has an elusive definition of a flow. RFC 2460 states that this 
allows ‚Äúlabeling of packets belonging to particular flows for which the sender

requests special handling, such as a non-default quality of service or real-time 
service.‚Äù For example, audio and video transmission might likely be treated as 
a flow. On the other hand, the more traditional applications, such as file transfer 
and e-mail, might not be treated as flows. It is possible that the traffic carried by a 
high-priority user (for example, someone paying for better service for their traffic)  
might also be treated as a flow. What is clear, however, is that the designers of 
IPv6 foresaw the eventual need to be able to differentiate among the flows, even 
if the exact meaning of a flow had yet to be determined.
As noted above, a comparison of Figure 4.26 with Figure 4.17 reveals the sim-
pler, more streamlined structure of the IPv6 datagram. The following fields are 
defined in IPv6:
‚Ä¢ Version. This 4-bit field identifies the IP version number. Not surprisingly, IPv6 
carries a value of 6 in this field. Note that putting a 4 in this field does not create 
a valid IPv4 datagram. (If it did, life would be a lot simpler‚Äîsee the discussion 
below regarding the transition from IPv4 to IPv6.)
‚Ä¢ Traffic class. The 8-bit traffic class field, like the TOS field in IPv4, can be used 
to give priority to certain datagrams within a flow, or it can be used to give pri-
ority to datagrams from certain applications (for example, voice-over-IP) over 
datagrams from other applications (for example, SMTP e-mail).
‚Ä¢ Flow label. As discussed above, this 20-bit field is used to identify a flow of datagrams.
‚Ä¢ Payload length. This 16-bit value is treated as an unsigned integer giving the 
number of bytes in the IPv6 datagram following the fixed-length, 40-byte data-
gram header.
Version
TrafÔ¨Åc class
Payload length
Next hdr
Hop limit
Flow label
32 bits
Source address
(128 bits)
Destination address
(128 bits)
Data
Figure 4.26 ‚ô¶ IPv6 datagram format

‚Ä¢ Next header. This field identifies the protocol to which the contents (data field) of 
this datagram will be delivered (for example, to TCP or UDP). The field uses the 
same values as the protocol field in the IPv4 header.
‚Ä¢ Hop limit. The contents of this field are decremented by one by each router that 
forwards the datagram. If the hop limit count reaches zero, a router must discard 
that datagram.
‚Ä¢ Source and destination addresses. The various formats of the IPv6 128-bit address 
are described in RFC 4291.
‚Ä¢ Data. This is the payload portion of the IPv6 datagram. When the datagram 
reaches its destination, the payload will be removed from the IP datagram and 
passed on to the protocol specified in the next header field.
The discussion above identified the purpose of the fields that are included in the 
IPv6 datagram. Comparing the IPv6 datagram format in Figure 4.26 with the IPv4 
datagram format that we saw in Figure 4.17, we notice that several fields appearing 
in the IPv4 datagram are no longer present in the IPv6 datagram:
‚Ä¢ Fragmentation/reassembly. IPv6 does not allow for fragmentation and reassem-
bly at intermediate routers; these operations can be performed only by the source 
and destination. If an IPv6 datagram received by a router is too large to be for-
warded over the outgoing link, the router simply drops the datagram and sends a 
‚ÄúPacket Too Big‚Äù ICMP error message (see Section 5.6) back to the sender. The 
sender can then resend the data, using a smaller IP datagram size. Fragmentation 
and reassembly is a time-consuming operation; removing this functionality from 
the routers and placing it squarely in the end systems considerably speeds up IP 
forwarding within the network.
‚Ä¢ Header checksum. Because the transport-layer (for example, TCP and UDP) and 
link-layer (for example, Ethernet) protocols in the Internet layers perform check-
summing, the designers of IP probably felt that this functionality was sufficiently 
redundant in the network layer that it could be removed. Once again, fast pro-
cessing of IP packets was a central concern. Recall from our discussion of IPv4 
in Section 4.3.1 that since the IPv4 header contains a TTL field (similar to the 
hop limit field in IPv6), the IPv4 header checksum needed to be recomputed at 
every router. As with fragmentation and reassembly, this too was a costly opera-
tion in IPv4.
‚Ä¢ Options. An options field is no longer a part of the standard IP header. How-
ever, it has not gone away. Instead, the options field is one of the possible next 
headers pointed to from within the IPv6 header. That is, just as TCP or UDP 
protocol headers can be the next header within an IP packet, so too can an 
options field. The removal of the options field results in a fixed-length, 40-byte 
IP header.

Transitioning from IPv4 to IPv6
Now that we have seen the technical details of IPv6, let us consider a very practi-
cal matter: How will the public Internet, which is based on IPv4, be transitioned to 
IPv6? The problem is that while new IPv6-capable systems can be made backward-
compatible, that is, can send, route, and receive IPv4 datagrams, already deployed 
IPv4-capable systems are not capable of handling IPv6 datagrams. Several options 
are possible [Huston 2011b, RFC 4213].
One option would be to declare a flag day‚Äîa given time and date when all 
Internet machines would be turned off and upgraded from IPv4 to IPv6. The last 
major technology transition (from using NCP to using TCP for reliable transport 
service) occurred almost 40 years ago. Even back then [RFC 801], when the Internet 
was tiny and still being administered by a small number of ‚Äúwizards,‚Äù it was real-
ized that such a flag day was not possible. A flag day involving billions of devices 
is even more unthinkable today.
The approach to IPv4-to-IPv6 transition that has been most widely adopted in 
practice involves tunneling [RFC 4213]. The basic idea behind tunneling‚Äîa key 
concept with applications in many other scenarios beyond IPv4-to-IPv6 transition, 
including wide use in the all-IP cellular networks that we‚Äôll cover in Chapter 7‚Äîis 
the following. Suppose two IPv6 nodes (in this example, B and E in Figure 4.27) 
want to interoperate using IPv6 datagrams but are connected to each other by inter-
vening IPv4 routers. We refer to the intervening set of IPv4 routers between two 
IPv6 routers as a tunnel, as illustrated in Figure 4.27. With tunneling, the IPv6 node 
on the sending side of the tunnel (in this example, B) takes the entire IPv6 datagram 
and puts it in the data (payload) field of an IPv4 datagram. This IPv4 datagram is 
then addressed to the IPv6 node on the receiving side of the tunnel (in this example, 
E) and sent to the first node in the tunnel (in this example, C). The intervening IPv4 
routers in the tunnel route this IPv4 datagram among themselves, just as they would 
any other datagram, blissfully unaware that the IPv4 datagram itself contains a com-
plete IPv6 datagram. The IPv6 node on the receiving side of the tunnel eventually 
receives the IPv4 datagram (it is the destination of the IPv4 datagram!), determines 
that the IPv4 datagram contains an IPv6 datagram (by observing that the protocol 
number field in the IPv4 datagram is 41 [RFC 4213], indicating that the IPv4 
payload is a IPv6 datagram), extracts the IPv6 datagram, and then routes the IPv6 
datagram exactly as it would if it had received the IPv6 datagram from a directly 
connected IPv6 neighbor.
We end this section by noting that while the adoption of IPv6 was initially slow 
to take off [Lawton 2001; Huston 2008b], momentum has been building. NIST 
[NIST IPv6 2020] reports that more than a third of US government second-level 
domains are IPv6-enabled. On the client side, Google reports that about 25 percent 
of the clients accessing Google services do so via IPv6 [Google IPv6 2020]. Other 
recent measurements [Czyz 2014] indicate that IPv6 adoption has been accelerating. 
The proliferation of devices such as IP-enabled phones and other portable devices

provides an additional push for more widespread deployment of IPv6. Europe‚Äôs 
Third Generation Partnership Program [3GPP 2020] has specified IPv6 as the stand-
ard addressing scheme for mobile multimedia.
One important lesson that we can learn from the IPv6 experience is that it is enor-
mously difficult to change network-layer protocols. Since the early 1990s, numerous 
new network-layer protocols have been trumpeted as the next major revolution for 
the Internet, but most of these protocols have had limited penetration to date. These 
protocols include IPv6, multicast protocols, and resource reservation protocols; a dis-
cussion of these latter two classes of protocols can be found in the online supplement 
to this text. Indeed, introducing new protocols into the network layer is like replac-
ing the foundation of a house‚Äîit is difficult to do without tearing the whole house 
down or at least temporarily relocating the house‚Äôs residents. On the other hand, the 
Internet has witnessed rapid deployment of new protocols at the application layer. 
The classic examples, of course, are the Web, instant messaging, streaming media, 
distributed games, and various forms of social media. Introducing new application-
layer protocols is like adding a new layer of paint to a house‚Äîit is relatively easy to 
do, and if you choose an attractive color, others in the neighborhood will copy you. 
A
B
C
D
E
F
IPv6
A to B: IPv6
Physical view
B to C: IPv4
(encapsulating IPv6)
D to E: IPv4
(encapsulating IPv6)
E to F: IPv6
IPv6
IPv4
IPv4
IPv6
IPv6
Flow: X
Source: A
Dest: F
data
Source: B
Dest: E
Source: B
Dest: E
A
B
E
F
IPv6
Logical view
IPv6
Tunnel
IPv6
IPv6
Flow: X
Source: A
Dest: F
data
Flow: X
Source: A
Dest: F
data
Flow: X
Source: A
Dest: F
data
Figure 4.27 ‚ô¶ Tunneling

In summary, in the future, we can certainly expect to see changes in the Internet‚Äôs 
network layer, but these changes will likely occur on a time scale that is much slower 
than the changes that will occur at the application layer.
4.4 Generalized Forwarding and SDN
Recall that Section 4.2.1 characterized destination-based forwarding as the two steps 
of looking up a destination IP address (‚Äúmatch‚Äù), then sending the packet into the 
switching fabric to the specified output port (‚Äúaction‚Äù). Let‚Äôs now consider a signifi-
cantly more general ‚Äúmatch-plus-action‚Äù paradigm, where the ‚Äúmatch‚Äù can be made 
over multiple header fields associated with different protocols at different layers in 
the protocol stack. The ‚Äúaction‚Äù can include forwarding the packet to one or more 
output ports (as in destination-based forwarding), load balancing packets across 
multiple outgoing interfaces that lead to a service (as in load balancing), rewriting 
header values (as in NAT), purposefully blocking/dropping a packet (as in a fire-
wall), sending a packet to a special server for further processing and action (as in 
DPI), and more.
In generalized forwarding, a match-plus-action table generalizes the notion of 
the destination-based forwarding table that we encountered in Section 4.2.1. Because 
forwarding decisions may be made using network-layer and/or link-layer source 
and destination addresses, the forwarding devices shown in Figure 4.28 are more 
accurately described as ‚Äúpacket switches‚Äù rather than layer 3 ‚Äúrouters‚Äù or layer 2 
‚Äúswitches.‚Äù Thus, in the remainder of this section, and in Section 5.5, we‚Äôll refer 
to these devices as packet switches, adopting the terminology that is gaining wide-
spread adoption in SDN literature.
Figure 4.28 shows a match-plus-action table in each packet switch, with the 
table being computed, installed, and updated by a remote controller. We note that 
while it is possible for the control components at the individual packet switches to 
interact with each other (e.g., in a manner similar to that in Figure 4.2), in practice, 
generalized match-plus-action capabilities are implemented via a remote controller 
that computes, installs, and updates these tables. You might take a minute to compare 
Figures 4.2, 4.3, and 4.28‚Äîwhat similarities and differences do you notice between 
destination-based forwarding shown in Figures 4.2 and 4.3, and generalized forward-
ing shown in Figure 4.28?
Our following discussion of generalized forwarding will be based on Open-
Flow [McKeown 2008, ONF 2020, Casado 2014, Tourrilhes 2014]‚Äîa highly visible  
standard that has pioneered the notion of the match-plus-action forwarding abstrac-
tion and controllers, as well as the SDN revolution more generally [Feamster 2013]. 
We‚Äôll primarily consider OpenFlow 1.0, which introduced key SDN abstractions 
and functionality in a particularly clear and concise manner. Later versions of 
 OpenFlow introduced additional capabilities as a result of experience gained through

implementation and use; current and earlier versions of the OpenFlow standard can 
be found at [ONF 2020].
Each entry in the match-plus-action forwarding table, known as a flow table in 
OpenFlow, includes:
‚Ä¢ A set of header field values to which an incoming packet will be matched. As in 
the case of destination-based forwarding, hardware-based matching is most rap-
idly performed in TCAM memory, with more than a million destination address 
entries being possible [Bosshart 2013]. A packet that matches no flow table entry 
can be dropped or sent to the remote controller for more processing. In practice, 
a flow table may be implemented by multiple flow tables for performance or cost 
reasons [Bosshart 2013], but we‚Äôll focus here on the abstraction of a single flow 
table.
1101
0100
Remote Controller
Values in arriving
packet‚Äôs header
1
2
3
Local Ô¨Çow table
...
...
...
...
...
...
...
...
...
...
...
...
Headers Counters Actions
Control plane
Data plane
Figure 4.28 ‚ô¶  Generalized forwarding: Each packet switch contains a 
match-plus-action table that is computed and distributed  
by a remote controller

‚Ä¢ A set of counters that are updated as packets are matched to flow table entries. 
These counters might include the number of packets that have been matched by 
that table entry, and the time since the table entry was last updated.
‚Ä¢ A set of actions to be taken when a packet matches a flow table entry. These 
actions might be to forward the packet to a given output port, to drop the packet, 
makes copies of the packet and sent them to multiple output ports, and/or to 
rewrite selected header fields.
We‚Äôll explore matching and actions in more detail in Sections 4.4.1 and 4.4.2, 
respectively. We‚Äôll then study how the network-wide collection of per-packet switch 
matching rules can be used to implement a wide range of functions including routing, 
layer-2 switching, firewalling, load-balancing, virtual networks, and more in Sec-
tion¬†4.4.3. In closing, we note that the flow table is essentially an API, the abstrac-
tion through which an individual packet switch‚Äôs behavior can be programmed; 
we‚Äôll see in Section 4.4.3 that network-wide behaviors can similarly be programmed 
by appropriately programming/configuring these tables in a collection of network 
packet switches [Casado 2014].
4.4.1 Match
Figure 4.29 shows the 11 packet-header fields and the incoming port ID that can 
be matched in an OpenFlow 1.0 match-plus-action rule. Recall from Section 1.5.2 
that a link-layer (layer 2) frame arriving to a packet switch will contain a net-
work-layer (layer 3) datagram as its payload, which in turn will typically con-
tain a transport-layer (layer 4) segment. The first observation we make is that 
OpenFlow‚Äôs match abstraction allows for a match to be made on selected fields 
from three layers of protocol headers (thus rather brazenly defying the layer-
ing principle we studied in Section 1.5). Since we‚Äôve not yet covered the link 
layer, suffice it to say that the source and destination MAC addresses shown in 
Figure 4.29 are the link-layer addresses associated with the frame‚Äôs sending and 
receiving interfaces; by forwarding on the basis of Ethernet addresses rather than 
IP addresses, we can see that an OpenFlow-enabled device can equally perform 
Ingress
Port
Src
MAC
Dst
MAC
Eth
Type
VLAN
ID
VLAN
Pri
IP Src
IP Dst
IP
Proto
IP
TOS
TCP/UDP
Src Port
TCP/UDP
Dst Port
Transport layer
Network layer
Link layer
Figure 4.29 ‚ô¶ Packet matching fields, OpenFlow 1.0 flow table

as a router (layer-3 device) forwarding datagrams as well as a switch (layer-2 
device) forwarding frames. The Ethernet type field corresponds to the upper layer 
protocol (e.g., IP) to which the frame‚Äôs payload will be de-multiplexed, and the 
VLAN fields are concerned with so-called virtual local area networks that we‚Äôll 
study in Chapter 6. The set of 12 values that can be matched in the OpenFlow 
1.0 specification has grown to 41 values in more recent OpenFlow specifications 
[Bosshart 2014].
The ingress port refers to the input port at the packet switch on which a packet 
is received. The packet‚Äôs IP source address, IP destination address, IP protocol field, 
and IP type of service fields were discussed earlier in Section 4.3.1. The transport-layer 
source and destination port number fields can also be matched.
Flow table entries may also have wildcards. For example, an IP address of 
128.119.*.* in a flow table will match the corresponding address field of any data-
gram that has 128.119 as the first 16 bits of its address. Each flow table entry also has 
an associated priority. If a packet matches multiple flow table entries, the selected 
match and corresponding action will be that of the highest priority entry with which 
the packet matches.
Lastly, we observe that not all fields in an IP header can be matched. For exam-
ple OpenFlow does not allow matching on the basis of TTL field or datagram length 
field. Why are some fields allowed for matching, while others are not? Undoubtedly, 
the answer has to do with the tradeoff between functionality and complexity. The 
‚Äúart‚Äù in choosing an abstraction is to provide for enough functionality to accomplish 
a task (in this case to implement, configure, and manage a wide range of network-
layer functions that had previously been implemented through an assortment of 
 network-layer devices), without over-burdening the abstraction with so much detail 
and generality that it becomes bloated and unusable. Butler Lampson has famously 
noted [Lampson 1983]:
Do one thing at a time, and do it well. An interface should capture the minimum 
essentials of an abstraction. Don‚Äôt generalize; generalizations are generally 
wrong.
Given OpenFlow‚Äôs success, one can surmise that its designers indeed chose their 
abstraction well. Additional details of OpenFlow matching can be found in [ONF 
2020].
4.4.2 Action
As shown in Figure 4.28, each flow table entry has a list of zero or more actions 
that determine the processing that is to be applied to a packet that matches a flow 
table entry. If there are multiple actions, they are performed in the order specified 
in the list.

Among the most important possible actions are:
‚Ä¢ Forwarding. An incoming packet may be forwarded to a particular physical 
output port, broadcast over all ports (except the port on which it arrived) or 
multicast over a selected set of ports. The packet may be encapsulated and sent 
to the remote controller for this device. That controller then may (or may not) 
take some action on that packet, including installing new flow table entries, and 
may return the packet to the device for forwarding under the updated set of flow 
table rules.
‚Ä¢ Dropping. A flow table entry with no action indicates that a matched packet 
should be dropped.
‚Ä¢ Modify-field. The values in 10 packet-header fields (all layer 2, 3, and 4 fields 
shown in Figure 4.29 except the IP Protocol field) may be re-written before the 
packet is forwarded to the chosen output port.
4.4.3 OpenFlow Examples of Match-plus-action in Action
Having now considered both the match and action components of generalized 
forwarding, let‚Äôs put these ideas together in the context of the sample network 
shown in Figure 4.30. The network has 6 hosts (h1, h2, h3, h4, h5 and h6) and 
three packet switches (s1, s2 and s3), each with four local interfaces (numbered 
1 through 4). We‚Äôll consider a number of network-wide behaviors that we‚Äôd like 
to implement, and the flow table entries in s1, s2 and s3 needed to implement this 
behavior.
1
4
s3
s3
s1
s2
2
3
1
2
3
4
Host h6
10.3.0.6
OpenFlow controller
Host h5
10.3.0.5
Host h1
10.1.0.1
Host h2
10.1.0.2
Host h3
10.2.0.3
Host h4
10.2.0.4
1
4
2
3
Figure 4.30 ‚ô¶  OpenFlow match-plus-action network with three packet 
switches, 6 hosts, and an OpenFlow controller

A First Example: Simple Forwarding
As a very simple example, suppose that the desired forwarding behavior is that  
packets from h5 or h6 destined to h3 or h4 are to be forwarded from s3 to s1, and then 
from s1 to s2 (thus completely avoiding the use of the link between s3 and s2). The 
flow table entry in s1 would be:
s1 Flow Table (Example 1)
Match
Action
Ingress Port = 1 ; IP Src = 10.3.*.* ; IP Dst = 10.2.*.*
Forward(4)
‚Ä¶
‚Ä¶
Of course, we‚Äôll also need a flow table entry in s3 so that datagrams sent from 
h5 or h6 are forwarded to s1 over outgoing interface 3:
s3 Flow Table (Example 1)
Match
Action
IP Src = 10.3.*.* ; IP Dst = 10.2.*.*
Forward(3)
‚Ä¶
‚Ä¶
Lastly, we‚Äôll also need a flow table entry in s2 to complete this first example, so 
that datagrams arriving from s1 are forwarded to their destination, either host h3 or h4:
s2 Flow Table (Example 1)
Match
Action
Ingress port = 2 ; IP Dst = 10.2.0.3
Forward(3)
Ingress port = 2 ; IP Dst = 10.2.0.4
Forward(4)
‚Ä¶
‚Ä¶
A Second Example: Load Balancing
As a second example, let‚Äôs consider a load-balancing scenario, where datagrams from 
h3 destined to 10.1.*.* are to be forwarded over the direct link between s2 and s1, while 
datagrams from h4 destined to 10.1.*.* are to be forwarded over the link between s2 
and s3 (and then from s3 to s1). Note that this behavior couldn‚Äôt be achieved with IP‚Äôs 
destination-based forwarding. In this case, the flow table in s2 would be:

s2 Flow Table (Example 2)
Match
Action
Ingress port = 3; IP Dst = 10.1.*.*
Forward(2)
Ingress port = 4; IP Dst = 10.1.*.*
Forward(1)
‚Ä¶
‚Ä¶
Flow table entries are also needed at s1 to forward the datagrams received from 
s2 to either h1 or h2; and flow table entries are needed at s3 to forward datagrams 
received on interface 4 from s2 over interface 3 toward s1. See if you can figure out 
these flow table entries at s1 and s3.
A Third Example: Firewalling
As a third example, let‚Äôs consider a firewall scenario in which s2 wants only to 
receive (on any of its interfaces) traffic sent from hosts attached to s3.
s2 Flow Table (Example 3)
Match
Action
IP Src = 10.3.*.* IP Dst = 10.2.0.3
Forward(3)
IP Src = 10.3.*.* IP Dst = 10.2.0.4
Forward(4)
‚Ä¶
‚Ä¶
If there were no other entries in s2‚Äôs flow table, then only traffic from 10.3.*.* would 
be forwarded to the hosts attached to s2.
Although we‚Äôve only considered a few basic scenarios here, the versatility and 
advantages of generalized forwarding are hopefully apparent. In homework prob-
lems, we‚Äôll explore how flow tables can be used to create many different logical 
behaviors, including virtual networks‚Äîtwo or more logically separate networks 
(each with their own independent and distinct forwarding behavior)‚Äîthat use the 
same physical set of packet switches and links. In Section 5.5, we‚Äôll return to flow 
tables when we study the SDN controllers that compute and distribute the flow tables, 
and the protocol used for communicating between a packet switch and its controller.
The match-plus-action flow tables that we‚Äôve seen in this section are actually 
a limited form of programmability, specifying how a router should forward and 
manipulate (e.g., change a header field) a datagram, based on the match between 
the datagram‚Äôs header values and the matching conditions. One could imagine an 
even richer form of programmability‚Äîa programming language with higher-level 
constructs such as variables, general purpose arithmetic and Boolean operations, 
variables, functions, and conditional statements, as well as constructs specifically

designed for datagram processing at line rate. P4 (Programming Protocol-independent 
Packet Processors) [P4 2020] is such a language, and has gained considerable inter-
est and traction since its introduction five years ago [Bosshart 2014].
4.5 Middleboxes
Routers are the workhorses of the network layer, and in this chapter, we‚Äôve learned 
how they accomplish their ‚Äúbread and butter‚Äù job of forwarding IP datagrams toward 
their destination. But in this chapter, and in earlier chapters, we‚Äôve also encoun-
tered other network equipment (‚Äúboxes‚Äù) within the network that sit on the data path 
and perform functions other than forwarding. We encountered Web caches in Sec-
tion 2.2.5; TCP connection splitters in section 3.7; and network address translation 
(NAT), firewalls, and intrusion detection systems in Section 4.3.4. We learned in 
Section 4.4 that generalized forwarding allows a modern router to easily and natu-
rally perform firewalling and load balancing with generalized ‚Äúmatch plus action‚Äù 
operations.
In the past 20 years, we‚Äôve seen tremendous growth in such middleboxes, which 
RFC 3234 defines as:
‚Äúany intermediary box performing functions apart from normal, standard func-
tions of an IP router on the data path between a source host and destination 
host‚Äù
We can broadly identify three types of services performed by middleboxes:
‚Ä¢ NAT Translation. As we saw in Section 4.3.4, NAT boxes implement private 
network addressing, rewriting datagram header IP addresses and port numbers.
‚Ä¢ Security Services. Firewalls block traffic based on header-field values or redirect 
packets for additional processing, such as deep packet inspection (DPI). Intru-
sion Detection Systems (IDS) are able to detect predetermined patterns and filter 
packets accordingly. Application-level e-mail filters block e-mails considered to 
be junk, phishing or otherwise posing a security threat.
‚Ä¢ Performance Enhancement. These middleboxes perform services such as com-
pression, content caching, and load balancing of service requests (e.g., an HTTP 
request, or a search engine query) to one of a set of servers that can provide the 
desired service.
Many other middleboxes [RFC 3234] provide capabilities belonging to these three 
types of services, in both wired and wireless cellular [Wang 2011] networks.
With the proliferation of middleboxes comes the attendant need to operate, 
manage, and upgrade this equipment. Separate specialized hardware boxes, separate

software stacks, and separate management/operation skills translate to significant 
operational and capital costs. It is perhaps not surprising then that researchers are 
exploring the use of commodity hardware (networking, computing, and storage) with 
specialized software built on top of a common software stack‚Äîexactly the approach 
taken in SDN a decade earlier‚Äîto implement these services. This approach has 
become known as network function virtualization (NFV) [Mijumbi 2016]. An 
alternate approach that has also been explored is to outsource middlebox functional-
ity to the cloud [Sherry 2012].
For many years, the Internet architecture had a clear separation between the 
network layer and the transport/application layers. In these ‚Äúgood old days,‚Äù the 
network layer consisted of routers, operating within the network core, to forward 
datagrams toward their destinations using fields only in the IP datagram header. The 
transport and application layers were implemented in hosts operating at the network 
edge. Hosts exchanged packets among themselves in transport-layer segments and 
application-layer messages. Today‚Äôs middleboxes clearly violate this separation: a 
NAT box, sitting between a router and host, rewrites network-layer IP addresses and 
transport-layer port numbers; an in-network firewall blocks suspect datagrams using 
application-layer (e.g., HTTP), transport-layer, and network-layer header fields; 
e-mail security gateways are injected between the e-mail sender (whether malicious 
or not) and the intended e-mail receiver, filtering application-layer e-mail messages 
based on whitelisted/blacklisted IP addresses as well as e-mail message content. 
While there are those who have considered such middleboxes as a bit of an archi-
tectural abomination [Garfinkel 2003], others have adopted the philosophy that such 
middleboxes ‚Äúexist for important and permanent reasons‚Äù‚Äîthat they fill an important 
need‚Äîand that we‚Äôll have more, not fewer, middleboxes in the future [Walfish 2004]. 
See the section in attached sidebar on ‚ÄúThe end-to-end argument‚Äù for a slightly differ-
ent lens on the question of where to place service functionality in a network.
ARCHITECTURAL PRINCIPLES OF THE INTERNET
Given the phenomenal success of the Internet, one might naturally wonder about the 
architectural principles that have guided the development of what is arguably the larg-
est and most complex engineered system ever built by humankind. RFC 1958, entitled 
‚ÄúArchitectural Principles of the Internet,‚Äù suggests that these principles, if indeed they exist, 
are truly minimal:
‚ÄúMany members of the Internet community would argue that there is no architecture, 
but only a tradition, which was not written down for the first 25 years (or at least not 
by the IAB). However, in very general terms, the community believes that the goal is 
connectivity, the tool is the Internet Protocol, and the intelligence is end to end rather 
than hidden in the network.‚Äù [RFC 1958]
PRINCIPLES IN PRACTICE

So there we have it! The goal was to provide connectivity, there would be just one net-
work-layer protocol (the celebrated IP protocol we have studied in this chapter), and ‚Äúintelli-
gence‚Äù (one might say the ‚Äúcomplexity‚Äù) would be placed at the network edge, rather than 
in the network core. Let‚Äôs look these last two considerations in a bit more detail.
THE IP HOURGLASS
By now, we‚Äôre well acquainted with the five-layer Internet protocol stack that we first 
encountered in Figure 1.23. Another visualization of this stack, shown in Figure 4.31 and 
sometimes known as the ‚ÄúIP hourglass,‚Äù illustrates the ‚Äúnarrow waist‚Äù of the layered 
Internet architecture. While the Internet has many protocols in the physical, link, transport, 
and application layers, there is only one network layer protocol‚Äîthe IP protocol. This is 
the one protocol that must be implemented by each and every of the billions of Internet-
connected devices. This narrow waist has played a critical role in the phenomenal growth 
of the Internet. The relative simplicity of the IP protocol, and the fact that it is the only 
universal requirement for Internet connectivity has allowed a rich variety of networks‚Äîwith 
very different underlying link-layer technologies, from Ethernet to WiFi to cellular to optical 
networks to become part of the Internet. [Clark 1997] notes that role of the narrow waist, 
which he refers to as a ‚Äúspanning layer,‚Äù is to ‚Äú‚Ä¶¬†hide the detailed differences among 
these various [underlying] technologies and present a uniform service interface to the appli-
cations above.‚Äù For the IP layer in particular: ‚ÄúHow does the IP spanning layer achieve 
its purpose? It defines a basic set of services, which were carefully designed so that they 
could be constructed from a wide range of underlying network technologies. Software, as 
a part of the Internet [i.e., network] layer, translates what each of these lower-layer tech-
nologies offers into the common service of the Internet layer.‚Äù
For a discussion the narrow waist, including examples beyond the Internet, see [Beck 2019; 
Akhshabi 2011]. We note here that as the Internet architecture enters mid-life (certainly, 
IP
TCP UDP
HTTP
SMTP
QUIC
DASH
RTP ‚Ä¶
Ethernet PPP ‚Ä¶
WiFi Bluetooth
PDCP
copper
Ô¨Åber
radio
Figure 4.31 ‚ô¶ The narrow-waisted Internet hourglass

the Internet‚Äôs age of 40 to 50 years qualifies it for middle age!), one might observe that 
its ‚Äúnarrow waist‚Äù may indeed be widening a bit (as often happens in middle age!) via 
the rise of middleboxes.
THE END-TO-END ARGUMENT
The third principle in RFC 1958‚Äîthat ‚Äúintelligence is end to end rather than hidden in the 
network‚Äù‚Äîspeaks to the placement of functionality within the network. Here, we‚Äôve seen 
that until the recent rise of middleboxes, most Internet functionality was indeed placed at 
the network‚Äôs edge. It‚Äôs worth noting that, in direct contrast with the 20th century telephone 
network‚Äîwhich had ‚Äúdumb‚Äù (non-programmable) endpoints and smart switches‚Äîthe 
Internet has always had smart endpoints (programmable computers), enabling complex 
functionality to be placed at those endpoints. But a more principled argument for actually 
placing functionality at the endpoints was made in an extremely influential paper [Saltzer 
1984] that articulated the ‚Äúend-to-end argument.‚Äù It stated:
‚Äú¬†.¬†.¬†.¬†there is a list of functions each of which might be implemented in any of several 
ways: by the communication subsystem, by its client, as a joint venture, or perhaps 
redundantly, each doing its own version. In reasoning about this choice, the require-
ments of the application provide the basis for a class of arguments, which go as follows:
The function in question can completely and correctly be implemented only with 
the knowledge and help of the application standing at the end points of the com-
munication system. Therefore, providing that questioned function as a feature of the 
communication system itself is not possible. (Sometimes an incomplete version of the 
function provided by the communication system may be useful as a performance 
enhancement.)
We call this line of reasoning against low-level function implementation the ‚Äúend-to-end 
argument.‚Äù
An example illustrating the end-to-end argument is that of reliable data transfer. Since 
packets can be lost within the network (e.g., even without buffer overflows, a router hold-
ing a queued packet could crash, or a portion of the network in which a packet is queued 
becomes detached due to link failures), the endpoints (in this case via the TCP protocol) 
must perform error control. As we will see in Chapter 6, some link-layer protocols do 
indeed perform local error control, but this local error control alone is ‚Äúincomplete‚Äù and 
not sufficient to provide end-to-end reliable data transfer. And so reliable data transfer must 
be implemented end to end.
RFC 1958 deliberately includes only two references, both of which are ‚Äúfundamental 
papers on the Internet architecture.‚Äù One of these is the end-to-end paper itself [Saltzer 1984];  
the second paper [Clark 1988] discusses the design philosophy of the DARPA Internet Protocols. 
Both are interesting ‚Äúmust reads‚Äù for anyone interested in Internet architecture. Follow-ons to 
[Clark 1988] are [Blumenthal 2001; Clark 2005] which reconsider Internet architecture in light 
of the much more complex environment in which today‚Äôs Internet must now operate.

4.6 Summary
In this chapter, we‚Äôve covered the data plane functions of the network layer‚Äîthe per-
router functions that determine how packets arriving on one of a router‚Äôs input links are 
forwarded to one of that router‚Äôs output links. We began by taking a detailed look at the 
internal operations of a router, studying input and output port functionality and destination-
based forwarding, a router‚Äôs internal switching mechanism, packet queue management and 
more. We covered both traditional IP forwarding (where forwarding is based on a data-
gram‚Äôs destination address) and generalized forwarding (where forwarding and other func-
tions may be performed using values in several different fields in the datagram‚Äôs header) 
and seen the versatility of the latter approach. We also studied the IPv4 and IPv6 protocols 
in detail, and Internet addressing, which we found to be much deeper, subtler, and more 
interesting than we might have expected. We completed our study of the network-layer 
data plane with a study of middleboxes, and a broad  discussion of Internet architecture.
With our newfound understanding of the network-layer‚Äôs data plane, we‚Äôre now 
ready to dive into the network layer‚Äôs control plane in Chapter 5!
Homework Problems and Questions
Chapter 4 Review Questions
SECTION 4.1
 R1. Let‚Äôs review some of the terminology used in this textbook. Recall that the 
name of a transport-layer packet is segment and that the name of a link-layer 
packet is frame. What is the name of a network-layer packet? Recall that both 
routers and link-layer switches are called packet switches. What is the funda-
mental difference between a router and link-layer switch?
 R2. We noted that network layer functionality can be broadly divided into  
data plane functionality and control plane functionality. What are the main 
functions of the data plane? Of the control plane?
 R3. We made a distinction between the forwarding function and the routing func-
tion performed in the network layer. What are the key differences between 
routing and forwarding?
 R4. What is the role of the forwarding table within a router?
 R5. We said that a network layer‚Äôs service model ‚Äúdefines the characteristics of 
end-to-end transport of packets between sending and receiving hosts.‚Äù What is 
the service model of the Internet‚Äôs network layer? What guarantees are made by 
the Internet‚Äôs service model regarding the host-to-host delivery of datagrams?
SECTION 4.2
 R6. In Section 4.2, we saw that a router typically consists of input ports, output ports, 
a switching fabric and a routing processor. Which of these are implemented in

hardware and which are implemented in software? Why? Returning to the 
notion of the network layer‚Äôs data plane and control plane, which are imple-
mented in hardware and which are implemented in software? Why?
  R7. Discuss why each input port in a high-speed router stores a shadow copy of 
the forwarding table.
  R8. What is meant by destination-based forwarding? How does this differ from 
generalized forwarding (assuming you‚Äôve read Section 4.4, which of the two 
approaches are adopted by Software-Defined Networking)?
  R9. Suppose that an arriving packet matches two or more entries in a router‚Äôs 
forwarding table. With traditional destination-based forwarding, what rule 
does a router apply to determine which of these rules should be applied 
to determine the output port to which the arriving packet should be 
switched?
 R10. Three types of switching fabrics are discussed in Section 4.2. List and briefly 
describe each type. Which, if any, can send multiple packets across the fabric 
in parallel?
 R11. Describe how packet loss can occur at input ports. Describe how packet loss 
at input ports can be eliminated (without using infinite buffers).
 R12. Describe how packet loss can occur at output ports. Can this loss be pre-
vented by increasing the switch fabric speed?
 R13. What is HOL blocking? Does it occur in input ports or output ports?
 R14. In Section 4.2, we studied FIFO, Priority, Round Robin (RR), and Weighted 
Fair Queueing (WFQ) packet scheduling disciplines? Which of these queueing 
disciplines ensure that all packets depart in the order in which they arrived?
 R15. Give an example showing why a network operator might want one class of 
packets to be given priority over another class of packets.
 R16. What is an essential different between RR and WFQ packet scheduling? Is 
there a case (Hint: Consider the WFQ weights) where RR and WFQ will 
behave exactly the same?
SECTION 4.3
 R17. Suppose Host A sends Host B a TCP segment encapsulated in an IP data-
gram. When Host B receives the datagram, how does the network layer in 
Host B know it should pass the segment (that is, the payload of the datagram) 
to TCP rather than to UDP or to some other upper-layer protocol?
 R18. What field in the IP header can be used to ensure that a packet is forwarded 
through no more than N routers?
 R19. Recall that we saw the Internet checksum being used in both transport-layer 
segment (in UDP and TCP headers, Figures 3.7 and 3.29 respectively) and in 
network-layer datagrams (IP header, Figure 4.17). Now consider a transport

layer segment encapsulated in an IP datagram. Are the checksums in the seg-
ment header and datagram header computed over any common bytes in the IP 
datagram? Explain your answer.
 R20. When a large datagram is fragmented into multiple smaller datagrams, where 
are these smaller datagrams reassembled into a single larger datagram?
 R21. Do routers have IP addresses? If so, how many?
 R22. What is the 32-bit binary equivalent of the IP address 223.1.3.27?
 R23. Visit a host that uses DHCP to obtain its IP address, network mask, default 
router, and IP address of its local DNS server. List these values.
 R24. Suppose there are three routers between a source host and a destination host. 
Ignoring fragmentation, an IP datagram sent from the source host to the desti-
nation host will travel over how many interfaces? How many forwarding tables 
will be indexed to move the datagram from the source to the  destination?
 R25. Suppose an application generates chunks of 40 bytes of data every 20 msec, 
and each chunk gets encapsulated in a TCP segment and then an IP datagram. 
What percentage of each datagram will be overhead, and what percentage 
will be application data?
 R26. Suppose you purchase a wireless router and connect it to your cable modem. 
Also suppose that your ISP dynamically assigns your connected device (that 
is, your wireless router) one IP address. Also suppose that you have five PCs 
at home that use 802.11 to wirelessly connect to your wireless router. How 
are IP addresses assigned to the five PCs? Does the wireless router use NAT? 
Why or why not?
 R27. What is meant by the term ‚Äúroute aggregation‚Äù? Why is it useful for a router 
to perform route aggregation?
 R28. What is meant by a ‚Äúplug-and-play‚Äù or ‚Äúzeroconf‚Äù protocol?
 R29. What is a private network address? Should a datagram with a private network 
address ever be present in the larger public Internet? Explain.
 R30. Compare and contrast the IPv4 and the IPv6 header fields. Do they have any 
fields in common?
 R31. It has been said that when IPv6 tunnels through IPv4 routers, IPv6 treats the 
IPv4 tunnels as link-layer protocols. Do you agree with this statement? Why 
or why not?
SECTION 4.4
 R32. How does generalized forwarding differ from destination-based  forwarding?
 R33. What is the difference between a forwarding table that we encountered in 
destination-based forwarding in Section 4.1 and OpenFlow‚Äôs flow table that 
we encountered in Section 4.4?

R34. What is meant by the ‚Äúmatch plus action‚Äù operation of a router or switch? In 
the case of destination-based forwarding packet switch, what is matched and 
what is the action taken? In the case of an SDN, name three fields that can be 
matched, and three actions that can be taken. 
 R35. Name three header fields in an IP datagram that can be ‚Äúmatched‚Äù in Open-
Flow 1.0 generalized forwarding. What are three IP datagram header fields 
that cannot be ‚Äúmatched‚Äù in OpenFlow?
Problems
 P1. Consider the network below.
a. Show the forwarding table in router A, such that all traffic destined to host 
H3 is forwarded through interface 3.
b. Can you write down a forwarding table in router A, such that all traffic 
from H1 destined to host H3 is forwarded through interface 3, while all 
traffic from H2 destined to host H3 is forwarded through interface 4? 
(Hint: This is a trick question.)
B
A
1
3
2
4
2
D
1
2
3
H3
H1
H2
1
1
2
C
 P2. Suppose two packets arrive to two different input ports of a router at exactly 
the same time. Also suppose there are no other packets anywhere in the 
router.
a. Suppose the two packets are to be forwarded to two different output ports. 
Is it possible to forward the two packets through the switch fabric at the 
same time when the fabric uses a shared bus?
b. Suppose the two packets are to be forwarded to two different output ports. 
Is it possible to forward the two packets through the switch fabric at the 
same time when the fabric uses switching via memory?
c. Suppose the two packets are to be forwarded to the same output port. Is it 
possible to forward the two packets through the switch fabric at the same 
time when the fabric uses a crossbar?

P3. In Section 4.2.4, it was said that if R_switch  is N  times faster than R_line, 
then only negligible queuing will occur at the input ports, even if all the 
packets are to be forwarded to the same output port. Now suppose that 
R_switch = R_line, but all packets are to be forwarded to different output 
ports. Let D be the time to transmit a packet. As a function of D, what is the 
maximum input queuing delay for a packet for the (a) memory, (b) bus, and 
(c) crossbar switching fabrics?
 P4. Consider the switch shown below. Suppose that all datagrams have the same 
fixed length, that the switch operates in a slotted, synchronous manner, and 
that in one time slot a datagram can be transferred from an input port to an 
output port. The switch fabric is a crossbar so that at most one datagram can 
be transferred to a given output port in a time slot, but different output ports 
can receive datagrams from different input ports in a single time slot. What is 
the minimal number of time slots needed to transfer the packets shown from 
input ports to their output ports, assuming any input queue scheduling order 
you want (i.e., it need not have HOL blocking)? What is the largest number 
of slots needed, assuming the worst-case scheduling order you can devise, 
assuming that a non-empty input queue is never idle?
X Y
Switch
fabric
Output port X
Output port Y
Output port Z
X
Y
Z
 P5. Suppose that the WEQ scheduling policy is applied to a buffer that supports 
three classes, and suppose the weights are 0.5, 0.25, and 0.25 for the three 
classes.
a. Suppose that each class has a large number of packets in the buffer. 
In what sequence might the three classes be served in order to achieve 
the WFQ weights? (For round robin scheduling, a natural sequence is 
123123123¬†.¬†.¬†.).
b. Suppose that classes 1 and 2 have a large number of packets in the buffer, 
and there are no class 3 packets in the buffer. In what sequence might the 
three classes be served in to achieve the WFQ weights?

P6. Consider the figure below. Answer the following questions:
Time
Arrivals
Departures
Packet
in service
Time
1
1
6
10
2
8
9
3
4
5
7
11
12
1
t = 0
t = 2
t = 4
t = 6
t = 8
t = 10
t = 12
t = 14
a. Assuming FIFO service, indicate the time at which packets 2 through 
12 each leave the queue. For each packet, what is the delay between its 
arrival and the beginning of the slot in which it is transmitted? What is the 
average of this delay over all 12 packets?
b. Now assume a priority service, and assume that odd-numbered packets 
are high priority, and even-numbered packets are low priority. Indicate the 
time at which packets 2 through 12 each leave the queue. For each packet, 
what is the delay between its arrival and the beginning of the slot in which 
it is transmitted? What is the average of this delay over all 12 packets?
c. Now assume round robin service. Assume that packets 1, 2, 3, 6, 11, and 
12 are from class 1, and packets 4, 5, 7, 8, 9, and 10 are from class 2. 
Indicate the time at which packets 2 through 12 each leave the queue. For 
each packet, what is the delay between its arrival and its departure? What 
is the average delay over all 12 packets?
d. Now assume weighted fair queueing (WFQ) service. Assume that odd-
numbered packets are from class 1, and even-numbered packets are from 
class 2. Class 1 has a WFQ weight of 2, while class 2 has a WFQ weight 
of 1. Note that it may not be possible to achieve an idealized WFQ sched-
ule as described in the text, so indicate why you have chosen the particu-
lar packet to go into service at each time slot. For each packet what is the 
delay between its arrival and its departure? What is the average delay over 
all 12 packets?
e. What do you notice about the average delay in all four cases (FIFO, RR, 
priority, and WFQ)?

P7. Consider again the figure for P6.
a. Assume a priority service, with packets 1, 4, 5, 6, and 11 being high-
priority packets. The remaining packets are low priority. Indicate the slots 
in which packets 2 through 12 each leave the queue.
b. Now suppose that round robin service is used, with packets 1, 4, 5, 6, and 
11 belonging to one class of traffic, and the remaining packets belonging 
to the second class of traffic. Indicate the slots in which packets 2 through 
12 each leave the queue.
c. Now suppose that WFQ service is used, with packets 1, 4, 5, 6, and 11 
belonging to one class of traffic, and the remaining packets belonging to the 
second class of traffic. Class 1 has a WFQ weight of 1, while class 2 has a 
WFQ weight of 2 (note that these weights are different than in the previous 
question). Indicate the slots in which packets 2 through 12 each leave the 
queue. See also the caveat in the question above regarding WFQ service.
 P8. Consider a datagram network using 32-bit host addresses. Suppose a router 
has four links, numbered 0 through 3, and packets are to be forwarded to the 
link interfaces as follows:
 
Destination Address Range 
Link Interface
 
11100000 00000000 00000000 00000000
 
through 
0
 
11100000 00111111 11111111 11111111
 
11100000 01000000 00000000 00000000
 
through 
1
 
11100000 01000000 11111111 11111111
 
11100000 01000001 00000000 00000000
 
through 
2
 
11100001 01111111 11111111 11111111
 
otherwise 
3
a. Provide a forwarding table that has five entries, uses longest prefix match-
ing, and forwards packets to the correct link interfaces.
b. Describe how your forwarding table determines the appropriate link inter-
face for datagrams with destination addresses:
 
11001000 10010001 01010001 01010101
 
11100001 01000000 11000011 00111100
 
11100001 10000000 00010001 01110111

P9. Consider a datagram network using 8-bit host addresses. Suppose a router 
uses longest prefix matching and has the following forwarding table:
Prefix Match
Interface
00
0
010
1
011
2
10
2
11
3
 
 For each of the four interfaces, give the associated range of destination host 
addresses and the number of addresses in the range.
 P10. Consider a datagram network using 8-bit host addresses. Suppose a router 
uses longest prefix matching and has the following forwarding table:
Prefix Match
Interface
1
0
10
1
111
2
otherwise
3
 
 For each of the four interfaces, give the associated range of destination host 
addresses and the number of addresses in the range.
 P11. Consider a router that interconnects three subnets: Subnet 1, Subnet 2, 
and Subnet 3. Suppose all of the interfaces in each of these three subnets 
are required to have the prefix 223.1.17/24. Also suppose that Subnet 1 is 
required to support at least 60 interfaces, Subnet 2 is to support at least 90 
interfaces, and Subnet 3 is to support at least 12 interfaces. Provide three 
network addresses (of the form a.b.c.d/x) that satisfy these constraints.
 P12. In Section 4.2.2, an example forwarding table (using longest prefix matching) 
is given. Rewrite this forwarding table using the a.b.c.d/x notation instead of 
the binary string notation.
 P13. In Problem P8, you are asked to provide a forwarding table (using longest 
prefix matching). Rewrite this forwarding table using the a.b.c.d/x notation 
instead of the binary string notation.
 P14. Consider a subnet with prefix 128.119.40.128/26. Give an example of one  
IP address (of form xxx.xxx.xxx.xxx) that can be assigned to this network.

Suppose an ISP owns the block of addresses of the form 128.119.40.64/26. 
Suppose it wants to create four subnets from this block, with each block 
having the same number of IP addresses. What are the prefixes (of form 
a.b.c.d/x) for the four subnets?
 P15. Consider the topology shown in Figure 4.20. Denote the three subnets with 
hosts (starting clockwise at 12:00) as Networks A, B, and C. Denote the  
subnets without hosts as Networks D, E, and F.
a. Assign network addresses to each of these six subnets, with the following 
constraints: All addresses must be allocated from 214.97.254/23; Subnet A 
should have enough addresses to support 250 interfaces; Subnet B should 
have enough addresses to support 120 interfaces; and Subnet C should 
have enough addresses to support 120 interfaces. Of course, subnets D, E 
and F should each be able to support two interfaces. For each subnet, the 
assignment should take the form a.b.c.d/x or a.b.c.d/x ‚Äì e.f.g.h/y.
b. Using your answer to part (a), provide the forwarding tables (using long-
est prefix matching) for each of the three routers.
 P16. Use the whois service at the American Registry for Internet Numbers  
(http://www.arin.net/whois) to determine the IP address blocks for three 
universities. Can the whois services be used to determine with certainty the 
geographical location of a specific IP address? Use www.maxmind.com to 
determine the locations of the Web servers at each of these universities.
 P17. Suppose datagrams are limited to 1,500 bytes (including header) between 
source Host A and destination Host B. Assuming a 20-byte IP header, how 
many datagrams would be required to send an MP3 consisting of 5 million 
bytes? Explain how you computed your answer.
 P18. Consider the network setup in Figure 4.25. Suppose that the ISP instead 
assigns the router the address 24.34.112.235 and that the network address  
of the home network is 192.168.1/24.
a. Assign addresses to all interfaces in the home network.
b. Suppose each host has two ongoing TCP connections, all to port 80 at 
host 128.119.40.86. Provide the six corresponding entries in the NAT 
translation table.
 P19. Suppose you are interested in detecting the number of hosts behind a NAT. 
You observe that the IP layer stamps an identification number sequentially on 
each IP packet. The identification number of the first IP packet generated by 
a host is a random number, and the identification numbers of the subsequent 
IP packets are sequentially assigned. Assume all IP packets generated by 
hosts behind the NAT are sent to the outside world.
a. Based on this observation, and assuming you can sniff all packets sent by 
the NAT to the outside, can you outline a simple technique that detects the 
number of unique hosts behind a NAT? Justify your answer.

b. If the identification numbers are not sequentially assigned but randomly 
assigned, would your technique work? Justify your answer.
 P20. In this problem, we‚Äôll explore the impact of NATs on P2P applications. 
Suppose a peer with username Arnold discovers through querying that a 
peer with username Bernard has a file it wants to download. Also suppose 
that Bernard and Arnold are both behind a NAT. Try to devise a technique 
that will allow Arnold to establish a TCP connection with Bernard without 
application-specific NAT configuration. If you have difficulty devising such 
a technique, discuss why.
 P21. Consider the SDN OpenFlow network shown in Figure 4.30. Suppose 
that the desired forwarding behavior for datagrams arriving at s2 is as 
follows:
‚Ä¢ any datagrams arriving on input port 1 from hosts h5 or h6 that are des-
tined to hosts h1 or h2 should be forwarded over output port 2;
‚Ä¢ any datagrams arriving on input port 2 from hosts h1 or h2 that are des-
tined to hosts h5 or h6 should be forwarded over output port 1;
‚Ä¢ any arriving datagrams on input ports 1 or 2 and destined to hosts h3 or h4 
should be delivered to the host specified;
‚Ä¢ hosts h3 and h4 should be able to send datagrams to each other.
Specify the flow table entries in s2 that implement this forwarding behavior.
 P22. Consider again the SDN OpenFlow network shown in Figure 4.30. Suppose 
that the desired forwarding behavior for datagrams arriving from hosts h3 or 
h4 at s2 is as follows:
‚Ä¢ any datagrams arriving from host h3 and destined for h1, h2, h5 or h6 
should be forwarded in a clockwise direction in the network;
‚Ä¢ any datagrams arriving from host h4 and destined for h1, h2, h5  
or h6 should be forwarded in a counter-clockwise direction in the 
network.
Specify the flow table entries in s2 that implement this forwarding behavior.
 P23. Consider again the scenario from P21 above. Give the flow tables entries at 
packet switches s1 and s3, such that any arriving datagrams with a source 
address of h3 or h4 are routed to the destination hosts specified in the desti-
nation address field in the IP datagram. (Hint: Your forwarding table rules 
should include the cases that an arriving datagram is destined for a directly 
attached host or should be forwarded to a neighboring router for eventual 
host delivery there.)
 P24. Consider again the SDN OpenFlow network shown in Figure 4.30. Suppose 
we want switch s2 to function as a firewall. Specify the flow table in s2 that 
implements the following firewall behaviors (specify a different flow table 
for each of the four firewalling behaviors below) for delivery of datagrams

destined to h3 and h4. You do not need to specify the forwarding behavior in 
s2 that forwards traffic to other routers.
‚Ä¢ Only traffic arriving from hosts h1 and h6 should be delivered to hosts h3 
or h4 (i.e., that arriving traffic from hosts h2 and h5 is blocked).
‚Ä¢ Only TCP traffic is allowed to be delivered to hosts h3 or h4 (i.e., that 
UDP traffic is blocked).
‚Ä¢ Only traffic destined to h3 is to be delivered (i.e., all traffic to h4 is 
blocked).
‚Ä¢ Only UDP traffic from h1 and destined to h3 is to be delivered. All other 
traffic is blocked.
 P25. Consider the Internet protocol stack in Figures 1.23 and 4.31.  Would you 
consider the ICMP protocol to be a network-layer protocol or a transport-
layer protocol?  Justify your answer.
Wireshark Lab: IP
In the Web site for this textbook, www.pearsonhighered.com/cs-resources, you‚Äôll 
find a Wireshark lab assignment that examines the operation of the IP protocol, and 
the IP datagram format in particular.

What brought you to specialize in networking?
I was working as a programmer at UCLA in the late 1960s. My job was supported by the 
US Defense Advanced Research Projects Agency (called ARPA then and DARPA now). I 
was working in the laboratory of Professor Leonard Kleinrock in the Network Measurement 
Center of the newly created ARPANet. The first node of the ARPANet was installed at 
UCLA on September 1, 1969. I was responsible for programming a computer that was 
used to capture performance information about the ARPANet and to report this information 
back for comparison with mathematical models and predictions of the performance of the 
network.
Several of the other graduate students and I were made responsible for working on 
the so-called host-level protocols of the ARPAnet‚Äîthe procedures and formats that would 
allow many different kinds of computers on the network to interact with each other. It 
was a fascinating exploration into a new world (for me) of distributed computing and 
communication.
Did you imagine that IP would become  as pervasive as it is today when you first 
designed the protocol?
When Bob Kahn and I first worked on this in 1973, I think we were mostly very focused on 
the central question: How can we make heterogeneous packet networks interoperate with 
one another, assuming we cannot actually change the networks themselves? We hoped that 
we could find a way to permit an arbitrary collection of packet-switched networks to be 
interconnected in a transparent fashion, so that host computers could communicate end-to-
end without having to do any translations in between. I think we knew that we were dealing 
Vinton G. Cerf has served as Vice President and Chief Internet 
Evangelist for Google since 2005.  He served for over 15 years 
at MCI in various positions, ending up his tenure there as Senior 
Vice President for Technology Strategy. He is widely known as 
the co-designer of the TCP/IP protocols and the architecture of the 
Internet. During his time from 1976 to 1982 at the US Department 
of Defense Advanced Research Projects Agency (DARPA), he played 
a key role leading the development of Internet and Internet-related  
packet communication and security techniques. He received the US 
Presidential Medal of Freedom in 2005 and the US National Medal 
of Technology in 1997. He holds a BS in Mathematics from Stanford 
University and an MS and PhD in computer science from UCLA.
Vinton G. Cerf
AN INTERVIEW WITH‚Ä¶
Courtesy of Vinton G. Cerf

world would be like with billions of computers all interlinked on the Internet.
What do you now envision for the future of networking and the Internet? What major 
challenges/obstacles do you think lie ahead in their development?
I believe the Internet itself and networks in general will continue to proliferate. There are 
already billions of Internet-enabled devices on the Internet, including appliances like cell 
phones, refrigerators, personal digital assistants, home servers, televisions, as well as the 
usual array of laptops, servers, and so on. Big challenges include support for mobility, bat-
tery life, capacity of the access links to the network, and ability to scale  the optical core of 
the network in an unlimited fashion. The interplanetary extension of the Internet is a project 
that is well underway at NASA and other space agencies. We still need to add IPv6 [128-
bit] addressing to the original IPv4 [32-bit addresses] packet format. The list is long!
Who has inspired you professionally?
My colleague Bob Kahn; my thesis advisor, Gerald Estrin; my best friend, Steve Crocker 
(we met in high school and he introduced me to computers in 1960!); and the thousands of 
engineers who continue to evolve the Internet today.
Do you have any advice for students  entering  the networking/Internet  field?
Think outside the limitations of existing systems‚Äîimagine what might be possible; but then 
do the hard work of figuring out how to get there from the current state of affairs. Dare to 
dream. The ‚ÄúInternet of Things‚Äù is the next big phase of Internet expansion. Safety, security, 
privacy, reliability, and autonomy all need attention. The interplanetary extension of the 
terrestrial Internet started as a speculative design but is becoming a reality. It may take 
decades to implement this, mission by mission, but to paraphrase: ‚ÄúA man‚Äôs reach should 
exceed his grasp, or what are the heavens for?‚Äù
376

control-plane component of the network layer‚Äîthe network-wide logic that con-
trols not only how a datagram is routed along an end-to-end path from the source 
host to the destination host, but also how network-layer components and services are 
configured and managed. In Section 5.2, we‚Äôll cover traditional routing algorithms 
for computing least cost paths in a graph; these algorithms are the basis for two 
widely deployed Internet routing protocols: OSPF and BGP, that we‚Äôll cover in Sec-
tions 5.3 and 5.4, respectively. As we‚Äôll see, OSPF is a routing protocol that operates 
within a single ISP‚Äôs network. BGP is a routing protocol that serves to interconnect 
all of the networks in the Internet; BGP is thus often referred to as the ‚Äúglue‚Äù that 
holds the Internet together. Traditionally, control-plane routing protocols have been 
implemented together with data-plane forwarding functions, monolithically, within a 
router. As we learned in the introduction to Chapter 4, software-defined networking 
(SDN) makes a clear separation between the data and control planes, implementing 
control-plane functions in a separate ‚Äúcontroller‚Äù service that is distinct, and remote, 
from the forwarding components of the routers it controls. We‚Äôll cover SDN control-
lers in Section 5.5.
In Sections 5.6 and 5.7, we‚Äôll cover some of the nuts and bolts of managing an 
IP network: ICMP (the Internet Control Message Protocol) and SNMP (the Simple 
Network Management Protocol).
The Network 
Layer: Control 
Plane
5
CHAPTER 
377

5.1 Introduction
Let‚Äôs quickly set the context for our study of the network control plane by recall-
ing Figures 4.2 and 4.3. There, we saw that the forwarding table (in the case of 
 destination-based forwarding) and the flow table (in the case of generalized forward-
ing) were the principal elements that linked the network layer‚Äôs data and control 
planes. We learned that these tables specify the local data-plane forwarding behavior 
of a router. We saw that in the case of generalized forwarding, the actions taken 
could include not only forwarding a packet to a router‚Äôs output port, but also drop-
ping a packet, replicating a packet, and/or rewriting layer 2, 3 or 4 packet-header 
fields.
In this chapter, we‚Äôll study how those forwarding and flow tables are computed, 
maintained and installed. In our introduction to the network layer in Section 4.1, we 
learned that there are two possible approaches for doing so.
‚Ä¢ Per-router control. Figure 5.1 illustrates the case where a routing algorithm runs 
in each and every router; both a forwarding and a routing function are contained 
Control plane
Data plane
Routing
Algorithm
Forwarding
Table
Figure 5.1 ‚ô¶  Per-router control: Individual routing algorithm components 
interact in the control plane

within each router. Each router has a routing component that communicates with 
the routing components in other routers to compute the values for its forwarding 
table. This per-router control approach has been used in the Internet for decades. 
The OSPF and BGP protocols that we‚Äôll study in Sections 5.3 and 5.4 are based 
on this per-router approach to control.
‚Ä¢ Logically centralized control. Figure 5.2 illustrates the case in which a logically 
centralized controller computes and distributes the forwarding tables to be used 
by each and every router. As we saw in Sections 4.4 and 4.5, the generalized 
match-plus-action abstraction allows the router to perform traditional IP forward-
ing as well as a rich set of other functions (load sharing, firewalling, and NAT) 
that had been previously implemented in separate middleboxes.
Logically centralized routing controller
Control plane
Data plane
Control
Agent (CA)
CA
CA
CA
CA
Figure 5.2 ‚ô¶  Logically centralized control: A distinct, typically remote,  
controller interacts with local control agents (CAs)

The controller interacts with a control agent (CA) in each of the routers via a 
well-defined protocol to configure and manage that router‚Äôs flow table. Typically, 
the CA has minimum functionality; its job is to communicate with the controller, 
and to do as the controller commands. Unlike the routing algorithms in Figure 5.1, 
the CAs do not directly interact with each other nor do they actively take part 
in computing the forwarding table. This is a key distinction between per-router 
control and logically centralized control.
By ‚Äúlogically centralized‚Äù control [Levin 2012] we mean that the routing 
control service is accessed as if it were a single central service point, even though 
the service is likely to be implemented via multiple servers for fault-tolerance, 
and performance scalability reasons. As we will see in Section 5.5, SDN adopts 
this notion of a logically centralized controller‚Äîan approach that is finding 
increased use in production deployments. Google uses SDN to control the rout-
ers in its internal B4 global wide-area network that interconnects its data centers  
[Jain 2013]. SWAN [Hong 2013], from Microsoft Research, uses a logically 
 centralized controller to manage routing and forwarding between a wide area 
network and a data center network. Major ISP deployments, including COM-
CAST‚Äôs ActiveCore and Deutsche Telecom‚Äôs Access 4.0 are actively integrating 
SDN into their networks. And as we‚Äôll see in Chapter 8, SDN control is central to  
4G/5G cellular networking as well. [AT&T 2019] notes, ‚Äú ‚Ä¶ SDN, isn‚Äôt a vision, 
a goal, or a promise. It‚Äôs a reality. By the end of next year, 75% of our network 
functions will be fully virtualized and software-controlled.‚Äù China Telecom and 
China Unicom are using SDN both within data centers and between data centers 
[Li 2015].
5.2 Routing Algorithms
In this section, we‚Äôll study routing algorithms, whose goal is to determine good 
paths (equivalently, routes), from senders to receivers, through the network of 
routers. Typically, a ‚Äúgood‚Äù path is one that has the least cost. We‚Äôll see that in 
practice, however, real-world concerns such as policy issues (for example, a rule 
such as ‚Äúrouter x, belonging to organization Y, should not forward any packets 
originating from the network owned by organization Z ‚Äù) also come into play. We 
note that whether the network control plane adopts a per-router control approach or 
a logically centralized approach, there must always be a well-defined sequence of 
routers that a packet will cross in traveling from sending to receiving host. Thus, 
the routing algorithms that compute these paths are of fundamental importance, 
and another candidate for our top-10 list of fundamentally important networking 
concepts.
A graph is used to formulate routing problems. Recall that a graph G = (N, E) 
is a set N of nodes and a collection E of edges, where each edge is a pair of nodes 
from N. In the context of network-layer routing, the nodes in the graph represent

routers‚Äîthe points at which packet-forwarding decisions are made‚Äîand the edges 
connecting these nodes represent the physical links between these routers. Such a 
graph abstraction of a computer network is shown in Figure 5.3. When we study the 
BGP inter-domain routing protocol, we‚Äôll see that nodes represent networks, and the 
edge connecting two such nodes represents direction connectivity (know as peering) 
between the two networks. To view some graphs representing real network maps, see 
[CAIDA 2020]; for a discussion of how well different graph-based models model the 
Internet, see [Zegura 1997, Faloutsos 1999, Li 2004].
As shown in Figure 5.3, an edge also has a value representing its cost. Typically, 
an edge‚Äôs cost may reflect the physical length of the corresponding link (for example, 
a transoceanic link might have a higher cost than a short-haul terrestrial link), the link 
speed, or the monetary cost associated with a link. For our purposes, we‚Äôll simply 
take the edge costs as a given and won‚Äôt worry about how they are determined. For 
any edge (x, y) in E, we denote c(x, y) as the cost of the edge between nodes x and y. 
If the pair (x, y) does not belong to E, we set c(x, y) = ‚àû. Also, we‚Äôll only consider 
undirected graphs (i.e., graphs whose edges do not have a direction) in our discussion 
here, so that edge (x, y) is the same as edge (y, x) and that c(x, y) = c(y, x); however, 
the algorithms we‚Äôll study can be easily extended to the case of directed links with a 
different cost in each direction. Also, a node y is said to be a neighbor of node x if 
(x, y) belongs to E.
Given that costs are assigned to the various edges in the graph abstraction, 
a natural goal of a routing algorithm is to identify the least costly paths between 
sources and destinations. To make this problem more precise, recall that a path 
in a graph G = (N, E) is a sequence of nodes (x1, x2, g, xp) such that each 
of the pairs (x1, x2), (x2, x3), g, (xp-1, xp) are edges in E. The cost of a path 
(x1, x2, g, xp) is simply the sum of all the edge costs along the path, that is, 
x
y
v
3
5
2
5
2
3
1
1
2
1
u
z
w
Figure 5.3 ‚ô¶ Abstract graph model of a computer network

c(x1, x2) + c(x2, x3) + g+ c(xp-1, xp). Given any two nodes x and y, there are typi-
cally many paths between the two nodes, with each path having a cost. One or more 
of these paths is a least-cost path. The least-cost problem is therefore clear: Find a 
path between the source and destination that has least cost. In Figure 5.3, for exam-
ple, the least-cost path between source node u and destination node w is (u, x, y, w) 
with a path cost of 3. Note that if all edges in the graph have the same cost, the least-
cost path is also the shortest path (that is, the path with the smallest number of links 
between the source and the destination).
As a simple exercise, try finding the least-cost path from node u to z in 
Figure 5.3 and reflect for a moment on how you calculated that path. If you are 
like most people, you found the path from u to z by examining Figure 5.3, tracing 
a few routes from u to z, and somehow convincing yourself that the path you had 
chosen had the least cost among all possible paths. (Did you check all of the 17 pos-
sible paths between u and z? Probably not!) Such a calculation is an example of a 
centralized routing algorithm‚Äîthe routing algorithm was run in one location, your 
brain, with complete information about the network. Broadly, one way in which 
we can classify routing algorithms is according to whether they are centralized or 
decentralized.
‚Ä¢ A centralized routing algorithm computes the least-cost path between a source 
and destination using complete, global knowledge about the network. That is, the 
algorithm takes the connectivity between all nodes and all link costs as inputs. 
This then requires that the algorithm somehow obtain this information before 
actually performing the calculation. The calculation itself can be run at one site 
(e.g., a logically centralized controller as in Figure 5.2) or could be replicated in 
the routing component of each and every router (e.g., as in Figure 5.1). The key 
distinguishing feature here, however, is that the algorithm has complete informa-
tion about connectivity and link costs. Algorithms with global state information 
are often referred to as link-state (LS) algorithms, since the algorithm must 
be aware of the cost of each link in the network. We‚Äôll study LS algorithms in  
Section 5.2.1.
‚Ä¢ In a decentralized routing algorithm, the calculation of the least-cost path is 
carried out in an iterative, distributed manner by the routers. No node has com-
plete information about the costs of all network links. Instead, each node begins 
with only the knowledge of the costs of its own directly attached links. Then, 
through an iterative process of calculation and exchange of information with its 
neighboring nodes, a node gradually calculates the least-cost path to a destination 
or set of destinations. The decentralized routing algorithm we‚Äôll study below in  
Section 5.2.2 is called a distance-vector (DV) algorithm, because each node main-
tains a vector of estimates of the costs (distances) to all other nodes in the net-
work. Such decentralized algorithms, with interactive message exchange between

neighboring routers is perhaps more naturally suited to control planes where the 
routers interact directly with each other, as in Figure 5.1.
A second broad way to classify routing algorithms is according to whether they 
are static or dynamic. In static routing algorithms, routes change very slowly over 
time, often as a result of human intervention (for example, a human manually editing 
a link costs). Dynamic routing algorithms change the routing paths as the network 
traffic loads or topology change. A dynamic algorithm can be run either periodically 
or in direct response to topology or link cost changes. While dynamic algorithms 
are more responsive to network changes, they are also more susceptible to problems 
such as routing loops and route oscillation.
A third way to classify routing algorithms is according to whether they are load-
sensitive or load-insensitive. In a load-sensitive algorithm, link costs vary dynami-
cally to reflect the current level of congestion in the underlying link. If a high cost 
is associated with a link that is currently congested, a routing algorithm will tend 
to choose routes around such a congested link. While early ARPAnet routing algo-
rithms were load-sensitive [McQuillan 1980], a number of difficulties were encoun-
tered [Huitema 1998]. Today‚Äôs Internet routing algorithms (such as RIP, OSPF, and 
BGP) are load-insensitive, as a link‚Äôs cost does not explicitly reflect its current (or 
recent past) level of congestion.
5.2.1 The Link-State (LS) Routing Algorithm
Recall that in a link-state algorithm, the network topology and all link costs are 
known, that is, available as input to the LS algorithm. In practice, this is accom-
plished by having each node broadcast link-state packets to all other nodes in 
the network, with each link-state packet containing the identities and costs of 
its attached links. In practice (for example, with the Internet‚Äôs OSPF routing 
protocol, discussed in Section 5.3), this is often accomplished by a link-state 
broadcast algorithm  [Perlman 1999]. The result of the nodes‚Äô broadcast is that 
all nodes have an identical and complete view of the network. Each node can 
then run the LS algorithm and compute the same set of least-cost paths as every 
other node.
The link-state routing algorithm we present below is known as Dijkstra‚Äôs 
algorithm, named after its inventor. A closely related algorithm is Prim‚Äôs algo-
rithm; see [Cormen 2001] for a general discussion of graph algorithms. Dijkstra‚Äôs 
algorithm computes the least-cost path from one node (the source, which we will 
refer to as u) to all other nodes in the network. Dijkstra‚Äôs algorithm is iterative and 
has the property that after the kth iteration of the algorithm, the least-cost paths 
are known to k destination nodes, and among the least-cost paths to all destination

nodes, these k paths will have the k smallest costs. Let us define the following 
notation:
‚Ä¢ D(v): cost of the least-cost path from the source node to destination v as of this 
iteration of the algorithm.
‚Ä¢ p(v): previous node (neighbor of v) along the current least-cost path from the 
source to v.
‚Ä¢ N‚Ä≤: subset of nodes; v is in N‚Ä≤ if the least-cost path from the source to v is defini-
tively known.
The centralized routing algorithm consists of an initialization step followed by 
a loop. The number of times the loop is executed is equal to the number of nodes in 
the network. Upon termination, the algorithm will have calculated the shortest paths 
from the source node u to every other node in the network.
Link-State (LS) Algorithm for Source Node u
1  Initialization: 
2   N‚Äô = {u}
3   for all nodes v
4     if v is a neighbor of u
5       then D(v) = c(u,v)
6     else D(v) = ‚àû
7
8  Loop
9   Ô¨Ånd w not in N‚Äô such that D(w) is a minimum
10  add w to N‚Äô
11  update D(v) for each neighbor v of w and not in N‚Äô:
12        D(v) = min(D(v), D(w)+ c(w,v) )
13   /* new cost to v is either old cost to v or known
14    least path cost to w plus cost from w to v */
15 until N‚Äô= N
As an example, let‚Äôs consider the network in Figure 5.3 and compute the least-
cost paths from u to all possible destinations. A tabular summary of the algorithm‚Äôs 
computation is shown in Table 5.1, where each line in the table gives the values of 
the algorithm‚Äôs variables at the end of the iteration. Let‚Äôs consider the few first steps 
in detail.
‚Ä¢ In the initialization step, the currently known least-cost paths from u to its directly 
attached neighbors, v, x, and w, are initialized to 2, 1, and 5, respectively. Note in

particular that the cost to w is set to 5 (even though we will soon see that a lesser-cost  
path does indeed exist) since this is the cost of the direct (one hop) link from u to 
w. The costs to y and z are set to infinity because they are not directly connected 
to u.
‚Ä¢ In the first iteration, we look among those nodes not yet added to the set N‚Ä≤ and 
find that node with the least cost as of the end of the previous iteration. That node 
is x, with a cost of 1, and thus x is added to the set N‚Ä≤. Line 12 of the LS algorithm 
is then performed to update D(v) for all nodes v, yielding the results shown in the 
second line (Step 1) in Table 5.1. The cost of the path to v is unchanged. The cost 
of the path to w (which was 5 at the end of the initialization) through node x is 
found to have a cost of 4. Hence this lower-cost path is selected and w‚Äôs predeces-
sor along the shortest path from u is set to x. Similarly, the cost to y (through x) is 
computed to be 2, and the table is updated accordingly.
‚Ä¢ In the second iteration, nodes v and y are found to have the least-cost paths (2), 
and we break the tie arbitrarily and add y to the set N‚Ä≤ so that N‚Ä≤ now contains u, 
x, and y. The cost to the remaining nodes not yet in N‚Ä≤, that is, nodes v, w, and z, 
are updated via line 12 of the LS algorithm, yielding the results shown in the third 
row in Table 5.1.
‚Ä¢ And so on . . . 
When the LS algorithm terminates, we have, for each node, its predecessor 
along the least-cost path from the source node. For each predecessor, we also have its 
predecessor, and so in this manner we can construct the entire path from the source to 
all destinations. The forwarding table in a node, say node u, can then be constructed 
from this information by storing, for each destination, the next-hop node on the least-
cost path from u to the destination. Figure 5.4 shows the resulting least-cost paths 
and forwarding table in u for the network in Figure 5.3.
Table 5.1 ‚ô¶ Running the link-state algorithm on the network in Figure 5.3
step
N‚Äô
D (v), p (v)
D (w), p (w)
D (x), p (x)
D (y), p (y)
D (z), p (z)
0
u
2, u
5, u
1,u
‚àû
‚àû
1
ux
2, u
4, x
2, x
‚àû
2
uxy
2, u
3, y
4, y
3
uxyv
3, y
4, y
4
uxyvw
4, y
5
uxyvwz

What is the computational complexity of this algorithm? That is, given n nodes 
(not counting the source), how much computation must be done in the worst case to 
find the least-cost paths from the source to all destinations? In the first iteration, we 
need to search through all n nodes to determine the node, w, not in N‚Ä≤ that has the 
minimum cost. In the second iteration, we need to check n - 1 nodes to determine 
the minimum cost; in the third iteration n - 2 nodes, and so on. Overall, the total 
number of nodes we need to search through over all the iterations is n(n + 1)/2, and 
thus we say that the preceding implementation of the LS algorithm has worst-case 
complexity of order n squared: O(n2). (A more sophisticated implementation of this 
algorithm, using a data structure known as a heap, can find the minimum in line 9 in 
logarithmic rather than linear time, thus reducing the complexity.)
Before completing our discussion of the LS algorithm, let us consider a pathol-
ogy that can arise. Figure 5.5 shows a simple network topology where link costs are 
equal to the load carried on the link, for example, reflecting the delay that would 
be experienced. In this example, link costs are not symmetric; that is, c(u,v) equals 
c(v,u) only if the load carried on both directions on the link (u,v) is the same. In this 
example, node z originates a unit of traffic destined for w, node x also originates a 
unit of traffic destined for w, and node y injects an amount of traffic equal to e, also 
destined for w. The initial routing is shown in Figure 5.5(a) with the link costs cor-
responding to the amount of traffic carried.
When the LS algorithm is next run, node y determines (based on the link costs 
shown in Figure 5.5(a)) that the clockwise path to w has a cost of 1, while the coun-
terclockwise path to w (which it had been using) has a cost of 1 + e. Hence y‚Äôs least-
cost path to w is now clockwise. Similarly, x determines that its new least-cost path to 
w is also clockwise, resulting in costs shown in Figure 5.5(b). When the LS algorithm 
is run next, nodes x, y, and z all detect a zero-cost path to w in the counterclockwise 
direction, and all route their traffic to the counterclockwise routes. The next time the 
LS algorithm is run, x, y, and z all then route their traffic to the clockwise routes.
What can be done to prevent such oscillations (which can occur in any algo-
rithm, not just an LS algorithm, that uses a congestion or delay-based link metric)? 
One solution would be to mandate that link costs not depend on the amount of traffic 
Destination 
Link
v
w
x
y
z
(u, v)
(u, x)
(u, x)
(u, x)
(u, x)
x
y
v
u
z
w
Figure 5.4 ‚ô¶ Least cost path and forwarding table for node u

carried‚Äîan unacceptable solution since one goal of routing is to avoid highly con-
gested (for example, high-delay) links. Another solution is to ensure that not all rout-
ers run the LS algorithm at the same time. This seems a more reasonable solution, 
since we would hope that even if routers ran the LS algorithm with the same perio-
dicity, the execution instance of the algorithm would not be the same at each node. 
Interestingly, researchers have found that routers in the Internet can self-synchronize 
among themselves [Floyd Synchronization 1994]. That is, even though they initially 
execute the algorithm with the same period but at different instants of time, the algo-
rithm execution instance can eventually become, and remain, synchronized at the 
routers. One way to avoid such self-synchronization is for each router to randomize 
the time it sends out a link advertisement.
Having studied the LS algorithm, let‚Äôs consider the other major routing algo-
rithm that is used in practice today‚Äîthe distance-vector routing algorithm.
w
y
z
x
1
0
0
0
e
1 + e
1
a.  Initial routing
1
e
w
y
z
x
2 + e
1 + e
1
0
0
0
b.  x, y detect better path
     to w, clockwise
w
y
z
x
0
0
0
1
1 + e
2+ e
c.  x, y, z detect better path
     to w, counterclockwise
w
y
z
x
2 + e
1 + e
1
0
0
0
d.  x, y, z, detect better path
     to w, clockwise
1
1
e
1
1
e
1
1
e
Figure 5.5 ‚ô¶ Oscillations with congestion-sensitive routing

5.2.2 The Distance-Vector (DV) Routing Algorithm
Whereas the LS algorithm is an algorithm using global information, the distance-
vector (DV) algorithm is iterative, asynchronous, and distributed. It is distributed in 
that each node receives some information from one or more of its directly attached 
neighbors, performs a calculation, and then distributes the results of its calculation 
back to its neighbors. It is iterative in that this process continues on until no more 
information is exchanged between neighbors. (Interestingly, the algorithm is also 
self-terminating‚Äîthere is no signal that the computation should stop; it just stops.) 
The algorithm is asynchronous in that it does not require all of the nodes to operate in 
lockstep with each other. We‚Äôll see that an asynchronous, iterative, self-terminating, 
distributed algorithm is much more interesting and fun than a centralized algorithm!
Before we present the DV algorithm, it will prove beneficial to discuss an impor-
tant relationship that exists among the costs of the least-cost paths. Let dx(y) be the 
cost of the least-cost path from node x to node y. Then the least costs are related by 
the celebrated Bellman-Ford equation, namely,
 
dx(y) = minv5c(x, v) + dv( y)6, 
(5.1)
where the minv in the equation is taken over all of x‚Äôs neighbors. The Bellman-
Ford equation is rather intuitive. Indeed, after traveling from x to v, if we then take 
the least-cost path from v to y, the path cost will be c(x, v) + dv(y). Since we must 
begin by traveling to some neighbor v, the least cost from x to y is the minimum of 
c(x, v) + dv(y) taken over all neighbors v.
But for those who might be skeptical about the validity of the equation, let‚Äôs 
check it for source node u and destination node z in Figure 5.3. The source node u 
has three neighbors: nodes v, x, and w. By walking along various paths in the graph, 
it is easy to see that dv(z) = 5, dx(z) = 3, and dw(z) = 3. Plugging these values into 
Equation 5.1, along with the costs c(u, v) = 2, c(u, x) = 1, and c(u, w) = 5, gives 
du(z) = min52 + 5, 5 + 3, 1 + 36 = 4, which is obviously true and which is 
exactly what the Dijskstra algorithm gave us for the same network. This quick veri-
fication should help relieve any skepticism you may have.
The Bellman-Ford equation is not just an intellectual curiosity. It actually has signif-
icant practical importance: the solution to the Bellman-Ford equation provides the entries 
in node x‚Äôs forwarding table. To see this, let v* be any neighboring node that achieves 
the minimum in Equation 5.1. Then, if node x wants to send a packet to node y along a 
least-cost path, it should first forward the packet to node v*. Thus, node x‚Äôs forwarding 
table would specify node v* as the next-hop router for the ultimate destination y. Another 
important practical contribution of the Bellman-Ford equation is that it suggests the form 
of the neighbor-to-neighbor communication that will take place in the DV algorithm.
The basic idea is as follows. Each node x begins with Dx(y), an estimate of the cost 
of the least-cost path from itself to node y, for all nodes, y, in N. Let Dx = [Dx(y): y in N] 
be node x‚Äôs distance vector, which is the vector of cost estimates from x to all other nodes, 
y, in N. With the DV algorithm, each node x maintains the following routing information:

‚Ä¢ For each neighbor v, the cost c(x,v) from x to directly attached neighbor, v
‚Ä¢ Node x‚Äôs distance vector, that is, Dx = [Dx(y): y in N], containing x‚Äôs estimate of 
its cost to all destinations, y, in N
‚Ä¢ The distance vectors of each of its neighbors, that is, Dv = [Dv(y): y in N] for 
each neighbor v of x
In the distributed, asynchronous algorithm, from time to time, each node sends a 
copy of its distance vector to each of its neighbors. When a node x receives a new 
distance vector from any of its neighbors w, it saves w‚Äôs distance vector, and then 
uses the Bellman-Ford equation to update its own distance vector as follows:
Dx(y) = minv5c(x, v) + Dv(y)6 for each node y in N
If node x‚Äôs distance vector has changed as a result of this update step, node x will then 
send its updated distance vector to each of its neighbors, which can in turn update 
their own distance vectors. Miraculously enough, as long as all the nodes continue 
to exchange their distance vectors in an asynchronous fashion, each cost estimate 
Dx(y) converges to dx(y), the actual cost of the least-cost path from node x to node y 
[Bertsekas 1991]!
Distance-Vector (DV) Algorithm
At each node, x:
1  Initialization:
2    for all destinations y in N:
3       Dx(y)= c(x,y)/* if y is not a neighbor then c(x,y)= ‚àû */
4    for each neighbor w
5       Dw(y) = ? for all destinations y in N
6    for each neighbor w
7       send distance vector  Dx = [Dx(y): y in N] to w
8
9  loop 
10    wait  (until I see a link cost change to some neighbor w or
11            until I receive a distance vector from some neighbor w)
12
13    for each y in N:
14        Dx(y) = minv{c(x,v) + Dv(y)}
15
16 if Dx(y) changed for any destination y
17       send distance vector Dx  = [Dx(y): y in N] to all neighbors
18
19 forever

In the DV algorithm, a node x updates its distance-vector estimate when it either 
sees a cost change in one of its directly attached links or receives a distance-vector 
update from some neighbor. But to update its own forwarding table for a given des-
tination y, what node x really needs to know is not the shortest-path distance to y but 
instead the neighboring node v*(y) that is the next-hop router along the shortest path 
to y. As you might expect, the next-hop router v*(y) is the neighbor v that achieves 
the minimum in Line 14 of the DV algorithm. (If there are multiple neighbors v that 
achieve the minimum, then v*(y) can be any of the minimizing neighbors.) Thus, 
in Lines 13‚Äì14, for each destination y, node x also determines v*(y) and updates its 
forwarding table for destination y.
Recall that the LS algorithm is a centralized algorithm in the sense that it 
requires each node to first obtain a complete map of the network before running the 
Dijkstra algorithm. The DV algorithm is decentralized and does not use such global 
information. Indeed, the only information a node will have is the costs of the links 
to its directly attached neighbors and information it receives from these neighbors. 
Each node waits for an update from any neighbor (Lines 10‚Äì11), calculates its new 
distance vector when receiving an update (Line 14), and distributes its new distance 
vector to its neighbors (Lines 16‚Äì17). DV-like algorithms are used in many routing 
protocols in practice, including the Internet‚Äôs RIP and BGP, ISO IDRP, Novell IPX, 
and the original ARPAnet.
Figure 5.6 illustrates the operation of the DV algorithm for the simple three-
node network shown at the top of the figure. The operation of the algorithm is illus-
trated in a synchronous manner, where all nodes simultaneously receive distance 
vectors from their neighbors, compute their new distance vectors, and inform their 
neighbors if their distance vectors have changed. After studying this example, you 
should convince yourself that the algorithm operates correctly in an asynchronous 
manner as well, with node computations and update generation/reception occurring 
at any time.
The leftmost column of the figure displays three initial routing tables for each 
of the three nodes. For example, the table in the upper-left corner is node x‚Äôs ini-
tial routing table. Within a specific routing table, each row is a distance vector‚Äî 
specifically, each node‚Äôs routing table includes its own distance vector and that 
of each of its neighbors. Thus, the first row in node x‚Äôs initial routing table is 
Dx = [Dx(x), Dx(y), Dx(z)] = [0, 2, 7]. The second and third rows in this table are 
the most recently received distance vectors from nodes y and z, respectively. Because 
at initialization node x has not received anything from node y or z, the entries in  
the second and third rows are initialized to infinity.
After initialization, each node sends its distance vector to each of its two neigh-
bors. This is illustrated in Figure 5.6 by the arrows from the first column of tables 
to the second column of tables. For example, node x sends its distance vector Dx = 
[0, 2, 7] to both nodes y and z. After receiving the updates, each node recomputes its 
own distance vector. For example, node x computes

Dx(x) = 0
 Dx(y) = min5c(x,y) + Dy(y), c(x,z) + Dz(y)6 = min52 + 0, 7 + 16 = 2
 Dx(z) = min5c(x,y) + Dy(z), c(x,z) + Dz(z)6 = min52 + 1, 7 + 06 = 3
The second column therefore displays, for each node, the node‚Äôs new distance vector 
along with distance vectors just received from its neighbors. Note, for example, that 
Node y table
Node x table
0 2 7
x y z
` ` `
` ` `
Time
7
2
1
y
x
z
Node z table
from
cost to
x
y
z
0 2 3
x y z
2 0 1
7 1 0
from
cost to
x
y
z
0 2 3
x y z
2 0 1
3 1 0
from
cost to
x
y
z
2 0 1
x y z
` ` `
` ` `
from
cost to
x
y
z
0 2 7
x y z
2 0 1
7 1 0
from
cost to
x
y
z
0 2 3
x y z
2 0 1
3 1 0
from
cost to
x
y
z
7 1 0
x y z
` ` `
` ` `
from
cost to
x
y
z
0 2 7
x y z
2 0 1
3 1 0
from
cost to
x
y
z
0 2 3
x y z
2 0 1
3 1 0
from
cost to
x
y
z
Figure 5.6 ‚ô¶ Distance-vector (DV) algorithm in operation

node x‚Äôs estimate for the least cost to node z, Dx(z), has changed from 7 to 3. Also 
note that for node x, neighboring node y achieves the minimum in line 14 of the DV 
algorithm; thus, at this stage of the algorithm, we have at node x that v*(y) = y and 
v*(z) = y.
After the nodes recompute their distance vectors, they again send their updated 
distance vectors to their neighbors (if there has been a change). This is illustrated in 
Figure 5.6 by the arrows from the second column of tables to the third column of 
tables. Note that only nodes x and z send updates: node y‚Äôs distance vector didn‚Äôt 
change so node y doesn‚Äôt send an update. After receiving the updates, the nodes then 
recompute their distance vectors and update their routing tables, which are shown in 
the third column.
The process of receiving updated distance vectors from neighbors, recomputing 
routing table entries, and informing neighbors of changed costs of the least-cost path 
to a destination continues until no update messages are sent. At this point, since no 
update messages are sent, no further routing table calculations will occur and the 
algorithm will enter a quiescent state; that is, all nodes will be performing the wait in 
Lines 10‚Äì11 of the DV algorithm. The algorithm remains in the quiescent state until 
a link cost changes, as discussed next.
Distance-Vector Algorithm: Link-Cost Changes and Link Failure
When a node running the DV algorithm detects a change in the link cost from 
itself to a neighbor (Lines 10‚Äì11), it updates its distance vector (Lines 13‚Äì14) 
and, if there‚Äôs a change in the cost of the least-cost path, informs its neighbors 
(Lines¬†16‚Äì17) of its new distance vector. Figure 5.7(a) illustrates a scenario where 
the link cost from y to x changes from 4 to 1. We focus here only on y‚Äô and z‚Äôs 
distance table entries to destination x. The DV algorithm causes the following 
sequence of events to occur:
‚Ä¢ At time t0, y detects the link-cost change (the cost has changed from 4 to 1), 
updates its distance vector, and informs its neighbors of this change since its dis-
tance vector has changed.
‚Ä¢ At time t1, z receives the update from y and updates its table. It computes a new 
least cost to x (it has decreased from a cost of 5 to a cost of 2) and sends its new 
distance vector to its neighbors.
‚Ä¢ At time t2, y receives z‚Äôs update and updates its distance table. y‚Äôs least costs do 
not change and hence y does not send any message to z. The algorithm comes to 
a quiescent state.
Thus, only two iterations are required for the DV algorithm to reach a quiescent  
state. The good news about the decreased cost between x and y has propagated 
quickly through the network.

Let‚Äôs now consider what can happen when a link cost increases. Suppose that 
the link cost between x and y increases from 4 to 60, as shown in Figure 5.7(b).
 1. Before the link cost changes, Dy(x) = 4, Dy(z) = 1, Dz(y) = 1, and Dz(x) = 5. 
At time t0, y detects the link-cost change (the cost has changed from 4 to 60). y 
computes its new minimum-cost path to x to have a cost of
Dy(x) = min5c(y,x) + Dx(x), c(y,z) + Dz(x)6 = min560 + 0, 1 + 56 = 6
 
 Of course, with our global view of the network, we can see that this new cost via 
z is wrong. But the only information node y has is that its direct cost to x is 60 
and that z has last told y that z could get to x with a cost of 5. So in order to get 
to x, y would now route through z, fully expecting that z will be able to get to x 
with a cost of 5. As of t1 we have a routing loop‚Äîin order to get to x, y routes 
through z, and z routes through y. A routing loop is like a black hole‚Äîa packet 
destined for x arriving at y or z as of t1 will bounce back and forth between these 
two nodes forever (or until the forwarding tables are changed).
 2. Since node y has computed a new minimum cost to x, it informs z of its new 
distance vector at time t1.
 3. Sometime after t1, z receives y‚Äôs new distance vector, which indicates that y‚Äôs 
minimum cost to x is 6. z knows it can get to y with a cost of 1 and hence com-
putes a new least cost to x of Dz(x) = min550 + 0,1 + 66 = 7. Since z‚Äôs 
least cost to x has increased, it then informs y of its new distance vector at t2.
 4. In a similar manner, after receiving z‚Äôs new distance vector, y determines 
Dy(x) = 8 and sends z its distance vector. z then determines Dz(x) = 9 and 
sends y its distance vector, and so on.
How long will the process continue? You should convince yourself that the loop will 
persist for 44 iterations (message exchanges between y and z)‚Äîuntil z eventually 
computes the cost of its path via y to be greater than 50. At this point, z will (finally!) 
determine that its least-cost path to x is via its direct connection to x. y will then 
50
4
1
60
1
y
x
a.
b.
z
50
4
1
y
x
z
Figure 5.7 ‚ô¶ Changes in link cost

route to x via z. The result of the bad news about the increase in link cost has indeed  
traveled slowly! What would have happened if the link cost c(y, x) had changed from 
4 to 10,000 and the cost c(z, x) had been 9,999? Because of such scenarios, the prob-
lem we have seen is sometimes referred to as the count-to-infinity  problem.
Distance-Vector Algorithm: Adding Poisoned Reverse
The specific looping scenario just described can be avoided using a technique known 
as poisoned reverse. The idea is simple‚Äîif z routes through y to get to destination x, 
then z will advertise to y that its distance to x is infinity, that is, z will advertise to y 
that Dz(x) = ‚àû (even though z knows Dz(x) = 5 in truth). z will continue telling this 
little white lie to y as long as it routes to x via y. Since y believes that z has no path 
to x, y will never attempt to route to x via z, as long as z continues to route to x via y 
(and lies about doing so).
Let‚Äôs now see how poisoned reverse solves the particular looping problem we 
encountered before in Figure 5.5(b). As a result of the poisoned reverse, y‚Äôs distance 
table indicates Dz(x) = ‚àû. When the cost of the (x, y) link changes from 4 to 60 at 
time t0, y updates its table and continues to route directly to x, albeit at a higher cost 
of 60, and informs z of its new cost to x, that is, Dy(x) = 60. After receiving the 
update at t1, z immediately shifts its route to x to be via the direct (z, x) link at a cost 
of 50. Since this is a new least-cost path to x, and since the path no longer passes 
through y, z now informs y that Dz(x) = 50 at t2. After receiving the update from 
z, y updates its distance table with Dy(x) = 51. Also, since z is now on y‚Äôs least-
cost path to x, y poisons the reverse path from z to x by informing z at time t3 that 
Dy(x) = ‚àû (even though y knows that Dy(x) = 51 in truth).
Does poisoned reverse solve the general count-to-infinity problem? It does not. 
You should convince yourself that loops involving three or more nodes (rather than 
simply two immediately neighboring nodes) will not be detected by the poisoned 
reverse technique.
A Comparison of LS and DV Routing Algorithms
The DV and LS algorithms take complementary approaches toward computing rout-
ing. In the DV algorithm, each node talks to only its directly connected neighbors, 
but it provides its neighbors with least-cost estimates from itself to all the nodes (that 
it knows about) in the network. The LS algorithm requires global information. Con-
sequently, when implemented in each and every router, for example, as in Figures 4.2 
and 5.1, each node would need to communicate with all other nodes (via broadcast), 
but it tells them only the costs of its directly connected links. Let‚Äôs conclude our 
study of LS and DV algorithms with a quick comparison of some of their attributes. 
Recall that N is the set of nodes (routers) and E is the set of edges (links).
‚Ä¢ Message complexity. We have seen that LS requires each node to know the 
cost of each link in the network. This requires O(|N| |E|) messages to be sent.

Also, whenever a link cost changes, the new link cost must be sent to all nodes. 
The DV algorithm requires message exchanges between directly connected 
neighbors at each iteration. We have seen that the time needed for the algo-
rithm to converge can depend on many factors. When link costs change, the 
DV algorithm will propagate the results of the changed link cost only if the 
new link cost results in a changed least-cost path for one of the nodes attached 
to that link.
‚Ä¢ Speed of convergence. We have seen that our implementation of LS is an O(|N|2) 
algorithm requiring O(|N| |E|)) messages. The DV algorithm can converge slowly 
and can have routing loops while the algorithm is converging. DV also suffers 
from the count-to-infinity problem.
‚Ä¢ Robustness. What can happen if a router fails, misbehaves, or is sabotaged? 
Under LS, a router could broadcast an incorrect cost for one of its attached links 
(but no others). A node could also corrupt or drop any packets it received as part 
of an LS broadcast. But an LS node is computing only its own forwarding tables; 
other nodes are performing similar calculations for themselves. This means route 
calculations are somewhat separated under LS, providing a degree of robustness. 
Under DV, a node can advertise incorrect least-cost paths to any or all destina-
tions. (Indeed, in 1997, a malfunctioning router in a small ISP provided national 
backbone routers with erroneous routing information. This caused other routers 
to flood the malfunctioning router with traffic and caused large portions of the 
Internet to become disconnected for up to several hours [Neumann 1997].) More 
generally, we note that, at each iteration, a node‚Äôs calculation in DV is passed on 
to its neighbor and then indirectly to its neighbor‚Äôs neighbor on the next iteration. 
In this sense, an incorrect node calculation can be diffused through the entire 
network under DV.
In the end, neither algorithm is an obvious winner over the other; indeed, both algo-
rithms are used in the Internet.
5.3 Intra-AS Routing in the Internet: OSPF
In our study of routing algorithms so far, we‚Äôve viewed the network simply as a 
collection of interconnected routers. One router was indistinguishable from another 
in the sense that all routers executed the same routing algorithm to compute routing 
paths through the entire network. In practice, this model and its view of a homog-
enous set of routers all executing the same routing algorithm is simplistic for two 
important reasons:
‚Ä¢ Scale. As the number of routers becomes large, the overhead involved in communi-
cating, computing, and storing routing information becomes prohibitive. Today‚Äôs

Internet consists of hundreds of millions of routers. Storing routing information  
for possible destinations at each of these routers would clearly require enormous 
amounts of memory. The overhead required to broadcast connectivity and link 
cost updates among all of the routers would be huge! A distance-vector algorithm 
that iterated among such a large number of routers would surely never converge. 
Clearly, something must be done to reduce the complexity of route computation 
in a network as large as the Internet.
‚Ä¢ Administrative autonomy. As described in Section 1.3, the Internet is a network 
of ISPs, with each ISP consisting of its own network of routers. An ISP generally 
desires to operate its network as it pleases (for example, to run whatever rout-
ing algorithm it chooses within its network) or to hide aspects of its network‚Äôs 
internal organization from the outside. Ideally, an organization should be able to 
operate and administer its network as it wishes, while still being able to connect 
its network to other outside networks.
Both of these problems can be solved by organizing routers into autonomous 
 systems (ASs), with each AS consisting of a group of routers that are under the same 
administrative control. Often the routers in an ISP, and the links that interconnect 
them, constitute a single AS. Some ISPs, however, partition their network into multi-
ple ASs. In particular, some tier-1 ISPs use one gigantic AS for their entire network, 
whereas others break up their ISP into tens of interconnected ASs. An autonomous 
system is identified by its globally unique autonomous system number (ASN) [RFC 
1930]. AS numbers, like IP addresses, are assigned by ICANN regional registries 
[ICANN 2020].
Routers within the same AS all run the same routing algorithm and have infor-
mation about each other. The routing algorithm  running within an autonomous sys-
tem is called an intra-autonomous system routing  protocol.
Open Shortest Path First (OSPF) 
OSPF routing and its closely related cousin, IS-IS, are widely used for intra-AS 
routing in the Internet. The Open in OSPF indicates that the routing protocol speci-
fication is publicly available (for example, as opposed to Cisco‚Äôs EIGRP protocol, 
which was only recently became open [Savage 2015], after roughly 20 years as a 
Cisco-proprietary protocol). The most recent version of OSPF, version 2, is defined 
in [RFC 2328], a public document.
OSPF is a link-state protocol that uses flooding of link-state information 
and a Dijkstra‚Äôs least-cost path algorithm. With OSPF, each router constructs 
a complete topological map (that is, a graph) of the entire autonomous system. 
Each router then locally runs Dijkstra‚Äôs shortest-path algorithm to determine a 
shortest-path tree to all subnets, with itself as the root node. Individual link costs 
are configured by the network administrator (see sidebar, Principles and Practice:

Setting OSPF Weights). The administrator might choose to set all link costs to 1, 
thus achieving minimum-hop routing, or might choose to set the link weights to 
be inversely proportional to link capacity in order to discourage traffic from using 
low-bandwidth links. OSPF does not mandate a policy for how link weights are 
set (that is the job of the  network administrator), but instead provides the mecha-
nisms (protocol) for determining least-cost path routing for the given set of link 
weights.
With OSPF, a router broadcasts routing information to all other routers in the 
autonomous system, not just to its neighboring routers. A router broadcasts link-state 
information whenever there is a change in a link‚Äôs state (for example, a change in 
cost or a change in up/down status). It also broadcasts a link‚Äôs state periodically (at 
least once every 30 minutes), even if the link‚Äôs state has not changed. RFC 2328 
notes that ‚Äúthis periodic updating of link state advertisements adds robustness to the 
link state algorithm.‚Äù OSPF advertisements are contained in OSPF messages that are 
SETTING OSPF LINK WEIGHTS
Our discussion of link-state routing has implicitly assumed that link weights are set, a 
routing algorithm such as OSPF is run, and traffic flows according to the routing tables 
computed by the LS algorithm. In terms of cause and effect, the link weights are given (i.e., 
they come first) and result (via Dijkstra‚Äôs algorithm) in routing paths that minimize overall 
cost. In this viewpoint, link weights reflect the cost of using a link (for example, if link 
weights are inversely proportional to capacity, then the use of high-capacity links would 
have smaller weight and thus be more attractive from a routing standpoint) and Dijsktra‚Äôs 
algorithm serves to minimize overall cost.
In practice, the cause and effect relationship between link weights and routing paths 
may be reversed, with network operators configuring link weights in order to obtain rout-
ing paths that achieve certain traffic engineering goals [Fortz 2000, Fortz 2002]. For 
example, suppose a network operator has an estimate of traffic flow entering the network 
at each ingress point and destined for each egress point. The operator may then want 
to put in place a specific routing of ingress-to-egress flows that minimizes the maximum 
utilization over all of the network‚Äôs links. But with a routing algorithm such as OSPF, the 
operator‚Äôs main ‚Äúknobs‚Äù for tuning the routing of flows through the network are the link 
weights. Thus, in order to achieve the goal of minimizing the maximum link utilization, the 
operator must find the set of link weights that achieves this goal. This is a reversal of the 
cause and effect relationship‚Äîthe desired routing of flows is known, and the OSPF link 
weights must be found such that the OSPF routing algorithm results in this desired routing 
of flows.
PRINCIPLES IN PRACTICE

carried directly by IP, with an upper-layer protocol of 89 for OSPF. Thus, the OSPF 
protocol must itself implement functionality such as reliable message transfer and 
link-state broadcast. The OSPF protocol also checks that links are operational (via a 
HELLO message that is sent to an attached neighbor) and allows an OSPF router to 
obtain a neighboring router‚Äôs database of network-wide link state.
Some of the advances embodied in OSPF include the following:
‚Ä¢ Security. Exchanges between OSPF routers (for example, link-state updates) can 
be authenticated. With authentication, only trusted routers can participate in the 
OSPF protocol within an AS, thus preventing malicious intruders (or networking 
students taking their newfound knowledge out for a joyride) from injecting incor-
rect information into router tables. By default, OSPF packets between routers 
are¬†not authenticated and could be forged. Two types of authentication can be 
configured‚Äîsimple and MD5 (see Chapter 8 for a discussion on MD5 and 
authentication in general). With simple authentication, the same password is con-
figured on each router. When a router sends an OSPF packet, it includes the 
password in plaintext. Clearly, simple authentication is not very secure. MD5 
authentication is based on shared secret keys that are configured in all the routers. 
For each OSPF packet that it sends, the router computes the MD5 hash of the 
content of the OSPF packet appended with the secret key. (See the discussion of 
message authentication codes in Chapter 8.) Then the router includes the resulting 
hash value in the OSPF packet. The receiving router, using the preconfigured 
secret key, will compute an MD5 hash of the packet and compare it with the hash 
value that the packet carries, thus verifying the packet‚Äôs authenticity. Sequence 
numbers are also used with MD5 authentication to protect against replay attacks.
‚Ä¢ Multiple same-cost paths. When multiple paths to a destination have the same 
cost, OSPF allows multiple paths to be used (that is, a single path need not be 
chosen for carrying all traffic when multiple equal-cost paths exist).
‚Ä¢ Integrated support for unicast and multicast routing. Multicast OSPF (MOSPF) 
[RFC 1584] provides simple extensions to OSPF to provide for multicast routing. 
MOSPF uses the existing OSPF link database and adds a new type of link-state 
advertisement to the existing OSPF link-state broadcast mechanism.
‚Ä¢ Support for hierarchy within a single AS. An OSPF autonomous system can 
be configured hierarchically into areas. Each area runs its own OSPF link-state 
routing algorithm, with each router in an area broadcasting its link state to all 
other routers in that area. Within each area, one or more area border routers are 
responsible for routing packets outside the area. Lastly, exactly one OSPF area 
in the AS is configured to be the backbone area. The primary role of the back-
bone area is to route traffic between the other areas in the AS. The backbone 
always contains all area border routers in the AS and may contain non-border 
routers as well. Inter-area routing within the AS requires that the packet be first

routed to an area border router (intra-area routing), then routed through the back-
bone to the area border router that is in the destination area, and then routed to 
the final destination.
OSPF is a relatively complex protocol, and our coverage here has been necessar-
ily brief; [Huitema 1998; Moy 1998; RFC 2328] provide additional details.
5.4 Routing Among the ISPs: BGP
We just learned that OSPF is an example of an intra-AS routing protocol. When 
routing a packet between a source and destination within the same AS, the route 
the packet follows is entirely determined by the intra-AS routing protocol. How-
ever, to route a packet across multiple ASs, say from a smartphone in Timbuktu to 
a server in a datacenter in Silicon Valley, we need an inter-autonomous  system 
routing protocol. Since an inter-AS routing protocol involves coordination 
among multiple ASs, communicating ASs must run the same inter-AS routing 
protocol. In fact, in the Internet, all ASs run the same inter-AS routing protocol, 
called the Border Gateway Protocol, more commonly known as BGP [RFC 4271; 
Stewart 1999].
BGP is arguably the most important of all the Internet protocols (the only other 
contender would be the IP protocol that we studied in Section 4.3), as it is the pro-
tocol that glues the thousands of ISPs in the Internet together. As we will soon see, 
BGP is a decentralized and asynchronous protocol in the vein of distance-vector 
routing described in Section 5.2.2. Although BGP is a complex and challenging pro-
tocol, to understand the Internet on a deep level, we need to become familiar with 
its underpinnings and operation. The time we devote to learning BGP will be well 
worth the effort.
5.4.1 The Role of BGP
To understand the responsibilities of BGP, consider an AS and an arbitrary router 
in that AS. Recall that every router has a forwarding table, which plays the central 
role in the process of forwarding arriving packets to outbound router links. As we 
have learned, for destinations that are within the same AS, the entries in the router‚Äôs 
forwarding table are determined by the AS‚Äôs intra-AS routing protocol. But what 
about destinations that are outside of the AS? This is precisely where BGP comes to 
the rescue.
In BGP, packets are not routed to a specific destination address, but instead to 
CIDRized prefixes, with each prefix representing a subnet or a collection of subnets. 
Gluing the Internet 
Together: BGP
VideoNote

In the world of BGP, a destination may take the form 138.16.68/22, which for this 
example includes 1,024 IP addresses. Thus, a router‚Äôs forwarding table will have 
entries of the form (x, I), where x is a prefix (such as 138.16.68/22) and I is an inter-
face number for one of the router‚Äôs interfaces.
As an inter-AS routing protocol, BGP provides each router a means to:
 1. Obtain prefix reachability information from neighboring ASs. In particular, 
BGP allows each subnet to advertise its existence to the rest of the Internet. A 
subnet screams, ‚ÄúI exist and I am here,‚Äù and BGP makes sure that all the rout-
ers in the Internet know about this subnet. If it weren‚Äôt for BGP, each subnet 
would be an isolated island‚Äîalone, unknown and unreachable by the rest of the 
Internet.
 2. Determine the ‚Äúbest‚Äù routes to the prefixes. A router may learn about two or 
more different routes to a specific prefix. To determine the best route, the router 
will locally run a BGP route-selection procedure (using the prefix reachability 
information it obtained via neighboring routers). The best route will be deter-
mined based on policy as well as the reachability information.
Let us now delve into how BGP carries out these two tasks.
5.4.2 Advertising BGP Route Information
Consider the network shown in Figure 5.8. As we can see, this simple network has 
three autonomous systems: AS1, AS2, and AS3. As shown, AS3 includes a subnet 
with prefix x. For each AS, each router is either a gateway router or an internal 
router. A gateway router is a router on the edge of an AS that directly connects to 
one or more routers in other ASs. An internal router connects only to hosts and 
routers within its own AS. In AS1, for example, router 1c is a gateway router; routers 
1a, 1b, and 1d are internal routers.
Let‚Äôs consider the task of advertising reachability information for prefix x to 
all of the routers shown in Figure 5.8. At a high level, this is straightforward. First, 
AS3 sends a BGP message to AS2, saying that x exists and is in AS3; let‚Äôs denote 
this message as ‚ÄúAS3 x‚Äù. Then AS2 sends a BGP message to AS1, saying that x 
exists and that you can get to x by first passing through AS2 and then going to AS3; 
let‚Äôs denote that message as ‚ÄúAS2 AS3 x‚Äù. In this manner, each of the autonomous 
systems will not only learn about the existence of x, but also learn about a path of 
autonomous systems that leads to x. 
Although the discussion in the above paragraph about advertising BGP reacha-
bility information should get the general idea across, it is not precise in the sense that 
autonomous systems do not actually send messages to each other, but instead routers 
do. To understand this, let‚Äôs now re-examine the example in Figure 5.8. In BGP,

pairs of routers exchange routing information over semi-permanent TCP connections 
using port 179. Each such TCP connection, along with all the BGP messages sent 
over the connection, is called a BGP connection. Furthermore, a BGP connection 
that spans two ASs is called an external BGP (eBGP) connection, and a BGP ses-
sion between routers in the same AS is called an internal BGP (iBGP) connection. 
Examples of BGP connections for the network in Figure 5.8 are shown in Figure 5.9. 
There is typically one eBGP connection for each link that directly connects gateway 
routers in different ASs; thus, in Figure 5.9, there is an eBGP connection between 
gateway routers 1c and 2a and an eBGP connection between gateway routers 2c  
and 3a.
There are also iBGP connections between routers within each of the ASs. In 
particular, Figure 5.9 displays a common configuration of one BGP connection for 
each pair of routers internal to an AS, creating a mesh of TCP connections within 
each AS. In Figure 5.9, the eBGP connections are shown with the long dashes; the 
iBGP connections are shown with the short dashes. Note that iBGP connections do 
not always correspond to physical links.
In order to propagate the reachability information, both iBGP and eBGP 
sessions are used. Consider again advertising the reachability information for 
prefix x to all routers in AS1 and AS2. In this process, gateway router 3a first 
sends an eBGP message ‚ÄúAS3 x‚Äù to gateway router 2c. Gateway router 2c then 
sends the iBGP message ‚ÄúAS3 x‚Äù to all of the other routers in AS2, including 
to gateway router 2a. Gateway router 2a then sends the eBGP message ‚ÄúAS2 
AS3 x‚Äù to gateway router 1c. Finally, gateway router 1c uses iBGP to send the 
2b
2d
2a
2c
AS2
1b
1d
1a
1c
AS1
3b
3d
3a
3c
AS3
X
Figure 5.8 ‚ô¶  Network with three autonomous systems. AS3 includes a  
subnet with prefix x

message ‚ÄúAS2 AS3 x‚Äù to all the routers in AS1. After this process is complete, 
each router in AS1 and AS2 is aware of the existence of x and is also aware of 
an AS path that leads to x.
Of course, in a real network, from a given router there may be many different 
paths to a given destination, each through a different sequence of ASs. For example, 
consider the network in Figure 5.10, which is the original network in Figure 5.8, with 
an additional physical link from router 1d to router 3d. In this case, there are two 
paths from AS1 to x: the path ‚ÄúAS2 AS3 x‚Äù via router 1c; and the new path ‚ÄúAS3 x‚Äù 
via the router 1d.
5.4.3 Determining the Best Routes
As we have just learned, there may be many paths from a given router to a destina-
tion subnet. In fact, in the Internet, routers often receive reachability information 
about dozens of different possible paths. How does a router choose among these 
paths (and then configure its forwarding table accordingly)?
Before addressing this critical question, we need to introduce a little more 
BGP terminology. When a router advertises a prefix across a BGP connection, it 
includes with the prefix several BGP attributes. In BGP jargon, a prefix along with 
its attributes is called a route. Two of the more important attributes are AS-PATH 
and NEXT-HOP. The AS-PATH attribute contains the list of ASs through which the 
eBGP
Key:
iBGP
2b
2d
2a
2c
AS2
1b
1d
1a
1c
AS1
3b
3d
3a
3c
AS3
X
Figure 5.9 ‚ô¶ eBGP and iBGP connections

advertisement has passed, as we‚Äôve seen in our examples above. To generate the AS-
PATH value, when a prefix is passed to an AS, the AS adds its ASN to the existing 
list in the AS-PATH. For example, in Figure 5.10, there are two routes from AS1 
to subnet x: one which uses the AS-PATH ‚ÄúAS2 AS3‚Äù; and another that uses the 
AS-PATH ‚ÄúA3‚Äù. BGP routers also use the AS-PATH attribute to detect and prevent 
looping advertisements; specifically, if a router sees that its own AS is contained in 
the path list, it will reject the advertisement.
Providing the critical link between the inter-AS and intra-AS routing protocols, 
the NEXT-HOP attribute has a subtle but important use. The NEXT-HOP is the IP 
address of the router interface that begins the AS-PATH. To gain insight into this 
attribute, let‚Äôs again refer to Figure 5.10. As indicated in Figure 5.10, the NEXT-
HOP attribute for the route ‚ÄúAS2 AS3 x‚Äù from AS1 to x that passes through AS2 
is the IP address of the left interface on router 2a. The NEXT-HOP attribute for the 
route ‚ÄúAS3 x‚Äù from AS1 to x that bypasses AS2 is the IP address of the leftmost 
interface of router 3d. In summary, in this toy example, each router in AS1 becomes 
aware of two BGP routes to prefix x:
IP address of leftmost interface for router 2a; AS2 AS3; x
IP address of leftmost interface of router 3d; AS3; x
Here, each BGP route is written as a list with three components: NEXT-HOP; AS-
PATH; destination prefix. In practice, a BGP route includes additional attributes, 
which we will ignore for the time being. Note that the NEXT-HOP attribute is an IP 
NEXT-HOP
NEXT-HOP
2b
2d
2a
2c
AS2
1b
1d
1a
1c
AS1
3b
3d
3a
3c
AS3
X
Figure 5.10 ‚ô¶  Network augmented with peering link between AS1  
and AS3

address of a router that does not belong to AS1; however, the subnet that contains 
this IP address directly attaches to AS1.
Hot Potato Routing
We are now finally in position to talk about BGP routing algorithms in a precise 
manner. We will begin with one of the simplest routing algorithms, namely, hot 
potato routing.
Consider router 1b in the network in Figure 5.10. As just described, this router 
will learn about two possible BGP routes to prefix x. In hot potato routing, the route 
chosen (from among all possible routes) is that route with the least cost to the NEXT-
HOP router beginning that route. In this example, router 1b will consult its intra-AS 
routing information to find the least-cost intra-AS path to NEXT-HOP router 2a and 
the least-cost intra-AS path to NEXT-HOP router 3d, and then select the route with 
the smallest of these least-cost paths. For example, suppose that cost is defined as the 
number of links traversed. Then the least cost from router 1b to router 2a is 2, the least 
cost from router 1b to router 2d is 3, and router 2a would therefore be selected. Router 
1b would then consult its forwarding table (configured by its intra-AS algorithm) and 
find the interface I that is on the least-cost path to router 2a. It then adds (x, I) to its 
forwarding table.
The steps for adding an outside-AS prefix in a router‚Äôs forwarding table for hot 
potato routing are summarized in Figure 5.11. It is important to note that when add-
ing an outside-AS prefix into a forwarding table, both the inter-AS routing protocol 
(BGP) and the intra-AS routing protocol (e.g., OSPF) are used.
The idea behind hot-potato routing is for router 1b to get packets out of its 
AS as quickly as possible (more specifically, with the least cost possible) without 
worrying about the cost of the remaining portions of the path outside of its AS to 
the destination. In the name ‚Äúhot potato routing,‚Äù a packet is analogous to a hot 
potato that is burning in your hands. Because it is burning hot, you want to pass it 
off to another person (another AS) as quickly as possible. Hot potato routing is thus 
Learn from inter-AS
protocol that subnet
x is reachable via
multiple gateways.
Use routing info from
intra-AS protocol to
determine costs of
least-cost paths to
each of the gateways.
Hot potato routing:
Choose the gateway
that has the
smallest least cost.
Determine from
forwarding table the
interface I that leads
to least-cost gateway.
Enter (x,I) in
forwarding table.
Figure 5.11 ‚ô¶  Steps in adding outside-AS destination in a router‚Äôs 
 forwarding table

a selfish  algorithm‚Äîit tries to reduce the cost in its own AS while ignoring the other 
components of the end-to-end costs outside its AS. Note that with hot potato routing, 
two routers in the same AS may choose two different AS paths to the same prefix. 
For example, we just saw that router 1b would send packets through AS2 to reach  
x. However, router 1d would bypass AS2 and send packets directly to AS3 to reach x.
Route-Selection Algorithm
In practice, BGP uses an algorithm that is more complicated than hot potato routing, 
but nevertheless incorporates hot potato routing. For any given destination prefix, the 
input into BGP‚Äôs route-selection algorithm is the set of all routes to that prefix that have 
been learned and accepted by the router. If there is only one such route, then BGP obvi-
ously selects that route. If there are two or more routes to the same prefix, then BGP 
sequentially invokes the following elimination rules until one route remains:
 1. A route is assigned a local preference value as one of its attributes (in addition 
to the AS-PATH and NEXT-HOP attributes). The local preference of a route 
could have been set by the router or could have been learned from another router 
in the same AS. The value of the local preference attribute is a policy decision 
that is left entirely up to the AS‚Äôs network administrator. (We will shortly dis-
cuss BGP policy issues in some detail.) The routes with the highest local prefer-
ence values are selected.
 2. From the remaining routes (all with the same highest local preference value), 
the route with the shortest AS-PATH is selected. If this rule were the only rule 
for route selection, then BGP would be using a DV algorithm for path determi-
nation, where the distance metric uses the number of AS hops rather than the 
number of router hops.
 3. From the remaining routes (all with the same highest local preference value and 
the same AS-PATH length), hot potato routing is used, that is, the route with the 
closest NEXT-HOP router is selected.
 4. If more than one route still remains, the router uses BGP identifiers to select the 
route; see [Stewart 1999].
As an example, let‚Äôs again consider router 1b in Figure 5.10. Recall that there 
are exactly two BGP routes to prefix x, one that passes through AS2 and one that 
bypasses AS2. Also recall that if hot potato routing on its own were used, then BGP 
would route packets through AS2 to prefix x. But in the above route-selection algo-
rithm, rule 2 is applied before rule 3, causing BGP to select the route that bypasses 
AS2, since that route has a shorter AS PATH. So we see that with the above route-
selection algorithm, BGP is no longer a selfish algorithm‚Äîit first looks for routes 
with short AS paths (thereby likely reducing end-to-end delay).
As noted above, BGP is the de facto standard for inter-AS routing for the 
Internet. To see the contents of various BGP routing tables (large!) extracted from

routers in tier-1 ISPs, see http://www.routeviews.org. BGP routing tables often 
contain over half a million routes (that is, prefixes and corresponding attributes). 
Statistics about the size and characteristics of BGP routing tables are presented in 
[Huston 2019b].
5.4.4 IP-Anycast
In addition to being the Internet‚Äôs inter-AS routing protocol, BGP is often used to 
implement the IP-anycast service [RFC 1546, RFC 7094], which is commonly used 
in DNS. To motivate IP-anycast, consider that in many applications, we are interested 
in (1) replicating the same content on different servers in many different dispersed 
geographical locations, and (2) having each user access the content from the server 
that is closest. For example, a CDN may replicate videos and other objects on servers 
in different countries. Similarly, the DNS system can replicate DNS records on DNS 
servers throughout the world. When a user wants to access this replicated content, it 
is desirable to point the user to the ‚Äúnearest‚Äù server with the replicated content. BGP‚Äôs 
route-selection algorithm provides an easy and natural mechanism for doing so.
To make our discussion concrete, let‚Äôs describe how a CDN might use IP- 
anycast. As shown in Figure 5.12, during the IP-anycast configuration stage, the 
CDN company assigns the same IP address to each of its servers, and uses stand-
ard BGP to advertise this IP address from each of the servers. When a BGP router 
receives multiple route advertisements for this IP address, it treats these advertise-
ments as providing different paths to the same physical location (when, in fact, 
the advertisements are for different paths to different physical locations). When 
configuring its routing table, each router will locally use the BGP route-selec-
tion algorithm to pick the ‚Äúbest‚Äù (for example, closest, as determined by AS-hop 
counts) route to that IP address. For example, if one BGP route (corresponding to 
one location) is only one AS hop away from the router, and all other BGP routes 
(corresponding to other locations) are two or more AS hops away, then the BGP 
router would choose to route packets to the location that is one hop away. After 
this initial BGP address-advertisement phase, the CDN can do its main job of dis-
tributing content. When a client requests the video, the CDN returns to the client 
the common IP address used by the geographically dispersed servers, no matter 
where the client is located. When the client sends a request to that IP address, 
Internet routers then forward the request packet to the ‚Äúclosest‚Äù server, as defined 
by the BGP route-selection algorithm.
Although the above CDN example nicely illustrates how IP-anycast can be 
used, in practice, CDNs generally choose not to use IP-anycast because BGP routing 
changes can result in different packets of the same TCP connection arriving at differ-
ent instances of the Web server. But IP-anycast is extensively used by the DNS system 
to direct DNS queries to the closest root DNS server. Recall from Section 2.4, there 
are currently 13 IP addresses for root DNS servers. But corresponding to each of these 
addresses, there are multiple DNS root servers, with some of these addresses having

over 100 DNS root servers scattered over all corners of the world. When a DNS query 
is sent to one of these 13 IP addresses, IP anycast is used to route the query to the 
nearest of the DNS root servers that is responsible for that address. [Li 2018] presents 
recent measurements illustrating Internet anycast, use, performance, and challenges. 
5.4.5 Routing Policy
When a router selects a route to a destination, the AS routing policy can trump all 
other considerations, such as shortest AS path or hot potato routing. Indeed, in the 
route-selection algorithm, routes are first selected according to the local-preference 
attribute, whose value is fixed by the policy of the local AS.
Let‚Äôs illustrate some of the basic concepts of BGP routing policy with a simple 
example. Figure 5.13 shows six interconnected autonomous systems: A, B, C, W, X, 
and Y. It is important to note that A, B, C, W, X, and Y are ASs, not routers. Let‚Äôs 
AS1
AS3
3b
3c
3a
1a
1c
1b
1d
AS2
AS4
2a
2c
4a
4c
4b
Advertise
212.21.21.21
CDN Server B
CDN Server A
Advertise
212.21.21.21
Receive BGP 
advertisements for
212.21.21.21 from
AS1 and from AS4.
Forward toward
Server B since it is
closer.
2b
Figure 5.12 ‚ô¶ Using IP-anycast to bring users to the closest CDN server

assume that autonomous systems W, X, and Y are access ISPs and that A, B, and C 
are backbone provider networks. We‚Äôll also assume that A, B, and C, directly send 
traffic to each other, and provide full BGP information to their customer networks. 
All traffic entering an ISP access network must be destined for that network, and  
all traffic leaving an ISP access network must have originated in that network.  
W and Y are clearly access ISPs. X is a multi-homed access ISP, since it is con-
nected to the rest of the network via two different providers (a scenario that is becom-
ing increasingly common in practice). However, like W and Y, X itself must be the 
source/destination of all traffic leaving/entering X. But how will this stub network 
behavior be implemented and enforced? How will X be prevented from forwarding 
traffic between B and C? This can easily be accomplished by controlling the manner 
in which BGP routes are advertised. In particular, X will function as an access ISP 
network if it advertises (to its neighbors B and C) that it has no paths to any other 
destinations except itself. That is, even though X may know of a path, say XCY, that 
reaches network Y, it will not advertise this path to B. Since B is unaware that X has 
a path to Y, B would never forward traffic destined to Y (or C) via X. This simple 
example illustrates how a selective route advertisement policy can be used to imple-
ment customer/provider routing relationships.
Let‚Äôs next focus on a provider network, say AS B. Suppose that B has learned 
(from A) that A has a path AW to W. B can thus install the route AW into its routing 
information base. Clearly, B also wants to advertise the path BAW to its customer, 
X, so that X knows that it can route to W via B. But should B advertise the path 
BAW to C? If it does so, then C could route traffic to W via BAW. If A, B, and C are 
all backbone providers, than B might rightly feel that it should not have to shoulder 
the burden (and cost!) of carrying transit traffic between A and C. B might rightly 
feel that it is A‚Äôs and C‚Äôs job (and cost!) to make sure that C can route to/from A‚Äôs 
customers via a direct connection between A and C. There are currently no official 
standards that govern how backbone ISPs route among themselves. However, a rule 
of thumb followed by commercial ISPs is that any traffic flowing across an ISP‚Äôs 
backbone network must have either a source or a destination (or both) in a network 
that is a customer of that ISP; otherwise the traffic would be getting a free ride on 
the ISP‚Äôs network. Individual peering agreements (that would govern questions such 
A
W
X
Y
B
Key:
Provider
network
Customer
network
C
Figure 5.13 ‚ô¶ A simple BGP policy scenario

as those raised above) are typically negotiated between pairs of ISPs and are often 
confidential; [Huston 1999a; Huston 2012] provide an interesting discussion of peer-
ing agreements. For a detailed description of how routing policy reflects commercial 
relationships among ISPs, see [Gao 2001; Dmitiropoulos 2007]. For a discussion of 
BGP routing polices from an ISP standpoint, see [Caesar 2005b].
WHY ARE THERE DIFFERENT INTER-AS AND INTRA-AS ROUTING 
PROTOCOLS?
Having now studied the details of specific inter-AS and intra-AS routing protocols deployed 
in today‚Äôs Internet, let‚Äôs conclude by considering perhaps the most fundamental question 
we could ask about these protocols in the first place (hopefully, you have been wondering 
this all along, and have not lost the forest for the trees!): Why are different inter-AS and 
intra-AS routing protocols used?
The answer to this question gets at the heart of the differences between the goals of 
routing within an AS and among ASs:
‚Ä¢ 
Policy. Among ASs, policy issues dominate. It may well be important that traffic origi-
nating in a given AS not be able to pass through another specific AS. Similarly, a 
given AS may well want to control what transit traffic it carries between other ASs. We 
have seen that BGP carries path attributes and provides for controlled distribution of 
routing information so that such policy-based routing decisions can be made. Within 
an AS, everything is nominally under the same administrative control, and thus policy 
issues play a much less important role in choosing routes within the AS.
‚Ä¢ 
Scale. The ability of a routing algorithm and its data structures to scale to handle 
routing to/among large numbers of networks is a critical issue in inter-AS routing. 
Within an AS, scalability is less of a concern. For one thing, if a single ISP becomes 
too large, it is always possible to divide it into two ASs and perform inter-AS routing 
between the two new ASs. (Recall that OSPF allows such a hierarchy to be built by 
splitting an AS into areas.)
‚Ä¢ 
Performance. Because inter-AS routing is so policy oriented, the quality (for example, 
performance) of the routes used is often of secondary concern (that is, a longer or 
more costly route that satisfies certain policy criteria may well be taken over a route 
that is shorter but does not meet that criteria). Indeed, we saw that among ASs, there 
is not even the notion of cost (other than AS hop count) associated with routes. Within 
a single AS, however, such policy concerns are of less importance, allowing routing to 
focus more on the level of performance realized on a route.
PRINCIPLES IN PRACTICE

This completes our brief introduction to BGP. Understanding BGP is important 
because it plays a central role in the Internet. We encourage you to see the references 
[Stewart 1999; Huston 2019a; Labovitz 1997; Halabi 2000; Huitema 1998; Gao 2001; 
Feamster 2004; Caesar 2005b; Li 2007] to learn more about BGP.
5.4.6  Putting the Pieces Together: Obtaining  
Internet Presence
Although this subsection is not about BGP per se, it brings together many of the 
protocols and concepts we‚Äôve seen thus far, including IP addressing, DNS, and BGP.
Suppose you have just created a small company that has a number of servers, 
including a public Web server that describes your company‚Äôs products and services, 
a mail server from which your employees obtain their e-mail messages, and a DNS 
server. Naturally, you would like the entire world to be able to visit your Web site in 
order to learn about your exciting products and services. Moreover, you would like your 
employees to be able to send and receive e-mail to potential customers throughout the 
world.
To meet these goals, you first need to obtain Internet connectivity, which is 
done by contracting with, and connecting to, a local ISP. Your company will have 
a gateway router, which will be connected to a router in your local ISP. This con-
nection might be a DSL connection through the existing telephone infrastructure, a 
leased line to the ISP‚Äôs router, or one of the many other access solutions described 
in Chapter 1. Your local ISP will also provide you with an IP address range, for 
example, a /24 address range consisting of 256 addresses. Once you have your physi-
cal connectivity and your IP address range, you will assign one of the IP addresses 
(in your address range) to your Web server, one to your mail server, one to your 
DNS server, one to your gateway router, and other IP addresses to other servers and 
 networking devices in your company‚Äôs network.
In addition to contracting with an ISP, you will also need to contract with an 
Internet registrar to obtain a domain name for your company, as described in Chapter 2. 
For example, if your company‚Äôs name is, say, Xanadu Inc., you will naturally try 
to obtain the domain name xanadu.com. Your company must also obtain presence 
in the DNS system. Specifically, because outsiders will want to contact your DNS 
server to obtain the IP addresses of your servers, you will also need to provide your 
registrar with the IP address of your DNS server. Your registrar will then put an 
entry for your DNS server (domain name and corresponding IP address) in the .com 
top-level-domain servers, as described in Chapter 2. After this step is completed, any 
user who knows your domain name (e.g., xanadu.com) will be able to obtain the IP 
address of your DNS server via the DNS system.
So that people can discover the IP addresses of your Web server, in your DNS 
server you will need to include entries that map the host name of your Web server 
(e.g., www.xanadu.com) to its IP address. You will want to have similar entries for

other publicly available servers in your company, including your mail server. In this 
manner, if Alice wants to browse your Web server, the DNS system will contact your 
DNS server, find the IP address of your Web server, and give it to Alice. Alice can 
then establish a TCP connection directly with your Web server.
However, there still remains one other necessary and crucial step to allow out-
siders from around the world to access your Web server. Consider what happens 
when Alice, who knows the IP address of your Web server, sends an IP datagram 
(e.g., a TCP SYN segment) to that IP address. This datagram will be routed through 
the Internet, visiting a series of routers in many different ASs, and eventually reach 
your Web server. When any one of the routers receives the datagram, it is going 
to look for an entry in its forwarding table to determine on which outgoing port it 
should forward the datagram. Therefore, each of the routers needs to know about the 
existence of your company‚Äôs /24 prefix (or some aggregate entry). How does a router 
become aware of your company‚Äôs prefix? As we have just seen, it becomes aware of 
it from BGP! Specifically, when your company contracts with a local ISP and gets 
assigned a prefix (i.e., an address range), your local ISP will use BGP to advertise 
your prefix to the ISPs to which it connects. Those ISPs will then, in turn, use BGP 
to propagate the advertisement. Eventually, all Internet routers will know about your 
prefix (or about some aggregate that includes your prefix) and thus be able to appro-
priately forward datagrams destined to your Web and mail servers.
5.5 The SDN Control Plane
In this section, we‚Äôll dive into the SDN control plane‚Äîthe network-wide logic that 
controls packet forwarding among a network‚Äôs SDN-enabled devices, as well as the 
configuration and management of these devices and their services. Our study here 
builds on our earlier discussion of generalized SDN forwarding in Section 4.4, so you 
might want to first review that section, as well as Section 5.1 of this chapter, before 
continuing on. As in Section 4.4, we‚Äôll again adopt the terminology used in the SDN 
literature and refer to the network‚Äôs forwarding devices as ‚Äúpacket switches‚Äù (or just 
switches, with ‚Äúpacket‚Äù being understood), since forwarding decisions can be made 
on the basis of network-layer source/destination addresses, link-layer source/destina-
tion addresses, as well as many other values in transport-, network-, and link-layer 
packet-header fields.
Four key characteristics of an SDN architecture can be identified [Kreutz 2015]:
‚Ä¢ Flow-based forwarding. Packet forwarding by SDN-controlled switches can be 
based on any number of header field values in the transport-layer, network-layer, 
or link-layer header. We saw in Section 4.4 that the OpenFlow1.0 abstraction 
allows forwarding based on eleven different header field values. This contrasts

sharply with the traditional approach to router-based forwarding that we studied 
in Sections 5.2‚Äì5.4, where forwarding of IP datagrams was based solely on a 
datagram‚Äôs destination IP address. Recall from Figure 5.2 that packet forwarding 
rules are specified in a switch‚Äôs flow table; it is the job of the SDN control plane 
to compute, manage and install flow table entries in all of the network‚Äôs switches.
‚Ä¢ Separation of data plane and control plane. This separation is shown clearly 
in Figures 5.2 and 5.14. The data plane consists of the network‚Äôs switches‚Äî 
relatively simple (but fast) devices that execute the ‚Äúmatch plus action‚Äù rules in 
their flow tables. The control plane consists of servers and software that deter-
mine and manage the switches‚Äô flow tables.
‚Ä¢ Network control functions: external to data-plane switches. Given that the ‚ÄúS‚Äù in 
SDN is for ‚Äúsoftware,‚Äù it‚Äôs perhaps not surprising that the SDN control plane is 
implemented in software. Unlike traditional routers, however, this software exe-
cutes on servers that are both distinct and remote from the network‚Äôs switches. As 
shown in Figure 5.14, the control plane itself consists of two components‚Äîan SDN 
controller (or network operating system [Gude 2008]) and a set of network-control 
applications. The controller maintains accurate network state information (e.g., the 
state of remote links, switches, and hosts); provides this information to the network-
control applications running in the control plane; and provides the means through 
which these applications can monitor, program, and control the underlying network 
devices. Although the controller in Figure 5.14 is shown as a single central server, 
in practice the controller is only logically centralized; it is typically implemented on 
several servers that provide coordinated, scalable performance and high availability.
‚Ä¢ A programmable network. The network is programmable through the network-
control applications running in the control plane. These applications represent the 
‚Äúbrains‚Äù of the SDN control plane, using the APIs provided by the SDN controller 
to specify and control the data plane in the network devices. For example, a routing 
network-control application might determine the end-end paths between sources 
and destinations (for example, by executing Dijkstra‚Äôs algorithm using the node-
state and link-state information maintained by the SDN controller). Another net-
work application might perform access control, that is, determine which packets 
are to be blocked at a switch, as in our third example in Section 4.4.3. Yet another 
application might have switches forward packets in a manner that performs server 
load balancing (the second example we considered in Section 4.4.3).
From this discussion, we can see that SDN represents a significant ‚Äúunbundling‚Äù 
of network functionality‚Äîdata plane switches, SDN controllers, and network-control  
applications are separate entities that may each be provided by different vendors 
and organizations. This contrasts with the pre-SDN model in which a switch/router 
(together with its embedded control plane software and protocol implementations) 
was monolithic, vertically integrated, and sold by a single vendor. This unbundling

of network functionality in SDN has been likened to the earlier evolution from main-
frame computers (where hardware, system software, and applications were provided 
by a single vendor) to personal computers (with their separate hardware, operating 
systems, and applications). The unbundling of computing hardware, system soft-
ware, and applications has led to a rich, open ecosystem driven by innovation in all 
three of these areas; one hope for SDN is that it will continue to drive and enable 
such rich innovation.
Given our understanding of the SDN architecture of Figure 5.14, many questions 
naturally arise. How and where are the flow tables actually computed? How are these 
tables updated in response to events at SDN-controlled devices (e.g., an attached link 
going up/down)? And how are the flow table entries at multiple switches coordinated 
in such a way as to result in orchestrated and consistent network-wide functionality 
(e.g., end-to-end paths for forwarding packets from sources to destinations, or coor-
dinated distributed firewalls)? It is the role of the SDN control plane to provide these, 
and many other, capabilities.
Routing
Network-control Applications
Control
plane
Data
plane
SDN-Controlled Switches
Access
Control
Load
Balancer
Northbound
API
Southbound
API
SDN Controller
(network operating system)
Figure 5.14 ‚ô¶  Components of the SDN architecture: SDN-controlled 
switches, the SDN controller, network-control applications

5.5.1  The SDN Control Plane: SDN Controller and  
SDN Network-control Applications
Let‚Äôs begin our discussion of the SDN control plane in the abstract, by consider-
ing the generic capabilities that the control plane must provide. As we‚Äôll see, this 
abstract, ‚Äúfirst principles‚Äù approach will lead us to an overall architecture that reflects 
how SDN control planes have been implemented in practice.
As noted above, the SDN control plane divides broadly into two components‚Äî
the SDN controller and the SDN network-control applications. Let‚Äôs explore the 
controller first. Many SDN controllers have been developed since the earliest SDN 
controller [Gude 2008]; see [Kreutz 2015] for an extremely thorough survey. Figure 5.15 
provides a more detailed view of a generic SDN controller. A controller‚Äôs function-
ality can be broadly organized into three layers. Let‚Äôs consider these layers in an 
uncharacteristically bottom-up fashion:
‚Ä¢ A communication layer: communicating between the SDN controller and con-
trolled network devices. Clearly, if an SDN controller is going to control the 
operation of a remote SDN-enabled switch, host, or other device, a protocol is 
needed to transfer information between the controller and that device. In addition, 
a device must be able to communicate locally-observed events to the controller  
(for example, a message indicating that an attached link has gone up or down, 
that a device has just joined the network, or a heartbeat indicating that a device 
is up and operational). These events provide the SDN controller with an up-to-
date view of the network‚Äôs state. This protocol constitutes the lowest layer of the 
controller architecture, as shown in Figure 5.15. The communication between 
the controller and the controlled devices cross what has come to be known as the 
controller‚Äôs ‚Äúsouthbound‚Äù interface. In Section 5.5.2, we‚Äôll study OpenFlow‚Äîa 
specific protocol that provides this communication functionality. OpenFlow is 
implemented in most, if not all, SDN controllers.
‚Ä¢ A network-wide state-management layer. The ultimate control decisions made by 
the SDN control plane‚Äîfor example, configuring flow tables in all switches to 
achieve the desired end-end forwarding, to implement load balancing, or to imple-
ment a particular firewalling capability‚Äîwill require that the controller have up-
to-date information about state of the networks‚Äô hosts, links, switches, and other 
SDN-controlled devices. A switch‚Äôs flow table contains counters whose values 
might also be profitably used by network-control applications; these values should 
thus be available to the applications. Since the ultimate aim of the control plane is 
to determine flow tables for the various controlled devices, a controller might also 
maintain a copy of these tables. These pieces of information all constitute exam-
ples of the network-wide ‚Äústate‚Äù maintained by the SDN controller.
‚Ä¢ The interface to the network-control application layer. The controller interacts 
with network-control applications through its ‚Äúnorthbound‚Äù interface. This API

allows network-control applications to read/write network state and flow tables 
within the state-management layer. Applications can register to be notified when 
state-change events occur, so that they can take actions in response to network 
event notifications sent from SDN-controlled devices. Different types of APIs 
may be provided; we‚Äôll see that two popular SDN controllers communicate with 
their applications using a REST [Fielding 2000] request-response interface.
We have noted several times that an SDN controller can be considered to be 
 ‚Äúlogically centralized,‚Äù that is, that the controller may be viewed externally (for exam-
ple, from the point of view of SDN-controlled devices and external network-control 
Routing
Access
Control
Load
Balancer
Interface, abstractions for  network control apps
Network
graph
RESTful
API
Intent
Communication to/from controlled devices
Network-wide distributed, robust state management
Link-state
info
Host info
Switch
info
Statistics
Flow
tables
OpenFlow
SNMP
SDN Controller
Northbound
API
Southbound
API
Figure 5.15 ‚ô¶ Components of an SDN controller

applications) as a single, monolithic service. However, these services and the data-
bases used to hold state information are implemented in practice by a distributed 
set of servers for fault tolerance, high availability, or for performance reasons. With 
controller functions being implemented by a set of servers, the semantics of the con-
troller‚Äôs internal operations (e.g., maintaining logical time ordering of events, con-
sistency, consensus, and more) must be considered [Panda 2013]. Such concerns are 
common across many different distributed systems; see [Lamport 1989, Lampson 
1996] for elegant solutions to these challenges. Modern controllers such as Open-
Daylight [OpenDaylight 2020] and ONOS [ONOS 2020] (see sidebar) have placed 
considerable emphasis on architecting a logically centralized but physically distrib-
uted controller platform that provides scalable services and high availability to the 
controlled devices and network-control applications alike.
The architecture depicted in Figure 5.15 closely resembles the architecture of the 
originally proposed NOX controller in 2008 [Gude 2008], as well as that of today‚Äôs 
OpenDaylight [OpenDaylight 2020] and ONOS [ONOS 2020] SDN controllers (see  
sidebar). We‚Äôll cover an example of controller operation in Section 5.5.3. First, how-
ever, let‚Äôs examine the OpenFlow protocol, the earliest and now one of several pro-
tocols that can be used for communication between an SDN controller and a controlled 
device, which lies in the controller‚Äôs communication layer.
5.5.2 OpenFlow Protocol
The OpenFlow protocol [OpenFlow 2009, ONF 2020] operates between an SDN 
controller and an SDN-controlled switch or other device implementing the Open-
Flow API that we studied earlier in Section 4.4. The OpenFlow protocol operates 
over TCP, with a default port number of 6653.
Among the important messages flowing from the controller to the controlled 
switch are the following:
‚Ä¢ Configuration. This message allows the controller to query and set a switch‚Äôs 
configuration parameters.
‚Ä¢ Modify-State. This message is used by a controller to add/delete or modify entries 
in the switch‚Äôs flow table, and to set switch port properties.
‚Ä¢ Read-State. This message is used by a controller to collect statistics and counter 
values from the switch‚Äôs flow table and ports.
‚Ä¢ Send-Packet. This message is used by the controller to send a specific packet out 
of a specified port at the controlled switch. The message itself contains the packet 
to be sent in its payload.
Among the messages flowing from the SDN-controlled switch to the controller 
are the following:
‚Ä¢ Flow-Removed. This message informs the controller that a flow table entry has been 
removed, for example by a timeout or as the result of a received modify-state message.

‚Ä¢ Port-status. This message is used by a switch to inform the controller of a change 
in port status.
‚Ä¢ Packet-in. Recall from Section 4.4 that a packet arriving at a switch port and not 
matching any flow table entry is sent to the controller for additional processing. 
Matched packets may also be sent to the controller, as an action to be taken on a 
match. The packet-in message is used to send such packets to the controller.
Additional OpenFlow messages are defined in [OpenFlow 2009, ONF 2020].
GOOGLE‚ÄôS SOFTWARE-DEFINED GLOBAL NETWORK
Recall from the case study in Section 2.6 that Google deploys a dedicated wide-area 
network (WAN) that interconnects its data centers and server clusters (in IXPs and ISPs). 
This network, called B4, has a Google-designed SDN control plane built on OpenFlow. 
Google‚Äôs network is able to drive WAN links at near 70% utilization over the long run  
(a two to three fold increase over typical link utilizations) and split application flows among 
multiple paths based on application priority and existing flow demands [Jain 2013].
The Google B4 network is particularly it well-suited for SDN: (i) Google controls all 
devices from the edge servers in IXPs and ISPs to routers in their network core; (ii) the 
most bandwidth-intensive applications are large-scale data copies between sites that can 
defer to higher-priority interactive applications during times of resource congestion;  
(iii) with only a few dozen data centers being connected, centralized control is feasible.
Google‚Äôs B4 network uses custom-built switches, each implementing a slightly extended ver-
sion of OpenFlow, with a local Open Flow Agent (OFA) that is similar in spirit to the control 
agent we encountered in Figure 5.2. Each OFA in turn connects to an Open Flow Controller 
(OFC) in the network control server (NCS), using a separate ‚Äúout of band‚Äù network, distinct 
from the network that carries data-center traffic between data centers. The OFC thus provides 
the services used by the NCS to communicate with its controlled switches, similar in spirit to 
the lowest layer in the SDN architecture shown in Figure 5.15. In B4, the OFC also performs 
state management functions, keeping node and link status in a Network Information Base 
(NIB). Google‚Äôs implementation of the OFC is based on the ONIX SDN controller [Koponen 
2010]. Two routing protocols, BGP (for routing between the data centers) and IS-IS (a close 
relative of OSPF, for routing within a data center), are implemented. Paxos [Chandra 2007] is 
used to execute hot replicas of NCS components to protect against failure.
A traffic engineering network-control application, sitting logically above the set of 
network control servers, interacts with these servers to provide global, network-wide band-
width provisioning for groups of application flows. With B4, SDN made an important 
leap forward into the operational networks of a global network provider. See [Jain 2013; 
Hong 2018] for a detailed description of B4.
PRINCIPLES IN PRACTICE

5.5.3 Data and Control Plane Interaction: An Example
In order to solidify our understanding of the interaction between SDN-controlled 
switches and the SDN controller, let‚Äôs consider the example shown in Figure 5.16, 
in which Dijkstra‚Äôs algorithm (which we studied in Section 5.2) is used to determine 
shortest path routes. The SDN scenario in Figure 5.16 has two important differ-
ences from the earlier per-router-control scenario of Sections 5.2.1 and 5.3, where 
 Dijkstra‚Äôs algorithm was implemented in each and every router and link-state updates 
were flooded among all network routers:
‚Ä¢ Dijkstra‚Äôs algorithm is executed as a separate application, outside of the packet 
switches.
‚Ä¢ Packet switches send link updates to the SDN controller and not to each other.
In this example, let‚Äôs assume that the link between switch s1 and s2 goes 
down; that shortest path routing is implemented, and consequently and that incom-
ing and outgoing flow forwarding rules at s1, s3, and s4 are affected, but that s2‚Äôs 
Figure 5.16 ‚ô¶ SDN controller scenario: Link-state change
Network
graph
RESTful
API
Intent
Statistics
Flow
tables
OpenFlow
SNMP
Dijkstra‚Äôs link-state
Routing
4
3
2
1
5
s1
s2
s3
s4
6
Link-state
info
Host info
Switch
info

operation is unchanged. Let‚Äôs also assume that OpenFlow is used as the communi-
cation layer protocol, and that the control plane performs no other function other 
than link-state routing.
 1. Switch s1, experiencing a link failure between itself and s2, notifies the SDN 
controller of the link-state change using the OpenFlow port-status message.
 2. The SDN controller receives the OpenFlow message indicating the link-state 
change, and notifies the link-state manager, which updates a link-state  database.
 3. The network-control application that implements Dijkstra‚Äôs link-state routing 
has previously registered to be notified when link state changes. That applica-
tion receives the notification of the link-state change.
 4. The link-state routing application interacts with the link-state manager to get 
updated link state; it might also consult other components in the state- management 
layer. It then computes the new least-cost paths.
 5. The link-state routing application then interacts with the flow table manager, 
which determines the flow tables to be updated.
 6. The flow table manager then uses the OpenFlow protocol to update flow table 
entries at affected switches‚Äîs1 (which will now route packets destined to s2 via s4), 
s2 (which will now begin receiving packets from s1 via intermediate switch s4), and 
s4 (which must now forward packets from s1 destined to s2).
This example is simple but illustrates how the SDN control plane provides control-
plane services (in this case, network-layer routing) that had been previously imple-
mented with per-router control exercised in each and every network router. One can 
now easily appreciate how an SDN-enabled ISP could easily switch from least-cost 
path routing to a more hand-tailored approach to routing. Indeed, since the controller 
can tailor the flow tables as it pleases, it can implement any form of forwarding that 
it pleases‚Äîsimply by changing its application-control software. This ease of change 
should be contrasted to the case of a traditional per-router control plane, where soft-
ware in all routers (which might be provided to the ISP by multiple independent 
vendors) must be changed.
5.5.4 SDN: Past and Future
Although the intense interest in SDN is a relatively recent phenomenon, the techni-
cal roots of SDN, and the separation of the data and control planes in particular, go  
back considerably further. In 2004, [Feamster 2004, Lakshman 2004, RFC 3746] all  
argued for the separation of the network‚Äôs data and control planes. [van der Merwe  
1998] describes a control framework for ATM networks [Black 1995] with multiple  
controllers, each controlling a number of ATM switches. The Ethane project [Casado  
2007] pioneered the notion of a network of simple flow-based Ethernet switches 
with¬† match-plus-action flow tables, a centralized controller that managed flow

admission and routing, and the forwarding of unmatched packets from the switch 
to the controller. A network of more than 300 Ethane switches was operational in 
2007. Ethane quickly evolved into the OpenFlow project, and the rest (as the saying 
goes) is history!
Numerous research efforts are aimed at developing future SDN architectures 
and capabilities. As we have seen, the SDN revolution is leading to the disruptive 
replacement of dedicated monolithic switches and routers (with both data and control 
planes) by simple commodity switching hardware and a sophisticated software con-
trol plane. A generalization of SDN known as network functions virtualization (NFV) 
(which we discussed earlier in Section 4.5) similarly aims at disruptive replacement 
of sophisticated middleboxes (such as middleboxes with dedicated hardware and 
proprietary software for media caching/service) with simple commodity servers, 
switching, and storage. A second area of important research seeks to extend SDN 
concepts from the intra-AS setting to the inter-AS setting [Gupta 2014].
SDN CONTROLLER CASE STUDIES: THE OPENDAYLIGHT  
AND ONOS CONTROLLERS
In the earliest days of SDN, there was a single SDN protocol (OpenFlow [McKeown 
2008; OpenFlow 2009]) and a single SDN controller (NOX [Gude 2008]). Since then, 
the number of SDN controllers in particular has grown significantly [Kreutz 2015]. Some 
SDN controllers are company-specific and proprietary, particularly when used to control 
internal proprietary networks (e.g., within or among a company‚Äôs data centers). But many 
more controllers are open-source and implemented in a variety of programming languages 
[Erickson 2013]. Most recently, the OpenDaylight controller [OpenDaylight 2020] and the 
ONOS controller [ONOS 2020] have found considerable industry support. They are both 
open-source and are being developed in partnership with the Linux Foundation.
The OpenDaylight Controller
Figure 5.17 presents a simplified view of the OpenDaylight (ODL) controller platform 
[OpenDaylight 2020, Eckel 2017].
ODL‚Äôs Basic Network Functions are at the heart of the controller, and correspond 
closely to the network-wide state management capabilities that we encountered in Figure 5.15. 
The Service Abstraction Layer (SAL) is the controller‚Äôs nerve center, allowing controller 
components and applications to invoke each other‚Äôs services, access configuration and 
operational data, and to subscribe to events they generate. The SAL also provides a uni-
form abstract interface to specific protocols operating between the ODL controller and the 
controlled devices. These protocols include OpenFlow (which we covered in Section 4.5), 
PRINCIPLES IN PRACTICE

and the Simple Network Management Protocol (SNMP) and the Network Configuration 
(NETCONF) protocol, both of which we‚Äôll cover in Section 5.7. The Open vSwitch 
Database Management Protocol (OVSDB) is used to manage data center switching, an 
important application area for SDN technology. We‚Äôll introduce data center networking in 
Chapter 6.¬†
Network Orchestrations and Applications determine how data-plane forwarding 
and other services, such as firewalling and load balancing, are accomplished in the 
controlled devices. ODL provides two ways in which applications can interoperate with 
native controller services (and hence devices) and with each other. In the API-Driven 
(AD-SAL) approach, shown in Figure 5.17, applications communicate with controller 
modules using a REST request-response API running over HTTP. Initial releases of the 
OpenDaylight controller provided only the AD-SAL. As ODL became increasingly used 
for network configuration and management, later ODL releases introduced a Model-
Driven (MD-SAL) approach. Here, the YANG data modeling language [RFC 6020] 
defines models of device, protocol, and network configuration and operational state 
data. Devices are then configured and managed by manipulating this data using the 
NETCONF protocol.
Figure 5.17 ‚ô¶ A simplified view of the OpenDaylight controller
REST/RESTCONF/NETCONF APIs
Enhanced
Services
Basic Network Functions
Topology
Processing
Switch
mgr.
Stats
mgr.
AAA
Device
Discovery
Forwarding
rules mgr.
Host
Tracker
Network
Orchestrations and
Applications
Northbound APIs
Southbound APIs
and
Protocols Plugins
Service Abstraction
Layer (SAL)
messaging
OpenÔ¨Çow
NETCONF
SNMP
OVSDB
TrafÔ¨Åc
Engineering
Firewalling
Load
Balancing
conÔ¨Åg. and
operational data
store

The ONOS Controller
Figure 5.18 presents a simplified view of the ONOS controller ONOS 2020]. Similar 
to the canonical controller in Figure 5.15, three layers can be identified in the ONOS 
 controller:
‚Ä¢ 
Northbound abstractions and protocols. A unique feature of ONOS is its intent 
framework, which allows an application to request a high-level service (e.g., to setup 
a connection between host A and Host B, or conversely to not allow Host A and host 
B to communicate) without having to know the details of how this service is performed. 
State information is provided to network-control applications across the northbound API 
either synchronously (via query) or asynchronously (via listener callbacks, e.g., when 
network state changes).
‚Ä¢ 
Distributed core. The state of the network‚Äôs links, hosts, and devices is maintained 
in ONOS‚Äôs distributed core. ONOS is deployed as a service on a set of intercon-
nected servers, with each server running an identical copy of the ONOS software; an 
increased number of servers offers an increased service capacity. The ONOS core 
Figure 5.18 ‚ô¶ ONOS controller architecture
Intent
REST   API
Hosts
Paths
Topology
Devices
Links
Flow rules
Statistics
Device
Link
Host
Flow
Packet
OpenFlow
Netconf
OVSDB
Network
control apps
Northbound
abstractions,
protocols
ONOS
distributed
core
Southbound
abstractions,
protocols

5.6 ICMP: The Internet Control Message Protocol
The Internet Control Message Protocol (ICMP), specified in [RFC 792], is used by 
hosts and routers to communicate network-layer information to each other. The most 
typical use of ICMP is for error reporting. For example, when running an HTTP 
session, you may have encountered an error message such as ‚ÄúDestination network 
unreachable.‚Äù This message had its origins in ICMP. At some point, an IP router was 
unable to find a path to the host specified in your HTTP request. That router created 
and sent an ICMP message to your host indicating the error.
ICMP is often considered part of IP, but architecturally it lies just above IP, as 
ICMP messages are carried inside IP datagrams. That is, ICMP messages are carried 
as IP payload, just as TCP or UDP segments are carried as IP payload. Similarly, 
when a host receives an IP datagram with ICMP specified as the upper-layer protocol 
(an upper-layer protocol number of 1), it demultiplexes the datagram‚Äôs contents to 
ICMP, just as it would demultiplex a datagram‚Äôs content to TCP or UDP.
ICMP messages have a type and a code field, and contain the header and the first 
8 bytes of the IP datagram that caused the ICMP message to be generated in the first 
place (so that the sender can determine the datagram that caused the error). Selected 
ICMP message types are shown in Figure 5.19. Note that ICMP messages are used 
not only for signaling error conditions.
The well-known ping program sends an ICMP type 8 code 0 message to the 
specified host. The destination host, seeing the echo request, sends back a type 0 
code 0 ICMP echo reply. Most TCP/IP implementations support the ping server 
directly in the operating system; that is, the server is not a process. Chapter 11 of 
[Stevens 1990] provides the source code for the ping client program. Note that the 
client program needs to be able to instruct the operating system to generate an ICMP 
message of type 8 code 0.
Another interesting ICMP message is the source quench message. This message 
is seldom used in practice. Its original purpose was to perform congestion control‚Äîto  
allow a congested router to send an ICMP source quench message to a host to force 
provides the mechanisms for service replication and coordination among instances, 
providing the applications above and the network devices below with the abstraction 
of logically centralized core services.
‚Ä¢ 
Southbound abstractions and protocols. The southbound abstractions mask the hetero-
geneity of the underlying hosts, links, switches, and protocols, allowing the distributed 
core to be both device and protocol agnostic. Because of this abstraction, the south-
bound interface below the distributed core is logically higher than in our canonical 
controller in Figure 5.14 or the ODL controller in Figure 5.17.

that host to reduce its transmission rate. We have seen in Chapter 3 that TCP has its 
own congestion-control mechanism that operates at the transport layer, and that Explicit 
Congestion Notification bits can be used by network-later devices to signal congestion.
In Chapter 1, we introduced the Traceroute program, which allows us to trace a 
route from a host to any other host in the world. Interestingly, Traceroute is imple-
mented with ICMP messages. To determine the names and addresses of the routers 
between source and destination, Traceroute in the source sends a series of ordinary IP 
datagrams to the destination. Each of these datagrams carries a UDP segment with an 
unlikely UDP port number. The first of these datagrams has a TTL of 1, the second of 2, 
the third of 3, and so on. The source also starts timers for each of the datagrams. When 
the nth datagram arrives at the nth router, the nth router observes that the TTL of the 
datagram has just expired. According to the rules of the IP protocol, the router discards 
the datagram and sends an ICMP warning message to the source (type 11 code 0). This 
warning message includes the name of the router and its IP address. When this ICMP 
message arrives back at the source, the source obtains the round-trip time from the 
timer and the name and IP address of the nth router from the ICMP message.
How does a Traceroute source know when to stop sending UDP segments? 
Recall that the source increments the TTL field for each datagram it sends. Thus, one 
of the datagrams will eventually make it all the way to the destination host. Because 
this datagram contains a UDP segment with an unlikely port number, the destination 
Figure 5.19 ‚ô¶ ICMP message types
ICMP Type
Code
Description
0
3
3
3
3
3
3
4
8
9
10
11
12
0
0
1
2
3
6
7
0
0
0
0
0
0
echo reply (to ping)
destination network unreachable
destination host unreachable
destination protocol unreachable
destination port unreachable
destination network unknown
destination host unknown
source quench (congestion control)
echo request
router advertisement
router discovery
TTL expired
IP header bad

host sends a port unreachable ICMP message (type 3 code 3) back to the source. 
When the source host receives this particular ICMP message, it knows it does not 
need to send additional probe packets. (The standard Traceroute program actually 
sends sets of three packets with the same TTL; thus, the Traceroute output provides 
three results for each TTL.)
In this manner, the source host learns the number and the identities of routers 
that lie between it and the destination host and the round-trip time between the two 
hosts. Note that the Traceroute client program must be able to instruct the operating 
system to generate UDP datagrams with specific TTL values and must also be able to 
be notified by its operating system when ICMP messages arrive. Now that you under-
stand how Traceroute works, you may want to go back and play with it some more.
A new version of ICMP has been defined for IPv6 in RFC 4443. In addition to 
reorganizing the existing ICMP type and code definitions, ICMPv6 also added new 
types and codes required by the new IPv6 functionality. These include the ‚ÄúPacket 
Too Big‚Äù type and an ‚Äúunrecognized IPv6 options‚Äù error code.
5.7 Network Management and SNMP,  
NETCONF/YANG
Having now made our way to the end of our study of the network layer, with only the 
link-layer before us, we‚Äôre well aware that a network consists of many complex, interact-
ing pieces of hardware and software‚Äîfrom the links, switches, routers, hosts, and other 
devices that comprise the physical components of the network to the many protocols that 
control and coordinate these devices. When hundreds or thousands of such components 
are brought together by an organization to form a network, the job of the network admin-
istrator to keep the network ‚Äúup and running‚Äù is surely a challenge. We saw in Section¬†5.5 
that the logically centralized controller can help with this process in an SDN context. But 
the challenge of network management has been around long before SDN, with a rich 
set of network management tools and approaches that help the network administrator 
monitor, manage, and control the network. We‚Äôll study these tools and techniques in this 
section, as well as new tools and techniques that have co-evolved along with SDN.
An often-asked question is ‚ÄúWhat is network management?‚Äù A well-conceived, 
single-sentence (albeit a rather long run-on sentence) definition of network manage-
ment from [Saydam 1996] is:
Network management includes the deployment, integration, and coordination of 
the hardware, software, and human elements to monitor, test, poll, configure, ana-
lyze, evaluate, and control the network and element resources to meet the real-time, 
operational performance, and Quality of Service requirements at a reasonable cost.
Given this broad definition, we‚Äôll cover only the rudiments of network man-
agement in this section‚Äîthe architecture, protocols, and data used by a network

administrator in performing their task. We‚Äôll not cover the administrator‚Äôs decision-
making processes, where topics such as fault identification [Labovitz 1997; Steinder 
2002; Feamster 2005; Wu 2005; Teixeira 2006], anomaly detection [Lakhina 2005; 
Barford 2009], network design/engineering to meet contracted Service Level Agree-
ments (SLA‚Äôs) [Huston 1999a], and more come into consideration. Our focus is thus 
purposefully narrow; the interested reader should consult these references, the excel-
lent overviews in [Subramanian 2000; Schonwalder 2010; Claise 2019], and the more 
detailed treatment of network management available on the Web site for this text.
5.7.1 The Network Management Framework
Figure 5.20 shows the key components of network management:
‚Ä¢ Managing server. The managing server is an application, typically with network 
managers (humans) in the loop, running in a centralized network management 
station in the network operations center (NOC). The managing server is the locus 
of activity for network management: it controls the collection, processing, analy-
sis, and dispatching of network management information and commands. It is 
here that actions are initiated to configure, monitor, and control the network‚Äôs 
managed devices. In practice, a network may have several such managing servers.
‚Ä¢ Managed device. A managed device is a piece of network equipment (including 
its software) that resides on a managed network. A managed device might be 
a host, router, switch, middlebox, modem, thermometer, or other network-con-
nected device. The device itself will have many manageable components (e.g., 
a network interface is but one component of a host or router), and configuration 
parameters for these hardware and software components (e.g., an intra-AS rout-
ing protocol, such as OSPF).
‚Ä¢ Data. Each managed device will have data, also known as ‚Äústate,‚Äù associated with it. 
There are several different types of data. Configuration data is device information 
explicitly configured by the network manager, for example, a manager-assigned/
configured IP address or interface speed for a device interface. Operational data is 
information that the device acquires as it operates, for example, the list of immedi-
ate neighbors in OSPF protocol. Device statistics are status indicators and counts 
that are updated as the device operators (e.g., the number of dropped packets on 
an interface, or the device‚Äôs cooling fan speed). The network manager can query 
remote device data, and in some cases, control the remote device by writing device 
data values, as discussed below. As shown in Figure 5.17, the managing server also 
maintains its own copy of configuration, operational and statistics data from its 
managed devices as well as network-wide data (e.g., the network‚Äôs topology).
‚Ä¢ Network management agent. The network management agent is a software pro-
cess running in the managed device that communicates with the managing server, 
taking local actions at the managed device under the command and control of the 
managing server. The network management agent is similar to the routing agent 
that we saw in Figure 5.2.

‚Ä¢ Network management protocol. The final component of a network management 
framework is the network management protocol. This protocol runs between 
the managing server and the managed devices, allowing the managing server to 
query the status of managed devices and take actions at these devices via its 
agents. Agents can use the network management protocol to inform the managing 
server of exceptional events (e.g., component failures or violation of performance 
thresholds). It‚Äôs important to note that the network management protocol does not 
itself manage the network. Instead, it provides capabilities that network managers 
can use to manage (‚Äúmonitor, test, poll, configure, analyze, evaluate, and con-
trol‚Äù) the network. This is a subtle, but important, distinction.¬†¬†
In practice, there are three commonly used ways in a network operator can man-
age the network, using the components described above:
‚Ä¢ CLI. A network operator may issue direct Command Line Interface (CLI) 
commands to the device. These commands can be typed directly on a managed 
device‚Äôs console (if the operator is physically present at the device), or over a 
Telnet or secure shell (SSH) connection, possibly via scripting, between the 
Figure 5.20 ‚ô¶  Elements of network management
Agent
Device
data
Agent
Device
data
Agent
Device
data
Managing
server/controller
Managed
device
Managed
device
Managed
device
Managed
device
Agent
Controller-to-device protocol
Key:
Managed
device
Network managers
conÔ¨Åg. and
operational data
store
Device
data
Agent
Device
data

managing server/controller and the managed device. CLI commands are vendor- 
and device-specific and can be rather arcane. While seasoned network wizards 
may be able to use CLI to flawlessly configure network devices, CLI use is prone 
to errors, and it is difficult to automate or efficiently scale for large networks. Con-
sumer-oriented network devices, such as your wireless home router, may export a 
management menu that you (the network manager!) can access via HTTP to con-
figure that device. While this approach may work well for single, simple devices 
and is less error-prone than CLI, it also doesn‚Äôt scale to larger-sized networks.
‚Ä¢ SNMP/MIB. In this approach, the network operator can query/set the data con-
tained in a device‚Äôs Management Information Base (MIB) objects using the 
Simple Network Management Protocol (SNMP). Some MIBs are device- and 
vendor-specific, while other MIBs (e.g., the number of IP datagrams discarded at 
a router due to errors in an IP datagram header, or the number of UDP segments 
received at a host) are device-agnostic, providing abstraction and generality. A net-
work operator would most typically use this approach to query and monitor opera-
tional state and device statistics, and then use CLI to actively control/configure the 
device. We note, importantly, that both approaches manage devices individually. 
We‚Äôll cover the SNMP and MIBs, which have been in use since the late 1980s, in 
Section 5.7.2 below. A network-management workshop convened by the Internet 
Architecture Board in 2002 [RFC 3535] noted not only the value of the SNMP/
MIB approach for device monitoring but also noted its shortcomings, particularly 
for device configuration and network management at scale. This gave rise to the 
most recent approach for network management, using NETCONF and YANG.
‚Ä¢ NETCONF/YANG. The NETCONF/YANG approach takes a more abstract, net-
work-wide, and holistic view toward network management, with a much stronger 
emphasis on configuration management, including specifying correctness con-
straints and providing atomic management operations over multiple controlled 
devices. YANG [RFC 6020] is a data modeling language used to model configu-
ration and operational data. The NETCONF protocol [RFC 6241] is used to com-
municate YANG-compatible actions and data to/from/among remote devices. We 
briefly encountered NETCONF and YANG in our case study of OpenDaylight 
Controller in Figure 5.17 and will study them in Section 5.7.3 below.
5.7.2  The Simple Network Management Protocol (SNMP) 
and the Management Information Base (MIB)
The Simple Network Management Protocol version 3 (SNMPv3) [RFC 3410] 
is an application-layer protocol used to convey network-management control and 
information messages between a managing server and an agent executing on behalf 
of that managing server. The most common usage of SNMP is in a request-response 
mode in which an SNMP managing server sends a request to an SNMP agent, who

receives the request, performs some action, and sends a reply to the request. Typi-
cally, a request will be used to query (retrieve) or modify (set) MIB object values 
associated with a managed device. A second common usage of SNMP is for an agent 
to send an unsolicited message, known as a trap message, to a managing server. Trap 
messages are used to notify a managing server of an exceptional situation (e.g., a 
link interface going up or down) that has resulted in changes to MIB object values.
MIB objects are specified in a data description language known as SMI (Structure 
of Management Information) [RFC 2578; RFC 2579; RFC 2580], a rather oddly named 
component of the network management framework whose name gives no hint¬†of its 
functionality. A formal definition language is used to ensure that the syntax and seman-
tics of the network management data are well defined and unambiguous. Related MIB 
objects are gathered into MIB modules. As of late 2019, there are more than 400 MIB-
related RFCs and a much larger number of vendor-specific (private) MIB modules.
SNMPv3 defines seven types of messages, known generically as protocol data 
units‚ÄîPDUs‚Äîas shown in Table 5.2 and described below. The format of the PDU 
is shown in Figure 5.21.
‚Ä¢ The GetRequest, GetNextRequest, and GetBulkRequest PDUs are 
all sent from a managing server to an agent to request the value of one or more 
Table 5.2 ‚ô¶ SNMPv3 PDU types
SNMPv3 PDU Type
Sender-receiver
Description
GetRequest
manager-to-agent
get value of one or more MIB object instances
GetNextRequest
manager-to-agent
get value of next MIB object instance in list or table
GetBulkRequest
manager-to-agent
get values in large block of data, for example, values 
in a large table
InformRequest
manager-to-manager
inform remote managing entity of MIB values remote 
to its access
SetRequest
manager-to-agent
set value of one or more MIB object instances
Response
agent-to-manager or
generated in response to 
manager-to-manager
 GetRequest,  
 GetNextRequest,  
 GetBulkRequest,  
 SetRequest PDU, or  
 InformRequest
SNMPv2-Trap
agent-to-manager
inform manager of an exceptional event #

MIB objects at the agent‚Äôs managed device. The MIB objects whose values 
are being requested are specified in the variable binding portion of the PDU. 
 GetRequest, GetNextRequest, and GetBulkRequest differ in the 
granularity of their data requests. GetRequest can request an arbitrary set of 
MIB values; multiple GetNextRequests can be used to sequence through 
a list or table of MIB objects; GetBulkRequest allows a large block of 
data to be returned, avoiding the overhead incurred if multiple GetRequest 
or  GetNextRequest messages were to be sent. In all three cases, the agent 
responds with a Response PDU containing the object identifiers and their 
associated values.
‚Ä¢ The SetRequest PDU is used by a managing server to set the value of one or 
more MIB objects in a managed device. An agent replies with a Response PDU 
with the ‚ÄúnoError‚Äù error status to confirm that the value has indeed been set.
‚Ä¢ The InformRequest PDU is used by a managing server to notify another 
managing server of MIB information that is remote to the receiving server.
‚Ä¢ The Response PDU is typically sent from a managed device to the managing 
server in response to a request message from that server, returning the requested 
information.
‚Ä¢ The final type of SNMPv3 PDU is the trap message. Trap messages are gener-
ated asynchronously; that is, they are not generated in response to a received 
request but rather in response to an event for which the managing server requires 
notification. RFC 3418 defines well-known trap types that include a cold or 
warm start by a device, a link going up or down, the loss of a neighbor, or an 
authentication failure event. A received trap request has no required response 
from a managing server.
Figure 5.21 ‚ô¶ SNMP PDU format
PDU
type
(0‚Äì3)
Request
Id
Error
Status
(0‚Äì5)
Error
Index
Name
Value
Name
Name
Value
PDU
Type
(4)
Enterprise
Agent
Addr
Trap
Type
(0‚Äì7)
SpeciÔ¨Åc
code
Time
stamp
Value
Get/set header
Trap header
Trap information
SNMP PDU
Variables to get/set

Given the request-response nature of SNMP, it is worth noting here that although 
SNMP PDUs can be carried via many different transport protocols, the SNMP PDU 
is typically carried in the payload of a UDP datagram. Indeed, RFC 3417 states 
that UDP is ‚Äúthe  preferred transport mapping.‚Äù However, since UDP is an unreli-
able transport protocol, there is no guarantee that a request, or its response, will be 
received at the intended destination. The request ID field of the PDU (see Figure 5.21) is 
used by the managing server to number its requests to an agent; the agent‚Äôs response 
takes its request ID from that of the received request. Thus, the request ID field can 
be used by the managing server to detect lost requests or replies. It is up to the man-
aging server to decide whether to retransmit a request if no corresponding response 
is received after a given amount of time. In particular, the SNMP standard does not 
mandate any particular procedure for retransmission, or even if retransmission is 
to be done in the first place. It only requires that the managing server ‚Äúneeds to act 
responsibly in respect to the frequency and duration of retransmissions.‚Äù This, of 
course, leads one to wonder how a ‚Äúresponsible‚Äù protocol should act!
SNMP has evolved through three versions. The designers of SNMPv3 have said 
that ‚ÄúSNMPv3 can be thought of as SNMPv2 with additional security and admin-
istration capabilities‚Äù [RFC 3410]. Certainly, there are changes in SNMPv3 over 
SNMPv2, but nowhere are those changes more evident than in the area of administra-
tion and security. The central role of security in SNMPv3 was particularly important, 
since the lack of adequate security resulted in SNMP being used primarily for moni-
toring rather than control (for example, SetRequest is rarely used in SNMPv1). 
Once again, we see that  security‚Äîa topic we‚Äôll cover in detail in Chapter 8 ‚Äî is of 
critical concern, but once again a concern whose importance had been realized per-
haps a bit late and only then ‚Äúadded on.‚Äù
The Management Information Base (MIB)
We learned earlier that a managed device‚Äôs operational state data (and to some extent 
its configuration data) in the SNMP/MIB approach to network management are rep-
resented as objects that are gathered together into an MIB for that device. An MIB 
object might be a counter, such as the number of IP datagrams discarded at a router 
due to errors in an IP datagram header; or the number of carrier sense errors in an 
Ethernet interface card; descriptive information such as the version of the software 
running on a DNS server; status information such as whether a particular device is 
functioning correctly; or protocol-specific information such as a routing path to a 
destination. Related MIB objects are gathered into MIB modules. There are over 
400 MIB modules defined in various IETC RFC‚Äôs; there are many more device- and 
vendor-specific MIBs. [RFC 4293] specifies the MIB module that defines managed 
objects (including ipSystemStatsInDelivers) for managing implementations of the 
Internet Protocol (IP) and its associated Internet Control Message Protocol (ICMP). 
[RFC 4022] specifies the MIB module for TCP, and [RFC 4113] specifies the MIB 
module for UDP.

While MIB-related RFCs make for rather tedious and dry reading, it is nonethe-
less instructive (i.e., like eating vegetables, it is ‚Äúgood for you‚Äù) to consider an exam-
ple of a MIB object, The ipSystem-StatsInDelivers object-type definition 
from [RFC 4293] defines a 32-bit read-only counter that keeps track of the number 
of IP datagrams that were received at the managed device and were successfully 
delivered to an upper-layer protocol. In the example below, Counter32 is one of the 
basic data types defined in the SMI.
ipSystemStatsInDelivers OBJECT-TYPE
    SYNTAX Counter32
    MAX-ACCESS read-only
    STATUS current
    DESCRIPTION
           ‚ÄúThe total number of datagrams successfully de-
livered to IPuser-protocols (including ICMP).
           When tracking interface statistics, the coun-
ter of the interface to which these datagrams 
were addressed is incremented. This interface 
might not be the same as the input interface 
for some of the datagrams.
           Discontinuities in the value of this counter can  
occur at re-initialization of the management 
system, and at other times as indicated by the 
value of ipSystemStatsDiscontinuityTime.‚Äù
    ::= { ipSystemStatsEntry 18 }
5.7.3  The Network Configuration Protocol (NETCONF)  
and YANG
The NETCONF protocol operates between the managing server and the man-
aged network devices, providing messaging to (i) retrieve, set, and modify con-
figuration data at managed devices; (ii) to query operational data and statistics 
at managed devices; and (iii) to subscribe to notifications generated by managed 
devices. The managing server actively controls a managed device by sending it 
configurations, which are specified in a structured XML document, and activat-
ing a configuration at the managed device. NETCONF uses a remote procedure 
call (RPC) paradigm, where protocol messages are also encoded in XML and 
exchanged between the managing server and a managed device over a secure, 
connection-oriented session such as the TLS (Transport Layer Security) protocol 
(discussed in Chapter 8) over TCP.

Figure 5.22 shows an example NETCONF session. First, the managing server 
establishes a secure connection to the managed device. (In NETCONF parlance, the 
managing server is actually referred to as the ‚Äúclient‚Äù and the managed device as 
the ‚Äúserver,‚Äù since the managing server establishes the connection to the managed 
device. But we‚Äôll ignore that here for consistency with the longer-standing network-
management server/client terminology shown in Figure 5.20.) Once a secure con-
nection has been established, the managing server and the managed device exchange 
<hello> messages, declaring their ‚Äúcapabilities‚Äù‚ÄîNETCONF functionality that sup-
plements the base NETCONF specification in [RFC 6241]. Interactions between the 
managing server and managed device take the form of a remote procedure call, using 
the <rpc> and <rpc-response> messages. These messages are used to retrieve, set, 
query and modify device configurations, operational data and statistics, and to sub-
scribe to device notifications. Device notifications themselves are proactively sent 
from managed device to the managing server using NETCONF <notification> mes-
sages. A session is closed with the <session-close message>.
Figure 5.22 ‚ô¶  NETCONF session between managing server/controller 
and managed device
Session initiation,
capabilities exchange: <hello>
Session close:  <close-session>
<rpc>
<rpc-reply>
<notiÔ¨Åcation>
Agent
Device
data
Managing
server/controller
<rpc>
<rpc>
<rpc-reply>
<rpc-reply>
conÔ¨Åg. and
operational data
store

Table 5.3 shows a number of the important NETCONF operations that a man-
aging server can perform at a managed device. As in the case of SNMP, we see 
operations for retrieving operational state data (<get>), and for event notification. 
However, the <get-config>, <edit-config>, <lock> and <unlock> operation demon-
strate NETCONF‚Äôs particular emphasis on device configuration. Using the basic 
operations shown in Table 5.3, it is also possible to create a set of more sophisticated 
network management transactions that either complete atomically (i.e., as a group) 
and successfully on a set of devices, or are fully reversed and leave the devices in 
their pre-transaction state. Such multi-device transactions‚Äî‚Äúenabl[ing] operators to 
concentrate on the configuration of the network as a whole rather than individual 
devices‚Äù was an important operator requirement put forth in [RFC 3535].
A full description of NETCONF is beyond our scope here; [RFC 6241, RFC 
5277, Claise 2019; Schonwalder 2010] provide more in-depth coverage.
But since this is the first time we‚Äôve seen protocol messages formatted as an 
XML document (rather than the traditional message with header fields and message 
body, e.g., as shown in Figure 5.21 for the SNMP PDU), let‚Äôs conclude our brief 
study of NETCONF with two examples.¬†
In the first example, the XML document sent from the managing server to the 
managed device is a NETCONF <get> command requesting all device configuration 
Table 5.3 ‚ô¶  Selected NETCONF operations
NETCONF Operation
Description
<get-config>
Retrieve all or part of a given configuration. A device may have multiple 
configurations. There is always a running/ configuration that describes 
the devices current (running) configuration.
<get>
Retrieve all or part of both configuration state and operational state data.
<edit-config>
Change all or part of a specified configuration at the managed device. If 
the running/configuration is specified, then the device‚Äôs current (running) 
configuration will be changed. If the managed device was able to satisfy the 
request, an <rpc-reply> is sent containing an <ok> element; otherwise <rpc-
error> response is returned. On error, the device‚Äôs configuration state can be 
roll-ed-back to its previous state.
<lock>, <unlock>
The <lock> (<unlock>) operation allows the managing server to lock 
(unlock) the entire configuration datastore system of a managed device. 
Locks are intended to be short-lived and allow a client to make a change 
without fear of interaction with other NETCONF, SNMP, or CLIs commands 
from other sources.
<create-subscription> ,
<notification>
This operation initiates an event notification subscription that will send 
asynchronous event <notification> for specified events of interest from the 
managed device to the managing server, until the subscription is terminated.

and operational data. With this command, the server can learn about the device‚Äôs 
configuration.
 
01 <?xml version=‚Äù1.0‚Äù encoding=‚ÄùUTF-8‚Äù?>
 
02 <rpc message-id=‚Äù101‚Äù
 
03 
xmlns=‚Äùurn:ietf:params:xml:ns:netconf:base:1.0‚Äù>
 
04 <get/>
 
05 </rpc>
Although few people can completely parse XML directly, we see that the NET-
CONF command is relatively human-readable, and is much more reminiscent of HTTP 
and HTML than the protocol message formats that we saw for SNMP PDU format in 
Figure 5.21. The RPC message itself spans lines 02‚Äì05 (we have added line numbers 
here for pedagogical purposes). The RPC has a message ID value of 101, declared in 
line 02, and contains a single NETCONF <get> command. The reply from the device 
contains a matching ID number (101), and all of the device‚Äôs configuration data (in 
XML format, of course), starting in line 04, ultimately with a closing </rpc-reply>.
 
01 <?xml version=‚Äù1.0‚Äù encoding=‚ÄùUTF-8‚Äù?>
 
02 <rpc-reply message-id=‚Äù101‚Äù
 
03 
xmlns=‚Äùurn:ietf:params:xml:ns:netconf:base:1.0‚Äù>
 
04 <!--¬†.¬†.¬†.¬†all conÔ¨Åguration data returned... -->
 
¬†.¬†.¬†.¬†
 
</rpc-reply>
In the second example below, adapted from [RFC 6241], the XML document 
sent from the managing server to the managed device sets the Maximum Transmis-
sion Unit (MTU) of an interface named ‚ÄúEthernet0/0‚Äù to 1500 bytes:
 
01 <?xml version=‚Äù1.0‚Äù encoding=‚ÄùUTF-8‚Äù?>
 
02 <rpc message-id=‚Äù101‚Äù
 
03 
xmlns=‚Äùurn:ietf:params:xml:ns:netconf:base:1.0‚Äù>
 
04 
<edit-conÔ¨Åg>
 
05 
<target>
 
06 
<running/>
 
07 
</target>
 
08 
<conÔ¨Åg>
 
09 
 <top xmlns=‚Äùhttp://example.com/schema/ 
1.2/conÔ¨Åg‚Äù>
 
10 
<interface>
 
11 
<name>Ethernet0/0</name>
 
12 
<mtu>1500</mtu>
 
13 
</interface>
 
14 
</top>
 
15 
</conÔ¨Åg>
 
16 
</edit-conÔ¨Åg>
 
17 </rpc>

The RPC message itself spans lines 02‚Äì17, has a message ID value of 101, and 
contains a single NETCONF <edit-config> command, spanning lines 04‚Äì15. Line 
06 indicates that the running device configuration at the managed device will be 
changed. Lines 11 and 12 specify the MTU size to be set of the Ethernet0/0 interface.
Once the managed device has changed the interface‚Äôs MTU size in the configu-
ration, it responds back to the managing server with an OK reply (line 04 below), 
again within an XML document:
 
01 <?xml version=‚Äù1.0‚Äù encoding=‚ÄùUTF-8‚Äù?>
 
02 <rpc-reply message-id=‚Äù101‚Äù
 
03 
xmlns=‚Äùurn:ietf:params:xml:ns:netconf:base:1.0‚Äù>
 
04 
<ok/>
 
05 
</rpc-reply>
YANG
YANG is the data modeling language used to precisely specify the structure, syntax, 
and semantics of network management data used by NETCONF, in much the same 
way that the SMI is used to specify MIBs in SNMP. All YANG definitions are con-
tained in modules, and an XML document describing a device and its capabilities can 
be generated from a YANG module.
YANG features a small set of built-in data types (as in the case of SMI) and also 
allows data modelers to express constraints that must be satisfied by a valid NET-
CONF configuration‚Äîa powerful aid in helping ensure that NETCONF configura-
tions satisfy specified correctness and consistency constraints. YANG is also used to 
specify NETCONF notifications.
A fuller discussion of YANG is beyond our scope here. For more information, 
we refer the interested reader to the excellent book [Claise 2019].
5.8 Summary
We have now completed our two-chapter journey into the network core‚Äîa journey 
that began with our study of the network layer‚Äôs data plane in Chapter 4 and finished 
here with our study of the network layer‚Äôs control plane. We learned that the control 
plane is the network-wide logic that controls not only how a datagram is forwarded 
among routers along an end-to-end path from the source host to the destination host, 
but also how network-layer components and services are configured and managed.
We learned that there are two broad approaches towards building a control plane: 
traditional per-router control (where a routing algorithm runs in each and every router 
and the routing component in the router communicates with the routing components in 
other routers) and software-defined networking (SDN) control (where a logically cen-
tralized controller computes and distributes the forwarding tables to be used by each 
and every router). We studied two fundamental routing algorithms for computing least 
cost paths in a graph‚Äîlink-state routing and distance-vector routing‚Äîin Section 5.2;

these algorithms find application in both per-router control and in SDN control. These 
algorithms are the basis for two widely deployed Internet routing protocols, OSPF and 
BGP, that we covered in Sections 5.3 and 5.4. We covered the SDN approach to the 
network-layer control plane in Section 5.5, investigating SDN network-control appli-
cations, the SDN controller, and the OpenFlow protocol for communicating between 
the controller and SDN-controlled devices. In Sections 5.6 and 5.7, we covered some 
of the nuts and bolts of managing an IP network: ICMP (the Internet Control Message 
Protocol) and network management using SNMP and NETCONF/YANG.
Having completed our study of the network layer, our journey now takes us 
one step further down the protocol stack, namely, to the link layer. Like the network 
layer, the link layer is part of each and every network-connected device. But we will 
see in the next chapter that the link layer has the much more localized task of moving 
packets between nodes on the same link or LAN. Although this task may appear on 
the surface to be rather simple compared with that of the network layer‚Äôs tasks, we 
will see that the link layer involves a number of important and fascinating issues that 
can keep us busy for a long time.
Homework Problems and Questions
Chapter 5 Review Questions
SECTION 5.1
 R1. What is meant by a control plane that is based on per-router control? In such 
cases, when we say the network control and data planes are implemented 
‚Äúmonolithically,‚Äù what do we mean?
 R2. What is meant by a control plane that is based on logically centralized 
control? In such cases, are the data plane and the control plane implemented 
within the same device or in separate devices? Explain.
SECTION 5.2
 R3. Compare and contrast the properties of a centralized and a distributed routing 
algorithm. Give an example of a routing protocol that takes a centralized and 
a decentralized approach.
 R4. Compare and contrast link-state and distance-vector routing algorithms.
 R5. What is the ‚Äúcount to infinity‚Äù problem in distance vector routing?
 R6. Is it necessary that every autonomous system use the same intra-AS routing 
algorithm? Why or why not?
SECTIONS 5.3‚Äì5.4
 R7. Why are different inter-AS and intra-AS protocols used in the Internet?
 R8. True or false: When an OSPF route sends its link state information, it is sent 
only to those nodes directly attached neighbors. Explain.

R9. What is meant by an area in an OSPF autonomous system? Why was the 
concept of an area introduced?
 R10. Define and contrast the following terms: subnet, prefix, and BGP route.
 R11. How does BGP use the NEXT-HOP attribute? How does it use the AS-PATH 
attribute?
 R12. Describe how a network administrator of an upper-tier ISP can implement 
policy when configuring BGP.
 R13. True or false: When a BGP router receives an advertised path from its neigh-
bor, it must add its own identity to the received path and then send that new 
path on to all of its neighbors. Explain.
SECTION 5.5
 R14. Describe the main role of the communication layer, the network-wide state- 
management layer, and the network-control application layer in an SDN 
controller.
 R15. Suppose you wanted to implement a new routing protocol in the SDN control 
plane. At which layer would you implement that protocol? Explain.
 R16. What types of messages flow across an SDN controller‚Äôs northbound and 
southbound APIs? Who is the recipient of these messages sent from the 
controller across the southbound interface, and who sends messages to the 
controller across the northbound interface?
 R17. Describe the purpose of two types of OpenFlow messages (of your choosing) 
that are sent from a controlled device to the controller. Describe the purpose 
of two types of Openflow messages (of your choosing) that are send from the 
controller to a controlled device.
 R18. What is the purpose of the service abstraction layer in the OpenDaylight SDN 
controller?
SECTIONS 5.6‚Äì5.7
 R19. Names four different types of ICMP messages
 R20. What two types of ICMP messages are received at the sending host executing 
the Traceroute program?
 R21. Define the following terms in the context of SNMP: managing server, 
 managed device, network management agent and MIB.
 R22. What are the purposes of the SNMP GetRequest and SetRequest messages?
 R23. What is the purpose of the SNMP trap message?

Problems
 P1. Looking at Figure 5.3, enumerate the paths from y to u that do not contain 
any loops.
 P2. Repeat Problem P1 for paths from x to z, z to u, and z to w.
 P3. Consider the following network. With the indicated link costs, use Dijkstra‚Äôs 
shortest-path algorithm to compute the shortest path from x to all network nodes. 
Show how the algorithm works by computing a table similar to Table 5.1. 
x
v
t
y
z
u
w
6
12
8
7
8
3
6
4
3
2
4
3
 P4. Consider the network shown in Problem P3. Using Dijkstra‚Äôs algorithm, and 
showing your work using a table similar to Table 5.1, do the following:
a. Compute the shortest path from t to all network nodes.
b. Compute the shortest path from u to all network nodes.
c. Compute the shortest path from v to all network nodes.
d. Compute the shortest path from w to all network nodes.
e. Compute the shortest path from y to all network nodes.
f. Compute the shortest path from z to all network nodes.
 P5. Consider the network shown below, and assume that each node initially 
knows the costs to each of its neighbors. Consider the distance-vector algo-
rithm and show the distance table entries at node z.
u
z
v
y
2
3
6
2
3
1
x
Dijkstra‚Äôs algorithm: 
discussion and example
VideoNote

P6. Consider a general topology (that is, not the specific network shown above) and a 
synchronous version of the distance-vector algorithm. Suppose that at each itera-
tion, a node exchanges its distance vectors with its neighbors and receives their 
distance vectors. Assuming that the algorithm begins with each node knowing 
only the costs to its immediate neighbors, what is the maximum number of itera-
tions required before the distributed algorithm converges? Justify your answer.
 P7. Consider the network fragment shown below. x has only two attached neigh-
bors, w and y. w has a minimum-cost path to destination u (not shown) of 5, 
and y has a minimum-cost path to u of 6. The complete paths from w and y 
to u (and between w and y) are not shown. All link costs in the network have 
strictly positive integer values.
x
y
w
2
2
5
a. Give x‚Äôs distance vector for destinations w, y, and u.
b. Give a link-cost change for either c(x,w) or c(x,y) such that x will inform 
its neighbors of a new minimum-cost path to u as a result of executing the 
distance-vector algorithm.
c. Give a link-cost change for either c(x,w) or c(x,y) such that x will not 
inform its neighbors of a new minimum-cost path to u as a result of 
executing the distance-vector algorithm.
 P8. Consider the three-node topology shown in Figure 5.6. Rather than having 
the link costs shown in Figure 5.6, the link costs are c(x,y) = 3, c(y,z) = 6, 
c(z,x) = 4. Compute the distance tables after the initialization step and after 
each iteration of a synchronous version of the distance-vector algorithm (as 
we did in our earlier discussion of Figure 5.6).
 P9. Consider the count-to-infinity problem in the distance vector routing. Will 
the count-to-infinity problem occur if we decrease the cost of a link? Why? 
How about if we connect two nodes which do not have a link?
 P10. Argue that for the distance-vector algorithm in Figure 5.6, each value in the 
distance vector D(x) is non-increasing and will eventually stabilize in a finite 
number of steps.
 P11. Consider Figure 5.7. Suppose there is another router w, connected to router 
y and z. The costs of all links are given as follows: c(x,y) = 4, c(x,z) = 50, 
c(y,w) = 1, c(z,w) = 1, c(y,z) = 3. Suppose that poisoned reverse is used in 
the distance-vector routing algorithm.

a. When the distance vector routing is stabilized, router w, y, and z inform their 
distances to x to each other. What distance values do they tell each other?
b. Now suppose that the link cost between x and y increases to 60. Will there be 
a¬†count-to-infinity problem even if poisoned reverse is used? Why or why not? 
If there is a count-to-infinity problem, then how many iterations are needed for 
the distance-vector routing to reach a stable state again? Justify your answer.
c. How do you modify c(y,z) such that there is no count-to-infinity problem 
at all if c(y,x) changes from 4 to 60?
 P12. Describe how loops in paths can be detected in BGP.
 P13. Will a BGP router always choose the loop-free route with the shortest ASpath 
length? Justify your answer.
 P14. Consider the network shown below. Suppose AS3 and AS2 are running 
OSPF for their intra-AS routing protocol. Suppose AS1 and AS4 are running 
RIP for their intra-AS routing protocol. Suppose eBGP and iBGP are used 
for the inter-AS routing protocol. Initially suppose there is no physical link 
between AS2 and AS4.
a. Router 3c learns about prefix x from which routing protocol: OSPF, RIP, 
eBGP, or iBGP?
b. Router 3a learns about x from which routing protocol?
c. Router 1c learns about x from which routing protocol?
d. Router 1d learns about x from which routing protocol?
AS4
AS3
AS1
AS2
x
4b
4c
4a
3c
3b
3a
1c
1b
1d
1a
I1
I2
2c
2a
2b

P15. Referring to the previous problem, once router 1d learns about x it will put an 
entry (x, I) in its forwarding table.
a. Will I be equal to I1 or I2 for this entry? Explain why in one sentence.
b. Now suppose that there is a physical link between AS2 and AS4, shown 
by the dotted line. Suppose router 1d learns that x is accessible via AS2 as 
well as via AS3. Will I be set to I1 or I2? Explain why in one sentence.
c. Now suppose there is another AS, called AS5, which lies on the path 
between AS2 and AS4 (not shown in diagram). Suppose router 1d learns 
that x is accessible via AS2 AS5 AS4 as well as via AS3 AS4. Will I be 
set to I1 or I2? Explain why in one sentence.
P16. Consider the following network. ISP B provides national backbone service 
to regional ISP A. ISP C provides national backbone service to regional 
ISP D. Each ISP consists of one AS. B and C peer with each other in two 
places using BGP. Consider traffic going from A to D. B would prefer 
to hand that traffic over to C on the West Coast (so that C would have 
to absorb the cost of carrying the traffic cross-country), while C would 
prefer to get the traffic via its East Coast peering point with B (so that B 
would have carried the traffic across the country). What BGP mechanism 
might C use, so that B would hand over A-to-D traffic at its East Coast 
peering point? To answer this question, you will need to dig into the BGP 
 specification.
ISP B
ISP C
ISP D
ISP A

P17. In Figure 5.13, consider the path information that reaches stub networks W, 
X, and Y. Based on the information available at W and X, what are their 
respective views of the network topology? Justify your answer. The topology 
view at Y is shown below.
W
Y
X
A
C
Stub network
Y‚Äôs view of
the topology
 P18. Consider Figure 5.13. B would never forward traffic destined to Y via X based 
on BGP routing. But there are some very popular applications for which data 
packets go to X first and then flow to Y. Identify one such application, and 
describe how data packets follow a path not given by BGP routing.
 P19. In Figure 5.13, suppose that there is another stub network V that is a cus-
tomer of ISP A. Suppose that B and C have a peering relationship, and A is 
a customer of both B and C. Suppose that A would like to have the traffic 
destined to W to come from B only, and the traffic destined to V from either 
B or C. How should A advertise its routes to B and C? What AS routes does 
C receive?
 P20. Suppose ASs X and Z are not directly connected but instead are connected 
by AS Y. Further suppose that X has a peering agreement with Y, and that Y 
has a peering agreement with Z. Finally, suppose that Z wants to transit all 
of Y‚Äôs traffic but does not want to transit X‚Äôs traffic. Does BGP allow Z to 
 implement this policy?
 P21. Consider the two ways in which communication occurs between a managing 
entity and a managed device: request-response mode and trapping. What are 
the pros and cons of these two approaches, in terms of (1) overhead, (2) noti-
fication time when exceptional events occur, and (3) robustness with respect 
to lost messages between the managing entity and the device?
 P22. In Section 5.7, we saw that it was preferable to transport SNMP messages in 
unreliable UDP datagrams. Why do you think the designers of SNMP chose 
UDP rather than TCP as the transport protocol of choice for SNMP?
Socket Programming Assignment 5: ICMP Ping
At the end of Chapter 2, there are four socket programming assignments. Here 
you¬†will find a fifth assignment which employs ICMP, a protocol discussed in this 
chapter.

Ping is a popular networking application used to test from a remote location 
whether a particular host is up and reachable. It is also often used to measure latency 
between the client host and the target host. It works by sending ICMP ‚Äúecho request‚Äù 
packets (i.e., ping packets) to the target host and listening for ICMP ‚Äúecho response‚Äù 
replies (i.e., pong packets). Ping measures the RRT, records packet loss, and calcu-
lates a statistical summary of multiple ping-pong exchanges (the minimum, mean, 
max, and standard deviation of the round-trip times).
In this lab, you will write your own Ping application in Python. Your application 
will use ICMP. But in order to keep your program simple, you will not exactly fol-
low the official specification in RFC 1739. Note that you will only need to write the 
client side of the program, as the functionality needed on the server side is built into 
almost all operating systems. You can find full details of this assignment, as well as 
important snippets of the Python code, at the Web site http://www.pearsonhighered.
com/cs-resources.
Programming Assignment: Routing
In this programming assignment, you will be writing a ‚Äúdistributed‚Äù set of proce-
dures that implements a distributed asynchronous distance-vector routing for the 
network shown below.
You are to write the following routines that will ‚Äúexecute‚Äù asynchronously 
within the emulated environment provided for this assignment. For node 0, you will 
write the routines:
3
2
0
1
7
3
1
2
1
‚Ä¢ rtinit0(). This routine will be called once at the beginning of the emulation. 
rtinit0() has no arguments. It should initialize your distance table in node 0 to 
reflect the direct costs of 1, 3, and 7 to nodes 1, 2, and 3, respectively. In the 
figure above, all links are bidirectional and the costs in both directions are identi-
cal. After initializing the distance table and any other data structures needed by 
your node 0 routines, it should then send its directly connected neighbors (in this 
case, 1, 2, and 3) the cost of its minimum-cost paths to all other network nodes.

This minimum-cost information is sent to neighboring nodes in a routing update 
packet by calling the routine tolayer2(), as described in the full assignment. The 
format of the routing update packet is also described in the full assignment.
‚Ä¢ rtupdate0(struct rtpkt *rcvdpkt). This routine will be called when node 0 receives 
a routing packet that was sent to it by one of its directly connected neighbors. 
The parameter *rcvdpkt is a pointer to the packet that was received. rtupdate0() 
is the ‚Äúheart‚Äù of the distance-vector algorithm. The values it receives in a routing 
update packet from some other node i contain i‚Äôs current shortest-path costs to 
all other network nodes. rtupdate0() uses these received values to update its own 
distance table (as specified by the distance-vector algorithm). If its own minimum 
cost to another node changes as a result of the update, node 0 informs its directly 
connected neighbors of this change in minimum cost by sending them a rout-
ing packet. Recall that in the distance-vector algorithm, only directly connected 
nodes will exchange routing packets. Thus, nodes 1 and 2 will communicate with 
each other, but nodes 1 and 3 will not communicate with each other.
Similar routines are defined for nodes 1, 2, and 3. Thus, you will write eight pro-
cedures in all: rtinit0(), rtinit1(), rtinit2(), rtinit3(), rtupdate0(), rtupdate1(), rtup-
date2(), and rtupdate3(). These routines will together implement a distributed, 
asynchronous computation of the distance tables for the topology and costs shown in 
the figure on the preceding page.
You can find the full details of the programming assignment, as well as C code 
that you will need to create the simulated hardware/software environment, at http://
www.pearsonhighered.com/cs-resource. A Java version of the assignment is also 
available.
Wireshark Lab: ICMP
In the Web site for this textbook, www.pearsonhighered.com/cs-resources, you‚Äôll 
find a Wireshark lab assignment that examines the use of the ICMP protocol in the 
ping and traceroute commands.

Please describe one or two of the most exciting projects you have worked on during your 
career. What were the biggest challenges?
When I was a researcher at AT&T, a group of us designed a new way to manage rout-
ing in Internet Service Provider backbone networks. Traditionally, network operators 
configure each router individually, and these routers run distributed protocols to compute 
paths through the network. We believed that network management would be simpler and 
more flexible if network operators could exercise direct control over how routers forward 
traffic based on a network-wide view of the topology and traffic. The Routing Control 
Platform (RCP) we designed and built could compute the routes for all of AT&T‚Äôs 
Jennifer Rexford is a Professor in the Computer Science department 
at Princeton University. Her research has the broad goal of making 
computer networks easier to design and manage, with particular 
emphasis on programmable neworks. From 1996‚Äì2004, she  
was a member of the Network Management and Performance 
department at AT&T Labs‚ÄìResearch. While at AT&T, she designed 
techniques and tools for network measurement, traffic engineering, 
and router configuration that were deployed in AT&T‚Äôs backbone 
network. Jennifer is co-author of the book ‚ÄúWeb Protocols and 
Practice: Networking Protocols, Caching, and Traffic Measurement,‚Äù 
published by Addison-Wesley in May 2001. She served as the 
chair of ACM SIGCOMM from 2003 to 2007. She received her 
BSE degree in electrical engineering from Princeton University in 
1991, and her PhD degree in electrical engineering and computer 
science from the University of Michigan in 1996. Jennifer was the 
2004 winner of ACM‚Äôs Grace Murray Hopper Award for outstand-
ing young computer professional, the ACM Athena Lecturer Award 
(2016), the NCWIT Harrold and Notkin Research and Graduate 
Mentoring Award (2017), the ACM SIGCOMM award for lifetime 
contributions (2018), and the IEEE Internet Award (2019). She is 
an ACM Fellow (2008), an IEEE Fellow (2018), and the National 
Academy of Engineering (2014).
Jennifer Rexford
AN INTERVIEW WITH‚Ä¶
Courtesy of Jennifer Rexford

backbone on a single commodity computer, and could control legacy routers without 
modification. To me, this project was exciting because we had a provocative idea, a 
working system, and ultimately a real deployment in an operational network. Fast forward 
a few years, and software-defined networking (SDN) has become a mainstream technology, 
and standard protocols (like standard protocols (like OpenFlow) and languages (like P4) 
have made it much easier to tell the underlying switches what to do.
How do you think software-defined networking should evolve in the future?
In a major break from the past, the software controlling network devices can be created by 
many different programmers, not just at companies selling network equipment. Yet, unlike 
the applications running on a server or a smart phone, SDN applications must work together 
to handle the same traffic. Network operators do not want to perform load balancing on 
some traffic and routing on other traffic; instead, they want to perform load balancing and 
routing, together, on the same traffic. Future SDN platforms should offer good program-
ming abstractions for composing independently written multiple applications together. More 
broadly, good programming abstractions can make it easier to create applications, without 
having to worry about low-level details like flow table entries, traffic counters, bit patterns 
in packet headers, and so on. Also, while an SDN controller is logically centralized, the 
network still consists of a distributed collection of devices. Future programmable networks 
should offer good abstractions for updating a distributed set of devices, so network admin-
istrators can reason about what happens to packets in flight while the devices are updated. 
Programming abstractions for programmable network is an exciting area for interdisciplinary 
research between computer networking, distributed systems, and programming languages, 
with a real chance for practical impact in the years ahead.
Where do you see the future of networking and the Internet?
Networking is an exciting field because the applications and the underlying technologies 
change all the time. We are always reinventing ourselves! Who would have predicted even 
ten years ago the dominance of smart phones, allowing mobile users to access existing 
applications as well as new location-based services? The emergence of cloud computing is 
fundamentally changing the relationship between users and the applications they run, and 
networked sensors and actuators (the ‚ÄúInternet of Things‚Äù) are enabling a wealth of new 
applications (and security vulnerabilities!). The pace of innovation is truly inspiring.
The underlying network is a crucial component in all of these innovations. Yet, the 
network is notoriously ‚Äúin the way‚Äù‚Äîlimiting performance, compromising reliability, con-
straining applications, and complicating the deployment and management of services. We 
should strive to make the network of the future as invisible as the air we breathe, so it never 
stands in the way of new ideas and valuable services. To do this, we need to raise the level 
of abstraction above individual network devices and protocols (and their attendant acro-
nyms!), so we can reason about the network and the user‚Äôs high-level goals as a whole.

What people inspired you professionally?
I‚Äôve long been inspired by Sally Floyd who worked for many years at the International 
Computer Science Institute. Her research was always purposeful, focusing on the important 
challenges facing the Internet. She dug deeply into hard questions until she understood the 
problem and the space of solutions completely, and she devoted serious energy into ‚Äúmak-
ing things happen,‚Äù such as pushing her ideas into protocol standards and network equip-
ment. Also, she gave back to the community, through professional service in numerous 
standards and research organizations and by creating tools (such as the widely used ns-2 and 
ns-3 simulators) that enable other researchers to succeed. She retired in 2009, and passed 
away in 2019, but her influence on the field will be felt for years to come.
What are your recommendations for students who want careers in computer science and 
networking?
Networking is an inherently interdisciplinary field. Applying techniques from other disci-
pline‚Äôs breakthroughs in networking come from such diverse areas as queuing theory, game 
theory, control theory, distributed systems, network optimization, programming languages, 
machine learning, algorithms, data structures, and so on. I think that becoming conversant 
in a related field, or collaborating closely with experts in those fields, is a wonderful way 
to put networking on a stronger foundation, so we can learn how to build networks that are 
worthy of society‚Äôs trust. Beyond the theoretical disciplines, networking is exciting because 
we create real artifacts that real people use. Mastering how to design and build systems‚Äîby 
gaining experience in operating systems, computer architecture, and so on‚Äîis another fan-
tastic way to amplify your knowledge of networking to help make the world a better place.

nication service between any two network hosts. Between the two hosts, datagrams 
travel over a series of communication links, some wired and some wireless, starting 
at the source host, passing through a series of packet switches (switches and routers) 
and ending at the destination host. As we continue down the protocol stack, from 
the network layer to the link layer, we naturally wonder how packets are sent across 
the individual links that make up the end-to-end communication path. How are the 
network-layer datagrams encapsulated in the link-layer frames for transmission over 
a single link? Are different link-layer protocols used in the different links along the 
communication path? How are transmission conflicts in broadcast links resolved? 
Is there addressing at the link layer and, if so, how does the link-layer addressing 
operate with the network-layer addressing we learned about in Chapter 4? And what 
exactly is the difference between a switch and a router? We‚Äôll answer these and other 
important questions in this chapter.
In discussing the link layer, we‚Äôll see that there are two fundamentally  different 
types of link-layer channels. The first type are broadcast channels, which connect 
multiple hosts in wireless LANs, in satellite networks, and in hybrid fiber-coaxial 
cable (HFC) access networks. Since many hosts are connected to the same broadcast 
communication channel, a so-called medium access protocol is needed to coordinate 
frame transmission. In some cases, a central controller may be used to coordinate 
The Link Layer 
and LANs
6
CHAPTER
449

transmissions; in other cases, the hosts themselves coordinate transmissions. The 
second type of link-layer channel is the point-to-point communication link, such as 
that often found between two routers connected by a long-distance link, or between 
a user‚Äôs office computer and the nearby Ethernet switch to which it is connected. 
Coordinating access to a point-to-point link is simpler; the reference material on this 
book‚Äôs Web site has a detailed discussion of the Point-to-Point Protocol (PPP), which 
is used in settings ranging from dial-up service over a telephone line to high-speed 
point-to-point frame transport over fiber-optic links.
We‚Äôll explore several important link-layer concepts and technologies in this 
 chapter. We‚Äôll dive deeper into error detection and correction, a topic we touched on 
briefly in Chapter 3. We‚Äôll consider multiple access networks and switched LANs, 
including Ethernet‚Äîby far the most prevalent wired LAN technology. We‚Äôll also 
look at virtual LANs, and data center networks. Although WiFi, and more generally 
wireless LANs, are link-layer topics, we‚Äôll postpone our study of these important 
topics until Chapter 7.
6.1 Introduction to the Link Layer
Let‚Äôs begin with some important terminology. We‚Äôll find it convenient in this chapter 
to refer to any device that runs a link-layer (i.e., layer 2) protocol as a node. Nodes 
include hosts, routers, switches, and WiFi access points (discussed in Chapter 7). 
We will also refer to the communication channels that connect adjacent nodes along 
the communication path as links. In order for a datagram to be transferred from 
source host to destination host, it must be moved over each of the individual links 
in the end-to-end path. As an example, in the company network shown at the bot-
tom of Figure 6.1, consider sending a datagram from one of the wireless hosts to 
one of the servers. This datagram will actually pass through six links: a WiFi link 
between sending host and WiFi access point, an Ethernet link between the access 
point and a link-layer switch; a link between the link-layer switch and the router, a 
link between the two routers; an Ethernet link between the router and a link-layer 
switch; and finally an Ethernet link between the switch and the server. Over a given 
link, a transmitting node encapsulates the datagram in a link-layer frame and trans-
mits the frame into the link.
In order to gain further insight into the link layer and how it relates to the 
 network layer, let‚Äôs consider a transportation analogy. Consider a travel agent who 
is planning a trip for a tourist traveling from Princeton, New Jersey, to Lausanne, 
Switzerland. The travel agent decides that it is most convenient for the tourist to take 
a limousine from Princeton to JFK airport, then a plane from JFK airport to Geneva‚Äôs 
airport, and finally a train from Geneva‚Äôs airport to Lausanne‚Äôs train station. Once

Figure 6.1 ‚ô¶ Six link-layer hops between wireless host and server
the travel agent makes the three reservations, it is the responsibility of the Princeton 
limousine company to get the tourist from Princeton to JFK; it is the responsibility of 
the airline company to get the tourist from JFK to Geneva; and it is the responsibility 
of the Swiss train service to get the tourist from Geneva to Lausanne. Each of the 
three segments of the trip is ‚Äúdirect‚Äù between two ‚Äúadjacent‚Äù locations. Note that the 
three transportation segments are managed by different companies and use entirely

different transportation modes (limousine, plane, and train). Although the transporta-
tion modes are different, they each provide the basic service of moving passengers 
from one location to an adjacent location. In this transportation analogy, the tourist is 
a datagram, each transportation segment is a link, the transportation mode is a link-
layer protocol, and the travel agent is a routing protocol.
6.1.1 The Services Provided by the Link Layer
Although the basic service of any link layer is to move a datagram from one node 
to an adjacent node over a single communication link, the details of the provided 
service can vary from one link-layer protocol to the next. Possible services that can 
be offered by a link-layer protocol include:
‚Ä¢ Framing. Almost all link-layer protocols encapsulate each network-layer data-
gram within a link-layer frame before transmission over the link. A frame consists 
of a data field, in which the network-layer datagram is inserted, and a number of 
header fields. The structure of the frame is specified by the link-layer protocol. 
We‚Äôll see several different frame formats when we examine specific link-layer 
protocols in the second half of this chapter.
‚Ä¢ Link access. A medium access control (MAC) protocol specifies the rules by 
which a frame is transmitted onto the link. For point-to-point links that have 
a single sender at one end of the link and a single receiver at the other end of 
the link, the MAC protocol is simple (or nonexistent)‚Äîthe sender can send a 
frame whenever the link is idle. The more interesting case is when multiple 
nodes share a single broadcast link‚Äîthe so-called multiple access problem. 
Here, the MAC protocol serves to coordinate the frame transmissions of the 
many nodes.
‚Ä¢ Reliable delivery. When a link-layer protocol provides reliable delivery service, 
it guarantees to move each network-layer datagram across the link without error. 
Recall that certain transport-layer protocols (such as TCP) also provide a reliable 
delivery service. Similar to a transport-layer reliable delivery service, a link-layer 
reliable delivery service can be achieved with acknowledgments and retransmis-
sions (see Section 3.4). A link-layer reliable delivery service is often used for 
links that are prone to high error rates, such as a wireless link, with the goal of 
correcting an error locally‚Äîon the link where the error occurs‚Äîrather than forc-
ing an end-to-end retransmission of the data by a transport- or application-layer 
protocol. However, link-layer reliable delivery can be considered an unnecessary 
overhead for low bit-error links, including fiber, coax, and many twisted-pair 
copper links. For this reason, many wired link-layer protocols do not provide a 
reliable delivery service.
‚Ä¢ Error detection and correction. The link-layer hardware in a receiving node 
can incorrectly decide that a bit in a frame is zero when it was transmitted as

a one, and vice versa. Such bit errors are introduced by signal attenuation and 
electromagnetic noise. Because there is no need to forward a datagram that 
has an error, many link-layer protocols provide a mechanism to detect such 
bit errors. This is done by having the transmitting node include error-detection 
bits in the frame, and having the receiving node perform an error check. Recall 
from Chapters 3 and 4 that the Internet‚Äôs transport layer and network layer 
also provide a limited form of error detection‚Äîthe Internet checksum. Error 
detection in the link layer is usually more sophisticated and is implemented in 
hardware. Error correction is similar to error detection, except that a receiver 
not only detects when bit errors have occurred in the frame but also deter-
mines exactly where in the frame the errors have occurred (and then corrects 
these errors).
6.1.2 Where Is the Link Layer Implemented?
Before diving into our detailed study of the link layer, let‚Äôs conclude this introduc-
tion by considering the question of where the link layer is implemented. Is a host‚Äôs 
link layer implemented in hardware or software? Is it implemented on a separate card 
or chip, and how does it interface with the rest of a host‚Äôs hardware and operating 
system components?
Figure 6.2 shows a typical host architecture. The Ethernet capabilities are either 
integrated into the motherboard chipset or implemented via a low-cost dedicated 
Ethernet chip. For the most part, the link layer is implemented on a chip called the 
network adapter, also sometimes known as a network interface controller (NIC). 
The network adapter implements many link layer services including framing, link 
access, error detection, and so on. Thus, much of a link-layer controller‚Äôs functional-
ity is implemented in hardware. For example, Intel‚Äôs 700 series adapters [Intel 2020] 
implements the Ethernet protocols we‚Äôll study in Section 6.5; the Atheros AR5006 
[Atheros 2020] controller implements the 802.11 WiFi protocols we‚Äôll study in 
Chapter 7.
On the sending side, the controller takes a datagram that has been created and 
stored in host memory by the higher layers of the protocol stack, encapsulates the 
datagram in a link-layer frame (filling in the frame‚Äôs various fields), and then trans-
mits the frame into the communication link, following the link-access protocol. On 
the receiving side, a controller receives the entire frame, and extracts the network-
layer datagram. If the link layer performs error detection, then it is the sending con-
troller that sets the error-detection bits in the frame header and it is the receiving 
controller that performs error detection.
Figure 6.2 shows that while most of the link layer is implemented in hardware, 
part of the link layer is implemented in software that runs on the host‚Äôs CPU. The 
software components of the link layer implement higher-level link-layer func-
tionality such as assembling link-layer addressing information and activating the

controller hardware. On the receiving side, link-layer software responds to con-
troller interrupts (for example, due to the receipt of one or more frames), handling 
error conditions and passing a datagram up to the network layer. Thus, the link 
layer is a combination of hardware and software‚Äîthe place in the protocol stack 
where software meets hardware. [Intel 2020] provides a readable overview (as 
well as a detailed description) of the XL710 controller from a software-program-
ming point of view.
6.2 Error-Detection and -Correction Techniques
In the previous section, we noted that bit-level error detection and correction‚Äî
detecting and correcting the corruption of bits in a link-layer frame sent from one 
node to another physically connected neighboring node‚Äîare two services often 
 provided by the link layer. We saw in Chapter 3 that error-detection and -correction 
services are also often offered at the transport layer as well. In this section, we‚Äôll 
examine a few of the simplest techniques that can be used to detect and, in some 
Figure 6.2 ‚ô¶  Network adapter: Its relationship to other host components 
and to protocol stack functionality
Host
Memory
Motherboard bus
CPU
Controller
Physical
transmission
Network adapter
Link
Physical
Transport
Network
Link
Application

cases, correct such bit errors. A full treatment of the theory and implementation of 
this topic is itself the topic of many textbooks (e.g., [Schwartz 1980] or [Bertsekas 
1991]), and our treatment here is necessarily brief. Our goal here is to develop an 
intuitive feel for the capabilities that error-detection and -correction techniques pro-
vide and to see how a few simple techniques work and are used in practice in the 
link layer.
Figure 6.3 illustrates the setting for our study. At the sending node, data, D, to 
be protected against bit errors is augmented with error-detection and -correction bits 
(EDC). Typically, the data to be protected includes not only the datagram passed 
down from the network layer for transmission across the link, but also link-level 
addressing information, sequence numbers, and other fields in the link frame header. 
Both D and EDC are sent to the receiving node in a link-level frame. At the receiv-
ing node, a sequence of bits, D‚Ä≤ and EDC‚Ä≤ is received. Note that D‚Ä≤ and EDC‚Ä≤ may 
differ from the original D and EDC as a result of in-transit bit flips.
The receiver‚Äôs challenge is to determine whether or not D‚Ä≤ is the same as the 
original D, given that it has only received D‚Ä≤ and EDC‚Ä≤. The exact wording of the 
receiver‚Äôs decision in Figure 6.3 (we ask whether an error is detected, not whether an 
error has occurred!) is important. Error-detection and -correction techniques allow 
the receiver to sometimes, but not always, detect that bit errors have occurred. Even 
with the use of error-detection bits there still may be undetected bit errors; that is, 
the receiver may be unaware that the received information contains bit errors. As a 
Figure 6.3 ‚ô¶ Error-detection and -correction scenario
EDC'
D'
Detected error
Datagram
EDC
D
d data bits
Bit error-prone link
all
bits in D'
OK
?
N
Y
Datagram
HI

consequence, the receiver might deliver a corrupted datagram to the network layer, 
or be unaware that the contents of a field in the frame‚Äôs header has been corrupted. 
We thus want to choose an error-detection scheme that keeps the probability of such 
occurrences small. Generally, more sophisticated error-detection and -correction 
techniques (that is, those that have a smaller probability of allowing undetected bit 
errors) incur a larger overhead‚Äîmore computation is needed to compute and trans-
mit a larger number of error-detection and -correction bits.
Let‚Äôs now examine three techniques for detecting errors in the transmitted data‚Äî
parity checks (to illustrate the basic ideas behind error detection and correction), check-
summing methods (which are more typically used in the transport layer), and cyclic 
redundancy checks (which are more typically used in the link layer in an adapter).
6.2.1 Parity Checks
Perhaps the simplest form of error detection is the use of a single parity bit. Suppose 
that the information to be sent, D in Figure 6.4, has d bits. In an even parity scheme, 
the sender simply includes one additional bit and chooses its value such that the total 
number of 1s in the d + 1 bits (the original information plus a parity bit) is even. For 
odd parity schemes, the parity bit value is chosen such that there is an odd number 
of 1s. Figure 6.4 illustrates an even parity scheme, with the single parity bit being 
stored in a separate field.
Receiver operation is also simple with a single parity bit. The receiver need only 
count the number of 1s in the received d + 1 bits. If an odd number of 1-valued bits 
are found with an even parity scheme, the receiver knows that at least one bit error has 
occurred. More precisely, it knows that some odd number of bit errors have occurred.
But what happens if an even number of bit errors occur? You should convince 
yourself that this would result in an undetected error. If the probability of bit errors is 
small and errors can be assumed to occur independently from one bit to the next, the 
probability of multiple bit errors in a packet would be extremely small. In this case, 
a single parity bit might suffice. However, measurements have shown that, rather 
than occurring independently, errors are often clustered together in ‚Äúbursts.‚Äù Under 
burst error conditions, the probability of undetected errors in a frame protected by 
single-bit parity can approach 50 percent [Spragins 1991]. Clearly, a more robust 
error-detection scheme is needed (and, fortunately, is used in practice!). But before 
examining error-detection schemes that are used in practice, let‚Äôs consider a simple 
Figure 6.4 ‚ô¶ One-bit even parity
0 1 1 1 0 0 0 1 1 0 1 0 1 0 1 1
1
d data bits
Parity
bit

generalization of one-bit parity that will provide us with insight into error-correction 
techniques.
Figure 6.5 shows a two-dimensional generalization of the single-bit parity 
scheme. Here, the d bits in D are divided into i rows and j columns. A parity value 
is computed for each row and for each column. The resulting i + j + 1 parity bits 
comprise the link-layer frame‚Äôs error-detection bits.
Suppose now that a single bit error occurs in the original d bits of information. 
With this two-dimensional parity scheme, the parity of both the column and the row 
containing the flipped bit will be in error. The receiver can thus not only detect the 
fact that a single bit error has occurred, but can use the column and row indices of 
the column and row with parity errors to actually identify the bit that was corrupted 
and correct that error! Figure 6.5 shows an example in which the 1-valued bit in 
position (2,2) is corrupted and switched to a 0‚Äîan error that is both detectable and 
correctable at the receiver. Although our discussion has focused on the original d bits 
of information, a single error in the parity bits themselves is also detectable and cor-
rectable. Two-dimensional parity can also detect (but not correct!) any combination 
of two errors in a packet. Other properties of the two-dimensional parity scheme are 
explored in the problems at the end of the chapter.
Figure 6.5 ‚ô¶ Two-dimensional even parity
1  0  1  0  1  1
1  1  1  1  0  0
0  1  1  1  0  1
0  0  1  0  1  0
1  0  1  0  1  1
1  0  1  1  0  0
0  1  1  1  0  1
0  0  1  0  1  0
Row parity
Parity 
error
Parity 
error
No errors
Correctable
single-bit error
d1,1
d2,1
. . .
di,1
di+1,1
. . .
. . .
. . .
. . .
. . .
d1, j
d2, j
. . .
di, j
di+1,j
d1,j+1
d2,j+1
. . .
di,j+1
di+1,j+1
Column parity

The ability of the receiver to both detect and correct errors is known as forward 
error correction (FEC). These techniques are commonly used in audio storage and 
playback devices such as audio CDs. In a network setting, FEC techniques can be 
used by themselves, or in conjunction with link-layer ARQ techniques similar to 
those we examined in Chapter 3. FEC techniques are valuable because they can 
decrease the number of sender retransmissions required. Perhaps more important, 
they allow for immediate correction of errors at the receiver. This avoids having to 
wait for the round-trip propagation delay needed for the sender to receive a NAK 
packet and for the retransmitted packet to propagate back to the receiver‚Äîa poten-
tially important advantage for real-time network applications [Rubenstein 1998] or 
links (such as deep-space links) with long propagation delays. Research examining 
the use of FEC in error-control protocols includes [Biersack 1992; Nonnenmacher 
1998; Byers 1998; Shacham 1990].
6.2.2 Checksumming Methods
In checksumming techniques, the d bits of data in Figure 6.4 are treated as a sequence 
of k-bit integers. One simple checksumming method is to simply sum these k-bit inte-
gers and use the resulting sum as the error-detection bits. The Internet checksum is 
based on this approach‚Äîbytes of data are treated as 16-bit integers and summed. The 
1s complement of this sum then forms the Internet checksum that is carried in the 
segment header. As discussed in Section 3.3, the receiver checks the checksum by 
taking the 1s complement of the sum of the received data (including the checksum) 
and checking whether the result is all 0 bits. If any of the bits are 1, an error is indi-
cated. RFC 1071 discusses the Internet checksum algorithm and its implementation 
in detail. In the TCP and UDP protocols, the Internet checksum is computed over all 
fields (header and data fields included). In IP, the checksum is computed over the IP 
header (since the UDP or TCP segment has its own checksum). In other protocols, 
for example, XTP [Strayer 1992], one checksum is computed over the header and 
another checksum is computed over the entire packet.
Checksumming methods require relatively little packet overhead. For example, 
the checksums in TCP and UDP use only 16 bits. However, they provide relatively 
weak protection against errors as compared with cyclic redundancy check, which is 
discussed below and which is often used in the link layer. A natural question at this 
point is, Why is checksumming used at the transport layer and cyclic redundancy 
check used at the link layer? Recall that the transport layer is typically implemented 
in software in a host as part of the host‚Äôs operating system. Because transport-layer 
error detection is implemented in software, it is important to have a simple and fast 
error-detection scheme such as checksumming. On the other hand, error detection at 
the link layer is implemented in dedicated hardware in adapters, which can rapidly 
perform the more complex CRC operations. Feldmeier [Feldmeier 1995] presents 
fast software implementation techniques for not only weighted checksum codes, but 
CRC (see below) and other codes as well.

6.2.3 Cyclic Redundancy Check (CRC)
An error-detection technique used widely in today‚Äôs computer networks is based on 
cyclic redundancy check (CRC) codes. CRC codes are also known as polynomial 
codes, since it is possible to view the bit string to be sent as a polynomial whose 
coefficients are the 0 and 1 values in the bit string, with operations on the bit string 
interpreted as polynomial arithmetic.
CRC codes operate as follows. Consider the d-bit piece of data, D, that the send-
ing node wants to send to the receiving node. The sender and receiver must first 
agree on an r + 1 bit pattern, known as a generator, which we will denote as G. 
We will require that the most significant (leftmost) bit of G be a 1. The key idea 
behind CRC codes is shown in Figure 6.6. For a given piece of data, D, the sender 
will choose r additional bits, R, and append them to D such that the resulting d + r  
bit pattern (interpreted as a binary number) is exactly divisible by G (i.e., has no 
remainder) using modulo-2 arithmetic. The process of error checking with CRCs is 
thus simple: The receiver divides the d + r received bits by G. If the remainder is 
nonzero, the receiver knows that an error has occurred; otherwise the data is accepted 
as being correct.
All CRC calculations are done in modulo-2 arithmetic without carries in addi-
tion or borrows in subtraction. This means that addition and subtraction are identical, 
and both are equivalent to the bitwise exclusive-or (XOR) of the operands. Thus, for 
example,
1011 XOR 0101 = 1110
1001 XOR 1101 = 0100
Also, we similarly have
1011 - 0101 = 1110
1001 - 1101 = 0100
Multiplication and division are the same as in base-2 arithmetic, except that any 
required addition or subtraction is done without carries or borrows. As in regular 
Figure 6.6 ‚ô¶ CRC
d bits
r bits
D: Data bits to be sent
D ‚Ä¢ 2r  XOR    R
R: CRC bits
Bit pattern
Mathematical formula

binary arithmetic, multiplication by 2k left shifts a bit pattern by k places. Thus, given 
D and R, the quantity D #  2r XOR R yields the d + r bit pattern shown in Figure 6.6. 
We‚Äôll use this algebraic characterization of the d + r bit pattern from Figure 6.6 in 
our discussion below.
Let us now turn to the crucial question of how the sender computes R. Recall 
that we want to find R such that there is an n such that
D #  2r XOR R = nG
That is, we want to choose R such that G divides into D #  2r XOR R without 
remainder. If we XOR (that is, add modulo-2, without carry) R to both sides of the 
above equation, we get
D #  2r = nG XOR R
This equation tells us that if we divide D #  2r by G, the value of the remainder 
is precisely R. In other words, we can calculate R as
R = remainder D # 2r
G
Figure 6.7 illustrates this calculation for the case of D = 101110, d = 6, 
G = 1001, and r = 3. The 9 bits transmitted in this case are 101 110  011. 
You should check these calculations for yourself and also check that indeed 
D #  2r = 101011 #  G XOR R.
Figure 6.7 ‚ô¶ A sample CRC calculation
1  0  0  1
1  0  1  1  1  0  0  0  0
1  0  1  0  1  1  
1  0  0  1  
1  0  1
0  0  0
1  0  1  0
1  0  0  1
1  1  0
0  0  0
1  1  0  0
1  0  0  1
1  0  1  0
1  0  0  1
 0  1  1
G
D
R

International standards have been defined for 8-, 12-, 16-, and 32-bit generators, 
G. The CRC-32 32-bit standard, which has been adopted in a number of link-level 
IEEE protocols, uses a generator of
GCRC@32 = 100000100110000010001110110110111
Each of the CRC standards can detect burst errors of fewer than r + 1 bits. (This 
means that all consecutive bit errors of r bits or fewer will be detected.) Furthermore, 
under appropriate assumptions, a burst of length greater than r + 1 bits is detected 
with probability 1 - 0.5r. Also, each of the CRC standards can detect any odd num-
ber of bit errors. See [Williams 1993] for a discussion of implementing CRC checks. 
The theory behind CRC codes and even more powerful codes is beyond the scope of 
this text. The text [Schwartz 1980] provides an excellent introduction to this topic.
6.3 Multiple Access Links and Protocols
In the introduction to this chapter, we noted that there are two types of network links: 
point-to-point links and broadcast links. A point-to-point link consists of a single 
sender at one end of the link and a single receiver at the other end of the link. Many 
link-layer protocols have been designed for point-to-point links; the point-to-point 
protocol (PPP) and high-level data link control (HDLC) are two such protocols. The 
second type of link, a broadcast link, can have multiple sending and receiving nodes 
all connected to the same, single, shared broadcast channel. The term broadcast is 
used here because when any one node transmits a frame, the channel broadcasts the 
frame and each of the other nodes receives a copy. Ethernet and wireless LANs are  
examples of broadcast link-layer technologies. In this section, we‚Äôll take a step back 
from specific link-layer protocols and first examine a problem of central importance 
to the link layer: how to coordinate the access of multiple sending and receiving 
nodes to a shared broadcast channel‚Äîthe multiple access problem. Broadcast chan-
nels are often used in LANs, networks that are geographically concentrated in a 
single building (or on a corporate or university campus). Thus, we‚Äôll look at how 
multiple access channels are used in LANs at the end of this section.
We are all familiar with the notion of broadcasting‚Äîtelevision has been using it 
since its invention. But traditional television is a one-way broadcast (that is, one fixed 
node transmitting to many receiving nodes), while nodes on a computer network 
broadcast channel can both send and receive. Perhaps a more apt human analogy for 
a broadcast channel is a cocktail party, where many people gather in a large room 
(the air providing the broadcast medium) to talk and listen. A second good analogy is 
something many readers will be familiar with‚Äîa classroom‚Äîwhere teacher(s) and 
student(s) similarly share the same, single, broadcast medium. A central problem in

both scenarios is that of determining who gets to talk (that is, transmit into the chan-
nel) and when. As humans, we‚Äôve evolved an elaborate set of protocols for sharing 
the broadcast channel:
‚ÄúGive everyone a chance to speak.‚Äù
‚ÄúDon‚Äôt speak until you are spoken to.‚Äù
‚ÄúDon‚Äôt monopolize the conversation.‚Äù
‚ÄúRaise your hand if you have a question.‚Äù
‚ÄúDon‚Äôt interrupt when someone is speaking.‚Äù
‚ÄúDon‚Äôt fall asleep when someone is talking.‚Äù
Computer networks similarly have protocols‚Äîso-called multiple access 
 protocols‚Äîby which nodes regulate their transmission into the shared broadcast 
channel. As shown in Figure 6.8, multiple access protocols are needed in a wide 
variety of network settings, including both wired and wireless access networks, and 
satellite networks. Although technically each node accesses the broadcast chan-
nel¬†through its adapter, in this section, we will refer to the node as the sending and 
Figure 6.8 ‚ô¶ Various multiple access channels
Shared wire
(e.g., cable access network)
Shared wireless
(e.g., WiFi)
Satellite
Cocktail party
Head
end

receiving device. In practice, hundreds or even thousands of nodes can directly com-
municate over a broadcast channel.
Because all nodes are capable of transmitting frames, more than two nodes can 
transmit frames at the same time. When this happens, all of the nodes receive multiple 
frames at the same time; that is, the transmitted frames collide at all of the receiv-
ers. Typically, when there is a collision, none of the receiving nodes can make any 
sense of any of the frames that were transmitted; in a sense, the signals of the col-
liding frames become inextricably tangled together. Thus, all the frames involved in 
the collision are lost, and the broadcast channel is wasted during the collision inter-
val. Clearly, if many nodes want to transmit frames frequently, many transmissions 
will result in collisions, and much of the bandwidth of the broadcast channel will be 
wasted.
In order to ensure that the broadcast channel performs useful work when mul-
tiple nodes are active, it is necessary to somehow coordinate the transmissions of 
the active nodes. This coordination job is the responsibility of the multiple access 
protocol. Over the past 40 years, thousands of papers and hundreds of PhD disserta-
tions have been written on multiple access protocols; a comprehensive survey of the 
first 20 years of this body of work is [Rom 1990]. Furthermore, active research in 
multiple access protocols continues due to the continued emergence of new types of 
links, particularly new wireless links.
Over the years, dozens of multiple access protocols have been implemented in 
a variety of link-layer technologies. Nevertheless, we can classify just about any 
multiple access protocol as belonging to one of three categories: channel partition-
ing protocols, random access protocols, and taking-turns protocols. We‚Äôll cover 
these categories of multiple access protocols in the following three subsections.
Let‚Äôs conclude this overview by noting that, ideally, a multiple access protocol 
for a broadcast channel of rate R bits per second should have the following desirable 
characteristics:
 1. When only one node has data to send, that node has a throughput of R bps.
 2. When M nodes have data to send, each of these nodes has a throughput of R/M 
bps. This need not necessarily imply that each of the M nodes always has an 
instantaneous rate of R/M, but rather that each node should have an average 
transmission rate of R/M over some suitably defined interval of time.
 3. The protocol is decentralized; that is, there is no master node that represents a 
single point of failure for the network.
 4. The protocol is simple, so that it is inexpensive to implement.
6.3.1 Channel Partitioning Protocols
Recall from our early discussion back in Section 1.3 that time-division  multiplexing 
(TDM) and frequency-division multiplexing (FDM) are two techniques that can

be used to partition a broadcast channel‚Äôs bandwidth among all nodes sharing that 
channel. As an example, suppose the channel supports N nodes and that the trans-
mission rate of the channel is R bps. TDM divides time into time frames and further 
divides each time frame into N time slots. (The TDM time frame should not be 
confused with the link-layer unit of data exchanged between sending and receiving 
adapters, which is also called a frame. In order to reduce confusion, in this subsec-
tion we‚Äôll refer to the link-layer unit of data exchanged as a packet.) Each time slot 
is then assigned to one of the N nodes. Whenever a node has a packet to send, it 
transmits the packet‚Äôs bits during its assigned time slot in the revolving TDM frame. 
Typically, slot sizes are chosen so that a single packet can be transmitted during a 
slot time. Figure 6.9 shows a simple four-node TDM example. Returning to our 
cocktail party analogy, a TDM-regulated cocktail party would allow one partygoer 
to speak for a fixed period of time, then allow another partygoer to speak for the 
same amount of time, and so on. Once everyone had had a chance to talk, the  pattern 
would repeat.
TDM is appealing because it eliminates collisions and is perfectly fair: Each 
node gets a dedicated transmission rate of R/N bps during each frame time. However, 
it has two major drawbacks. First, a node is limited to an average rate of R/N bps 
even when it is the only node with packets to send. A second drawback is that a node 
must always wait for its turn in the transmission sequence‚Äîagain, even when it is 
the only node with a frame to send. Imagine the partygoer who is the only one with 
anything to say (and imagine that this is the even rarer circumstance where everyone 
Figure 6.9 ‚ô¶ A four-node TDM and FDM example
4KHz
FDM
TDM
Link
4KHz
Slot
All slots labeled ‚Äú2‚Äù are dedicated
to a speciÔ¨Åc sender-receiver pair.
Frame
1
2
2
3
4
1
2
3
4
1
2
3
4
1
2
3
4
Key:

wants to hear what that one person has to say). Clearly, TDM would be a poor choice 
for a multiple access protocol for this particular party.
While TDM shares the broadcast channel in time, FDM divides the R bps chan-
nel into different frequencies (each with a bandwidth of R/N) and assigns each fre-
quency to one of the N nodes. FDM thus creates N smaller channels of R/N bps out 
of the single, larger R bps channel. FDM shares both the advantages and drawbacks 
of TDM. It avoids collisions and divides the bandwidth fairly among the N nodes. 
However, FDM also shares a principal disadvantage with TDM‚Äîa node is limited to 
a bandwidth of R/N, even when it is the only node with packets to send.
A third channel partitioning protocol is code division multiple access 
(CDMA). While TDM and FDM assign time slots and frequencies, respectively, 
to the nodes, CDMA assigns a different code to each node. Each node then uses 
its unique code to encode the data bits it sends. If the codes are chosen carefully, 
CDMA networks have the wonderful property that different nodes can transmit 
simultaneously and yet have their respective receivers correctly receive a send-
er‚Äôs encoded data bits (assuming the receiver knows the sender‚Äôs code) in spite 
of interfering transmissions by other nodes. CDMA has been used in military 
systems for some time (due to its anti-jamming properties) and now has wide-
spread civilian use, particularly in cellular telephony. Because CDMA‚Äôs use is so 
tightly tied to wireless channels, we‚Äôll save our discussion of the technical details 
of CDMA until Chapter 7. For now, it will suffice to know that CDMA codes, 
like time slots in TDM and frequencies in FDM, can be allocated to the multiple 
access channel users.
6.3.2 Random Access Protocols
The second broad class of multiple access protocols are random access protocols. 
In a random access protocol, a transmitting node always transmits at the full rate 
of the channel, namely, R bps. When there is a collision, each node involved in the 
collision repeatedly retransmits its frame (that is, packet) until its frame gets through 
without a collision. But when a node experiences a collision, it doesn‚Äôt necessarily 
retransmit the frame right away. Instead it waits a random delay before retrans-
mitting the frame. Each node involved in a collision chooses independent random 
delays. Because the random delays are independently chosen, it is possible that one 
of the nodes will pick a delay that is sufficiently less than the delays of the other col-
liding nodes and will therefore be able to sneak its frame into the channel without a 
collision.
There are dozens if not hundreds of random access protocols described in the 
literature [Rom 1990; Bertsekas 1991]. In this section we‚Äôll describe a few of the 
most commonly used random access protocols‚Äîthe ALOHA protocols [Abram-
son 1970; Abramson 1985; Abramson 2009] and the carrier sense multiple access 
(CSMA) protocols [Kleinrock 1975b]. Ethernet [Metcalfe 1976] is a popular and 
widely deployed CSMA protocol.

Slotted ALOHA
Let‚Äôs begin our study of random access protocols with one of the simplest random 
access protocols, the slotted ALOHA protocol. In our description of slotted ALOHA, 
we assume the following:
‚Ä¢ All frames consist of exactly L bits.
‚Ä¢ Time is divided into slots of size L/R seconds (that is, a slot equals the time to 
transmit one frame).
‚Ä¢ Nodes start to transmit frames only at the beginnings of slots.
‚Ä¢ The nodes are synchronized so that each node knows when the slots begin.
‚Ä¢ If two or more frames collide in a slot, then all the nodes detect the collision event 
before the slot ends.
Let p be a probability, that is, a number between 0 and 1. The operation of slotted 
ALOHA in each node is simple:
‚Ä¢ When the node has a fresh frame to send, it waits until the beginning of the next 
slot and transmits the entire frame in the slot.
‚Ä¢ If there isn‚Äôt a collision, the node has successfully transmitted its frame and thus 
need not consider retransmitting the frame. (The node can prepare a new frame 
for transmission, if it has one.)
‚Ä¢ If there is a collision, the node detects the collision before the end of the slot. The 
node retransmits its frame in each subsequent slot with probability p until the 
frame is transmitted without a collision.
By retransmitting with probability p, we mean that the node effectively tosses 
a biased coin; the event heads corresponds to ‚Äúretransmit,‚Äù which occurs with prob-
ability p. The event tails corresponds to ‚Äúskip the slot and toss the coin again in the 
next slot‚Äù; this occurs with probability (1 - p). All nodes involved in the collision 
toss their coins independently.
Slotted ALOHA would appear to have many advantages. Unlike channel par-
titioning, slotted ALOHA allows a node to transmit continuously at the full rate, R, 
when that node is the only active node. (A node is said to be active if it has frames 
to send.) Slotted ALOHA is also highly decentralized, because each node detects 
collisions and independently decides when to retransmit. (Slotted ALOHA does, 
however, require the slots to be synchronized in the nodes; shortly we‚Äôll discuss 
an unslotted version of the ALOHA protocol, as well as CSMA protocols, none of 
which require such synchronization.) Slotted ALOHA is also an extremely simple 
protocol.
Slotted ALOHA works well when there is only one active node, but how 
 efficient is it when there are multiple active nodes? There are two possible efficiency

concerns here. First, as shown in Figure 6.10, when there are multiple active nodes, 
a certain fraction of the slots will have collisions and will therefore be ‚Äúwasted.‚Äù The 
second concern is that another fraction of the slots will be empty because all active 
nodes refrain from transmitting as a result of the probabilistic transmission policy. 
The only ‚Äúunwasted‚Äù slots will be those in which exactly one node transmits. A slot 
in which exactly one node transmits is said to be a successful slot. The efficiency of 
a slotted multiple access protocol is defined to be the long-run fraction of successful 
slots in the case when there are a large number of active nodes, each always having 
a large number of frames to send. Note that if no form of access control were used, 
and each node were to immediately retransmit after each collision, the efficiency 
would be zero. Slotted ALOHA clearly increases the efficiency beyond zero, but by 
how much?
We now proceed to outline the derivation of the maximum efficiency of slotted 
ALOHA. To keep this derivation simple, let‚Äôs modify the protocol a little and assume 
that each node attempts to transmit a frame in each slot with probability p. (That is, 
we assume that each node always has a frame to send and that the node transmits 
with probability p for a fresh frame as well as for a frame that has already suffered a 
collision.) Suppose there are N nodes. Then the probability that a given slot is a suc-
cessful slot is the probability that one of the nodes transmits and that the remaining 
N - 1 nodes do not transmit. The probability that a given node transmits is p; the 
probability that the remaining nodes do not transmit is (1 - p)N-1. Therefore, the 
probability a given node has a success is p(1 - p)N-1. Because there are N nodes, 
the probability that any one of the N nodes has a success is Np(1 - p)N-1.
Figure 6.10 ‚ô¶  Nodes 1, 2, and 3 collide in the first slot. Node 2 finally 
succeeds in the fourth slot, node 1 in the eighth slot, and 
node 3 in the ninth slot
Node 3
Key:
C = Collision slot
E = Empty slot
S = Successful slot
Node 2
Node 1
2
2
2
1
1
1
1
3
3
3
Time
C
E
C
S
E
C
E
S
S

Thus, when there are N active nodes, the efficiency of slotted ALOHA is 
Np(1 - p)N-1. To obtain the maximum efficiency for N active nodes, we have to find the 
p* that maximizes this expression. (See the homework problems for a general outline of  
this derivation.) And to obtain the maximum efficiency for a large number of active 
nodes, we take the limit of Np*(1 - p*)N-1 as N approaches infinity. (Again, see the 
homework problems.) After performing these calculations, we‚Äôll find that the maximum 
efficiency of the protocol is given by 1/e = 0.37. That is, when a large number of nodes 
have many frames to transmit, then (at best) only 37 percent of the slots do useful work. 
Thus, the effective transmission rate of the channel is not R bps but only 0.37 R bps! 
A similar analysis also shows that 37 percent of the slots go empty and 26 percent 
of slots have collisions. Imagine the poor network administrator who has purchased a 
100-Mbps slotted ALOHA system, expecting to be able to use the network to transmit 
data among a large number of users at an aggregate rate of, say, 80 Mbps! Although the 
channel is capable of transmitting a given frame at the full channel rate of 100 Mbps, in 
the long run, the successful throughput of this channel will be less than 37 Mbps.
ALOHA
The slotted ALOHA protocol required that all nodes synchronize their transmissions 
to start at the beginning of a slot. The first ALOHA protocol [Abramson 1970] was 
actually an unslotted, fully decentralized protocol. In pure ALOHA, when a frame 
first arrives (that is, a network-layer datagram is passed down from the network layer 
at the sending node), the node immediately transmits the frame in its entirety into the 
broadcast channel. If a transmitted frame experiences a collision with one or more 
other transmissions, the node will then immediately (after completely transmitting 
its collided frame) retransmit the frame with probability p. Otherwise, the node waits 
for a frame transmission time. After this wait, it then transmits the frame with prob-
ability p, or waits (remaining idle) for another frame time with probability 1 ‚Äì p.
To determine the maximum efficiency of pure ALOHA, we focus on an individual 
node. We‚Äôll make the same assumptions as in our slotted ALOHA analysis and take the 
frame transmission time to be the unit of time. At any given time, the probability that a 
node is transmitting a frame is p. Suppose this frame begins transmission at time t0. As 
shown in Figure 6.11, in order for this frame to be successfully transmitted, no other 
nodes can begin their transmission in the interval of time [t0 - 1, t0]. Such a transmis-
sion would overlap with the beginning of the transmission of node i‚Äôs frame. The prob-
ability that all other nodes do not begin a transmission in this interval is (1 - p)N-1. 
Similarly, no other node can begin a transmission while node i is transmitting, as such a 
transmission would overlap with the latter part of node i‚Äôs transmission. The probabil-
ity that all other nodes do not begin a transmission in this interval is also (1 - p)N-1. 
Thus, the probability that a given node has a successful transmission is p(1 - p)2(N-1). 
By taking limits as in the slotted ALOHA case, we find that the maximum efficiency 
of the pure ALOHA protocol is only 1/(2e)‚Äîexactly half that of slotted ALOHA. This 
then is the price to be paid for a fully decentralized ALOHA protocol.

Carrier Sense Multiple Access (CSMA)
In both slotted and pure ALOHA, a node‚Äôs decision to transmit is made indepen-
dently of the activity of the other nodes attached to the broadcast channel. In particu-
lar, a node neither pays attention to whether another node happens to be transmitting 
when it begins to transmit, nor stops transmitting if another node begins to interfere 
with its transmission. In our cocktail party analogy, ALOHA protocols are quite 
like a boorish partygoer who continues to chatter away regardless of whether other 
people are talking. As humans, we have human protocols that allow us not only to 
behave with more civility, but also to decrease the amount of time spent ‚Äúcolliding‚Äù 
with each other in conversation and, consequently, to increase the amount of data we 
exchange in our conversations. Specifically, there are two important rules for polite 
human conversation:
‚Ä¢ Listen before speaking. If someone else is speaking, wait until they are finished. 
In the networking world, this is called carrier sensing‚Äîa node listens to the 
channel before transmitting. If a frame from another node is currently being trans-
mitted into the channel, a node then waits until it detects no transmissions for a 
short amount of time and then begins transmission.
‚Ä¢ If someone else begins talking at the same time, stop talking. In the network-
ing world, this is called collision detection‚Äîa transmitting node listens to the 
channel while it is transmitting. If it detects that another node is transmitting an 
interfering frame, it stops transmitting and waits a random amount of time before 
repeating the sense-and-transmit-when-idle cycle.
These two rules are embodied in the family of carrier sense multiple access 
(CSMA) and CSMA with collision detection (CSMA/CD) protocols [Kleinrock 
1975b; Metcalfe 1976; Lam 1980; Rom 1990]. Many variations on CSMA and 
Figure 6.11 ‚ô¶ Interfering transmissions in pure ALOHA
Time
Will overlap
with start of 
i ‚Äôs frame
t0 ‚Äì 1
t0
t0 + 1
Will overlap
with end of 
i‚Äôs frame
Node i frame

CSMA/CD have been proposed. Here, we‚Äôll consider a few of the most important, 
and fundamental, characteristics of CSMA and CSMA/CD.
The first question that you might ask about CSMA is why, if all nodes perform 
carrier sensing, do collisions occur in the first place? After all, a node will refrain 
from transmitting whenever it senses that another node is transmitting. The answer 
to the question can best be illustrated using space-time diagrams [Molle 1987]. 
 Figure 6.12 shows a space-time diagram of four nodes (A, B, C, D) attached to a 
linear broadcast bus. The horizontal axis shows the position of each node in space; 
the vertical axis represents time.
At time t0, node B senses the channel is idle, as no other nodes are currently trans-
mitting. Node B thus begins transmitting, with its bits propagating in both directions 
along the broadcast medium. The downward propagation of B‚Äôs bits in Figure 6.12 
with increasing time indicates that a nonzero amount of time is needed for B‚Äôs bits 
actually to propagate (albeit at near the speed of light) along the broadcast medium. At 
time t1 (t1 7 t0), node D has a frame to send. Although node B is currently transmit-
ting at time t1, the bits being transmitted by B have yet to reach D, and thus D senses 
NORM ABRAMSON AND ALOHANET
Norm Abramson, a PhD engineer, had a passion for surfing and an interest in 
packet switching. This combination of interests brought him to the University of 
Hawaii in 1969. Hawaii consists of many mountainous islands, making it difficult 
to install and operate land-based networks. When not surfing, Abramson thought 
about how to design a network that does packet switching over radio. The network 
he designed had one central host and several secondary nodes scattered over the 
Hawaiian Islands. The network had two channels, each using a different frequency 
band. The downlink channel broadcasted packets from the central host to the sec-
ondary hosts; and the upstream channel sent packets from the secondary hosts to 
the central host. In addition to sending informational packets, the central host also 
sent on the downstream channel an acknowledgment for each packet successfully 
received from the secondary hosts.
Because the secondary hosts transmitted packets in a decentralized fashion, col-
lisions on the upstream channel inevitably occurred. This observation led Abramson 
to devise the pure ALOHA protocol, as described in this chapter. In 1970, with 
continued funding from ARPA, Abramson connected his ALOHAnet to the ARPAnet. 
Abramson‚Äôs work is important not only because it was the first example of a radio 
packet network, but also because it inspired Bob Metcalfe. A few years later, 
Metcalfe modified the ALOHA protocol to create the CSMA/CD protocol and the 
Ethernet LAN.
CASE HISTORY

the channel idle at t1. In accordance with the CSMA protocol, D thus begins transmit-
ting its frame. A short time later, B‚Äôs transmission begins to interfere with D‚Äôs trans-
mission at D. From Figure 6.12, it is evident that the end-to-end channel propagation 
delay of a broadcast channel‚Äîthe time it takes for a signal to propagate from one of 
the nodes to another‚Äîwill play a crucial role in determining its performance. The 
longer this propagation delay, the larger the chance that a carrier-sensing node is not 
yet able to sense a transmission that has already begun at another node in the network.
Carrier Sense Multiple Access with Collision Detection (CSMA/CD)
In Figure 6.12, nodes do not perform collision detection; both B and D continue to 
transmit their frames in their entirety even though a collision has occurred. When a 
node performs collision detection, it ceases transmission as soon as it detects a col-
lision. Figure 6.13 shows the same scenario as in Figure 6.12, except that the two 
Figure 6.12 ‚ô¶  Space-time diagram of two CSMA nodes with colliding 
transmissions
A
Time
Time
Space
t 0
t 1
B
C
D

nodes each abort their transmission a short time after detecting a collision. Clearly, 
adding collision detection to a multiple access protocol will help protocol perfor-
mance by not transmitting a useless, damaged (by interference with a frame from 
another node) frame in its entirety.
Before analyzing the CSMA/CD protocol, let us now summarize its operation 
from the perspective of an adapter (in a node) attached to a broadcast channel:
 1. The adapter obtains a datagram from the network layer, prepares a link-layer 
frame, and puts the frame adapter buffer.
 2. If the adapter senses that the channel is idle (that is, there is no signal energy 
entering the adapter from the channel), it starts to transmit the frame. If, on the 
other hand, the adapter senses that the channel is busy, it waits until it senses 
no signal energy and then starts to transmit the frame.
 3. While transmitting, the adapter monitors for the presence of signal energy 
coming from other adapters using the broadcast channel.
Figure 6.13 ‚ô¶ CSMA with collision detection
A
Time
Time
Collision
detect/abort
time
Space
t 0
t 1
B
C
D

4. If the adapter transmits the entire frame without detecting signal energy from 
other adapters, the adapter is finished with the frame. If, on the other hand, the 
adapter detects signal energy from other adapters while transmitting, it aborts 
the transmission (that is, it stops transmitting its frame).
 5. After aborting, the adapter waits a random amount of time and then returns to 
step 2.
The need to wait a random (rather than fixed) amount of time is hopefully clear‚Äîif 
two nodes transmitted frames at the same time and then both waited the same fixed 
amount of time, they‚Äôd continue colliding forever. But what is a good interval of 
time from which to choose the random backoff time? If the interval is large and the 
number of colliding nodes is small, nodes are likely to wait a large amount of time 
(with the channel remaining idle) before repeating the sense-and-transmit-when-
idle step. On the other hand, if the interval is small and the number of colliding 
nodes is large, it‚Äôs likely that the chosen random values will be nearly the same, 
and transmitting nodes will again collide. What we‚Äôd like is an interval that is short 
when the number of colliding nodes is small, and long when the number of colliding 
nodes is large.
The binary exponential backoff algorithm, used in Ethernet as well as in DOC-
SIS cable network multiple access protocols [DOCSIS 3.1 2014], elegantly solves 
this problem. Specifically, when transmitting a frame that has already experienced n 
collisions, a node chooses the value of K at random from {0,1,2, . . . . 2n-1}. Thus, 
the more collisions experienced by a frame, the larger the interval from which K is 
chosen. For Ethernet, the actual amount of time a node waits is K # 512 bit times (i.e., 
K times the amount of time needed to send 512 bits into the Ethernet) and the maxi-
mum value that n can take is capped at 10.
Let‚Äôs look at an example. Suppose that a node attempts to transmit a frame for 
the first time and while transmitting it detects a collision. The node then chooses  
K = 0 with probability 0.5 or chooses K = 1 with probability 0.5. If the node 
chooses K = 0, then it immediately begins sensing the channel. If the node chooses 
K = 1, it waits 512 bit times (e.g., 5.12 microseconds for a 100 Mbps Ethernet) 
before beginning the sense-and-transmit-when-idle cycle. After a second collision, 
K is chosen with equal probability from {0,1,2,3}. After three collisions, K is chosen 
with equal probability from {0,1,2,3,4,5,6,7}. After 10 or more collisions, K is cho-
sen with equal probability from {0,1,2, . . . , 1023}. Thus, the size of the sets from 
which K is chosen grows exponentially with the number of collisions; for this reason 
this algorithm is referred to as binary exponential backoff.
We also note here that each time a node prepares a new frame for transmission, 
it runs the CSMA/CD algorithm, not taking into account any collisions that may 
have occurred in the recent past. So it is possible that a node with a new frame will 
immediately be able to sneak in a successful transmission while several other nodes 
are in the exponential backoff state.

CSMA/CD Efficiency
When only one node has a frame to send, the node can transmit at the full channel 
rate (e.g., for Ethernet typical rates are 10 Mbps, 100 Mbps, or 1 Gbps). However, if 
many nodes have frames to transmit, the effective transmission rate of the channel 
can be much less. We define the efficiency of CSMA/CD to be the long-run fraction 
of time during which frames are being transmitted on the channel without collisions 
when there is a large number of active nodes, with each node having a large number 
of frames to send. In order to present a closed-form approximation of the efficiency 
of Ethernet, let dprop denote the maximum time it takes signal energy to propagate 
between any two adapters. Let dtrans be the time to transmit a maximum-size frame 
(approximately 1.2 msecs for a 10 Mbps Ethernet). A derivation of the efficiency of 
CSMA/CD is beyond the scope of this book (see [Lam 1980] and [Bertsekas 1991]). 
Here we simply state the following approximation:
Efficiency =
1
1 + 5dprop>dtrans
We see from this formula that as dprop approaches 0, the efficiency approaches 1. 
This matches our intuition that if the propagation delay is zero, colliding nodes will 
abort immediately without wasting the channel. Also, as dtrans becomes very large, 
efficiency approaches 1. This is also intuitive because when a frame grabs the chan-
nel, it will hold on to the channel for a very long time; thus, the channel will be doing 
productive work most of the time.
6.3.3 Taking-Turns Protocols
Recall that two desirable properties of a multiple access protocol are (1) when only 
one node is active, the active node has a throughput of R bps, and (2) when M nodes 
are active, then each active node has a throughput of nearly R/M bps. The ALOHA 
and CSMA protocols have this first property but not the second. This has motivated 
researchers to create another class of protocols‚Äîthe taking-turns protocols. As with 
random access protocols, there are dozens of taking-turns protocols, and each one of 
these protocols has many variations. We‚Äôll discuss two of the more important protocols 
here. The first one is the polling protocol. The polling protocol requires one of the 
nodes to be designated as a master node. The master node polls each of the nodes in 
a round-robin fashion. In particular, the master node first sends a message to node 1, 
saying that it (node 1) can transmit up to some maximum number of frames. After node 
1 transmits some frames, the master node tells node 2 it (node 2) can transmit up to the 
maximum number of frames. (The master node can determine when a node has finished 
sending its frames by observing the lack of a signal on the channel.) The procedure con-
tinues in this manner, with the master node polling each of the nodes in a cyclic manner.
The polling protocol eliminates the collisions and empty slots that plague ran-
dom access protocols. This allows polling to achieve a much higher efficiency. But

it¬†also has a few drawbacks. The first drawback is that the protocol introduces a 
polling delay‚Äîthe amount of time required to notify a node that it can transmit. If, 
for example, only one node is active, then the node will transmit at a rate less than 
R bps, as the master node must poll each of the inactive nodes in turn each time the 
active node has sent its maximum number of frames. The second drawback, which is 
potentially more serious, is that if the master node fails, the entire channel becomes 
inoperative. The Bluetooth protocol, which we will study in Section¬†6.3, is an exam-
ple of a polling protocol.
The second taking-turns protocol is the token-passing protocol. In this pro-
tocol there is no master node. A small, special-purpose frame known as a token is 
exchanged among the nodes in some fixed order. For example, node 1 might always 
send the token to node 2, node 2 might always send the token to node 3, and node 
N might always send the token to node 1. When a node receives a token, it holds 
onto the token only if it has some frames to transmit; otherwise, it immediately for-
wards the token to the next node. If a node does have frames to transmit when it 
receives the token, it sends up to a maximum number of frames and then forwards 
the token to the next node. Token passing is decentralized and highly efficient. But 
it has its problems as well. For example, the failure of one node can crash the entire 
channel. Or if a node accidentally neglects to release the token, then some recovery 
procedure must be invoked to get the token back in circulation. Over the years many 
token-passing protocols have been developed, including the fiber distributed data 
interface (FDDI) protocol [Jain 1994] and the IEEE 802.5 token ring protocol [IEEE 
802.5 2012], and each one had to address these as well as other sticky issues.
6.3.4  DOCSIS: The Link-Layer Protocol for Cable  
Internet Access
In the previous three subsections, we‚Äôve learned about three broad classes of mul-
tiple access protocols: channel partitioning protocols, random access protocols, and 
taking turns protocols. A cable access network will make for an excellent case study 
here, as we‚Äôll find aspects of each of these three classes of multiple access protocols 
with the cable access network!
Recall from Section 1.2.1 that a cable access network typically connects several 
thousand residential cable modems to a cable modem termination system (CMTS) 
at the cable network headend. The Data-Over-Cable Service Interface Specifica-
tions (DOCSIS) [DOCSIS 3.1 2014; Hamzeh 2015] specifies the cable data network 
architecture and its protocols. DOCSIS uses FDM to divide the downstream (CMTS 
to modem) and upstream (modem to CMTS) network segments into multiple fre-
quency channels. Each downstream channel is between 24 MHz and 192 MHz wide, 
with a maximum throughput of approximately 1.6 Gbps per channel; each upstream 
channel has channel widths ranging from 6.4 MHz to 96 MHz, with a maximum 
upstream throughput of approximately 1 Gbps. Each upstream and downstream

channel is a broadcast channel. Frames transmitted on the downstream channel by 
the CMTS are received by all cable modems receiving that channel; since there is 
just a single CMTS transmitting into the downstream channel, however, there is no 
multiple access problem. The upstream direction, however, is more interesting and 
technically challenging, since multiple cable modems share the same upstream chan-
nel (frequency) to the CMTS, and thus collisions can potentially occur.
As illustrated in Figure 6.14, each upstream channel is divided into intervals 
of time (TDM-like), each containing a sequence of mini-slots during which cable 
modems can transmit to the CMTS. The CMTS explicitly grants permission to indi-
vidual cable modems to transmit during specific mini-slots. The CMTS accomplishes 
this by sending a control message known as a MAP message on a downstream chan-
nel to specify which cable modem (with data to send) can transmit during which 
mini-slot for the interval of time specified in the control message. Since mini-slots 
are explicitly allocated to cable modems, the CMTS can ensure there are no colliding 
transmissions during a mini-slot.
But how does the CMTS know which cable modems have data to send in the 
first place? This is accomplished by having cable modems send mini-slot-request 
frames to the CMTS during a special set of interval mini-slots that are dedicated for 
this purpose, as shown in Figure 6.14. These mini-slot-request frames are transmit-
ted in a random access manner and so may collide with each other. A cable modem 
can neither sense whether the upstream channel is busy nor detect collisions. Instead, 
the cable modem infers that its mini-slot-request frame experienced a collision if it 
does not receive a response to the requested allocation in the next downstream con-
trol message. When a collision is inferred, a cable modem uses binary exponential 
Figure 6.14 ‚ô¶  Upstream and downstream channels between CMTS and 
cable modems
Residences with
cable modems
Minislots
containing
minislot
request frames
Assigned minislots
containing cable
modem upstream
data frames
Cable head end
MAP frame for 
interval [t1,t2]
CMTS
Downstream channel i
Upstream channel j
t1
t2

backoff to defer the retransmission of its mini-slot-request frame to a future time 
slot. When there is little traffic on the upstream channel, a cable modem may actually 
transmit data frames during slots nominally assigned for mini-slot-request frames 
(and thus avoid having to wait for a mini-slot assignment).
A cable access network thus serves as a terrific example of multiple access pro-
tocols in action‚ÄîFDM, TDM, random access, and centrally allocated time slots all 
within one network!
6.4 Switched Local Area Networks
Having covered broadcast networks and multiple access protocols in the previ-
ous section, let‚Äôs turn our attention next to switched local networks. Figure 6.15 
shows a¬†switched local network connecting three departments, two servers and a 
router with¬† four switches. Because these switches operate at the link layer, they 
switch  link-layer frames (rather than network-layer datagrams), don‚Äôt recognize 
network-layer addresses, and don‚Äôt use routing algorithms like OSPF to determine 
Figure 6.15 ‚ô¶ An institutional network connected together by four switches
Mail
server
To external
internet
1 Gbps
1
2
3
4
5
6
1 Gbps
1 Gbps
Electrical Engineering
Computer Science
100 Mbps
(Ô¨Åber)
100 Mbps
(Ô¨Åber)
100 Mbps
(Ô¨Åber)
Mixture of 10 Mbps,
100 Mbps, 1 Gbps,
Cat 5 cable
Web
server
Computer Engineering

paths¬†through the network of layer-2 switches. Instead of using IP addresses, we will 
soon see that they use link-layer addresses to forward link-layer frames through the 
network of switches. We‚Äôll begin our study of switched LANs by first covering link-
layer addressing (Section 6.4.1). We then examine the celebrated Ethernet protocol 
(Section 6.4.2). After examining link-layer addressing and Ethernet, we‚Äôll look at 
how link-layer switches operate (Section 6.4.3), and then see (Section 6.4.4) how 
these switches are often used to build large-scale LANs.
6.4.1 Link-Layer Addressing and ARP
Hosts and routers have link-layer addresses. Now you might find this surprising, 
recalling from Chapter 4 that hosts and routers have network-layer addresses as well. 
You might be asking, why in the world do we need to have addresses at both the 
network and link layers? In addition to describing the syntax and function of the 
link-layer addresses, in this section we hope to shed some light on why the two lay-
ers of addresses are useful and, in fact, indispensable. We‚Äôll also cover the Address 
Resolution Protocol (ARP), which provides a mechanism to translate IP addresses to 
link-layer addresses.
MAC Addresses
In truth, it is not hosts and routers that have link-layer addresses but rather their 
adapters (that is, network interfaces) that have link-layer addresses. A host or router 
with multiple network interfaces will thus have multiple link-layer addresses associ-
ated with it, just as it would also have multiple IP addresses associated with it. It‚Äôs 
important to note, however, that link-layer switches do not have link-layer addresses 
associated with their interfaces that connect to hosts and routers. This is because the 
job of the link-layer switch is to carry datagrams between hosts and routers; a switch 
does this job transparently, that is, without the host or router having to explicitly 
address the frame to the intervening switch. This is illustrated in Figure 6.16. A link-
layer address is variously called a LAN address, a physical address, or a MAC 
address. Because MAC address seems to be the most popular term, we‚Äôll henceforth 
refer to link-layer addresses as MAC addresses. For most LANs (including Ethernet 
and 802.11 wireless LANs), the MAC address is 6 bytes long, giving 248 possi-
ble MAC addresses. As shown in Figure 6.16, these 6-byte addresses are typically 
expressed in hexadecimal notation, with each byte of the address expressed as a pair 
of hexadecimal numbers. Although MAC addresses were designed to be permanent, 
it is now possible to change an adapter‚Äôs MAC address via software. For the rest of 
this section, however, we‚Äôll assume that an adapter‚Äôs MAC address is fixed.
One interesting property of MAC addresses is that no two adapters have the 
same address. This might seem surprising given that adapters are manufactured in 
many countries by many companies. How does a company manufacturing adapters in 
Taiwan make sure that it is using different addresses from a company manufacturing

adapters in Belgium? The answer is that the IEEE manages the MAC address space. 
In particular, when a company wants to manufacture adapters, it purchases a chunk 
of the address space consisting of 224 addresses for a nominal fee. IEEE allocates the 
chunk of 224 addresses by fixing the first 24 bits of a MAC address and letting the 
company create unique combinations of the last 24 bits for each adapter.
An adapter‚Äôs MAC address has a flat structure (as opposed to a hierarchical 
structure) and doesn‚Äôt change no matter where the adapter goes. A laptop with an 
Ethernet interface always has the same MAC address, no matter where the computer 
goes. A smartphone with an 802.11 interface always has the same MAC address, no 
matter where the smartphone goes. Recall that, in contrast, IP addresses have a hier-
archical structure (that is, a network part and a host part), and a host‚Äôs IP addresses 
needs to be changed when the host moves, i.e., changes the network to which it is 
attached. An adapter‚Äôs MAC address is analogous to a person‚Äôs social security num-
ber, which also has a flat addressing structure and which doesn‚Äôt change no matter 
where the person goes. An IP address is analogous to a person‚Äôs postal address, 
which is hierarchical and which must be changed whenever a person moves. Just as a 
person may find it useful to have both a postal address and a social security number, 
it is useful for a host and router interfaces to have both a network-layer address and 
a MAC address.
When an adapter wants to send a frame to some destination adapter, the sending 
adapter inserts the destination adapter‚Äôs MAC address into the frame and then sends the 
frame into the LAN. As we will soon see, a switch occasionally broadcasts an incom-
ing frame onto all of its interfaces. We‚Äôll see in Chapter 7 that 802.11 also broadcasts 
frames. Thus, an adapter may receive a frame that isn‚Äôt addressed to it. Thus, when 
an adapter receives a frame, it will check to see whether the destination MAC address 
Figure 6.16 ‚ô¶  Each interface connected to a LAN has a unique MAC 
address
88-B2-2F-54-1A-0F
5C-66-AB-90-75-B1
1A-23-F9-CD-06-9B
49-BD-D2-C7-56-2A

in the frame matches its own MAC address. If there is a match, the adapter extracts 
the enclosed datagram and passes the datagram up the protocol stack. If there isn‚Äôt a 
match, the adapter discards the frame, without passing the network-layer datagram up. 
Thus, the destination only will be interrupted when the frame is received.
However, sometimes a sending adapter does want all the other adapters on the 
LAN to receive and process the frame it is about to send. In this case, the sending 
adapter inserts a special MAC broadcast address into the destination address field 
of the frame. For LANs that use 6-byte addresses (such as Ethernet and 802.11), the 
broadcast address is a string of 48 consecutive 1s (that is, FF-FF-FF-FF-FF-FF in 
hexadecimal notation).
Address Resolution Protocol (ARP)
Because there are both network-layer addresses (for example, Internet IP addresses) 
and link-layer addresses (that is, MAC addresses), there is a need to translate between 
them. For the Internet, this is the job of the Address Resolution Protocol (ARP) 
[RFC 826].
To understand the need for a protocol such as ARP, consider the network 
shown in Figure 6.17. In this simple example, each host and router has a single IP 
address and single MAC address. As usual, IP addresses are shown in dotted-decimal 
KEEPING THE LAYERS INDEPENDENT
There are several reasons why hosts and router interfaces have MAC addresses in 
 addition to network-layer addresses. First, LANs are designed for arbitrary network-layer 
protocols, not just for IP and the Internet. If adapters were assigned IP addresses rather 
than ‚Äúneutral‚Äù MAC addresses, then adapters would not easily be able to support other 
network-layer protocols (for example, IPX or DECnet). Second, if adapters were to use 
network-layer addresses instead of MAC addresses, the network-layer address would have 
to be stored in the adapter RAM and reconfigured every time the adapter was moved (or 
powered up). Another option is to not use any addresses in the adapters and have each 
adapter pass the data (typically, an IP datagram) of each frame it receives up the protocol 
stack. The network layer could then check for a matching network-layer address. One 
problem with this option is that the host would be interrupted by every frame sent on the 
LAN, including by frames that were destined for other hosts on the same broadcast LAN. 
In summary, in order for the layers to be largely independent building blocks in a network 
architecture, different layers need to have their own addressing scheme. We have now 
seen three types of addresses: host names for the application layer, IP addresses for the 
network layer, and MAC addresses for the link layer.
PRINCIPLES IN PRACTICE

notation and MAC addresses are shown in hexadecimal notation. For the purposes of 
this discussion, we will assume in this section that the switch broadcasts all frames; 
that is, whenever a switch receives a frame on one interface, it forwards the frame 
on all of its other interfaces. In the next section, we will provide a more accurate 
explanation of how switches operate.
Now suppose that the host with IP address 222.222.222.220 wants to send an IP 
datagram to host 222.222.222.222. In this example, both the source and destination 
are in the same subnet, in the addressing sense of Section 4.3.3. To send a datagram, 
the source must give its adapter not only the IP datagram but also the MAC address 
for destination 222.222.222.222. The sending adapter will then construct a link-layer 
frame containing the destination‚Äôs MAC address and send the frame into the LAN.
The important question addressed in this section is, How does the sending host 
determine the MAC address for the destination host with IP address 222.222.222.222? 
As you might have guessed, it uses ARP. An ARP module in the sending host takes 
any IP address on the same LAN as input, and returns the corresponding MAC 
address. In the example at hand, sending host 222.222.222.220 provides its ARP 
module the IP address 222.222.222.222, and the ARP module returns the corre-
sponding MAC address 49-BD-D2-C7-56-2A.
So we see that ARP resolves an IP address to a MAC address. In many ways 
it is analogous to DNS (studied in Section 2.5), which resolves host names to IP 
addresses. However, one important difference between the two resolvers is that DNS 
resolves host names for hosts anywhere in the Internet, whereas ARP resolves IP 
addresses only for hosts and router interfaces on the same subnet. If a node in Cali-
fornia were to try to use ARP to resolve the IP address for a node in Mississippi, ARP 
would return with an error.
Figure 6.17 ‚ô¶  Each interface on a LAN has an IP address and a MAC 
address
IP:222.222.222.221
IP:222.222.222.220
IP:222.222.222.223
IP:222.222.222.222
5C-66-AB-90-75-B1
1A-23-F9-CD-06-9B
49-BD-D2-C7-56-2A
88-B2-2F-54-1A-0F
A
B
C

Now that we have explained what ARP does, let‚Äôs look at how it works. Each host 
and router has an ARP table in its memory, which contains mappings of IP addresses 
to MAC addresses. Figure 6.18 shows what an ARP table in host 222.222.222.220 
might look like. The ARP table also contains a time-to-live (TTL) value, which indi-
cates when each mapping will be deleted from the table. Note that a table does not 
necessarily contain an entry for every host and router on the subnet; some may have 
never been entered into the table, and others may have expired. A typical expiration 
time for an entry is 20 minutes from when an entry is placed in an ARP table.
Now suppose that host 222.222.222.220 wants to send a datagram that is IP-
addressed to another host or router on that subnet. The sending host needs to obtain the 
MAC address of the destination given the IP address. This task is easy if the sender‚Äôs 
ARP table has an entry for the destination node. But what if the ARP table doesn‚Äôt cur-
rently have an entry for the destination? In particular, suppose 222.222.222.220 wants 
to send a datagram to 222.222.222.222. In this case, the sender uses the ARP protocol 
to resolve the address. First, the sender constructs a special packet called an ARP 
packet. An ARP packet has several fields, including the sending and receiving IP and 
MAC addresses. Both ARP query and response packets have the same format. The pur-
pose of the ARP query packet is to query all the other hosts and routers on the subnet 
to determine the MAC address corresponding to the IP address that is being resolved.
Returning to our example, 222.222.222.220 passes an ARP query packet to 
the adapter along with an indication that the adapter should send the packet to the 
MAC broadcast address, namely, FF-FF-FF-FF-FF-FF. The adapter encapsulates the 
ARP packet in a link-layer frame, uses the broadcast address for the frame‚Äôs destina-
tion address, and transmits the frame into the subnet. Recalling our social security 
 number/postal address analogy, an ARP query is equivalent to a person shouting out 
in a crowded room of cubicles in some company (say, AnyCorp): ‚ÄúWhat is the social 
security number of the person whose postal address is Cubicle 13, Room 112, Any-
Corp, Palo Alto, California?‚Äù The frame containing the ARP query is received by all 
the other adapters on the subnet, and (because of the broadcast address) each adapter 
passes the ARP packet within the frame up to its ARP module. Each of these ARP 
modules checks to see if its IP address matches the destination IP address in the ARP 
packet. The one with a match sends back to the querying host a response ARP packet 
with the desired mapping. The querying host 222.222.222.220 can then update its 
ARP table and send its IP datagram, encapsulated in a link-layer frame whose desti-
nation MAC is that of the host or router responding to the earlier ARP query.
Figure 6.18 ‚ô¶ A possible ARP table in 222.222.222.220
IP Address
MAC Address
TTL
222.222.222.221
88-B2-2F-54-1A-0F
13:45:00
222.222.222.223
5C-66-AB-90-75-B1
13:52:00

There are a couple of interesting things to note about the ARP protocol. First, 
the query ARP message is sent within a broadcast frame, whereas the response 
ARP message is sent within a standard frame. Before reading on you should think 
about why this is so. Second, ARP is plug-and-play; that is, an ARP table gets built 
 automatically‚Äîit doesn‚Äôt have to be configured by a system administrator. And if a 
host becomes disconnected from the subnet, its entry is eventually deleted from the 
other ARP tables in the subnet.
Students often wonder if ARP is a link-layer protocol or a network-layer proto-
col. As we‚Äôve seen, an ARP packet is encapsulated within a link-layer frame and thus 
lies architecturally above the link layer. However, an ARP packet has fields contain-
ing link-layer addresses and thus is arguably a link-layer protocol, but it also contains 
network-layer addresses and thus is also arguably a network-layer protocol. In the 
end, ARP is probably best considered a protocol that straddles the boundary between 
the link and network layers‚Äînot fitting neatly into the simple layered protocol stack 
we studied in Chapter 1. Such are the complexities of real-world protocols!
Sending a Datagram off the Subnet
It should now be clear how ARP operates when a host wants to send a datagram to 
another host on the same subnet. But now let‚Äôs look at the more complicated situ-
ation when a host on a subnet wants to send a network-layer datagram to a host off 
the subnet (that is, across a router onto another subnet). Let‚Äôs discuss this issue in 
the context of Figure 6.19, which shows a simple network consisting of two subnets 
interconnected by a router.
There are several interesting things to note about Figure 6.19. Each host has 
exactly one IP address and one adapter. But, as discussed in Chapter 4, a router has 
an IP address for each of its interfaces. For each router interface there is also an ARP 
module (in the router) and an adapter. Because the router in Figure 6.19 has two 
interfaces, it has two IP addresses, two ARP modules, and two adapters. Of course, 
each adapter in the network has its own MAC address.
Figure 6.19 ‚ô¶ Two subnets interconnected by a router
IP:111.111.111.110
IP:111.111.111.111
IP:111.111.111.112
IP:222.222.222.221
IP:222.222.222.222
74-29-9C-E8-FF-55
CC-49-DE-D0-AB-7D
E6-E9-00-17-BB-4B
1A-23-F9-CD-06-9B
IP:222.222.222.220
88-B2-2F-54-1A-0F
49-BD-D2-C7-56-2A

Also note that Subnet 1 has the network address 111.111.111/24 and that Subnet 2  
has the network address 222.222.222/24. Thus, all of the interfaces connected to Sub-
net 1 have addresses of the form 111.111.111.xxx and all of the interfaces connected 
to Subnet 2 have addresses of the form 222.222.222.xxx.
Now let‚Äôs examine how a host on Subnet 1 would send a datagram to a host 
on Subnet 2. Specifically, suppose that host 111.111.111.111 wants to send an IP 
datagram to a host 222.222.222.222. The sending host passes the datagram to its 
adapter, as usual. But the sending host must also indicate to its adapter an appro-
priate destination MAC address. What MAC address should the adapter use? One 
might be tempted to guess that the appropriate MAC address is that of the adapter for 
host 222.222.222.222, namely, 49-BD-D2-C7-56-2A. This guess, however, would 
be wrong! If the sending adapter were to use that MAC address, then none of the 
 adapters on Subnet 1 would bother to pass the IP datagram up to its network layer, 
since the frame‚Äôs destination address would not match the MAC address of any 
adapter on Subnet 1. The datagram would just die and go to datagram heaven.
If we look carefully at Figure 6.19, we see that in order for a datagram to go from 
111.111.111.111 to a host on Subnet 2, the datagram must first be sent to the router 
interface 111.111.111.110, which is the IP address of the first-hop router on the 
path to the final destination. Thus, the appropriate MAC address for the frame is the 
address of the adapter for router interface 111.111.111.110, namely, E6-E9-00-17-
BB-4B. How does the sending host acquire the MAC address for 111.111.111.110? 
By using ARP, of course! Once the sending adapter has this MAC address, it cre-
ates a frame (containing the datagram addressed to 222.222.222.222) and sends the 
frame into Subnet 1. The router adapter on Subnet 1 sees that the link-layer frame 
is addressed to it, and therefore passes the frame to the network layer of the router. 
Hooray‚Äîthe IP datagram has successfully been moved from source host to the 
router! But we are not finished. We still have to move the datagram from the router 
to the destination. The router now has to determine the correct interface on which the 
datagram is to be forwarded. As discussed in Chapter 4, this is done by consulting a 
forwarding table in the router. The forwarding table tells the router that the datagram 
is to be forwarded via router interface 222.222.222.220. This interface then passes 
the datagram to its adapter, which encapsulates the datagram in a new frame and 
sends the frame into Subnet 2. This time, the destination MAC address of the frame 
is indeed the MAC address of the ultimate destination. And how does the router 
obtain this destination MAC address? From ARP, of course!
ARP for Ethernet is defined in RFC 826. A nice introduction to ARP is given in 
the TCP/IP tutorial, RFC 1180. We‚Äôll explore ARP in more detail in the homework 
problems.
6.4.2 Ethernet
Ethernet has pretty much taken over the wired LAN market. In the 1980s and the 
early 1990s, Ethernet faced many challenges from other LAN technologies,  including

token ring, FDDI, and ATM. Some of these other technologies succeeded in captur-
ing a part of the LAN market for a few years. But since its invention in the mid-
1970s, Ethernet has continued to evolve and grow and has held on to its dominant 
position. Today, Ethernet is by far the most prevalent wired LAN technology, and it 
is likely to remain so for the foreseeable future. One might say that Ethernet has been 
to local area networking what the Internet has been to global networking.
There are many reasons for Ethernet‚Äôs success. First, Ethernet was the first 
widely deployed high-speed LAN. Because it was deployed early, network admin-
istrators became intimately familiar with Ethernet‚Äîits wonders and its quirks‚Äîand 
were reluctant to switch over to other LAN technologies when they came on the 
scene. Second, token ring, FDDI, and ATM were more complex and expensive than 
Ethernet, which further discouraged network administrators from switching over. 
Third, the most compelling reason to switch to another LAN technology (such as 
FDDI or ATM) was usually the higher data rate of the new technology; however, 
Ethernet always fought back, producing versions that operated at equal data rates 
or higher. Switched Ethernet was also introduced in the early 1990s, which further 
increased its effective data rates. Finally, because Ethernet has been so popular, Eth-
ernet hardware (in particular, adapters and switches) has become a commodity and 
is remarkably cheap.
The original Ethernet LAN was invented in the mid-1970s by Bob Metcalfe 
and David Boggs. The original Ethernet LAN used a coaxial bus to interconnect the 
nodes. Bus topologies for Ethernet actually persisted throughout the 1980s and into 
the mid-1990s. Ethernet with a bus topology is a broadcast LAN‚Äîall transmitted 
frames travel to and are processed by all adapters connected to the bus. Recall that 
we covered Ethernet‚Äôs CSMA/CD multiple access protocol with binary exponential 
backoff in Section 6.3.2.
By the late 1990s, most companies and universities had replaced their LANs 
with Ethernet installations using a hub-based star topology. In such an installation 
the hosts (and routers) are directly connected to a hub with twisted-pair copper wire. 
A hub is a physical-layer device that acts on individual bits rather than frames. 
When a bit, representing a zero or a one, arrives from one interface, the hub sim-
ply re-creates the bit, boosts its energy strength, and transmits the bit onto all the 
other interfaces. Thus, Ethernet with a hub-based star topology is also a broadcast 
LAN‚Äîwhenever a hub receives a bit from one of its interfaces, it sends a copy out 
on all of its other interfaces. In particular, if a hub receives frames from two different 
interfaces at the same time, a collision occurs and the nodes that created the frames 
must retransmit.
In the early 2000s, Ethernet experienced yet another major evolutionary change. 
Ethernet installations continued to use a star topology, but the hub at the center was 
replaced with a switch. We‚Äôll be examining switched Ethernet in depth later in this 
chapter. For now, we only mention that a switch is not only ‚Äúcollision-less‚Äù but is 
also a bona-fide store-and-forward packet switch; but unlike routers, which operate 
up through layer 3, a switch operates only up through layer 2.

Ethernet Frame Structure
We can learn a lot about Ethernet by examining the Ethernet frame, which is shown 
in Figure 6.20. To give this discussion about Ethernet frames a tangible context, 
let‚Äôs consider sending an IP datagram from one host to another host, with both 
hosts on the same Ethernet LAN (for example, the Ethernet LAN in Figure¬†6.17.) 
(Although the payload of our Ethernet frame is an IP datagram, we note that an 
Ethernet frame can carry other network-layer packets as well.) Let the sending 
adapter, adapter A, have the MAC address AA-AA-AA-AA-AA-AA and the 
receiving adapter, adapter B, have the MAC address BB-BB-BB-BB-BB-BB. The 
sending adapter encapsulates the IP datagram within an Ethernet frame and passes 
the frame to the physical layer. The receiving adapter receives the frame from the 
physical layer, extracts the IP datagram, and passes the IP datagram to the network 
layer. In this context, let‚Äôs now examine the six fields of the Ethernet frame, as 
shown in Figure 6.20.
‚Ä¢ Data field (46 to 1,500 bytes). This field carries the IP datagram. The maxi-
mum transmission unit (MTU) of Ethernet is 1,500 bytes. This means that if the 
IP datagram exceeds 1,500 bytes, then the host has to fragment the datagram, 
as discussed in Section 4.3.2. The minimum size of the data field is 46 bytes. 
This means that if the IP datagram is less than 46 bytes, the data field has to be 
‚Äústuffed‚Äù to fill it out to 46 bytes. When stuffing is used, the data passed to the 
network layer contains the stuffing as well as an IP datagram. The network layer 
uses the length field in the IP datagram header to remove the stuffing.
‚Ä¢ Destination address (6 bytes). This field contains the MAC address of the 
destination adapter, BB-BB-BB-BB-BB-BB. When adapter B receives an Eth-
ernet frame whose destination address is either BB-BB-BB-BB-BB-BB or the 
MAC broadcast address, it passes the contents of the frame‚Äôs data field to the 
network layer; if it receives a frame with any other MAC address, it discards 
the frame.
‚Ä¢ Source address (6 bytes). This field contains the MAC address of the adapter that 
transmits the frame onto the LAN, in this example, AA-AA-AA-AA-AA-AA.
‚Ä¢ Type field (2 bytes). The type field permits Ethernet to multiplex network-layer 
protocols. To understand this, we need to keep in mind that hosts can use other 
network-layer protocols besides IP. In fact, a given host may support multi-
ple network-layer protocols using different protocols for different applications. 
Figure 6.20 ‚ô¶ Ethernet frame structure
Preamble
CRC
Dest.
address
Source
address
Type
Data

For this reason, when the Ethernet frame arrives at adapter B, adapter B needs  
to know to which network-layer protocol it should pass (that is, demultiplex)  
the contents of the data field. IP and other network-layer protocols (for exam-
ple, Novell IPX or AppleTalk) each have their own, standardized type number.  
Furthermore, the ARP protocol (discussed in the previous section) has its own  
type number, and if the arriving frame contains an ARP packet (i.e., has a type  
field of 0806 hexadecimal), the ARP packet will be demultiplexed up to 
the¬†ARP protocol. Note that the type field is analogous to the protocol field 
in¬†the network-layer datagram and the port-number fields in the transport-layer 
segment; all of these fields serve to glue a protocol at one layer to a protocol at 
the layer above.
‚Ä¢ Cyclic redundancy check (CRC) (4 bytes). As discussed in Section 6.2.3, the pur-
pose of the CRC field is to allow the receiving adapter, adapter B, to detect bit 
errors in the frame.
‚Ä¢ Preamble (8 bytes). The Ethernet frame begins with an 8-byte preamble field. 
Each of the first 7 bytes of the preamble has a value of 10101010; the last byte 
is 10101011. The first 7 bytes of the preamble serve to ‚Äúwake up‚Äù the receiv-
ing adapters and to synchronize their clocks to that of the sender‚Äôs clock. Why 
should the clocks be out of synchronization? Keep in mind that adapter A aims 
to transmit the frame at 10 Mbps, 100 Mbps, or 1 Gbps, depending on the type 
of Ethernet LAN. However, because nothing is absolutely perfect, adapter A will 
not transmit the frame at exactly the target rate; there will always be some drift 
from the target rate, a drift which is not known a priori by the other adapters on 
the LAN. A receiving adapter can lock onto adapter A‚Äôs clock simply by locking 
onto the bits in the first 7 bytes of the preamble. The last 2 bits of the eighth byte 
of the preamble (the first two consecutive 1s) alert adapter B that the ‚Äúimportant 
stuff‚Äù is about to come.
All of the Ethernet technologies provide connectionless service to the network 
layer. That is, when adapter A wants to send a datagram to adapter B, adapter A 
encapsulates the datagram in an Ethernet frame and sends the frame into the LAN, 
without first handshaking with adapter B. This layer-2 connectionless service is anal-
ogous to IP‚Äôs layer-3 datagram service and UDP‚Äôs layer-4 connectionless service.
Ethernet technologies provide an unreliable service to the network layer. Spe-
cifically, when adapter B receives a frame from adapter A, it runs the frame through 
a CRC check, but neither sends an acknowledgment when a frame passes the CRC 
check nor sends a negative acknowledgment when a frame fails the CRC check. 
When a frame fails the CRC check, adapter B simply discards the frame. Thus, 
adapter A has no idea whether its transmitted frame reached adapter B and passed 
the CRC check. This lack of reliable transport (at the link layer) helps to make Eth-
ernet simple and cheap. But it also means that the stream of datagrams passed to the 
network layer can have gaps.

If there are gaps due to discarded Ethernet frames, does the application at 
Host B see gaps as well? As we learned in Chapter 3, this depends on whether 
the application is using UDP or TCP. If the application is using UDP, then the 
application in Host B will indeed see gaps in the data. On the other hand, if the 
application is using TCP, then TCP in Host B will not acknowledge the data 
contained in discarded frames, causing TCP in Host A to retransmit. Note that 
when TCP retransmits data, the data will eventually return to the Ethernet adapter 
at which it was discarded. Thus, in this sense, Ethernet does retransmit data, 
although Ethernet is unaware of whether it is transmitting a brand-new datagram 
with brand-new data, or a datagram that contains data that has already been trans-
mitted at least once.
Ethernet Technologies
In our discussion above, we‚Äôve referred to Ethernet as if it were a single protocol 
standard. But in fact, Ethernet comes in many different flavors, with somewhat bewil-
dering acronyms such as 10BASE-T, 10BASE-2, 100BASE-T, 1000BASE-LX, 
BOB METCALFE AND ETHERNET
As a PhD student at Harvard University in the early 1970s, Bob Metcalfe worked 
on the ARPAnet at MIT. During his studies, he also became exposed to Abramson‚Äôs 
work on ALOHA and random access protocols. After completing his PhD and just 
before beginning a job at Xerox Palo Alto Research Center (Xerox PARC), he vis-
ited Abramson and his University of Hawaii colleagues for three months, getting a 
firsthand look at ALOHAnet. At Xerox PARC, Metcalfe became exposed to Alto com-
puters, which in many ways were the forerunners of the personal computers of the 
1980s. Metcalfe saw the need to network these computers in an inexpensive man-
ner. So armed with his knowledge about ARPAnet, ALOHAnet, and random access 
protocols, Metcalfe‚Äîalong with colleague David Boggs‚Äîinvented Ethernet.
Metcalfe and Boggs‚Äôs original Ethernet ran at 2.94 Mbps and linked up to 256 
hosts separated by up to one mile. Metcalfe and Boggs succeeded at getting most of 
the researchers at Xerox PARC to communicate through their Alto computers. Metcalfe 
then forged an alliance between Xerox, Digital, and Intel to establish Ethernet as a 
10 Mbps Ethernet standard, ratified by the IEEE. Xerox did not show much interest in 
commercializing Ethernet. In 1979, Metcalfe formed his own company, 3Com, which 
developed and commercialized networking technology, including Ethernet technol-
ogy. In particular, 3Com developed and marketed Ethernet cards in the early 1980s 
for the immensely popular IBM PCs.
CASE HISTORY

10GBASE-T and 40GBASE-T. These and many other Ethernet technologies have 
been standardized over the years by the IEEE 802.3 CSMA/CD (Ethernet) working 
group [IEEE 802.3 2020]. While these acronyms may appear bewildering, there is 
actually considerable order here. The first part of the acronym refers to the speed of 
the standard: 10, 100, 1000, or 10G, for 10 Megabit (per second), 100 Megabit, Giga-
bit, 10 Gigabit and 40 Gigibit Ethernet, respectively. ‚ÄúBASE‚Äù refers to baseband 
Ethernet, meaning that the physical media only carries Ethernet traffic; almost all of 
the 802.3 standards are for baseband Ethernet. The final part of the acronym refers to 
the physical media itself; Ethernet is both a link-layer and a physical-layer specifica-
tion and is carried over a variety of physical media including coaxial cable, copper 
wire, and fiber. Generally, a ‚ÄúT‚Äù refers to twisted-pair copper wires.
Historically, an Ethernet was initially conceived of as a segment of coaxial cable. 
The early 10BASE-2 and 10BASE-5 standards specify 10 Mbps Ethernet over two 
types of coaxial cable, each limited in length to 500 meters. Longer runs could be 
obtained by using a repeater‚Äîa physical-layer device that receives a signal on the 
input side, and regenerates the signal on the output side. A coaxial cable corresponds 
nicely to our view of Ethernet as a broadcast medium‚Äîall frames transmitted by one 
interface are received at other interfaces, and Ethernet‚Äôs CDMA/CD protocol nicely 
solves the multiple access problem. Nodes simply attach to the cable, and voila, we 
have a local area network!
Ethernet has passed through a series of evolutionary steps over the years, and 
today‚Äôs Ethernet is very different from the original bus-topology designs using coax-
ial cable. In most installations today, nodes are connected to a switch via point-to-
point segments made of twisted-pair copper wires or fiber-optic cables, as shown in 
Figures 6.15‚Äì6.17.
In the mid-1990s, Ethernet was standardized at 100 Mbps, 10 times faster than 
10 Mbps Ethernet. The original Ethernet MAC protocol and frame format were pre-
served, but higher-speed physical layers were defined for copper wire (100BASE-T) 
and fiber (100BASE-FX, 100BASE-SX, 100BASE-BX). Figure 6.21 shows these 
different standards and the common Ethernet MAC protocol and frame format. 
100 Mbps Ethernet is limited to a 100-meter distance over twisted pair, and to 
Physical
Transport
Network
Link
Application
100BASE-TX
100BASE-T4
100BASE-T2
MAC protocol
and frame format
100BASE-SX
100BASE-FX
100BASE-BX
Figure 6.21 ‚ô¶  100 Mbps Ethernet standards: A common link layer, 
 different physical layers

several kilometers over fiber, allowing Ethernet switches in different buildings to 
be connected.
Gigabit Ethernet is an extension to the highly successful 10 Mbps and 100 Mbps 
Ethernet standards. Offering a raw data rate of 40,000 Mbps, 40 Gigabit Ethernet 
maintains full compatibility with the huge installed base of Ethernet equipment. The 
standard for Gigabit Ethernet, referred to as IEEE 802.3z, does the following:
‚Ä¢ Uses the standard Ethernet frame format (Figure 6.20) and is backward com-
patible with 10BASE-T and 100BASE-T technologies. This allows for easy 
integration of Gigabit Ethernet with the existing installed base of Ethernet 
equipment.
‚Ä¢ Allows for point-to-point links as well as shared broadcast channels. Point-to-
point links use switches while broadcast channels use hubs, as described earlier. 
In Gigabit Ethernet jargon, hubs are called buffered distributors.
‚Ä¢ Uses CSMA/CD for shared broadcast channels. In order to have acceptable effi-
ciency, the maximum distance between nodes must be severely restricted.
‚Ä¢ Allows for full-duplex operation at 40 Gbps in both directions for point-to-point 
channels.
Initially operating over optical fiber, Gigabit Ethernet is now able to run over cat-
egory 5 UTP cabling (for 1000BASE-T and 10GBASE-T).
Let‚Äôs conclude our discussion of Ethernet technology by posing a question 
that may have begun troubling you. In the days of bus topologies and hub-based 
star topologies, Ethernet was clearly a broadcast link (as defined in Section 6.3) in 
which frame collisions occurred when nodes transmitted at the same time. To deal 
with these collisions, the Ethernet standard included the CSMA/CD protocol, which 
is particularly effective for a wired broadcast LAN spanning a small geographical 
region. But if the prevalent use of Ethernet today is a switch-based star topology, 
using store-and-forward packet switching, is there really a need anymore for an Eth-
ernet MAC protocol? As we‚Äôll see shortly, a switch coordinates its transmissions 
and never forwards more than one frame onto the same interface at any time. Fur-
thermore, modern switches are full-duplex, so that a switch and a node can each 
send frames to each other at the same time without interference. In other words, in 
a switch-based Ethernet LAN there are no collisions and, therefore, there is no need 
for a MAC protocol!
As we‚Äôve seen, today‚Äôs Ethernets are very different from the original Ethernet 
conceived by Metcalfe and Boggs more than 40 years ago‚Äîspeeds have increased 
by three orders of magnitude, Ethernet frames are carried over a variety of media, 
switched-Ethernets have become dominant, and now even the MAC protocol is often 
unnecessary! Is all of this really still Ethernet? The answer, of course, is ‚Äúyes, by 
definition.‚Äù It is interesting to note, however, that through all of these changes, there

has indeed been one enduring constant that has remained unchanged over 30 years‚Äî
Ethernet‚Äôs frame format. Perhaps this then is the one true and timeless centerpiece of 
the Ethernet standard.
6.4.3 Link-Layer Switches
Up until this point, we have been purposefully vague about what a switch actually 
does and how it works. The role of the switch is to receive incoming link-layer 
frames and forward them onto outgoing links; we‚Äôll study this forwarding function 
in detail in this subsection. We‚Äôll see that the switch itself is transparent to the 
hosts and routers in the subnet; that is, a host/router addresses a frame to another 
host/router (rather than addressing the frame to the switch) and happily sends the 
frame into the LAN, unaware that a switch will be receiving the frame and forward-
ing it. The rate at which frames arrive to any one of the switch‚Äôs output interfaces 
may temporarily exceed the link capacity of that interface. To accommodate this 
problem, switch output interfaces have buffers, in much the same way that router 
output interfaces have buffers for datagrams. Let‚Äôs now take a closer look at how 
switches operate.
Forwarding and Filtering
Filtering is the switch function that determines whether a frame should be for-
warded to some interface or should just be dropped. Forwarding is the switch 
function that determines the interfaces to which a frame should be directed, and 
then moves the frame to those interfaces. Switch filtering and forwarding are done 
with a switch table. The switch table contains entries for some, but not necessarily  
all,¬† of the hosts and routers on a LAN. An entry in the switch table contains 
(1)¬†a¬†MAC address, (2) the switch interface that leads toward that MAC address, 
and (3) the time at which the entry was placed in the table. An example switch table 
for the uppermost switch in Figure 6.15 is shown in Figure 6.22. This description 
of frame forwarding may sound similar to our discussion of datagram forwarding 
Figure 6.22 ‚ô¶  Portion of a switch table for the uppermost switch in  
Figure 6.15
Time
Interface
Address
62-FE-F7-11-89-A3
1
9:32
7C-BA-B2-B4-91-10
3
9:36
....
....
....

in Chapter 4. Indeed, in our discussion of generalized forwarding in Section 4.4, 
we learned that many modern packet switches can be configured to forward on the 
basis of layer-2 destination MAC addresses (i.e., function as a layer-2 switch) or 
layer-3 IP destination addresses (i.e., function as a layer-3 router). Nonetheless,  
we‚Äôll make the important distinction that switches forward packets based on MAC 
addresses rather than on IP addresses. We will also see that a traditional (i.e., in a 
non-SDN context) switch table is constructed in a very different manner from a 
router‚Äôs forwarding table.
To understand how switch filtering and forwarding work, suppose a frame with 
destination address DD-DD-DD-DD-DD-DD arrives at the switch on interface x. 
The switch indexes its table with the MAC address DD-DD-DD-DD-DD-DD. There 
are three possible cases:
‚Ä¢ There is no entry in the table for DD-DD-DD-DD-DD-DD. In this case, the switch 
forwards copies of the frame to the output buffers preceding all interfaces except 
for interface x. In other words, if there is no entry for the destination address, the 
switch broadcasts the frame.
‚Ä¢ There is an entry in the table, associating DD-DD-DD-DD-DD-DD with interface 
x. In this case, the frame is coming from a LAN segment that contains adapter 
DD-DD-DD-DD-DD-DD. There being no need to forward the frame to any of 
the other interfaces, the switch performs the filtering function by discarding the 
frame.
‚Ä¢ There is an entry in the table, associating DD-DD-DD-DD-DD-DD with interface 
y‚â†x. In this case, the frame needs to be forwarded to the LAN segment attached 
to interface y. The switch performs its forwarding function by putting the frame 
in an output buffer that precedes interface y.
Let‚Äôs walk through these rules for the uppermost switch in Figure 6.15 and its 
switch table in Figure 6.22. Suppose that a frame with destination address 62-FE-
F7-11-89-A3 arrives at the switch from interface 1. The switch examines its table 
and sees that the destination is on the LAN segment connected to interface 1 (that 
is, Electrical Engineering). This means that the frame has already been broadcast on 
the LAN segment that contains the destination. The switch therefore filters (that is, 
discards) the frame. Now suppose a frame with the same destination address arrives 
from interface 2. The switch again examines its table and sees that the destination 
is in the direction of interface 1; it therefore forwards the frame to the output buffer 
preceding interface 1. It should be clear from this example that as long as the switch 
table is complete and accurate, the switch forwards frames toward destinations 
without any broadcasting.
In this sense, a switch is ‚Äúsmarter‚Äù than a hub. But how does this switch table get 
configured in the first place? Are there link-layer equivalents to network-layer rout-
ing protocols? Or must an overworked manager manually configure the switch table?

Self-Learning
A switch has the wonderful property (particularly for the already-overworked network 
administrator) that its table is built automatically, dynamically, and autonomously‚Äî
without any intervention from a network administrator or from a configuration pro-
tocol. In other words, switches are self-learning. This capability is accomplished as 
follows:
 1. The switch table is initially empty.
 2. For each incoming frame received on an interface, the switch stores in its table 
(1) the MAC address in the frame‚Äôs source address field, (2) the interface from 
which the frame arrived, and (3) the current time. In this manner, the switch 
records in its table the LAN segment on which the sender resides. If every 
host in the LAN eventually sends a frame, then every host will eventually get 
recorded in the table.
 3. The switch deletes an address in the table if no frames are received with that 
address as the source address after some period of time (the aging time). In 
this manner, if a PC is replaced by another PC (with a different adapter), the 
MAC address of the original PC will eventually be purged from the switch 
table.
Let‚Äôs walk through the self-learning property for the uppermost switch in Fig-
ure¬†6.15 and its corresponding switch table in Figure 6.22. Suppose at time 9:39 a 
frame with source address 01-12-23-34-45-56 arrives from interface 2. Suppose that 
this address is not in the switch table. Then the switch adds a new entry to the table, 
as shown in Figure 6.23.
Continuing with this same example, suppose that the aging time for this switch 
is 60 minutes, and no frames with source address 62-FE-F7-11-89-A3 arrive to the 
switch between 9:32 and 10:32. Then at time 10:32, the switch removes this address 
from its table.
Figure 6.23 ‚ô¶  Switch learns about the location of an adapter with address 
01-12-23-34-45-56
Address
Interface
Time
01-12-23-34-45-56
2
9:39
62-FE-F7-11-89-A3
1
9:32
7C-BA-B2-B4-91-10
3
9:36
....
....
....

Switches are plug-and-play devices because they require no intervention 
from a network administrator or user. A network administrator wanting to install 
a switch need do nothing more than connect the LAN segments to the switch 
interfaces. The administrator need not configure the switch tables at the time of 
installation or when a host is removed from one of the LAN segments. Switches 
are also full-duplex, meaning any switch interface can send and receive at the 
same time.
Properties of Link-Layer Switching
Having described the basic operation of a link-layer switch, let‚Äôs now consider their 
features and properties. We can identify several advantages of using switches, rather 
than broadcast links such as buses or hub-based star topologies:
‚Ä¢ Elimination of collisions. In a LAN built from switches (and without hubs), there 
is no wasted bandwidth due to collisions! The switches buffer frames and never 
transmit more than one frame on a segment at any one time. As with a router, the 
maximum aggregate throughput of a switch is the sum of all the switch interface 
rates. Thus, switches provide a significant performance improvement over LANs 
with broadcast links.
‚Ä¢ Heterogeneous links. Because a switch isolates one link from another, the differ-
ent links in the LAN can operate at different speeds and can run over different 
media. For example, the uppermost switch in Figure 6.15 might have three1 Gbps 
1000BASE-T copper links, two 100 Mbps 100BASE-FX fiber links, and one 
100BASE-T copper link. Thus, a switch is ideal for mixing legacy equipment 
with new equipment.
‚Ä¢ Management. In addition to providing enhanced security (see sidebar on Focus on 
Security), a switch also eases network management. For example, if an adapter 
malfunctions and continually sends Ethernet frames (called a jabbering adapter), 
a switch can detect the problem and internally disconnect the malfunctioning 
adapter. With this feature, the network administrator need not get out of bed and 
drive back to work in order to correct the problem. Similarly, a cable cut discon-
nects only that host that was using the cut cable to connect to the switch. In the 
days of coaxial cable, many a network manager spent hours ‚Äúwalking the line‚Äù (or 
more accurately, ‚Äúcrawling the floor‚Äù) to find the cable break that brought down 
the entire network. Switches also gather statistics on bandwidth usage, collision 
rates, and traffic types, and make this information available to the network man-
ager. This information can be used to debug and correct problems, and to plan 
how the LAN should evolve in the future. Researchers are exploring adding yet 
more management functionality into Ethernet LANs in prototype deployments 
[Casado 2007; Koponen 2011].

Switches Versus Routers
As we learned in Chapter 4, routers are store-and-forward packet switches that for-
ward packets using network-layer addresses. Although a switch is also a store-and-
forward packet switch, it is fundamentally different from a router in that it forwards 
packets using MAC addresses. Whereas a router is a layer-3 packet switch, a switch 
is a layer-2 packet switch. Recall, however, that we learned in Section 4.4 that mod-
ern switches using the ‚Äúmatch plus action‚Äù operation can be used to forward a layer-2 
frame based on the frame's destination MAC address, as well as a layer-3 datagram 
using the datagram's destination IP address. Indeed, we saw that switches using the 
OpenFlow standard can perform generalized packet forwarding based on any of 
eleven different frame, datagram, and transport-layer header fields.
Even though switches and routers are fundamentally different, network admin-
istrators must often choose between them when installing an interconnection device. 
For example, for the network in Figure 6.15, the network administrator could just as 
easily have used a router instead of a switch to connect the department LANs, servers, 
and internet gateway router. Indeed, a router would permit interdepartmental commu-
nication without creating collisions. Given that both switches and routers are candi-
dates for interconnection devices, what are the pros and cons of the two approaches?
SNIFFING A SWITCHED LAN: SWITCH POISONING
When a host is connected to a switch, it typically only receives frames that are 
intended for it. For example, consider a switched LAN in Figure 6.17. When host 
A sends a frame to host B, and there is an entry for host B in the switch table, then 
the switch will forward the frame only to host B. If host C happens to be running a 
sniffer, host C will not be able to sniff this A-to-B frame. Thus, in a switched-LAN  
environment (in contrast to a broadcast link environment such as 802.11 LANs or  
hub‚Äìbased Ethernet LANs), it is more difficult for an attacker to sniff frames. However,  
because the switch broadcasts frames that have destination addresses that are not in 
the switch table, the sniffer at C can still sniff some frames that are not intended for 
C. Furthermore, a sniffer will be able sniff all Ethernet broadcast frames with broad-
cast destination address FF‚ÄìFF‚ÄìFF‚ÄìFF‚ÄìFF‚ÄìFF. A well-known attack against a switch, 
called switch poisoning, is to send tons of packets to the switch with many different 
bogus source MAC addresses, thereby filling the switch table with bogus entries 
and leaving no room for the MAC addresses of the legitimate hosts. This causes the 
switch to broadcast most frames, which can then be picked up by the sniffer [Skoudis 
2006]. As this attack is rather involved even for a sophisticated attacker, switches are 
significantly less vulnerable to sniffing than are hubs and wireless LANs.
FOCUS ON SECURITY

First consider the pros and cons of switches. As mentioned above, switches are 
plug-and-play, a property that is cherished by all the overworked network adminis-
trators of the world. Switches can also have relatively high filtering and forwarding 
rates‚Äîas shown in Figure 6.24, switches have to process frames only up through 
layer 2, whereas routers have to process datagrams up through layer 3. On the other 
hand, to prevent the cycling of broadcast frames, the active topology of a switched 
network is restricted to a spanning tree. Also, a large switched network would require 
large ARP tables in the hosts and routers and would generate substantial ARP traffic 
and processing. Furthermore, switches are susceptible to broadcast storms‚Äîif one 
host goes haywire and transmits an endless stream of Ethernet broadcast frames, the 
switches will forward all of these frames, causing the entire network to collapse.
Now consider the pros and cons of routers. Because network addressing is often 
hierarchical (and not flat, as is MAC addressing), packets do not normally cycle 
through routers even when the network has redundant paths. (However, packets can 
cycle when router tables are misconfigured; but as we learned in Chapter 4, IP uses 
a special datagram header field to limit the cycling.) Thus, packets are not restricted 
to a spanning tree and can use the best path between source and destination. Because 
routers do not have the spanning tree restriction, they have allowed the Internet to be 
built with a rich topology that includes, for example, multiple active links between 
Europe and North America. Another feature of routers is that they provide firewall 
protection against layer-2 broadcast storms. Perhaps the most significant drawback 
of routers, though, is that they are not plug-and-play‚Äîthey and the hosts that connect 
to them need their IP addresses to be configured. Also, routers often have a larger 
per-packet processing time than switches, because they have to process up through 
the layer-3 fields. Finally, there are two different ways to pronounce the word router, 
either as ‚Äúrootor‚Äù or as ‚Äúrowter,‚Äù and people waste a lot of time arguing over the 
proper pronunciation [Perlman 1999].
Given that both switches and routers have their pros and cons (as summarized in 
Table 6.1), when should an institutional network (for example, a university campus 
Figure 6.24 ‚ô¶ Packet processing in switches, routers, and hosts
Host
Application
Host
Transport
Network
Link
Physical
Link
Physical
Network
Switch
Router
Link
Physical
Application
Transport
Network
Link
Physical

network or a corporate campus network) use switches, and when should it use rout-
ers? Typically, small networks consisting of a few hundred hosts have a few LAN 
segments. Switches suffice for these small networks, as they localize traffic and 
increase aggregate throughput without requiring any configuration of IP addresses. 
But larger networks consisting of thousands of hosts typically include routers within 
the network (in addition to switches). The routers provide a more robust isolation of 
traffic, control broadcast storms, and use more ‚Äúintelligent‚Äù routes among the hosts 
in the network.
For more discussion of the pros and cons of switched versus routed networks, 
as well as a discussion of how switched LAN technology can be extended to accom-
modate two orders of magnitude more hosts than today‚Äôs Ethernets, see [Meyers 
2004; Kim 2008].
6.4.4 Virtual Local Area Networks (VLANs)
In our earlier discussion of Figure 6.15, we noted that modern institutional LANs 
are often configured hierarchically, with each workgroup (department) having its 
own switched LAN connected to the switched LANs of other groups via a switch 
hierarchy. While such a configuration works well in an ideal world, the real world 
is often far from ideal. Three drawbacks can be identified in the configuration in 
Figure 6.15:
‚Ä¢ Lack of traffic isolation. Although the hierarchy localizes group traffic to within 
a single switch, broadcast traffic (e.g., frames carrying ARP and DHCP mes-
sages or frames whose destination has not yet been learned by a self-learning 
switch) must still traverse the entire institutional network. Limiting the scope of 
such broadcast traffic would improve LAN performance. Perhaps more impor-
tantly, it also may be desirable to limit LAN broadcast traffic for security/privacy 
reasons. For example, if one group contains the company‚Äôs executive manage-
ment team and another group contains disgruntled employees running Wireshark 
packet sniffers, the network manager may well prefer that the executives‚Äô traffic 
never even reaches employee hosts. This type of isolation could be provided by 
Table 6.1 ‚ô¶  Comparison of the typical features of popular interconnection 
devices
Hubs
Routers
Switches
Traffic isolation
No
Yes
Yes
Plug and play
Yes
No
Yes
Optimal routing
No
Yes
No

replacing the center switch in Figure 6.15 with a router. We‚Äôll see shortly that this 
isolation also can be achieved via a switched (layer 2) solution.
‚Ä¢ Inefficient use of switches. If instead of three groups, the institution had 10 
groups, then 10 first-level switches would be required. If each group were 
small, say less than 10 people, then a single 96-port switch would likely be large 
enough to accommodate everyone, but this single switch would not provide 
traffic isolation.
‚Ä¢ Managing users. If an employee moves between groups, the physical cabling 
must be changed to connect the employee to a different switch in Figure 6.15. 
Employees belonging to two groups make the problem even harder.
Fortunately, each of these difficulties can be handled by a switch that supports 
virtual local area networks (VLANs). As the name suggests, a switch that sup-
ports VLANs allows multiple virtual local area networks to be defined over a sin-
gle physical local area network infrastructure. Hosts within a VLAN communicate 
with each other as if they (and no other hosts) were connected to the switch. In a 
port-based VLAN, the switch‚Äôs ports (interfaces) are divided into groups by the 
network manager. Each group constitutes a VLAN, with the ports in each VLAN 
forming a broadcast domain (i.e., broadcast traffic from one port can only reach 
other ports in the group). Figure 6.25 shows a single switch with 16 ports. Ports 2 
to 8 belong to the EE VLAN, while ports 9 to 15 belong to the CS VLAN (ports 1 
and 16 are unassigned). This VLAN solves all of the difficulties noted above‚ÄîEE 
and CS VLAN frames are isolated from each other, the two switches in Figure 6.15 
have been replaced by a single switch, and if the user at switch port 8 joins the CS 
Department, the network operator simply reconfigures the VLAN software so that 
port 8 is now associated with the CS VLAN. One can easily imagine how the VLAN 
switch is configured and operates‚Äîthe network manager declares a port to belong 
Figure 6.25 ‚ô¶ A single switch with two configured VLANs
1
Electrical Engineering
(VLAN ports 2‚Äì8)
Computer Science
(VLAN ports 9‚Äì15)
9
15
2
4
8
10
16

to a given VLAN (with undeclared ports belonging to a default VLAN) using switch 
management software, a table of port-to-VLAN mappings is maintained within the 
switch; and switch hardware only delivers frames between ports belonging to the 
same VLAN.
But by completely isolating the two VLANs, we have introduced a new dif-
ficulty! How can traffic from the EE Department be sent to the CS Department? 
One way to handle this would be to connect a VLAN switch port (e.g., port 1 in Fig-
ure¬†6.25) to an external router and configure that port to belong both the EE and CS 
VLANs. In this case, even though the EE and CS departments share the same physi-
cal switch, the logical configuration would look as if the EE and CS departments 
had separate switches connected via a router. An IP datagram going from the EE to 
the CS department would first cross the EE VLAN to reach the router and then be 
forwarded by the router back over the CS VLAN to the CS host. Fortunately, switch 
vendors make such configurations easy for the network manager by building a single 
device that contains both a VLAN switch and a router, so a separate external router 
is not needed. A homework problem at the end of the chapter explores this scenario 
in more detail.
Returning again to Figure 6.15, let‚Äôs now suppose that rather than having a sepa-
rate Computer Engineering department, some EE and CS faculty are housed in a 
separate building, where (of course!) they need network access, and (of course!) 
they‚Äôd like to be part of their department‚Äôs VLAN. Figure 6.26 shows a second 8-port 
switch, where the switch ports have been defined as belonging to the EE or the 
CS VLAN, as needed. But how should these two switches be interconnected? One 
easy solution would be to define a port belonging to the CS VLAN on each switch 
(similarly for the EE VLAN) and to connect these ports to each other, as shown in 
Figure¬†6.26(a). This solution doesn‚Äôt scale, however, since N VLANS would require 
N ports on each switch simply to interconnect the two switches.
A more scalable approach to interconnecting VLAN switches is known as 
VLAN trunking. In the VLAN trunking approach shown in Figure 6.26(b), a spe-
cial port on each switch (port 16 on the left switch and port 1 on the right switch) is 
configured as a trunk port to interconnect the two VLAN switches. The trunk port 
belongs to all VLANs, and frames sent to any VLAN are forwarded over the trunk 
link to the other switch. But this raises yet another question: How does a switch know 
that a frame arriving on a trunk port belongs to a particular VLAN? The IEEE has 
defined an extended Ethernet frame format, 802.1Q, for frames crossing a VLAN 
trunk. As shown in Figure 6.27, the 802.1Q frame consists of the standard Ethernet 
frame with a four-byte VLAN tag added into the header that carries the identity of 
the VLAN to which the frame belongs. The VLAN tag is added into a frame by the 
switch at the sending side of a VLAN trunk, parsed, and removed by the switch at 
the receiving side of the trunk. The VLAN tag itself consists of a 2-byte Tag Protocol 
Identifier (TPID) field (with a fixed hexadecimal value of 81-00), a 2-byte Tag Con-
trol Information field that contains a 12-bit VLAN identifier field, and a 3-bit priority 
field that is similar in intent to the IP datagram TOS field.

Figure 6.26 ‚ô¶  Connecting two VLAN switches with two VLANs:  
(a) two cables (b) trunked
1
16
1
8
1
Electrical Engineering
(VLAN ports 2‚Äì8)
b.
a.
Electrical Engineering
(VLAN ports 2, 3, 6)
Trunk
link
Computer Science
(VLAN ports 9‚Äì15)
9
15
2
4
8
10
16
1
2
3
4
5
6
8
7
Computer Science
(VLAN ports 4, 5, 7)
Figure 6.27 ‚ô¶  Original Ethernet frame (top), 802.1Q-tagged Ethernet 
VLAN frame (below)
Preamble
CRC
Dest.
address
Source
address
Type
Data
Preamble
CRC'
Dest.
address
Source
address
Type
Tag Control Information
Tag Protocol IdentiÔ¨Åer
Recomputed
CRT
Data

In this discussion, we‚Äôve only briefly touched on VLANs and have focused on port-
based VLANs. We should also mention that VLANs can be defined in several other 
ways. In MAC-based VLANs, the network manager specifies the set of MAC addresses 
that belong to each VLAN; whenever a device attaches to a port, the port is connected 
into the appropriate VLAN based on the MAC address of the device. VLANs can also 
be defined based on network-layer protocols (e.g., IPv4, IPv6, or Appletalk) and other 
criteria. It is also possible for VLANs to be extended across IP routers, allowing islands 
of LANs to be connected together to form a single VLAN that could span the globe  
[Yu 2011]. See the 802.1Q standard [IEEE 802.1q 2005] for more details.
6.5 Link Virtualization: A Network  
as a Link Layer
Because this chapter concerns link-layer protocols, and given that we‚Äôre now nearing 
the chapter‚Äôs end, let‚Äôs reflect on how our understanding of the term link has evolved. 
We began this chapter by viewing the link as a physical wire connecting two com-
municating hosts. In studying multiple access protocols, we saw that multiple hosts 
could be connected by a shared wire and that the ‚Äúwire‚Äù connecting the hosts could 
be radio spectra or other media. This led us to consider the link a bit more abstractly 
as a channel, rather than as a wire. In our study of Ethernet LANs (Figure 6.15), 
we saw that the interconnecting media could actually be a rather complex switched 
infrastructure. Throughout this evolution, however, the hosts themselves maintained 
the view that the interconnecting medium was simply a link-layer channel connect-
ing two or more hosts. We saw, for example, that an Ethernet host can be blissfully 
unaware of whether it is connected to other LAN hosts by a single short LAN seg-
ment (Figure 6.17) or by a geographically dispersed switched LAN (Figure 6.15) or 
by a VLAN (Figure 6.26).
In the case of a dialup modem connection between two hosts, the link connect-
ing the two hosts is actually the telephone network‚Äîa logically separate, global 
telecommunications network with its own switches, links, and protocol stacks for 
data transfer and signaling. From the Internet link-layer point of view, however, 
the dial-up connection through the telephone network is viewed as a simple ‚Äúwire.‚Äù 
In this sense, the Internet virtualizes the telephone network, viewing the telephone 
network as a link-layer technology providing link-layer connectivity between two 
Internet hosts. You may recall from our discussion of overlay networks in Chapter 2  
that an overlay network similarly views the Internet as a means for providing con-
nectivity between overlay nodes, seeking to overlay the Internet in the same way that 
the Internet overlays the telephone network.
In this section, we‚Äôll consider Multiprotocol Label Switching (MPLS) net-
works. Unlike the circuit-switched telephone network, MPLS is a packet-switched,

virtual-circuit network in its own right. It has its own packet formats and forwarding 
behaviors. Thus, from a pedagogical viewpoint, a discussion of MPLS fits well into a 
study of either the network layer or the link layer. From an Internet viewpoint, how-
ever, we can consider MPLS, like the telephone network and switched- Ethernets, 
as a link-layer technology that serves to interconnect IP devices. Thus, we‚Äôll con-
sider MPLS in our discussion of the link layer. Frame-relay and ATM networks 
can also be used to interconnect IP devices, though they represent a slightly older 
(but still deployed) technology and will not be covered here; see the very readable 
book [Goralski 1999] for details. Our treatment of MPLS will be necessarily brief, 
as entire books could be (and have been) written on these networks. We recommend 
[Davie 2000] for details on MPLS. We‚Äôll focus here primarily on how MPLS  servers 
interconnect to IP devices, although we‚Äôll dive a bit deeper into the underlying tech-
nologies as well.
6.5.1 Multiprotocol Label Switching (MPLS)
Multiprotocol Label Switching (MPLS) evolved from a number of industry efforts 
in the mid-to-late 1990s to improve the forwarding speed of IP routers by adopting a 
key concept from the world of virtual-circuit networks: a fixed-length label. The goal 
was not to abandon the destination-based IP datagram-forwarding infrastructure for 
one based on fixed-length labels and virtual circuits, but to augment it by selectively 
labeling datagrams and allowing routers to forward datagrams based on fixed-length 
labels (rather than destination IP addresses) when possible. Importantly, these tech-
niques work hand-in-hand with IP, using IP addressing and routing. The IETF uni-
fied these efforts in the MPLS protocol [RFC 3031, RFC 3032], effectively blending 
VC techniques into a routed datagram network.
Let‚Äôs begin our study of MPLS by considering the format of a link-layer frame 
that is handled by an MPLS-capable router. Figure 6.28 shows that a link-layer 
frame transmitted between MPLS-capable devices has a small MPLS header added 
between the layer-2 (e.g., Ethernet) header and layer-3 (i.e., IP) header. RFC 3032 
defines the format of the MPLS header for such links; headers are defined for ATM 
and frame-relayed networks as well in other RFCs. Among the fields in the MPLS 
PPP or Ethernet
header
MPLS header
IP header
Remainder of link-layer frame
Label
Exp
S
TTL
Figure 6.28 ‚ô¶  MPLS header: Located between link- and network-layer 
headers

header are the label, 3 bits reserved for experimental use, a single S bit, which is used 
to indicate the end of a series of ‚Äústacked‚Äù MPLS headers (an advanced topic that 
we‚Äôll not cover here), and a time-to-live field.
It‚Äôs immediately evident from Figure 6.28 that an MPLS-enhanced frame can 
only be sent between routers that are both MPLS capable (since a non-MPLS-capable 
router would be quite confused when it found an MPLS header where it had expected 
to find the IP header!). An MPLS-capable router is often referred to as a label-
switched router, since it forwards an MPLS frame by looking up the MPLS label 
in its forwarding table and then immediately passing the datagram to the appropriate 
output interface. Thus, the MPLS-capable router need not extract the destination IP 
address and perform a lookup of the longest prefix match in the forwarding table. But 
how does a router know if its neighbor is indeed MPLS capable, and how does a router 
know what label to associate with the given IP destination? To answer these questions, 
we‚Äôll need to take a look at the interaction among a group of MPLS-capable routers.
In the example in Figure 6.29, routers R1 through R4 are MPLS capable. R5 and 
R6 are standard IP routers. R1 has advertised to R2 and R3 that it (R1) can route to 
destination A, and that a received frame with MPLS label 6 will be forwarded to destina-
tion A. Router R3 has advertised to router R4 that it can route to destinations A and D, 
and that incoming frames with MPLS labels 10 and 12, respectively, will be switched 
toward those destinations. Router R2 has also advertised to router R4 that it (R2) can 
reach destination A, and that a received frame with MPLS label 8 will be switched 
toward A. Note that router R4 is now in the interesting position of having two MPLS 
Figure 6.29 ‚ô¶ MPLS-enhanced forwarding
R4
in
label
out
label
10
12
8
A
D
A
0
0
1
dest
out
interface
R6
R5
R3
R2
D
A
0
0
0
1
1
0
R1
in
label
out
label
6
9
A
D
1
0
10
12
dest
out
interface
in
label
out
label
‚Äì
A
0
6
dest
out
interface
in
label
out
label
6
A
0
8
dest
out
interface

paths to reach A: via interface 0 with outbound MPLS label 10, and via interface 1 with 
an MPLS label of 8. The broad picture painted in Figure 6.29 is that IP devices R5, R6, 
A, and D are connected together via an MPLS infrastructure (MPLS-capable routers R1, 
R2, R3, and R4) in much the same way that a switched LAN or an ATM network can 
connect together IP devices. And like a switched LAN or ATM network, the MPLS-
capable routers R1 through R4 do so without ever touching the IP header of a packet.
In our discussion above, we‚Äôve not specified the specific protocol used to distribute 
labels among the MPLS-capable routers, as the details of this signaling are well beyond the 
scope of this book. We note, however, that the IETF working group on MPLS has speci-
fied in [RFC 3468] that an extension of the RSVP protocol, known as RSVP-TE [RFC 
3209], will be the focus of its efforts for MPLS signaling. We‚Äôve also not discussed how 
MPLS actually computes the paths for packets among MPLS capable routers, nor how it 
gathers link-state information (e.g., amount of link bandwidth unreserved by MPLS) to 
use in these path computations. Existing link-state routing algorithms (e.g., OSPF) have 
been extended to flood this information to MPLS-capable routers. Interestingly, the actual 
path computation algorithms are not standardized, and are currently vendor-specific.
Thus far, the emphasis of our discussion of MPLS has been on the fact that MPLS 
performs switching based on labels, without needing to consider the IP address of a 
packet. The true advantages of MPLS and the reason for current interest in MPLS, 
however, lie not in the potential increases in switching speeds, but rather in the new 
traffic management capabilities that MPLS enables. As noted above, R4 has two MPLS 
paths to A. If forwarding were performed up at the IP layer on the basis of IP address, 
the IP routing protocols we studied in Chapter 5 would specify only a single, least-cost 
path to A. Thus, MPLS provides the ability to forward packets along routes that would 
not be possible using standard IP routing protocols. This is one simple form of traffic 
engineering using MPLS [RFC 3346; RFC 3272; RFC 2702; Xiao 2000], in which a 
network operator can override normal IP routing and force some of the traffic headed 
toward a given destination along one path, and other traffic destined toward the same 
destination along another path (whether for policy, performance, or some other reason).
It is also possible to use MPLS for many other purposes as well. It can be used 
to perform fast restoration of MPLS forwarding paths, e.g., to reroute traffic over a 
precomputed failover path in response to link failure [Kar 2000; Huang 2002; RFC 
3469]. Finally, we note that MPLS can, and has, been used to implement so-called 
 virtual private networks (VPNs). In implementing a VPN for a customer, an ISP uses 
its MPLS-enabled network to connect together the customer‚Äôs various networks. MPLS 
can be used to isolate both the resources and addressing used by the customer‚Äôs VPN 
from that of other users crossing the ISP‚Äôs network; see [DeClercq 2002] for details.
Our discussion of MPLS has been brief, and we encourage you to consult the ref-
erences we‚Äôve mentioned. We note that MPLS rose to prominence before the devel-
opment of software-defined networking, which we studied in Chapter 5, and that 
many of MPLS‚Äô traffic engineering capabilities can also be achieved via SDN and 
the generalized  forwarding paradigm we studied in Chapter 4. Only the future will 
tell whether MPLS and SDN will continue to co-exist, or whether newer technologies 
(such as SDN) will eventually replace MPLS.

6.6 Data Center Networking
Internet companies such as Google, Microsoft, Amazon, and Alibaba have built mas-
sive data centers, each housing tens to hundreds of thousands of hosts. As briefly 
discussed in the sidebar in Section 1.2, data centers are not only connected to the 
Internet, but also internally include complex computer networks, called data center 
networks, which interconnect their internal hosts. In this section, we provide a brief 
introduction to data center networking for cloud applications.
Broadly speaking, data centers serve three purposes. First, they provide 
content such as Web pages, search results, e-mail, or streaming video to users. 
Second, they serve as massively-parallel computing infrastructures for specific 
data processing tasks, such as distributed index computations for search engines. 
Third, they provide cloud computing to other companies. Indeed, today a major 
trend in computing is for companies to use a cloud provider such as Amazon 
Web Services, Microsoft Azure, and Alibaba Cloud to handle essentially all of 
their IT needs.
6.6.1 Data Center Architectures
Data center designs are carefully kept company secrets, as they often provide critical 
competitive advantages to leading cloud computing companies. The cost of a large 
data center is huge, exceeding $12 million per month for a 100,000 host data center 
in 2009 [Greenberg 2009a]. Of these costs, about 45 percent can be attributed to the 
hosts themselves (which need to be replaced every 3‚Äì4 years); 25 percent to infra-
structure, including transformers, uninterruptable power supplies (UPS) systems, 
generators for long-term outages, and cooling systems; 15 percent for electric utility 
costs for the power draw; and 15 percent for networking, including network gear 
(switches, routers, and load balancers), external links, and transit traffic costs. (In 
these percentages, costs for equipment are amortized so that a common cost metric 
is applied for one-time purchases and ongoing expenses such as power.) While net-
working is not the largest cost, networking innovation is the key to reducing overall 
cost and maximizing performance [Greenberg 2009a].
The worker bees in a data center are the hosts. The hosts in data centers, called 
blades and resembling pizza boxes, are generally commodity hosts that include 
CPU, memory, and disk storage. The hosts are stacked in racks, with each rack typi-
cally having 20 to 40 blades. At the top of each rack, there is a switch, aptly named 
the Top of Rack (TOR) switch, that interconnects the hosts in the rack with each 
other and with other switches in the data center. Specifically, each host in the rack 
has a network interface that connects to its TOR switch, and each TOR switch has 
additional ports that can be connected to other switches. Today, hosts typically have 
40 Gbps or 100 Gbps Ethernet connections to their TOR switches [FB 2019; Green-
berg 2015; Roy 2015; Singh 2015]. Each host is also assigned its own data-center-
internal IP address.

The data center network supports two types of traffic: traffic flowing between 
external clients and internal hosts and traffic flowing between internal hosts. To 
handle flows between external clients and internal hosts, the data center network 
includes one or more border routers, connecting the data center network to the 
public Internet. The data center network therefore interconnects the racks with each 
other and connects the racks to the border routers. Figure 6.30 shows an example of a 
data center network. Data center network design, the art of designing the intercon-
nection network and protocols that connect the racks with each other and with the 
border routers, has become an important branch of computer networking research in 
recent years. (See references in this section.)
Load Balancing
A cloud data center, such as one operated by Google, Microsoft, Amazon, and Ali-
baba, provides many applications concurrently, such as search, e-mail, and video 
applications. To support requests from external clients, each application is associ-
ated with a publicly visible IP address to which clients send their requests and from 
which they receive responses. Inside the data center, the external requests are first 
Figure 6.30 ‚ô¶ A data center network with a hierarchical topology
Internet
A
1
2
3
4
5
6
7
8
C
B
Server racks
TOR switches
Tier-2 switches
Tier-1 switches
Access router
Border router
Load
balancer

directed to a load balancer whose job it is to distribute requests to the hosts, balanc-
ing the load across the hosts as a function of their current load [Patel 2013; Eisenbud 
2016]. A large data center will often have several load balancers, each one devoted 
to a set of specific cloud applications. Such a load balancer is sometimes referred to 
as a ‚Äúlayer-4 switch‚Äù since it makes decisions based on the destination port number 
(layer 4) as well as destination IP address in the packet. Upon receiving a request for 
a particular application, the load balancer forwards it to one of the hosts that handles 
the application. (A host may then invoke the services of other hosts to help process 
the request.) The load balancer not only balances the work load across hosts, but also 
provides a NAT-like function, translating the public external IP address to the inter-
nal IP address of the appropriate host, and then translating back for packets traveling 
in the reverse direction back to the clients. This prevents clients from contacting 
hosts directly, which has the security benefit of hiding the internal network structure 
and preventing clients from directly interacting with the hosts.
Hierarchical Architecture
For a small data center housing only a few thousand hosts, a simple network consist-
ing of a border router, a load balancer, and a few tens of racks all interconnected by 
a single Ethernet switch could possibly suffice. But to scale to tens to hundreds of 
thousands of hosts, a data center often employs a hierarchy of routers and switches, 
such as the topology shown in Figure 6.30. At the top of the hierarchy, the border 
router connects to access routers (only two are shown in Figure 6.30, but there can be 
many more). Below each access router, there are three tiers of switches. Each access 
router connects to a top-tier switch, and each top-tier switch connects to multiple 
second-tier switches and a load balancer. Each second-tier switch in turn connects to 
multiple racks via the racks‚Äô TOR switches (third-tier switches). All links typically 
use Ethernet for their link-layer and physical-layer protocols, with a mix of copper 
and fiber cabling. With such a hierarchical design, it is possible to scale a data center 
to hundreds of thousands of hosts.
Because it is critical for a cloud application provider to continually provide appli-
cations with high availability, data centers also include redundant network equip-
ment and redundant links in their designs (not shown in Figure 6.30). For example, 
each TOR switch can connect to two tier-2 switches, and each access router, tier-1 
switch, and tier-2 switch can be duplicated and integrated into the design [Cisco 
2012; Greenberg 2009b]. In the hierarchical design in Figure 6.30, observe that the 
hosts below each access router form a single subnet. In order to localize ARP broad-
cast traffic, each of these subnets is further partitioned into smaller VLAN subnets, 
each comprising a few hundred hosts [Greenberg 2009a].
Although the conventional hierarchical architecture just described solves the 
problem of scale, it suffers from limited host-to-host capacity [Greenberg 2009b]. 
To understand this limitation, consider again Figure 6.30, and suppose each host 
connects to its TOR switch with a 10 Gbps link, whereas the links between switches

are 100 Gbps Ethernet links. Two hosts in the same rack can always communicate 
at a full 10 Gbps, limited only by the rate of the hosts‚Äô network interface controllers. 
However, if there are many simultaneous flows in the data center network, the maxi-
mum rate between two hosts in different racks can be much less. To gain insight into  
this issue, consider a traffic pattern consisting of 40 simultaneous flows between 
40¬†pairs of hosts in different racks. Specifically, suppose each of 10 hosts in rack 1 in 
Figure 6.30 sends a flow to a corresponding host in rack 5. Similarly, there are ten 
simultaneous flows between pairs of hosts in racks 2 and 6, ten simultaneous flows 
between racks 3 and 7, and ten simultaneous flows between racks 4 and 8. If each 
flow evenly shares a link‚Äôs capacity with other flows traversing that link, then the 
40 flows crossing the 100 Gbps A-to-B link (as well as the 100 Gbps B-to-C link) 
will each only receive 100 Gbps / 40 = 2.5 Gbps, which is significantly less than the 
10 Gbps network interface rate. The problem becomes even more acute for flows 
between hosts that need to travel higher up the hierarchy.
There are several possible solutions to this problem:
‚Ä¢ One possible solution to this limitation is to deploy higher-rate switches and 
routers. But this would significantly increase the cost of the data center, because 
switches and routers with high port speeds are very expensive.
‚Ä¢ A second solution to this problem, which can be adopted whenever possible, is 
to co-locate related services and data as close to one another as possible (e.g., in 
the same rack or in a nearby rack) [Roy 2015; Singh 2015] in order to minimize 
inter-rack communication via tier-2 or tier-1 switches. But this can only go so far, 
as a key requirement in data centers is flexibility in placement of computation and 
services [Greenberg 2009b; Farrington 2010]. For example, a large-scale Internet 
search engine may run on thousands of hosts spread across multiple racks with 
significant bandwidth requirements between all pairs of hosts. Similarly, a cloud 
computing service (such Amazon Web Services or Microsoft Azure) may wish to 
place the multiple virtual machines comprising a customer‚Äôs service on the physi-
cal hosts with the most capacity irrespective of their location in the data center. 
If these physical hosts are spread across multiple racks, network bottlenecks as 
described above may result in poor performance.
‚Ä¢ A final piece of the solution is to provide increased connectivity between the TOR 
switches and tier-2 switches, and between tier-2 switches and tier-1 switches. For 
example, as shown in Figure 6.31, each TOR switch could be connected to two 
tier-2 switches, which then provide for multiple link- and switch-disjoint paths 
between racks. In Figure 6.31, there are four distinct paths between the first tier-2 
switch and the second tier-2 switch, together providing an aggregate capacity of 
400 Gbps between the first two tier-2 switches. Increasing the degree of connectiv-
ity between tiers has two significant benefits: there is both increased capacity and 
increased reliability (because of path diversity) between switches. In Facebook‚Äôs 
data center [FB 2014; FB 2019], each TOR is connected to four different tier-2 
switches, and each tier-2 switch is connected to four different tier-1 switches.

A direct consequence of the increased connectivity between tiers in data center 
networks is that multi-path routing can become a first-class citizen in these net-
works. Flows are by default multipath flows. A very simple scheme to achieve 
multi-path routing is Equal Cost Multi Path (ECMP) [RFC 2992], which per-
forms a randomized next-hop selection along the switches between source and 
destination. Advanced schemes using finer-grained load balancing have also been 
proposed [Alizadeh 2014; Noormohammadpour 2018]. While these schemes per-
form multi-path routing at the flow level, there are also designs that route indi-
vidual packets within a flow among multiple paths [He 2015; Raiciu 2010].
6.6.2 Trends in Data Center Networking
Data center networking is evolving rapidly, with the trends being driven by cost 
reduction, virtualization, physical constraints, modularity, and customization.
Cost Reduction
In order to reduce the cost of data centers, and at the same time improve their delay 
and throughput performance, as well as ease of expansion and deployment, Internet 
cloud giants are continually deploying new data center network designs. Although 
some of these designs are proprietary, others (e.g., [FB 2019]) are explicitly open or 
described in the open literature (e.g., [Greenberg 2009b; Singh 2015]). Many impor-
tant trends can thus be identified.
Figure 6.31 illustrates one of the most important trends in data center network-
ing‚Äîthe emergence of a hierarchical, tiered network interconnecting the data center 
hosts. This hierarchy conceptually serves the same purpose as a single (very, very!), 
large crossbar switch that we studied in Section 4.2.2, allowing any host in the 
data center to communicate with any other host. But as we have seen, this tiered 
Figure 6.31 ‚ô¶ Highly interconnected data network topology
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
Server racks
TOR switches
Tier-2 switches
Tier-1 switches

interconnection network has many advantages over a conceptual crossbar switch, 
including multiple paths from source to destination and the increased capacity (due 
to multipath routing) and reliability (due to multiple switch- and link-disjoint paths 
between any two hosts).
The data center interconnection network is comprised of a large number of small-
sized switches. For example, in Google‚Äôs Jupiter datacenter fabric, one configuration has 
48 links between the ToR switch and its servers below, and connections up to 8 tier-2 
switches; a tier-2 switch has links to 256 ToR switches and links up to 16 tier-1 switches 
[Singh 2015]. In Facebook‚Äôs data center architecture, each ToR switch connects up to four 
different tier-2 switches (each in a different ‚Äúspline plane‚Äù), and each tier-2 switch connects 
up to 4 of the 48 tier-1 switches in its spline plane; there are four spline planes. Tier-1 and 
tier-2 switches connect down to a larger, scalable number of tier-2 or ToR switches, respec-
tively, below [FB 2019]. For some of the largest data center operators, these switches 
are being built in-house from commodity, off-the-shelf, merchant silicon [Greenberg 
2009b; Roy 2015; Singh 2015] rather than being purchased from switch vendors.
A multi-switch layered (tiered, multistage) interconnection network such as that 
in Figure 6.31 and as implemented in the data center architectures discussed above 
is known as Clos networks, named after Charles Clos, who studied such networks 
[Clos 1953] in the context of telephony switching. Since then, a rich theory of Clos 
networks has been developed, finding additional use in data center networking and 
in multiprocessor interconnection networks.
Centralized SDN Control and Management
Because a data center is managed by a single organization, it is perhaps natural that 
a number of the largest data center operators, including Google, Microsoft, and 
Facebook, are embracing the notion of SDN-like logically centralized control. Their 
architectures also reflect a clear separation of a data plane (comprised of relatively 
simple, commodity switches) and a software-based control plane, as we saw in Sec-
tion 5.5. Due to the immense-scale of their data centers, automated configuration and 
operational state management, as we encountered in Section 5.7, are also crucial.
Virtualization
Virtualization has been a driving force for much of the growth of cloud computing 
and data center networks more generally. Virtual Machines (VMs) decouple soft-
ware running applications from the physical hardware. This decoupling also allows 
seamless migration of VMs between physical servers, which might be located on 
different racks. Standard Ethernet and IP protocols have limitations in enabling the 
movement of VMs while maintaining active network connections across servers. 
Since all data center networks are managed by a single administrative authority, an 
elegant solution to the problem is to treat the entire data center network as a single, 
flat, layer-2 network. Recall that in a typical Ethernet network, the ARP protocol 
maintains the binding between the IP address and hardware (MAC) address on an

interface. To emulate the effect of having all hosts connect to a ‚Äúsingle‚Äù switch, the 
ARP mechanism is modified to use a DNS style query system instead of a broadcast, 
and the directory maintains a mapping of the IP address assigned to a VM and which 
physical switch the VM is currently connected to in the data center network. Scal-
able schemes that implement this basic design have been proposed in [Mysore 2009; 
Greenberg 2009b] and have been successfully deployed in modern data centers.
Physical Constraints
Unlike the wide area Internet, data center networks operate in environments that not 
only have very high capacity (40 Gbps and 100 Gbps links are now commonplace) 
but also have extremely low delays (microseconds). Consequently, buffer sizes are 
small and congestion control protocols such as TCP and its variants do not scale 
well in data centers. In data centers, congestion control protocols have to react fast 
and operate in extremely low loss regimes, as loss recovery and timeouts can lead  
to extreme inefficiency. Several approaches to tackle this issue have been proposed 
and deployed, ranging from data center-specific TCP variants [Alizadeh 2010] to 
implementing Remote Direct Memory Access (RDMA) technologies on standard 
Ethernet [Zhu 2015; Moshref 2016; Guo 2016]. Scheduling theory has also been 
applied to develop mechanisms that decouple flow scheduling from rate control, 
enabling very simple congestion control protocols while maintaining high utilization 
of the links [Alizadeh 2013; Hong 2012].
Hardware Modularity and Customization
Another major trend is to employ shipping container‚Äìbased modular data centers 
(MDCs) [YouTube 2009; Waldrop 2007]. In an MDC, a factory builds, within a 
standard 12-meter shipping container, a ‚Äúmini data center‚Äù and ships the container to 
the data center location. Each container has up to a few thousand hosts, stacked in tens 
of racks, which are packed closely together. At the data center location, multiple con-
tainers are interconnected with each other and also with the Internet. Once a prefabri-
cated container is deployed at a data center, it is often difficult to service. Thus, each 
container is designed for graceful performance degradation: as components (servers 
and switches) fail over time, the container continues to operate but with degraded per-
formance. When many components have failed and performance has dropped below a 
threshold, the entire container is removed and replaced with a fresh one.
Building a data center out of containers creates new networking challenges. 
With an MDC, there are two types of networks: the container-internal networks 
within each of the containers and the core network connecting each container [Guo 
2009; Farrington 2010]. Within each container, at the scale of up to a few thousand 
hosts, it is possible to build a fully connected network using inexpensive commodity 
Gigabit Ethernet switches. However, the design of the core network, interconnecting 
hundreds to thousands of containers while providing high host-to-host bandwidth 
across containers for typical workloads, remains a challenging problem. A hybrid

electrical/optical switch architecture for interconnecting the containers is described 
in [Farrington 2010].
Another important trend is that large cloud providers are increasingly building 
or customizing just about everything that is in their data centers, including network 
adapters, switches routers, TORs, software, and networking protocols [Greenberg 
2015; Singh 2015]. Another trend, pioneered by Amazon, is to improve reliability 
with ‚Äúavailability zones,‚Äù which essentially replicate distinct data centers in different 
nearby buildings. By having the buildings nearby (a few kilometers apart), trans-
actional data can be synchronized across the data centers in the same availability 
zone while providing fault tolerance [Amazon 2014]. Many more innovations in data 
center design are likely to continue to come.
6.7 Retrospective: A Day in the Life of a Web 
Page Request
Now that we‚Äôve covered the link layer in this chapter, and the network, transport and 
application layers in earlier chapters, our journey down the protocol stack is com-
plete! In the very beginning of this book (Section 1.1), we wrote ‚Äúmuch of this book 
is concerned with computer network protocols,‚Äù and in the first five chapters, we‚Äôve 
certainly seen that this is indeed the case! Before heading into the topical chapters in 
second part of this book, we‚Äôd like to wrap up our journey down the protocol stack by 
taking an integrated, holistic view of the protocols we‚Äôve learned about so far. One 
way then to take this ‚Äúbig picture‚Äù view is to identify the many (many!) protocols 
that are involved in satisfying even the simplest request: downloading a Web page. 
Figure 6.32 illustrates our setting: a student, Bob, connects a laptop to his school‚Äôs 
Ethernet switch and downloads a Web page (say the home page of www.google.com). 
As we now know, there‚Äôs a lot going on ‚Äúunder the hood‚Äù to satisfy this seemingly 
simple request. A Wireshark lab at the end of this chapter examines trace files con-
taining a number of the packets involved in similar scenarios in more detail.
6.7.1 Getting Started: DHCP, UDP, IP, and Ethernet
Let‚Äôs suppose that Bob boots up his laptop and then connects it to an Ethernet cable 
connected to the school‚Äôs Ethernet switch, which in turn is connected to the school‚Äôs 
router, as shown in Figure 6.32. The school‚Äôs router is connected to an ISP, in this 
example, comcast.net. In this example, comcast.net is providing the DNS service 
for the school; thus, the DNS server resides in the Comcast network rather than the 
school network. We‚Äôll assume that the DHCP server is running within the router, as 
is often the case.
When Bob first connects his laptop to the network, he can‚Äôt do anything  
(e.g., download a Web page) without an IP address. Thus, the first network-related

action taken by Bob‚Äôs laptop is to run the DHCP protocol to obtain an IP address, as 
well as other information, from the local DHCP server:
 1. The operating system on Bob‚Äôs laptop creates a DHCP request message 
 (Section 4.3.3) and puts this message within a UDP segment (Section 3.3) 
with destination port 67 (DHCP server) and source port 68 (DHCP client). The 
UDP segment is then placed within an IP datagram (Section 4.3.1) with a 
broadcast IP destination address (255.255.255.255) and a source IP address of 
0.0.0.0, since Bob‚Äôs laptop doesn‚Äôt yet have an IP address.
 2. The IP datagram containing the DHCP request message is then placed within 
an Ethernet frame (Section 6.4.2). The Ethernet frame has a destina-
tion MAC addresses of FF:FF:FF:FF:FF:FF so that the frame will be 
broadcast to all devices connected to the switch (hopefully including a 
DHCP server); the frame‚Äôs source MAC address is that of Bob‚Äôs laptop, 
00:16:D3:23:68:8A.
 3. The broadcast Ethernet frame containing the DHCP request is the first frame 
sent by Bob‚Äôs laptop to the Ethernet switch. The switch broadcasts the 
incoming frame on all outgoing ports, including the port connected to the 
router.
00:22:6B:45:1F:1B
68.85.2.1
00:16:D3:23:68:8A
68.85.2.101
comcast.net
DNS server
68.87.71.226
www.google.com
Web server
64.233.169.105
School network
68.80.2.0/24
Comcast‚Äôs network
68.80.0.0/13
Google‚Äôs network
64.233.160.0/19
1‚Äì7
8‚Äì13
18‚Äì24
14‚Äì17
Figure 6.32 ‚ô¶  A day in the life of a Web page request: Network setting 
and actions

4. The router receives the broadcast Ethernet frame containing the DHCP request 
on its interface with MAC address 00:22:6B:45:1F:1B and the IP datagram 
is extracted from the Ethernet frame. The datagram‚Äôs broadcast IP destina-
tion address indicates that this IP datagram should be processed by upper 
layer protocols at this node, so the datagram‚Äôs payload (a UDP segment) is 
thus demultiplexed (Section 3.2) up to UDP, and the DHCP request message 
is extracted from the UDP segment. The DHCP server now has the DHCP 
request message.
 5. Let‚Äôs suppose that the DHCP server running within the router can allocate IP 
addresses in the CIDR (Section 4.3.3) block 68.85.2.0/24. In this example, all 
IP addresses used within the school are thus within Comcast‚Äôs address block. 
Let‚Äôs suppose the DHCP server allocates address 68.85.2.101 to Bob‚Äôs laptop. 
The DHCP server creates a DHCP ACK message (Section 4.3.3) containing 
this IP address, as well as the IP address of the DNS server (68.87.71.226), 
the IP address for the default gateway router (68.85.2.1), and the subnet block 
(68.85.2.0/24) (equivalently, the ‚Äúnetwork mask‚Äù). The DHCP message is 
put inside a UDP segment, which is put inside an IP datagram, which is put 
inside an Ethernet frame. The Ethernet frame has a source MAC address of the 
router‚Äôs interface to the home network (00:22:6B:45:1F:1B) and a destination 
MAC address of Bob‚Äôs laptop (00:16:D3:23:68:8A).
 6. The Ethernet frame containing the DHCP ACK is sent (unicast) by the router 
to the switch. Because the switch is self-learning (Section 6.4.3) and previ-
ously received an Ethernet frame (containing the DHCP request) from Bob‚Äôs 
laptop, the switch knows to forward a frame addressed to 00:16:D3:23:68:8A 
only to the output port leading to Bob‚Äôs laptop.
 7. Bob‚Äôs laptop receives the Ethernet frame containing the DHCP ACK, extracts 
the IP datagram from the Ethernet frame, extracts the UDP segment from the 
IP datagram, and extracts the DHCP ACK message from the UDP segment. 
Bob‚Äôs DHCP client then records its IP address and the IP address of its DNS 
server. It also installs the address of the default gateway into its IP forward-
ing table (Section 4.1). Bob‚Äôs laptop will send all datagrams with destination 
address outside of its subnet 68.85.2.0/24 to the default gateway. At this point, 
Bob‚Äôs laptop has initialized its networking components and is ready to begin 
processing the Web page fetch. (Note that only the last two DHCP steps of the 
four presented in Chapter 4 are actually necessary.)
6.7.2 Still Getting Started: DNS and ARP
When Bob types the URL for www.google.com into his Web browser, he begins 
the long chain of events that will eventually result in Google‚Äôs home page being 
displayed by his Web browser. Bob‚Äôs Web browser begins the process by creating a  
TCP socket (Section 2.7) that will be used to send the HTTP request (Section 2.2) 
to¬†www.google.com. In order to create the socket, Bob‚Äôs laptop will need to know

the¬†IP address of www.google.com. We learned in Section 2.5, that the DNS  protocol 
is used to provide this name-to-IP-address translation service.
 8. The operating system on Bob‚Äôs laptop thus creates a DNS query message 
(Section 2.5.3), putting the string ‚Äúwww.google.com‚Äù in the question section 
of the DNS message. This DNS message is then placed within a UDP segment 
with a destination port of 53 (DNS server). The UDP segment is then placed 
within an IP datagram with an IP destination address of 68.87.71.226 (the 
address of the DNS server returned in the DHCP ACK in step 5) and a source 
IP address of 68.85.2.101.
 9. Bob‚Äôs laptop then places the datagram containing the DNS query message in 
an Ethernet frame. This frame will be sent (addressed, at the link layer) to the 
gateway router in Bob‚Äôs school‚Äôs network. However, even though Bob‚Äôs laptop 
knows the IP address of the school‚Äôs gateway router (68.85.2.1) via the DHCP 
ACK message in step 5 above, it doesn‚Äôt know the gateway router‚Äôs MAC 
address. In order to obtain the MAC address of the gateway router, Bob‚Äôs 
 laptop will need to use the ARP protocol (Section 6.4.1).
 10. Bob‚Äôs laptop creates an ARP query message with a target IP address of 
68.85.2.1 (the default gateway), places the ARP message within an Ethernet 
frame with a broadcast destination address (FF:FF:FF:FF:FF:FF) and sends the 
Ethernet frame to the switch, which delivers the frame to all connected devices, 
including the gateway router.
 11. The gateway router receives the frame containing the ARP query message on the 
interface to the school network, and finds that the target IP address of 68.85.2.1 in 
the ARP message matches the IP address of its interface. The gateway router thus 
prepares an ARP reply, indicating that its MAC address of 00:22:6B:45:1F:1B 
corresponds to IP address 68.85.2.1. It places the ARP reply message in an Eth-
ernet frame, with a destination address of 00:16:D3:23:68:8A (Bob‚Äôs laptop) and 
sends the frame to the switch, which delivers the frame to Bob‚Äôs laptop.
 12. Bob‚Äôs laptop receives the frame containing the ARP reply message and 
extracts the MAC address of the gateway router (00:22:6B:45:1F:1B) from the 
ARP reply message.
 13. Bob‚Äôs laptop can now (finally!) address the Ethernet frame containing the DNS 
query to the gateway router‚Äôs MAC address. Note that the IP datagram in this frame 
has an IP destination address of 68.87.71.226 (the DNS server), while the frame 
has a destination address of 00:22:6B:45:1F:1B (the gateway router). Bob‚Äôs laptop 
sends this frame to the switch, which delivers the frame to the gateway router.
6.7.3  Still Getting Started: Intra-Domain Routing to the 
DNS Server
 14. The gateway router receives the frame and extracts the IP datagram containing 
the DNS query. The router looks up the destination address of this datagram

(68.87.71.226) and determines from its forwarding table that the datagram 
should be sent to the leftmost router in the Comcast network in Figure 6.32. 
The IP datagram is placed inside a link-layer frame appropriate for the link 
connecting the school‚Äôs router to the leftmost Comcast router and the frame is 
sent over this link.
 15. The leftmost router in the Comcast network receives the frame, extracts the 
IP datagram, examines the datagram‚Äôs destination address (68.87.71.226) and 
determines the outgoing interface on which to forward the datagram toward the 
DNS server from its forwarding table, which has been filled in by  Comcast‚Äôs 
intra-domain protocol (such as RIP, OSPF or IS-IS, Section 5.3) as well as the 
Internet‚Äôs inter-domain protocol, BGP (Section 5.4).
 16. Eventually the IP datagram containing the DNS query arrives at the DNS  
server. The DNS server extracts the DNS query message, looks up the name 
www.google.com in its DNS database (Section 2.5), and finds the DNS resource  
record that contains the IP address (64.233.169.105) for www.google.com. 
(assuming that it is currently cached in the DNS server). Recall that this cached 
data originated in the authoritative DNS server (Section 2.5.2) for google.com. 
The DNS server forms a DNS reply message containing this hostname-to-IP-
address mapping, and places the DNS reply message in a UDP segment, and the 
segment within an IP datagram addressed to Bob‚Äôs laptop (68.85.2.101). This 
datagram will be forwarded back through the Comcast network to the school‚Äôs 
router and from there, via the Ethernet switch to Bob‚Äôs laptop.
 17. Bob‚Äôs laptop extracts the IP address of the server www.google.com from the 
DNS message. Finally, after a lot of work, Bob‚Äôs laptop is now ready to con-
tact the www.google.com server!
6.7.4 Web Client-Server Interaction: TCP and HTTP
 18. Now that Bob‚Äôs laptop has the IP address of www.google.com, it can create the 
TCP socket (Section 2.7) that will be used to send the HTTP GET message 
(Section 2.2.3) to www.google.com. When Bob creates the TCP socket, the 
TCP in Bob‚Äôs laptop must first perform a three-way handshake (Section 3.5.6) 
with the TCP in www.google.com. Bob‚Äôs laptop thus first creates a TCP SYN 
segment with destination port 80 (for HTTP), places the TCP segment inside an  
IP datagram with a destination IP address of 64.233.169.105 (www.google.com),  
places the datagram inside a frame with a destination MAC address of 
00:22:6B:45:1F:1B (the gateway router) and sends the frame to the switch.
 19. The routers in the school network, Comcast‚Äôs network, and Google‚Äôs network 
forward the datagram containing the TCP SYN toward www.google.com, 
using the forwarding table in each router, as in steps 14‚Äì16 above. Recall that 
the router forwarding table entries governing forwarding of packets over the 
inter-domain link between the Comcast and Google networks are determined 
by the BGP protocol (Chapter 5).

20. Eventually, the datagram containing the TCP SYN arrives at www.google.com.  
The TCP SYN message is extracted from the datagram and demultiplexed to 
the welcome socket associated with port 80. A connection socket (Section 2.7) 
is created for the TCP connection between the Google HTTP server and  
Bob‚Äôs laptop. A TCP SYNACK (Section 3.5.6) segment is generated, placed 
inside a datagram addressed to Bob‚Äôs laptop, and finally placed inside a 
link-layer frame appropriate for the link connecting www.google.com to its 
first-hop router.
 21. The datagram containing the TCP SYNACK segment is forwarded through 
the Google, Comcast, and school networks, eventually arriving at the Ethernet 
controller in Bob‚Äôs laptop. The datagram is demultiplexed within the operating 
system to the TCP socket created in step 18, which enters the connected state.
 22. With the socket on Bob‚Äôs laptop now (finally!) ready to send bytes to  
www.google.com, Bob‚Äôs browser creates the HTTP GET message (Section 2.2.3)  
containing the URL to be fetched. The HTTP GET message is then written  
into the socket, with the GET message becoming the payload of a TCP  
segment. The TCP segment is placed in a datagram and sent and delivered to 
www.google.com as in steps 18‚Äì20 above.
 23. The HTTP server at www.google.com reads the HTTP GET message from 
the TCP socket, creates an HTTP response message (Section 2.2), places the 
requested Web page content in the body of the HTTP response message, and 
sends the message into the TCP socket.
 24. The datagram containing the HTTP reply message is forwarded through the 
Google, Comcast, and school networks, and arrives at Bob‚Äôs laptop. Bob‚Äôs 
Web browser program reads the HTTP response from the socket, extracts 
the html for the Web page from the body of the HTTP response, and finally 
(finally!) displays the Web page!
Our scenario above has covered a lot of networking ground! If you‚Äôve understood 
most or all of the above example, then you‚Äôve also covered a lot of ground since you 
first read Section 1.1, where we wrote ‚Äúmuch of this book is concerned with computer 
network protocols‚Äù and you may have wondered what a protocol actually was! As 
detailed as the above example might seem, we‚Äôve omitted a number of possible addi-
tional protocols (e.g., NAT running in the school‚Äôs gateway router, wireless access to 
the school‚Äôs network, security protocols for accessing the school network or encrypt-
ing segments or datagrams, network management protocols), and considerations 
(Web caching, the DNS hierarchy) that one would encounter in the public  Internet. 
We‚Äôll cover a number of these topics and more in the second part of this book.
Lastly, we note that our example above was an integrated and holistic, but also very 
‚Äúnuts and bolts,‚Äù view of many of the protocols that we‚Äôve studied in the first part of this 
book. The example focused more on the ‚Äúhow‚Äù than the ‚Äúwhy.‚Äù For a broader, more 
reflective view on the design of network protocols in general, you might want to re-read 
the ‚ÄúArchitectural Principles of the Internet‚Äù in Section 4.5, and the references therein.

6.8 Summary
In this chapter, we‚Äôve examined the link layer‚Äîits services, the principles underly-
ing its operation, and a number of important specific protocols that use these princi-
ples in implementing link-layer services.
We saw that the basic service of the link layer is to move a network-layer data-
gram from one node (host, switch, router, WiFi access point) to an adjacent node. We 
saw that all link-layer protocols operate by encapsulating a network-layer datagram 
within a link-layer frame before transmitting the frame over the link to the adjacent 
node. Beyond this common framing function, however, we learned that different 
link-layer protocols provide very different link access, delivery, and transmission 
services. These differences are due in part to the wide variety of link types over 
which link-layer protocols must operate. A simple point-to-point link has a single 
sender and receiver communicating over a single ‚Äúwire.‚Äù A multiple access link is 
shared among many senders and receivers; consequently, the link-layer protocol for 
a multiple access channel has a protocol (its multiple access protocol) for coordinat-
ing link access. In the case of MPLS, the ‚Äúlink‚Äù connecting two adjacent nodes (for 
example, two IP routers that are adjacent in an IP sense‚Äîthat they are next-hop 
IP routers toward some destination) may actually be a network in and of itself. In 
one sense, the idea of a network being considered as a link should not seem odd. A 
telephone link connecting a home modem/computer to a remote modem/router, for 
example, is actually a path through a sophisticated and complex telephone network.
Among the principles underlying link-layer communication, we examined error-
detection and -correction techniques, multiple access protocols, link-layer address-
ing, virtualization (VLANs), and the construction of extended switched LANs and 
data center networks. Much of the focus today at the link layer is on these switched 
networks. In the case of error detection/correction, we examined how it is possible 
to add additional bits to a frame‚Äôs header in order to detect, and in some cases cor-
rect, bit-flip errors that might occur when the frame is transmitted over the link. We 
covered simple parity and checksumming schemes, as well as the more robust cyclic 
redundancy check. We then moved on to the topic of multiple access protocols. We 
identified and studied three broad approaches for coordinating access to a broadcast 
channel: channel partitioning approaches (TDM, FDM), random access approaches 
(the ALOHA protocols and CSMA protocols), and taking-turns approaches (poll-
ing and token passing). We studied the cable access network and found that it 
uses many of these multiple access methods. We saw that a consequence of hav-
ing multiple nodes share a single broadcast channel was the need to provide node 
addresses at the link layer. We learned that link-layer addresses were quite different 
from  network-layer addresses and that, in the case of the Internet, a special proto-
col (ARP‚Äîthe Address Resolution Protocol) is used to translate between these two 
forms of addressing and studied the hugely successful Ethernet protocol in detail. We 
then examined how nodes sharing a broadcast channel form a LAN and how multiple 
LANs can be connected together to form larger LANs‚Äîall without the intervention

of network-layer routing to interconnect these local nodes. We also learned how 
 multiple virtual LANs can be created on a single physical LAN infrastructure.
We ended our study of the link layer by focusing on how MPLS networks pro-
vide link-layer services when they interconnect IP routers and an overview of the 
network designs for today‚Äôs massive data centers. We wrapped up this chapter (and 
indeed the first five chapters) by identifying the many protocols that are needed to 
fetch a simple Web page. Having covered the link layer, our journey down the pro-
tocol stack is now over! Certainly, the physical layer lies below the link layer, but 
the¬†details of the physical layer are probably best left for another course (e.g., in com-
munication theory, rather than computer networking). We have, however, touched 
upon several aspects of the physical layer in this chapter and in Chapter 1 (our dis-
cussion of physical media in Section 1.2). We‚Äôll consider the physical layer again 
when we study wireless link characteristics in the next chapter.
Although our journey down the protocol stack is over, our study of computer 
networking is not yet at an end. In the following three chapters, we cover wireless 
networking, network security, and multimedia networking. These four topics do 
not fit conveniently into any one layer; indeed, each topic crosscuts many layers. 
Understanding these topics (billed as advanced topics in some networking texts) thus 
requires a firm foundation in all layers of the protocol stack‚Äîa foundation that our 
study of the link layer has now completed!
Homework Problems and Questions
Chapter 6 Review Questions
SECTIONS 6.1‚Äì6.2
 R1. Consider the transportation analogy in Section 6.1.1. If the passenger is 
analagous to a datagram, what is analogous to the link layer frame?
 R2. If all the links in the Internet were to provide reliable delivery service, would 
the TCP reliable delivery service be redundant? Why or why not?
 R3. What are some of the possible services that a link-layer protocol can offer 
to the network layer? Which of these link-layer services have corresponding 
services in IP? In TCP?
SECTION 6.3
 R4. Suppose two nodes start to transmit at the same time a packet of length L 
over a broadcast channel of rate R. Denote the propagation delay between the 
two nodes as dprop. Will there be a collision if dprop 6 L/R? Why or why not?
 R5. In Section 6.3, we listed four desirable characteristics of a broadcast channel. 
Which of these characteristics does slotted ALOHA have? Which of these 
characteristics does token passing have?

R6. In CSMA/CD, after the fifth collision, what is the probability that a node 
chooses K = 4? The result K = 4 corresponds to a delay of how many 
 seconds on a 10 Mbps Ethernet?
  R7. Describe polling and token-passing protocols using the analogy of cocktail 
party interactions.
  R8. Why would the token-ring protocol be inefficient if a LAN had a very large 
perimeter?
SECTION 6.4
  R9. How big is the MAC address space? The IPv4 address space? The IPv6 
address space?
 R10. Suppose nodes A, B, and C each attach to the same broadcast LAN (through 
their adapters). If A sends thousands of IP datagrams to B with each encap-
sulating frame addressed to the MAC address of B, will C‚Äôs adapter process 
these frames? If so, will C‚Äôs adapter pass the IP datagrams in these frames 
to the network layer C? How would your answers change if A sends frames 
with the MAC broadcast address?
 R11. Why is an ARP query sent within a broadcast frame? Why is an ARP 
response sent within a frame with a specific destination MAC address?
 R12. For the network in Figure 6.19, the router has two ARP modules, each with its 
own ARP table. Is it possible that the same MAC address appears in both tables?
 R13. Compare the frame structures for 10BASE-T, 100BASE-T, and Gigabit 
 Ethernet. How do they differ?
 R14. Consider Figure 6.15. How many subnetworks are there, in the addressing 
sense of Section 4.3?
 R15. What is the maximum number of VLANs that can be configured on a switch 
supporting the 802.1Q protocol? Why?
 R16. Suppose that N switches supporting K VLAN groups are to be connected via 
a trunking protocol. How many ports are needed to connect the switches? 
Justify your answer.
Problems
 P1. Suppose the information content of a packet is the bit pattern 1110 0110 1001 
0101 and an even parity scheme is being used. What would the value of the field 
containing the parity bits be for the case of a two-dimensional parity scheme? 
Your answer should be such that a minimum-length checksum field is used.
 P2. Show (give an example other than the one in Figure 6.5) that two-dimensional  
parity checks can correct and detect a single bit error. Show (give an example 
of) a double-bit error that can be detected but not corrected.

P3. Suppose the information portion of a packet (D in Figure 6.3) contains  
10 bytes consisting of the 8-bit unsigned binary ASCII representation of 
string ‚ÄúInternet.‚Äù Compute the Internet checksum for this data.
 P4. Consider the previous problem, but instead suppose these 10 bytes contain
a. the binary representation of the numbers 1 through 10.
b. the ASCII representation of the letters B through K (uppercase).
c. the ASCII representation of the letters b through k (lowercase).
d. Compute the Internet checksum for this data.
 P5. Consider the 5-bit generator, G = 10011, and suppose that D has the value 
1010101010. What is the value of R?
 P6. Consider the previous problem, but suppose that D has the value
a. 1000100101.
b. 0101101010.
c. 0110100011.
 P7. In this problem, we explore some of the properties of the CRC. For  
the  generator G (= 1001) given in Section 6.2.3, answer the following  
questions.
a. Why can it detect any single bit error in data D?
b. Can the above G detect any odd number of bit errors? Why?
 P8. In Section 6.3, we provided an outline of the derivation of the efficiency of 
slotted ALOHA. In this problem we‚Äôll complete the derivation.
a. Recall that when there are N active nodes, the efficiency of slotted 
ALOHA is Np(1 - p)N-1. Find the value of p that maximizes this  
expression.
b. Using the value of p found in (a), find the efficiency of slotted ALOHA 
by letting N approach infinity. Hint: (1 - 1/N)N approaches 1/e as N 
approaches infinity.
 P9. Show that the maximum efficiency of pure ALOHA is 1/(2e). Note: This 
problem is easy if you have completed the problem above!
P 10. Consider two nodes, A and B, that use the slotted ALOHA protocol to con-
tend for a channel. Suppose node A has more data to transmit than node B, 
and node A‚Äôs retransmission probability pA is greater than node B‚Äôs retrans-
mission probability, pB.
a. Provide a formula for node A‚Äôs average throughput. What is the total 
efficiency of the protocol with these two nodes?
b. If pA = 2pB, is node A‚Äôs average throughput twice as large as that of node 
B? Why or why not? If not, how can you choose pA and pB to make that 
happen?

c. In general, suppose there are N nodes, among which node A has retrans-
mission probability 2p and all other nodes have retransmission probability 
p. Provide expressions to compute the average throughputs of node A and 
of any other node.
 P11. Suppose four active nodes‚Äînodes A, B, C and D‚Äîare competing for access 
to a channel using slotted ALOHA. Assume each node has an infinite number 
of packets to send. Each node attempts to transmit in each slot with probabil-
ity p. The first slot is numbered slot 1, the second slot is numbered slot 2, and 
so on.
a. What is the probability that node A succeeds for the first time in slot 4?
b. What is the probability that some node (either A, B, C or D) succeeds in 
slot 5?
c. What is the probability that the first success occurs in slot 4?
d. What is the efficiency of this four-node system?
 P12. Graph the efficiency of slotted ALOHA and pure ALOHA as a function of  
p for the following values of N:
a. N = 10.
b. N = 30.
c. N = 50.
 P13. Consider a broadcast channel with N nodes and a transmission rate of R bps. 
Suppose the broadcast channel uses polling (with an additional polling node) 
for multiple access. Suppose the amount of time from when a node completes 
transmission until the subsequent node is permitted to transmit (that is, the 
polling delay) is dpoll. Suppose that within a polling round, a given node is 
allowed to transmit at most Q bits. What is the maximum throughput of the 
broadcast channel?
 P14. Consider three LANs interconnected by two routers, as shown in Figure 6.33.
a. Assign IP addresses to all of the interfaces. For Subnet 1 use 
addresses of the form 192.168.1.xxx; for Subnet 2 uses addresses of 
the form 192.168.2.xxx; and for Subnet 3 use addresses of the form 
192.168.3.xxx.
b. Assign MAC addresses to all of the adapters.
c. Consider sending an IP datagram from Host E to Host B. Suppose all of 
the ARP tables are up to date. Enumerate all the steps, as done for the 
single-router example in Section 6.4.1.
d. Repeat (c), now assuming that the ARP table in the sending host is empty 
(and the other tables are up to date).
 P15. Consider Figure 6.33. Now we replace the router between subnets 1 and 2 
with a switch S1, and label the router between subnets 2 and 3 as R1.

a. Consider sending an IP datagram from Host E to Host F. Will Host E ask router 
R1 to help forward the datagram? Why? In the Ethernet frame containing the 
IP datagram, what are the source and destination IP and MAC addresses?
b. Suppose E would like to send an IP datagram to B, and assume that E‚Äôs 
ARP cache does not contain B‚Äôs MAC address. Will E perform an ARP 
query to find B‚Äôs MAC address? Why? In the Ethernet frame (containing 
the IP datagram destined to B) that is delivered to router R1, what are the 
source and destination IP and MAC addresses?
c. Suppose Host A would like to send an IP datagram to Host B, and neither A‚Äôs 
ARP cache contains B‚Äôs MAC address nor does B‚Äôs ARP cache contain A‚Äôs 
MAC address. Further suppose that the switch S1‚Äôs forwarding table contains 
entries for Host B and router R1 only. Thus, A will broadcast an ARP request 
message. What actions will switch S1 perform once it receives the ARP 
request message? Will router R1 also receive this ARP request message? If 
so, will R1 forward the message to Subnet 3? Once Host B receives this ARP 
request message, it will send back to Host A an ARP response message. But 
will it send an ARP query message to ask for A‚Äôs MAC address? Why? What 
will switch S1 do once it receives an ARP response message from Host B?
 P16. Consider the previous problem, but suppose now that the router between sub-
nets 2 and 3 is replaced by a switch. Answer questions (a)‚Äì(c) in the previous 
problem in this new context.
Figure 6.33 ‚ô¶ Three subnets, interconnected by routers
Subnet 3
E
F
C
Subnet 2
D
A
B
Subnet 1

P17. Recall that with the CSMA/CD protocol, the adapter waits K #  512 bit times 
after a collision, where K is drawn randomly. For K = 100, how long does 
the adapter wait until returning to Step 2 for a 100 Mbps broadcast channel? 
For a 1 Gbps broadcast channel?
 P18. Suppose nodes A and B are on the same 10 Mbps broadcast channel, and the 
propagation delay between the two nodes is 325 bit times. Suppose CSMA/
CD and Ethernet packets are used for this broadcast channel. Suppose node 
A begins transmitting a frame and, before it finishes, node B begins transmit-
ting a frame. Can A finish transmitting before it detects that B has transmit-
ted? Why or why not? If the answer is yes, then A incorrectly believes that its 
frame was successfully transmitted without a collision. Hint: Suppose at time 
t = 0 bits, A begins transmitting a frame. In the worst case, A transmits a 
minimum-sized frame of 512 + 64 bit times. So A would finish transmitting 
the frame at t = 512 + 64 bit times. Thus, the answer is no, if B‚Äôs signal 
reaches A before bit time t = 512 + 64 bits. In the worst case, when does 
B‚Äôs signal reach A?
 P19. Suppose nodes A and B are on the same 10 Mbps broadcast channel, and the 
propagation delay between the two nodes is 245 bit times. Suppose A and  
B send Ethernet frames at the same time, the frames collide, and then A and  
B choose different values of K in the CSMA/CD algorithm. Assuming no  
other nodes are active, can the retransmissions from A and B collide? For our 
purposes, it suffices to work out the following example. Suppose A and B 
begin transmission at t = 0 bit times. They both detect collisions at t = 245
t bit times. Suppose KA = 0 and KB = 1. At what time does B schedule its 
retransmission? At what time does A begin transmission? (Note: The nodes 
must wait for an idle channel after returning to Step 2‚Äîsee protocol.) At 
what time does A‚Äôs signal reach B? Does B refrain from transmitting at its 
scheduled time?
 P20. In this problem, you will derive the efficiency of a CSMA/CD-like multiple 
access protocol. In this protocol, time is slotted and all adapters are synchro-
nized to the slots. Unlike slotted ALOHA, however, the length of a slot (in 
seconds) is much less than a frame time (the time to transmit a frame). Let S 
be the length of a slot. Suppose all frames are of constant length L = kRS,  
where R is the transmission rate of the channel and k is a large integer. Sup-
pose there are N nodes, each with an infinite number of frames to send. We 
also assume that dprop 6 S, so that all nodes can detect a collision before the 
end of a slot time. The protocol is as follows:
‚Ä¢ If, for a given slot, no node has possession of the channel, all nodes 
contend for the channel; in particular, each node transmits in the slot with 
probability p. If exactly one node transmits in the slot, that node takes 
possession of the channel for the subsequent k - 1 slots and transmits its 
entire frame.

from transmitting until the node that possesses the channel has finished 
transmitting its frame. Once this node has transmitted its frame, all nodes 
contend for the channel.
 
 Note that the channel alternates between two states: the productive state, 
which lasts exactly k slots, and the nonproductive state, which lasts for a ran-
dom number of slots. Clearly, the channel efficiency is the ratio of k/(k + x), 
where x is the expected number of consecutive unproductive slots.
a. For fixed N and p, determine the efficiency of this protocol.
b. For fixed N, determine the p that maximizes the efficiency.
c. Using the p (which is a function of N) found in (b), determine the effi-
ciency as N approaches infinity.
d. Show that this efficiency approaches 1 as the frame length becomes large.
 P21. Consider Figure 6.33 in problem P14. Provide MAC addresses and IP 
addresses for the interfaces at Host A, both routers, and Host F. Suppose 
Host A sends a datagram to Host F. Give the source and destination MAC 
addresses in the frame encapsulating this IP datagram as the frame is trans-
mitted (i) from A to the left router, (ii) from the left router to the right router, 
(iii) from the right router to F. Also give the source and destination IP 
addresses in the IP datagram encapsulated within the frame at each of these 
points in time.
 P22. Suppose now that the leftmost router in Figure 6.33 is replaced by a switch. 
Hosts A, B, C, and D and the right router are all star-connected into this 
switch. Give the source and destination MAC addresses in the frame encap-
sulating this IP datagram as the frame is transmitted (i) from A to the switch, 
(ii) from the switch to the right router, (iii) from the right router to F. Also 
give the source and destination IP addresses in the IP datagram encapsulated 
within the frame at each of these points in time.
 P23. Consider Figure 6.15. Suppose that all links are 1 Gbps. What is the maxi-
mum total aggregate throughput that can be achieved among the 9 hosts and 
2 servers in this network? You can assume that any host or server can send to 
any other host or server. Why?
 P24. Suppose the three departmental switches in Figure 6.15 are replaced by hubs. 
All links are 1 Gbps. Now answer the questions posed in problem P23.
 P25. Suppose that all the switches in Figure 6.15 are replaced by hubs. All links 
are 1 Gbps. Now answer the questions posed in problem P23.
 P26. Let‚Äôs consider the operation of a learning switch in the context of a network 
in which 6 nodes labeled A through F are star connected into an Ethernet 
switch. Suppose that (i) B sends a frame to E, (ii) E replies with a frame to B, 
(iii) A sends a frame to B, (iv) B replies with a frame to A. The switch table 
PROBLEMS     525

is initially empty. Show the state of the switch table before and after each 
of these events. For each of these events, identify the link(s) on which the 
transmitted frame will be forwarded, and briefly justify your answers.
 P27. In this problem, we explore the use of small packets for Voice-over-IP appli-
cations. One of the drawbacks of a small packet size is that a large fraction of 
link bandwidth is consumed by overhead bytes. To this end, suppose that the 
packet consists of P bytes and 5 bytes of header.
a. Consider sending a digitally encoded voice source directly. Suppose the 
source is encoded at a constant rate of 128 kbps. Assume each packet is 
entirely filled before the source sends the packet into the network. The 
time required to fill a packet is the packetization delay. In terms of L, 
determine the packetization delay in milliseconds.
b. Packetization delays greater than 20 msec can cause a noticeable and 
unpleasant echo. Determine the packetization delay for L = 1,500 bytes 
(roughly corresponding to a maximum-sized Ethernet packet) and for 
L = 50 (corresponding to an ATM packet).
c. Calculate the store-and-forward delay at a single switch for a link rate of 
R = 622 Mbps for L = 1,500 bytes, and for L = 50 bytes.
d. Comment on the advantages of using a small packet size.
 P28. Consider the single switch VLAN in Figure 6.25, and assume an external 
router is connected to switch port 1. Assign IP addresses to the EE and CS 
hosts and router interface. Trace the steps taken at both the network layer  
and the link layer to transfer an IP datagram from an EE host to a CS host  
(Hint: Reread the discussion of Figure 6.19 in the text).
 P29. Consider the MPLS network shown in Figure 6.29, and suppose that rout-
ers R5 and R6 are now MPLS enabled. Suppose that we want to perform 
traffic engineering so that packets from R6 destined for A are switched to 
A via R6-R4-R3-R1, and packets from R5 destined for A are switched via 
R5-R4-R2-R1. Show the MPLS tables in R5 and R6, as well as the modified 
table in R4, that would make this possible.
 P30. Consider again the same scenario as in the previous problem, but suppose 
that packets from R6 destined for D are switched via R6-R4-R3, while pack-
ets from R5 destined to D are switched via R4-R2-R1-R3. Show the MPLS 
tables in all routers that would make this possible.
 P31. In this problem, you will put together much of what you have learned about 
Internet protocols. Suppose you walk into a room, connect to Ethernet, and 
want to download a Web page. What are all the protocol steps that take place, 
starting from powering on your PC to getting the Web page? Assume there  
is nothing in our DNS or browser caches when you power on your PC.

(Hint: The steps include the use of Ethernet, DHCP, ARP, DNS, TCP, and 
HTTP protocols.) Explicitly indicate in your steps how you obtain the IP and 
MAC addresses of a gateway router.
 P32. Consider the data center network with hierarchical topology in Figure 6.30. 
Suppose now there are 80 pairs of flows, with ten flows between the first 
and ninth rack, ten flows between the second and tenth rack, and so on. 
Further suppose that all links in the network are 10 Gbps, except for the links 
between hosts and TOR switches, which are 1 Gbps.
a. Each flow has the same data rate; determine the maximum rate of a flow.
b. For the same traffic pattern, determine the maximum rate of a flow for the 
highly interconnected topology in Figure 6.31.
c. Now suppose there is a similar traffic pattern, but involving 20 hosts on 
each rack and 160 pairs of flows. Determine the maximum flow rates for 
the two topologies.
 P33. Consider the hierarchical network in Figure 6.30 and suppose that the data 
center needs to support e-mail and video distribution among other applica-
tions. Suppose four racks of servers are reserved for e-mail and four racks are 
reserved for video. For each of the applications, all four racks must lie below 
a single tier-2 switch since the tier-2 to tier-1 links do not have sufficient 
bandwidth to support the intra-application traffic. For the e-mail application, 
suppose that for 99.9 percent of the time only three racks are used, and that 
the video application has identical usage patterns.
a. For what fraction of time does the e-mail application need to use a fourth 
rack? How about for the video application?
b. Assuming e-mail usage and video usage are independent, for what fraction 
of time do (equivalently, what is the probability that) both applications 
need their fourth rack?
c. Suppose that it is acceptable for an application to have a shortage of serv-
ers for 0.001 percent of time or less (causing rare periods of performance 
degradation for users). Discuss how the topology in Figure 6.31 can be 
used so that only seven racks are collectively assigned to the two applica-
tions (assuming that the topology can support all the traffic).
Wireshark Labs: 802.11 Ethernet
At the Companion website for this textbook, http://www.pearsonhighered.com/ 
cs-resources/, you‚Äôll find a Wireshark lab that examines the operation of the IEEE 
802.3 protocol and the Wireshark frame format. A second Wireshark lab examines 
packet traces taken in a home network scenario.

Albert Greenberg
AN INTERVIEW WITH‚Ä¶
Albert Greenberg is Microsoft Corporate Vice President for Azure 
Networking. He leads development for the Azure Networking team, 
which is responsible for networking R&D at Microsoft - within and 
across data centers and edge sites; global terrestrial and subsea  
networks; optical networking; FPGA and SmartNIC offloads;  
access and hybrid cloud networking; host networking and network 
virtualization; application load balancers and network virtual appli-
ances; network services and analytics; security services; container 
networking; content distribution networks; edge networking including 
application acceleration and 5G, and first party networks. To meet 
the challenges of agility and quality that comes with cloud scale, 
his team has developed and embraced custom hardware, machine 
learning, and open source. Albert moved to Microsoft in 2007  
to innovate on Cloud and bring networking to the host (network  
virtualization), ideas that appeared, among many, in his VL2 paper, 
and which underly Cloud networking today.
Prior to joining Microsoft, Albert worked at Bell Labs and AT&T 
Labs as an AT&T Fellow. He helped build the systems and tools  
that run AT&T‚Äôs networks, and pioneered the architecture and systems  
at the foundations of software-defined networking. He holds an  
AB in Mathematics from Dartmouth College and a PhD in Computer 
Science from the University of Washington. 
Albert is a member of the National Academy of Engineering, 
and an ACM Fellow. He has received the IEEE Koji Kobayashi 
Computer and Communication Award, ACM Sigcomm Award, and 
ACM Sigcomm and Sigmetrics Test of Time paper awards. Albert 
and wife Kathryn are proud parents of four daughters. He grew up 
in New Orleans. While the Seattle Seahawks are his team, he  
cannot shake his fondness for the Saints.

What brought you to specialize in networking?
I‚Äôve always liked solving real-world problems, and also liked mathematics. I‚Äôve found 
that the field of networking has lots of room and scope to do both. That mix was very 
appealing to me. While working on a PhD at the University of Washington, I benefited 
from the influence of Ed Lazowska on the systems side, and Richard Ladner and Martin 
Tompa on the mathematical and theoretical side. One of my MS course projects was to 
get two machines from the same vendor to talk to each other. Now it seems you can‚Äôt stop 
machines from communicating!
Do you have any advice for students entering the networking/Internet field? 
The face of networking is changing. It‚Äôs becoming a very diverse, inclusive and open  
environment. I mean that in two ways. First, we will see far much more diversity among  
our network developers and researchers, including women and other underrepresented 
groups in technology. I‚Äôm proud of the diversity and inclusivity of the team at Microsoft, 
and my earlier teams at AT&T. Diversity makes us more resilient, better able to adapt to 
change, and makes our decisions better. Second, one can bring a diversity of technical 
skills and interests to networking. Those interests might be in architecture, programming 
languages, optics, formal methods, data science, AI, or in fault tolerant and reliable system 
design. Open source systems are having enormous impact. SONiC, a Linux-based an open 
source initiative for networking operating systems, is a great example. Read this book, and 
bring your whole set of skills, experience and knowledge set to creating the networks of the 
future. SDN and Disaggregation brings diversity and openness. So exciting. 
Can you describe one or two of the most exciting projects you have worked on during 
your career? What were the biggest challenges?
The cloud is by far the biggest thing to come along in a long time. The challenges there are  
head and shoulders above other system challenges I‚Äôve worked on, in part because the cloud 
incorporate so many aspects of systems. Cloud scenarios stretch tremendously the challenge 
of networking. Traditional networking technology is only part of it; in practice today there‚Äôs 
operating systems and distributed systems, architecture, performance, security, reliability, 
machine learning, data science, and management‚Äìthe whole stack. If we used to think of 
these individual areas as ‚Äúgardens‚Äù, we can think of the cloud as a ‚Äúfarm‚Äù made up of all 
of these wonderful gardens. And the operational concerns of designing, monitoring and 
managing an ultra-reliable global-scale system are crucial, as the cloud provides critically 
important infrastructure for government, industry, education and more. All of that has to 
be rock solid; it needs to be secure; it needs to be trustworthy. Software is, of course, key 
to effectively monitoring and managing such a massive cloud. Here, SDN plays the central 
role in managing and provisioning at scale, creating, in essence, a software-defined data 
center. Software allows us to also innovate rapidly.

How do you envision the future of networking and the Internet? What major challenges/
obstacles do you think lie ahead in their development, particularly in the areas of data 
center networking, and edge networks?
I‚Äôve already talked about Cloud, and we are just say 10% into its evolution. Yet, it‚Äôs clear 
that the division of work in the end-to-end system will be an increasingly important issue. 
How much computation and storage will happen in the application and at the end-host? 
How much will happen in cloud components at the network‚Äôs ‚Äúedge‚Äù, at or near the end 
host or container? And how much will happen in the data centers themselves. How will 
all of this be orchestrated? We‚Äôll see cloud computing being pushed closer to the edge and 
we‚Äôll see ‚Äúhorizontal‚Äù growth‚Äìa richer end-to-end computing/data/networking ecosystem‚Äì 
not just growth, say within a data center. This will be an area of great innovation.  
5G wireless will be an important part of this mix. 
Who has inspired you professionally?
I‚Äôve learned a tremendous amount, at both Microsoft and AT&T, from customers and from 
the live site. Interacting with engineers inspires me, for their passion for dev and dev-ops  
of the entire lifecycle (invention to development to deployment to ultimate decommission)  
of operational services and systems. These are the people who know architecture and  
systems from end to end, inside out. They‚Äôre great to work with and have so much insight, 
experience and knowledge to share, whether that be Microsoft‚Äôs Azure Cloud or earlier  
in my career AT&T‚Äôs networks. I‚Äôve also loved working with the researchers who have 
established some of the principles underlying the design and management of these  
at-scale systems.

telephony. The number of worldwide mobile cellular subscribers increased from 
34¬†million in 1993 to 8.3 billion subscribers in 2019. There are now a larger number 
of mobile phone subscriptions than there are people on our planet. The many advan-
tages of cell phones are evident to all‚Äîanywhere, anytime, untethered access to the 
global telephone network via a highly portable lightweight device. More recently, 
smartphones, tablets, and laptops have become wirelessly connected to the Internet 
via a cellular or WiFi network. And increasingly, devices such as gaming consoles, 
thermostats, home security systems, home appliances, watches, eye glasses, cars, 
traffic control systems and more are being wirelessly connected to the Internet.
From a networking standpoint, the challenges posed by networking these wire-
less and mobile devices, particularly at the link layer and the network layer, are so 
different from traditional wired computer networks that an individual chapter devoted 
to the study of wireless and mobile networks (i.e., this chapter) is appropriate.
We‚Äôll begin this chapter with a discussion of mobile users, wireless links, and 
networks, and their relationship to the larger (typically wired) networks to which 
they connect. We‚Äôll draw a distinction between the challenges posed by the  wireless 
nature of the communication links in such networks, and by the mobility that these 
wireless links enable. Making this important distinction‚Äîbetween wireless and 
mobility‚Äîwill allow us to better isolate, identify, and master the key concepts in 
each area.
We will begin with an overview of wireless access infrastructure and associ-
ated¬† terminology. We‚Äôll then consider the characteristics of this wireless link in 
7
CHAPTER
Wireless 
and Mobile 
Networks
531

Section 7.2. We include a brief introduction to code division multiple access (CDMA), 
a shared-medium access protocol that is often used in wireless networks, in Section 
7.2. In Section 7.3, we‚Äôll examine the link-level aspects of the IEEE 802.11 (WiFi) 
wireless LAN standard in some depth; we‚Äôll also say a few words about Bluetooth 
wireless personal area networks. In Section 7.4, we‚Äôll provide an overview of cellular 
Internet access, including 4G and emerging 5G cellular technologies that provide 
both voice and high-speed Internet access. In Section 7.5, we‚Äôll turn our attention to 
mobility, focusing on the problems of locating a mobile user, routing to the mobile 
user, and ‚Äúhanding over‚Äù the mobile user who dynamically moves from one point 
of attachment to the network to another. We‚Äôll examine how these mobility services 
are implemented in the 4G/5G cellular networks, and the in the Mobile IP standard 
in Section 7.6. Finally, we‚Äôll consider the impact of wireless links and mobility on 
transport-layer protocols and networked applications in Section 7.7.
7.1 Introduction
Figure 7.1 shows the setting in which we‚Äôll consider the topics of wireless data com-
munication and mobility. We‚Äôll begin by keeping our discussion general enough to 
cover a wide range of networks, including both wireless LANs such as WiFi and 4G 
and 5G cellular networks; we‚Äôll drill down into a more detailed discussion of specific 
wireless architectures in later sections. We can identify the following elements in a 
wireless network:
‚Ä¢ Wireless hosts. As in the case of wired networks, hosts are the end-system devices 
that run applications. A wireless host might be a smartphone, tablet, or laptop, or 
it could be an Internet of Things (IoT) device such as a sensor, appliance, auto-
mobile, or any other of the myriad devices being connected to the Internet. The 
hosts themselves may or may not be mobile.
‚Ä¢ Wireless links. A host connects to a base station (defined below) or to another 
wireless host through a wireless communication link. Different wireless link 
technologies have different transmission rates and can transmit over different 
distances. Figure 7.2 shows two key characteristics, link transmission rates and 
coverage ranges, of the more popular wireless network standards. (The figure is 
only meant to provide a rough idea of these characteristics. For example, some 
of these types of networks are only now being deployed, and some link rates 
can increase or decrease beyond the values shown depending on distance, chan-
nel conditions, and the number of users in the wireless network.) We‚Äôll cover 
these standards later in the first half of this chapter; we‚Äôll also consider other 
wireless link characteristics (such as their bit error rates and the causes of bit 
errors) in Section 7.2.

In Figure 7.1, wireless links connect wireless hosts located at the edge of the 
network into the larger network infrastructure. We hasten to add that wireless 
links are also sometimes used within a network to connect routers, switches, and 
other network equipment. However, our focus in this chapter will be on the use of 
wireless communication at the network edge, as it is here that many of the most 
exciting technical challenges, and most of the growth, are occurring.
‚Ä¢ Base station. The base station is a key part of the wireless network infrastructure. 
Unlike the wireless host and wireless link, a base station has no obvious counter-
part in a wired network. A base station is responsible for sending and receiving 
data (e.g., packets) to and from a wireless host that is associated with that base 
station. A base station will often be responsible for coordinating the transmission 
of multiple wireless hosts with which it is associated. When we say a wireless host 
is ‚Äúassociated‚Äù with a base station, we mean that (1) the host is within the wireless 
communication distance of the base station, and (2) the host uses that base station 
to relay data between it (the host) and the larger network. Cell towers in cellular 
networks and access points in 802.11 wireless LANs are examples of base stations.
Figure 7.1 ‚ô¶ Elements of a wireless network
Network
infrastructure
Key:
Wireless access point
Coverage area
Wireless host
Wireless host in motion

In Figure 7.1, the base station is connected to the larger network (e.g., the  Internet, 
corporate or home network), thus functioning as a link-layer relay between the 
wireless host and the rest of the world with which the host communicates.
 
Hosts associated with a base station are often referred to as operating in 
 infrastructure mode, since all traditional network services (e.g., address assign-
ment and routing) are provided by the network to which a host is connected via 
the base station. In ad hoc networks, wireless hosts have no such infrastructure 
with which to connect. In the absence of such infrastructure, the hosts themselves 
must provide for services such as routing, address assignment, DNS-like name 
translation, and more.
 
When a mobile host moves beyond the range of one base station and into the 
range of another, it will change its point of attachment into the larger network 
(i.e., change the base station with which it is associated)‚Äîa process referred to as 
handoff or handover. Such mobility raises many challenging questions. If a host 
can move, how does one find the mobile host‚Äôs current location in the network 
so that data can be forwarded to that mobile host? How is addressing performed, 
given that a host can be in one of many possible locations? If the host moves 
during a TCP connection or phone call, how is data routed so that the connection 
Figure 7.2 ‚ô¶  Wireless transmission rates and range for WiFi, cellular 
4G/5G and Bluetooth standards (note: axes are not linear)
802.11ax
802.11ac
802.11n
802.11 af,ah
5G
4G LTE
802.11g
802.11b
Bluetooth
Indoor
Outdoor
Mid range
outdoor
Long range
outdoor
10‚Äì30m
50‚Äì200m
200m‚Äì4Km
4Km‚Äì15Km
14 Gbps
10 Gbps
3.5 Gbps
600 Mbps
54 Mbps
2 Mbps
11 Mbps

continues uninterrupted? These and many (many!) other questions make wireless 
and mobile networking an area of exciting networking research.
‚Ä¢ Network infrastructure. This is the larger network with which a wireless host may 
wish to communicate.
Having discussed the ‚Äúpieces‚Äù of a wireless network, we note that these pieces 
can be combined in many different ways to form different types of wireless net-
works. You may find a taxonomy of these types of wireless networks useful as you 
read on in this chapter, or read/learn more about wireless networks beyond this book. 
At the highest level we can classify wireless networks according to two criteria: (i) 
whether a packet in the wireless network crosses exactly one wireless hop or multiple 
wireless hops, and (ii) whether there is infrastructure such as a base station in the 
network:
‚Ä¢ Single-hop, infrastructure-based. These networks have a base station that is con-
nected to a larger wired network (e.g., the Internet). Furthermore, all communica-
tion is between this base station and a wireless host over a single wireless hop. The 
802.11 networks you use in the classroom, caf√©, or library; and the 4G LTE data 
networks that we will learn about shortly all fall in this category. The vast majority 
of our daily interactions are with single-hop, infrastructure-based  wireless networks.
‚Ä¢ Single-hop, infrastructure-less. In these networks, there is no base station that 
is connected to a wireless network. However, as we will see, one of the nodes 
in this single-hop network may coordinate the transmissions of the other nodes. 
 Bluetooth networks (that connect small wireless devices such as keyboards, 
speakers, and headsets, and which we will study in Section 7.3.6) are single-hop, 
infrastructure-less networks.
‚Ä¢ Multi-hop, infrastructure-based. In these networks, a base station is present that 
is wired to the larger network. However, some wireless nodes may have to relay 
their communication through other wireless nodes in order to communicate via 
the base station. Some wireless sensor networks and so-called wireless mesh 
networks deployed in homes fall in this category.
‚Ä¢ Multi-hop, infrastructure-less. There is no base station in these networks, and 
nodes may have to relay messages among several other nodes in order to reach 
a destination. Nodes may also be mobile, with connectivity changing among 
nodes‚Äîa class of networks known as mobile ad hoc networks (MANETs). 
If the mobile nodes are vehicles, the network is a vehicular ad hoc network 
(VANET). As you might imagine, the development of protocols for such net-
works is challenging and is the subject of much ongoing research.
In this chapter, we‚Äôll mostly confine ourselves to single-hop networks, and then 
mostly to infrastructure-based networks.

Let‚Äôs now dig deeper into the technical challenges that arise in wireless and 
mobile networks. We‚Äôll begin by first considering the individual wireless link, defer-
ring our discussion of mobility until later in this chapter.
7.2 Wireless Links and Network Characteristics
Wireless links differ from their wired counterparts in a number important ways:
‚Ä¢ Decreasing signal strength. Electromagnetic radiation attenuates as it passes 
through matter (e.g., a radio signal passing through a wall). Even in free space, 
the signal will disperse, resulting in decreased signal strength (sometimes referred 
to as path loss) as the distance between sender and receiver increases.
‚Ä¢ Interference from other sources. Radio sources transmitting in the same fre-
quency band will interfere with each other. For example, 2.4 GHz wireless 
phones and 802.11b wireless LANs transmit in the same frequency band. Thus, 
the 802.11b wireless LAN user talking on a 2.4 GHz wireless phone can expect 
that neither the network nor the phone will perform particularly well. In addi-
tion to interference from transmitting sources, electromagnetic noise within the 
environment (e.g., a nearby motor, a microwave) can result in interference. For 
this reason, a number of more recent 802.11 standards operate in the 5GHz 
frequency band.
‚Ä¢ Multipath propagation. Multipath propagation occurs when portions of the 
electromagnetic wave reflect off objects and the ground, taking paths of different 
lengths between a sender and receiver. This results in the blurring of the received 
signal at the receiver. Moving objects between the sender and receiver can cause 
multipath propagation to change over time.
For a detailed discussion of wireless channel characteristics, models, and measure-
ments, see [Anderson 1995; Almers 2007].
The discussion above suggests that bit errors will be more common in wireless 
links than in wired links. For this reason, it is perhaps not surprising that wireless 
link protocols (such as the 802.11 protocol we‚Äôll examine in the following section) 
employ not only powerful CRC error detection codes, but also link-level relia-
ble-data-transfer protocols that retransmit corrupted frames.
Having considered the impairments that can occur on a wireless channel, let‚Äôs next 
turn our attention to the host receiving the wireless signal. This host receives an elec-
tromagnetic signal that is a combination of a degraded form of the original signal trans-
mitted by the sender (degraded due to the attenuation and multipath propagation effects 
that we discussed above, among others) and background noise in the environment.  
The signal-to-noise ratio (SNR) is a relative measure of the strength of the received 
signal (i.e., the information being transmitted) and this noise. The SNR is typically 
measured in units of decibels (dB), a unit of measure that some think is used by

electrical engineers primarily to confuse computer scientists. The SNR, measured 
in dB, is 20 times the ratio of the base-10 logarithm of the amplitude of the received 
signal to the amplitude of the noise. For our purposes here, we need only know that 
a larger SNR makes it easier for the receiver to extract the transmitted signal from 
the background noise.
Figure 7.3 (adapted from [Holland 2001]) shows the bit error rate (BER)‚Äî
roughly speaking, the probability that a transmitted bit is received in error at the 
receiver‚Äîversus the SNR for three different modulation techniques for encod-
ing information for transmission on an idealized wireless channel. The theory 
of modulation and coding, as well as signal extraction and BER, is well beyond 
the scope of this text (see [Schwartz 1980; Goldsmith 2005] for a discussion of 
these topics). Nonetheless, Figure 7.3 illustrates several physical-layer charac-
teristics that are important in understanding higher-layer wireless communica-
tion protocols:
‚Ä¢ For a given modulation scheme, the higher the SNR, the lower the BER. Since 
a sender can increase the SNR by increasing its transmission power, a sender 
can decrease the probability that a frame is received in error by increasing its 
transmission power. Note, however, that there is arguably little practical gain 
in increasing the power beyond a certain threshold, say to decrease the BER 
from 10-12 to 10-13. There are also disadvantages associated with increas-
ing the  transmission power: More energy must be expended by the sender  
Figure 7.3 ‚ô¶ Bit error rate, transmission rate, and SNR
10‚Äì7
10‚Äì6
10‚Äì5
10‚Äì4
10‚Äì3
10‚Äì2
10‚Äì1
10
20
30
40
0
SNR (dB)
BER
QAM16
(4 Mbps)
QAM256
(8 Mbps)
BPSK
(1 Mbps)

(an important concern for battery-powered mobile users), and the sender‚Äôs 
transmissions are more likely to interfere with the transmissions of another 
sender (see Figure 7.4(b)).
‚Ä¢ For a given SNR, a modulation technique with a higher bit transmission rate 
(whether in error or not) will have a higher BER. For example, in Figure 7.3, 
with an SNR of 10 dB, BPSK modulation with a transmission rate of 1 Mbps has 
a BER of less than 10-7, while with QAM16 modulation with a transmission rate 
of 4 Mbps, the BER is 10-1, far too high to be practically useful. However, with 
an SNR of 20 dB, QAM16 modulation has a transmission rate of 4 Mbps and a 
BER of 10-7, while BPSK modulation has a transmission rate of only 1 Mbps 
and a BER that is so low as to be (literally) ‚Äúoff the charts.‚Äù If one can tolerate a 
BER of 10-7, the higher transmission rate offered by QAM16 would make it the 
preferred modulation technique in this situation. These considerations give rise to 
the final characteristic, described next.
‚Ä¢ Dynamic selection of the physical-layer modulation technique can be used to 
adapt the modulation technique to channel conditions. The SNR (and hence 
the BER) may change as a result of mobility or due to changes in the environ-
ment. Adaptive modulation and coding are used in the 802.11 WiFi and in 4G 
and 5G cellular data networks that we‚Äôll study in Sections 7.3 and 7.4. This 
allows, for example, the selection of a modulation technique that provides the 
highest transmission rate possible subject to a constraint on the BER, for given 
channel characteristics.
A higher and time-varying bit error rate is not the only difference between a 
wired and wireless link. Recall that in the case of wired broadcast links, all nodes 
Figure 7.4 ‚ô¶ Hidden terminal problem caused by obstacle (a) and fading (b)
A
A
C
B
C
Location
b.
a.
0
Signal strength
B

receive the transmissions from all other nodes. In the case of wireless links, the situ-
ation is not as simple, as shown in Figure 7.4. Suppose that Station A is transmit-
ting to Station B. Suppose also that Station C is transmitting to Station B. With the 
so-called hidden terminal problem, physical obstructions in the environment (for 
example, a mountain or a building) may prevent A and C from hearing each other‚Äôs 
transmissions, even though A‚Äôs and C‚Äôs transmissions are indeed interfering at the 
destination, B. This is shown in Figure 7.4(a). A second scenario that results in unde-
tectable collisions at the receiver results from the fading of a signal‚Äôs strength as it 
propagates through the wireless medium. Figure 7.4(b) illustrates the case where A 
and C are placed such that their signals are not strong enough to detect each other‚Äôs 
transmissions, yet their signals are strong enough to interfere with each other at sta-
tion B. As we‚Äôll see in Section 7.3, the hidden terminal problem and fading make 
multiple access in a wireless network considerably more complex than in a wired 
network.
7.2.1 CDMA
Recall from Chapter 6 that when hosts communicate over a shared medium, a pro-
tocol is needed so that the signals sent by multiple senders do not interfere at the 
receivers. In Chapter 6, we described three classes of medium access protocols: 
channel partitioning, random access, and taking turns. Code division multiple access 
(CDMA) belongs to the family of channel partitioning protocols. It is prevalent in 
wireless LAN and cellular technologies. Because CDMA is so important in the wire-
less world, we‚Äôll take a quick look at CDMA now, before getting into specific wire-
less access technologies in the subsequent sections.
In a CDMA protocol, each bit being sent is encoded by multiplying the bit by 
a signal (the code) that changes at a much faster rate (known as the chipping rate) 
than the original sequence of data bits. Figure 7.5 shows a simple, idealized CDMA 
encoding/decoding scenario. Suppose that the rate at which original data bits reach 
the CDMA encoder defines the unit of time; that is, each original data bit to be 
transmitted requires a one-bit slot time. Let di be the value of the data bit for the 
ith bit slot. For mathematical convenience, we represent a data bit with a 0 value 
as -1. Each bit slot is further subdivided into M mini-slots; in Figure 7.5, M = 8, 
although in practice M is much larger. The CDMA code used by the sender con-
sists of a sequence of M values, cm, m = 1, . . . , M, each taking a +1 or -1 value. 
In the example in Figure 7.5, the M-bit CDMA code being used by the sender is 
(1, 1, 1, -1, 1, -1, -1, -1).
To illustrate how CDMA works, let us focus on the ith data bit, di. For the mth 
mini-slot of the bit-transmission time of di, the output of the CDMA encoder, Zi,m, is 
the value of di multiplied by the mth bit in the assigned CDMA code, cm:
 
Zi,m = di # cm 
(7.1)

In a simple world, with no interfering senders, the receiver would receive the encoded 
bits, Zi,m, and recover the original data bit, di, by computing:
 
di = 1
M a
M
m=1
Zi,m # cm 
(7.2)
The reader might want to work through the details of the example in Figure 7.5 to 
see that the original data bits are indeed correctly recovered at the receiver using 
Equation 7.2.
Figure 7.5 ‚ô¶ A simple CDMA example: Sender encoding, receiver decoding
1
1 1 1
‚Äì1
‚Äì1‚Äì1‚Äì1
1
1 1 1
‚Äì1
‚Äì1‚Äì1‚Äì1
1
‚Äì1
‚Äì1
‚Äì1
1 1 1
1
‚Äì1
‚Äì1‚Äì1‚Äì1
1 1 1
Time slot 1
received input
Time slot 0
received input
Code
1
‚Äì1
‚Äì1
‚Äì1‚Äì1
1 1 1
1
‚Äì1
‚Äì1‚Äì1‚Äì1
1 1 1
Data bits
Code
1
1 1 1
‚Äì1
‚Äì1‚Äì1‚Äì1
1
1 1 1
‚Äì1
‚Äì1‚Äì1‚Äì1
d1 = ‚Äì1
d0 = 1
Time slot 1
Sender
Channel output Zi,m
Receiver
Zi,m
di ‚Ä¢ cm
=
Zi,m ‚Ä¢ cm
d
M
i
m=1
M
5
S
Time slot 1
channel output
Time slot 0
channel output
Time slot 0
d1 = ‚Äì1
d0 = 1
‚Äì1

The world is far from ideal, however, and as noted above, CDMA must work in 
the presence of interfering senders that are encoding and transmitting their data using 
a different assigned code. But how can a CDMA receiver recover a sender‚Äôs original 
data bits when those data bits are being tangled with bits being transmitted by other 
senders? CDMA works under the assumption that the interfering transmitted bit sig-
nals are additive. This means, for example, that if three senders send a 1 value, and a 
fourth sender sends a -1 value during the same mini-slot, then the received signal at 
all receivers during that mini-slot is a 2 (since 1 + 1 + 1 - 1 = 2). In the presence 
of multiple senders, sender s computes its encoded transmissions, Zs
i,m, in exactly 
the same manner as in Equation 7.1. The value received at a receiver during the 
mth mini-slot of the ith bit slot, however, is now the sum of the transmitted bits from 
all N senders during that mini-slot:
Z*
i, m = a
N
s=1
Z s
i,m
Amazingly, if the senders‚Äô codes are chosen carefully, each receiver can recover the 
data sent by a given sender out of the aggregate signal simply by using the sender‚Äôs 
code in exactly the same manner as in Equation 7.2:
 
di = 1
M a
M
m=1
Zi,m
* # cm 
(7.3)
as shown in Figure 7.6, for a two-sender CDMA example. The M-bit CDMA code 
being used by the upper sender is (1, 1, 1, -1, 1, -1, -1, -1), while the CDMA code 
being used by the lower sender is (1, -1, 1, 1, 1, -1, 1, 1). Figure 7.6 illustrates a 
receiver recovering the original data bits from the upper sender. Note that the receiver 
is able to extract the data from sender 1 in spite of the interfering transmission from  
sender 2.
Recall our cocktail analogy from Chapter 6. A CDMA protocol is similar to 
having partygoers speaking in multiple languages; in such circumstances humans are 
actually quite good at locking into the conversation in the language they understand, 
while filtering out the remaining conversations. We see here that CDMA is a parti-
tioning protocol in that it partitions the codespace (as opposed to time or frequency) 
and assigns each node a dedicated piece of the codespace.
Our discussion here of CDMA is necessarily brief; in practice a number of dif-
ficult issues must be addressed. First, in order for the CDMA receivers to be able 
to extract a particular sender‚Äôs signal, the CDMA codes must be carefully chosen. 
 Second, our discussion has assumed that the received signal strengths from various 
senders are the same; in reality, this can be difficult to achieve. There is a consid-
erable body of literature addressing these and other issues related to CDMA; see 
 [Pickholtz 1982; Viterbi 1995] for details.

7.3 WiFi: 802.11 Wireless LANs
Pervasive in the workplace, the home, educational institutions, caf√©s, airports, and 
street corners, wireless LANs are now one of the most important access network 
technologies in the Internet today. Although many technologies and standards for 
Figure 7.6 ‚ô¶ A two-sender CDMA example
Receiver 1
1
1 1 1
‚Äì1
‚Äì1‚Äì1‚Äì1
1
1 1 1
‚Äì1
‚Äì1‚Äì1‚Äì1
Time slot 1
received input
Time slot 0
received input
Data bits
Data bits
1
1 1 1
‚Äì1
‚Äì1‚Äì1‚Äì1
1
1 1 1
‚Äì1
‚Äì1‚Äì1‚Äì1
Code
Senders
1 1 1
‚Äì1
1 1 1
‚Äì1
1
‚Äì1
‚Äì1
1 1 1
1 1
Code
Code
+
‚Äì2
2
2 2 2
2
‚Äì2
2
‚Äì2
2
2 2 2
2
‚Äì2
2
Channel, Zi,m
*
Zi,m
di  ‚Ä¢ cm
=
Zi,m ‚Ä¢ cm
d
M
i
m=1
M
5
S
d1 = ‚Äì1
d0 = 1
d1 = 1
2
1
1
*
2
2
2
Zi,m
= di  ‚Ä¢ cm
1
1
1
d0 = 1
2
1
1
d1 = ‚Äì1
d0 = 1
1
1

wireless LANs were developed in the 1990s, one particular class of standards has 
clearly emerged as the winner: the IEEE 802.11 wireless LAN, also known as WiFi. 
In this section, we‚Äôll take a close look at 802.11 wireless LANs, examining its frame 
structure, its medium access protocol, and its internetworking of 802.11 LANs with 
wired Ethernet LANs.
As summarized in Table 7.1, there are several 802.11 standards [IEEE 802.11 
2020]. The 802.11 b, g, n, ac, ax are successive generations of 802.11 technology 
aimed for wireless local area networks (WLANs), typically less than 70 m range in 
a home office, workplace, or business setting. The 802.11 n, ac, and ax standards 
have recently been branded as WiFi 4, 5 and 6, respectively‚Äîno doubt competing 
with 4G and 5G cellular network branding. The 802.11 af, ah standards operate over 
longer distances and are aimed at Internet of Things, sensor networks, and metering 
applications.
The different 802.11 b, g, n, ac, ax standards all share some common character-
istics, including the 802.11 frame format that we will study shortly, and are back-
ward compatible, meaning, for example, that a mobile capable only of 802.11 g may 
still interact with a newer 802.11 ac or 802.11 ax base station. They also all use the 
same medium access protocol, CSMA/CA, which we‚Äôll also discuss shortly, while 
also 802.11 ax also supports centralized scheduling by the base station of transmis-
sions from associated wireless devices.
However, as shown in Table 7.1, the standards have some major differences 
at the physical layer. 802.11 devices operate in two different frequency ranges:  
2.4‚Äì2.485 GHz (referred to as the 2.4 GHz range) and 5.1‚Äì5.8 GHz (referred to as 
the 5 GHz range). The 2.4 GHz range is an unlicensed frequency band, where 802.11 
devices may compete for frequency spectrum with 2.4 GHz phones and appli-
ances such as microwave ovens. At 5 GHz, 802.11 LANs have a shorter transmis-
sion distance for a given power level and suffer more from multipath propagation. 
The 802.11n, 802.11ac, and 802.11ax standards use multiple input multiple-output  
(MIMO) antennas; that is, two or more antennas on the sending side and two or 
more antennas on the receiving side that are transmitting/receiving different signals 
IEEE 802.11 standard
Year
Max data rate
Range
Frequency
802.11 b
1999
11 Mbps
30 m
2.4 Ghz
802.11 g
2003
54 Mbps
30 m
2.4 Ghz
802.11 n (WiFi 4)
2009
600
70 m
2.4, 5 Ghz
802.11 ac (WiFi 5)
2013
3.47 Gpbs
70 m
5 Ghz
802.11 ax (WiFi 6)
2020 (expected)
14 Gbps
70 m
2.4, 5 Ghz
802.11 af
2014
35‚Äì560 Mbps
1 Km
unused TV bands (54‚Äì790 MHz)
802.11 ah
2017
347 Mbps
1 Km
900 Mhz
Table 7.1 ‚ô¶ Summary of IEEE 802.11 standards

[Diggavi 2004]. 802.11ac and 802.11 ax base stations may transmit to multiple sta-
tions simultaneously, and use ‚Äúsmart‚Äù antennas to adaptively beamform to target 
transmissions in the direction of a receiver. This decreases interference and increases 
the distance reached at a given data rate. The data rates shown in Table 7.1 are  
for an idealized environment, for example, a receiver close to the base station, with no 
 interference‚Äîa scenario that we‚Äôre unlikely to experience in practice! So as the say-
ing goes, YMMV: Your Mileage (or in this case your wireless data rate) May Vary.
7.3.1 The 802.11 Wireless LAN Architecture
Figure 7.7 illustrates the principal components of the 802.11 wireless LAN architec-
ture. The fundamental building block of the 802.11 architecture is the basic service 
set (BSS). A BSS contains one or more wireless stations and a central base station, 
known as an access point (AP) in 802.11 parlance. Figure 7.7 shows the AP in each 
of two BSSs connecting to an interconnection device (such as a switch or router), 
which in turn leads to the Internet. In a typical home network, there is one AP and one 
router (typically integrated together as one unit) that connects the BSS to the Internet.
As with Ethernet devices, each 802.11 wireless station has a 6-byte MAC address 
that is stored in the firmware of the station‚Äôs adapter (that is, 802.11 network interface 
card). Each AP also has a MAC address for its wireless interface. As with Ethernet, 
these MAC addresses are administered by IEEE and are (in theory)  globally unique.
As noted in Section 7.1, wireless LANs that deploy APs are often referred to as 
infrastructure wireless LANs, with the ‚Äúinfrastructure‚Äù being the APs along with the 
Figure 7.7 ‚ô¶ IEEE 802.11 LAN architecture
Internet
Switch or router
AP
BSS 1
BSS 2
AP

wired Ethernet infrastructure that interconnects the APs and a router. Figure 7.8 shows 
that IEEE 802.11 stations can also group themselves together to form an ad hoc net-
work‚Äîa network with no central control and with no connections to the  ‚Äúoutside world.‚Äù 
Here, the network is formed ‚Äúon the fly,‚Äù by mobile devices that have found themselves 
in proximity to each other, that have a need to communicate, and that find no preexist-
ing network infrastructure in their location. An ad hoc network might be formed when 
people with laptops get together (e.g., in a conference room, a train, or a car) and want 
to exchange data in the absence of a centralized AP. There has been tremendous interest 
in ad hoc networking, as communicating portable devices continue to proliferate. In this 
section, though, we‚Äôll focus our attention on infrastructure wireless LANs.
Channels and Association
In 802.11, each wireless station needs to associate with an AP before it can send or 
receive network-layer data. Although all of the 802.11 standards use association, 
we‚Äôll discuss this topic specifically in the context of IEEE 802.11b, g, n, ac, ax.
When a network administrator installs an AP, the administrator assigns a one- 
or two-word Service Set Identifier (SSID) to the access point. (When you choose 
Wi-Fi under Setting on your iPhone, for example, a list is displayed showing the 
SSID of each AP in range.) The administrator must also assign a channel number 
to the AP. To understand channel numbers, recall that 802.11 operates in the fre-
quency range of 2.4 GHz to 2.4835 GHz. Within this 85 MHz band, 802.11 defines 
11 partially overlapping channels. Any two channels are non-overlapping if and 
only if they are separated by four or more channels. In particular, the set of chan-
nels 1, 6, and 11 is the only set of three non-overlapping channels. This means that 
an administrator could create a wireless LAN with an aggregate maximum trans-
mission rate of three times the maximum transmission rate shown in Table 7.1 by 
installing three 802.11 APs at the same physical location, assigning channels 1, 6, 
and 11 to the APs, and interconnecting each of the APs with a switch.
Figure 7.8 ‚ô¶ An IEEE 802.11 ad hoc network
BSS

Now that we have a basic understanding of 802.11 channels, let‚Äôs describe an 
interesting (and not completely uncommon) situation‚Äîthat of a WiFi jungle. A WiFi 
jungle is any physical location where a wireless station receives a sufficiently strong 
signal from two or more APs. For example, in many caf√©s in New York City, a wire-
less station can pick up a signal from numerous nearby APs. One of the APs might be 
managed by the caf√©, while the other APs might be in residential apartments near the 
caf√©. Each of these APs would likely be located in a different IP subnet and would 
have been independently assigned a channel.
Now suppose you enter such a WiFi jungle with your smartphone, tablet, or 
 laptop, seeking wireless Internet access and a blueberry muffin. Suppose there are  
five APs in the WiFi jungle. To gain Internet access, your wireless device needs to join 
exactly one of the subnets and hence needs to associate with exactly one of the APs. 
 Associating means the wireless device creates a virtual wire between itself and the 
AP. Specifically, only the associated AP will send data frames (that is, frames con-
taining data, such as a datagram) to your wireless device, and your wireless device 
will send data frames into the Internet only through the associated AP. But how does 
your wireless device associate with a particular AP? And more fundamentally, how 
does your wireless device know which APs, if any, are out there in the jungle?
The 802.11 standard requires that an AP periodically send beacon frames, each 
of which includes the AP‚Äôs SSID and MAC address. Your wireless device, know-
ing that APs are sending out beacon frames, scans the 11 channels, seeking beacon 
frames from any APs that may be out there (some of which may be transmitting 
on the same channel‚Äîit‚Äôs a jungle out there!). Having learned about available APs 
from the beacon frames, you (or your wireless device) select one of the APs for 
association.
The 802.11 standard does not specify an algorithm for selecting which of 
the available APs to associate with; that algorithm is left up to the designers of 
the 802.11 firmware and software in your wireless device. Typically, the device 
chooses the AP whose beacon frame is received with the highest signal strength. 
While a high signal strength is good (see, e.g., Figure 7.3), signal strength is not 
the only AP characteristic that will determine the performance a device receives.  
In particular, it‚Äôs possible that the selected AP may have a strong signal, but may be 
overloaded with other affiliated devices (that will need to share the wireless band-
width at that AP), while an unloaded AP is not selected due to a slightly weaker 
signal. A number of alternative ways of choosing APs have thus recently been pro-
posed [Vasudevan 2005; Nicholson 2006; Sundaresan 2006]. For an interesting and 
down-to-earth discussion of how signal strength is measured, see [Bardwell 2004].
The process of scanning channels and listening for beacon frames is known 
as passive scanning (see Figure 7.9a). A wireless device can also perform active 
scanning, by broadcasting a probe frame that will be received by all APs within the 
wireless device‚Äôs range, as shown in Figure 7.9b. APs respond to the probe request 
frame with a probe response frame. The wireless device can then choose the AP with 
which to associate from among the responding APs.

Figure 7.9 ‚ô¶ Active and passive scanning for access points
1
1
3
2
H1
AP 2
AP 1
BBS 1
a. Passive scanning
 
1. Beacon frames sent from APs
 
2. Association Request frame sent:
 
 
H1 to selected AP
 
3. Association Response frame sent:
 
 
Selected AP to H1
a. Active scanning
 
1. Probe Request frame broadcast from H1
 
2. Probes Response frame sent from APs
 
3. Association Request frame sent:
 
 
H1 to selected AP
 
4. Association Response frame sent:
 
 
Selected AP to H1
 
 
BBS 2
2
2
4
3
H1
AP 2
AP 1
BBS 1
BBS 2
1
After selecting the AP with which to associate, the wireless device sends an asso-
ciation request frame to the AP, and the AP responds with an association response 
frame. Note that this second request/response handshake is needed with active scan-
ning, since an AP responding to the initial probe request frame doesn‚Äôt know which 
of the (possibly many) responding APs the device will choose to associate with, in 
much the same way that a DHCP client can choose from among multiple DHCP 
servers (see Figure 4.21). Once associated with an AP, the device will want to join 
the subnet (in the IP addressing sense of Section 4.3.3) to which the AP belongs. 
Thus, the device will typically send a DHCP discovery message (see Figure 4.21) 
into the subnet via the AP in order to obtain an IP address on the subnet. Once the 
address is obtained, the rest of the world then views that device simply as another 
host with an IP address in that subnet.
In order to create an association with a particular AP, the wireless device may be 
required to authenticate itself to the AP. 802.11 wireless LANs provide a number of 
alternatives for authentication and access. One approach, used by many companies, is 
to permit access to a wireless network based on a device‚Äôs MAC address. A second 
approach, used by many Internet caf√©s, employs usernames and passwords. In both 
cases, the AP typically communicates with an authentication server, relaying informa-
tion between the wireless device and the authentication server using a protocol such as 
RADIUS [RFC 2865] or DIAMETER [RFC 6733]. Separating the authentication server 
from the AP allows one authentication server to serve many APs, centralizing the (often 
sensitive) decisions of authentication and access within the single server, and keeping

AP costs and complexity low. We‚Äôll see in chapter 8 that the new IEEE 802.11i protocol 
defining security aspects of the 802.11 protocol family takes precisely this approach.
7.3.2 The 802.11 MAC Protocol
Once a wireless device is associated with an AP, it can start sending and receiving 
data frames to and from the access point. But because multiple wireless devices, or the 
AP itself may want to transmit data frames at the same time over the same channel, a 
multiple access protocol is needed to coordinate the transmissions. In the following, 
we'll refer to the devices or the AP as wireless ‚Äústations‚Äù that share the multiple access 
channel. As discussed in Chapter 6 and Section 7.2.1, broadly speaking there are three 
classes of multiple access protocols: channel partitioning (including CDMA), random 
access, and taking turns. Inspired by the huge success of Ethernet and its random 
access protocol, the designers of 802.11 chose a random access protocol for 802.11 
wireless LANs. This random access protocol is referred to as CSMA with collision 
avoidance, or more succinctly as CSMA/CA. As with Ethernet‚Äôs CSMA/CD, the 
‚ÄúCSMA‚Äù in CSMA/CA stands for ‚Äúcarrier sense multiple access,‚Äù meaning that each 
station senses the channel before transmitting, and refrains from transmitting when the 
channel is sensed busy. Although both  Ethernet and 802.11 use carrier-sensing ran-
dom access, the two MAC protocols have important differences. First, instead of using 
collision detection, 802.11 uses collision-avoidance techniques. Second, because of 
the relatively high bit error rates of wireless channels, 802.11 (unlike Ethernet) uses 
a link-layer acknowledgment/retransmission (ARQ) scheme. We‚Äôll describe 802.11‚Äôs 
collision-avoidance and link-layer acknowledgment schemes below.
Recall from Sections 6.3.2 and 6.4.2 that with Ethernet‚Äôs collision-detection 
algorithm, an Ethernet station listens to the channel as it transmits. If, while transmit-
ting, it detects that another station is also transmitting, it aborts its transmission and 
tries to transmit again after waiting a small, random amount of time. Unlike the 802.3 
Ethernet protocol, the 802.11 MAC protocol does not implement collision detection. 
There are two important reasons for this:
‚Ä¢ The ability to detect collisions requires the ability to send (the station‚Äôs own 
 signal) and receive (to determine whether another station is also transmitting) at 
the same time. Because the strength of the received signal is typically very small 
compared to the strength of the transmitted signal at the 802.11 adapter, it is 
costly to build hardware that can detect a collision.
‚Ä¢ More importantly, even if the adapter could transmit and listen at the same time 
(and presumably abort transmission when it senses a busy channel), the adapter 
would still not be able to detect all collisions, due to the hidden terminal problem 
and fading, as discussed in Section 7.2.
Because 802.11wireless LANs do not use collision detection, once a station 
begins to transmit a frame, it transmits the frame in its entirety; that is, once a station

Figure 7.10 ‚ô¶ 802.11 uses link-layer acknowledgments
Destination
DIFS
SIFS
data
ack
Source
gets started, there is no turning back. As one might expect, transmitting entire frames 
(particularly long frames) when collisions are prevalent can significantly degrade a 
multiple access protocol‚Äôs performance. In order to reduce the likelihood of collisions, 
802.11 employs several collision-avoidance techniques, which we‚Äôll shortly discuss.
Before considering collision avoidance, however, we‚Äôll first need to examine 
802.11‚Äôs link-layer acknowledgment scheme. Recall from Section 7.2 that when a 
station in a wireless LAN sends a frame, the frame may not reach the destination sta-
tion intact for a variety of reasons. To deal with this non-negligible chance of failure, 
the 802.11 MAC protocol uses link-layer acknowledgments. As shown in Figure 7.10, 
when the destination station receives a frame that passes the CRC, it waits a short 
period of time known as the Short Inter-frame Spacing (SIFS) and then sends back 
an acknowledgment frame. If the transmitting station does not receive an acknowl-
edgment within a given amount of time, it assumes that an error has occurred and 
retransmits the frame, using the CSMA/CA protocol to access the channel. If an 
acknowledgment is not received after some fixed number of retransmissions, the trans-
mitting station gives up and discards the frame.

Having discussed how 802.11 uses link-layer acknowledgments, we‚Äôre now in a 
position to describe the 802.11 CSMA/CA protocol. Suppose that a station (wireless 
device or an AP) has a frame to transmit.
 1. If initially the station senses the channel idle, it transmits its frame after a  
short period of time known as the Distributed Inter-frame Space (DIFS);  
see  Figure 7.10.
 2. Otherwise, the station chooses a random backoff value using binary exponen-
tial backoff (as we encountered in Section 6.3.2) and counts down this value 
after DIFS when the channel is sensed idle. While the channel is sensed busy, 
the counter value remains frozen.
 3. When the counter reaches zero (note that this can only occur while the chan-
nel is sensed idle), the station transmits the entire frame and then waits for an 
acknowledgment.
 4. If an acknowledgment is received, the transmitting station knows that its frame 
has been correctly received at the destination station. If the station has another 
frame to send, it begins the CSMA/CA protocol at step 2. If the acknowledg-
ment isn‚Äôt received, the transmitting station reenters the backoff phase in step 2,  
with the random value chosen from a larger interval.
Recall that under Ethernet‚Äôs CSMA/CD, multiple access protocol (Section 6.3.2), 
a station begins transmitting as soon as the channel is sensed idle. With CSMA/CA, 
however, the station refrains from transmitting while counting down, even when it 
senses the channel to be idle. Why do CSMA/CD and CDMA/CA take such different 
approaches here?
To answer this question, let‚Äôs consider a scenario in which two stations each 
have a data frame to transmit, but neither station transmits immediately because each 
senses that a third station is already transmitting. With Ethernet‚Äôs CSMA/CD, the 
two stations would each transmit as soon as they detect that the third station has 
finished transmitting. This would cause a collision, which isn‚Äôt a serious issue in 
CSMA/CD, since both stations would abort their transmissions and thus avoid the 
useless transmissions of the remainders of their frames. In 802.11, however, the situ-
ation is quite different. Because 802.11 does not detect a collision and abort trans-
mission, a frame suffering a collision will be transmitted in its entirety. The goal 
in 802.11 is thus to avoid collisions whenever possible. In 802.11, if the two sta-
tions sense the channel busy, they both immediately enter random backoff, hopefully 
choosing different backoff values. If these values are indeed different, once the chan-
nel becomes idle, one of the two stations will begin transmitting before the other, and 
(if the two stations are not hidden from each other) the ‚Äúlosing station‚Äù will hear the  
‚Äúwinning station‚Äôs‚Äù signal, freeze its counter, and refrain from transmitting until the 
winning station has completed its transmission. In this manner, a costly collision is 
avoided. Of course, collisions can still occur with 802.11 in this scenario: The two 
stations could be hidden from each other, or the two stations could choose random

backoff values that are close enough that the transmission from the station starting 
first have yet to reach the second station. Recall that we encountered this problem 
earlier in our discussion of random access algorithms in the context of Figure 6.12.
Dealing with Hidden Terminals: RTS and CTS
The 802.11 MAC protocol also includes a nifty (but optional) reservation scheme 
that helps avoid collisions even in the presence of hidden terminals. Let‚Äôs investi-
gate this scheme in the context of Figure 7.11, which shows two wireless  stations 
and one access point. Both of the wireless stations are within range of the AP 
(whose  coverage is shown as a shaded circle) and both have associated with the AP. 
 However, due to fading, the signal ranges of wireless stations are limited to the inte-
riors of the shaded circles shown in Figure 7.11. Thus, each of the wireless stations 
is hidden from the other, although neither is hidden from the AP.
Let‚Äôs now consider why hidden terminals can be problematic. Suppose Station H1 is 
transmitting a frame and halfway through H1‚Äôs transmission, Station H2 wants to send a 
frame to the AP. H2, not hearing the transmission from H1, will first wait a DIFS interval 
and then transmit the frame, resulting in a collision. The channel will therefore be wasted 
during the entire period of H1‚Äôs transmission as well as during H2‚Äôs transmission.
In order to avoid this problem, the IEEE 802.11 protocol allows a station to 
use a short Request to Send (RTS) control frame and a short Clear to Send (CTS) 
control frame to reserve access to the channel. When a sender wants to send a DATA 
frame, it can first send an RTS frame to the AP, indicating the total time required 
to transmit the DATA frame and the acknowledgment (ACK) frame. When the AP 
receives the RTS frame, it responds by broadcasting a CTS frame. This CTS frame 
Figure 7.11 ‚ô¶  Hidden terminal example: H1 is hidden from H2, and  
vice versa
AP
H1
H2

serves two purposes: It gives the sender explicit permission to send and also instructs 
the other stations not to send for the reserved duration.
Thus, in Figure 7.12, before transmitting a DATA frame, H1 first broadcasts an RTS 
frame, which is heard by all stations in its circle, including the AP. The AP then responds 
with a CTS frame, which is heard by all stations within its range, including H1 and H2. 
Station H2, having heard the CTS, refrains from transmitting for the time specified in the 
CTS frame. The RTS, CTS, DATA, and ACK frames are shown in Figure 7.12.
Figure 7.12 ‚ô¶ Collision avoidance using the RTS and CTS frames
Destination
All other nodes
Defer access
Source
DIFS
ACK
SIFS
SIFS
SIFS
DATA
CTS
CTS
ACK
RTS

The use of the RTS and CTS frames can improve performance in two important 
ways:
‚Ä¢ The hidden station problem is mitigated, since a long DATA frame is transmitted 
only after the channel has been reserved.
‚Ä¢ Because the RTS and CTS frames are short, a collision involving an RTS or CTS 
frame will last only for the duration of the short RTS or CTS frame. Once the RTS 
and CTS frames are correctly transmitted, the following DATA and ACK frames 
should be transmitted without collisions.
You are encouraged to check out the 802.11 animation in the textbook‚Äôs Web site. 
This interactive animation illustrates the CSMA/CA protocol, including the RTS/
CTS exchange sequence.
Although the RTS/CTS exchange can help reduce collisions, it also introduces 
delay and consumes channel resources. For this reason, the RTS/CTS exchange is 
only used (if at all) to reserve the channel for the transmission of a long DATA 
frame. In practice, each wireless station can set an RTS threshold such that the RTS/
CTS sequence is used only when the frame is longer than the threshold. For many 
wireless stations, the default RTS threshold value is larger than the maximum frame 
length, so the RTS/CTS sequence is skipped for all DATA frames sent.
Using 802.11 as a Point-to-Point Link
Our discussion so far has focused on the use of 802.11 in a multiple access setting. We 
should mention that if two nodes each have a directional antenna, they can point their 
directional antennas at each other and run the 802.11 protocol over what is essentially 
a point-to-point link. Given the low cost of commodity 802.11 hardware, the use of 
directional antennas and an increased transmission power allow 802.11 to be used as an 
inexpensive means of providing wireless point-to-point connections over tens of kilo-
meters distance. [Raman 2007] describes one of the first such multi-hop wireless net-
works, operating in the rural Ganges plains in India using point-to-point 802.11 links.
7.3.3 The IEEE 802.11 Frame
Although the 802.11 frame shares many similarities with an Ethernet frame, it also con-
tains a number of fields that are specific to its use for wireless links. The 802.11 frame 
is shown in Figure 7.13. The numbers above each of the fields in the frame represent 
the lengths of the fields in bytes; the numbers above each of the subfields in the frame 
control field represent the lengths of the subfields in bits. Let‚Äôs now examine the fields 
in the frame as well as some of the more important subfields in the frame‚Äôs control field.
Payload and CRC Fields
At the heart of the frame is the payload, which typically consists of an IP datagram 
or an ARP packet. Although the field is permitted to be as long as 2,312 bytes, it is

typically fewer than 1,500 bytes, holding an IP datagram or an ARP packet. As with 
an Ethernet frame, an 802.11 frame includes a 32-bit cyclic redundancy check (CRC) 
so that the receiver can detect bit errors in the received frame. As we‚Äôve seen, bit 
errors are much more common in wireless LANs than in wired LANs, so the CRC is 
even more useful here.
Address Fields
Perhaps the most striking difference in the 802.11 frame is that it has four address 
fields, each of which can hold a 6-byte MAC address. But why four address 
fields? Doesn‚Äôt a source MAC field and destination MAC field suffice, as they do 
for  Ethernet? It turns out that three address fields are needed for internetworking 
 purposes‚Äîspecifically, for moving the network-layer datagram from a wireless sta-
tion through an AP to a router interface. The fourth address field is used when APs 
 forward frames to each other in ad hoc mode. Since we are only considering infra-
structure networks here, let‚Äôs focus our attention on the first three address fields. The 
802.11 standard defines these fields as follows:
‚Ä¢ Address 2 is the MAC address of the station that transmits the frame. Thus, if a 
wireless station transmits the frame, that station‚Äôs MAC address is inserted in the 
address 2 field. Similarly, if an AP transmits the frame, the AP‚Äôs MAC address is 
inserted in the address 2 field.
‚Ä¢ Address 1 is the MAC address of the wireless station that is to receive the frame. 
Thus if a mobile wireless station transmits the frame, address 1 contains the MAC 
address of the destination AP. Similarly, if an AP transmits the frame, address 1 
contains the MAC address of the destination wireless station.
‚Ä¢ To understand address 3, recall that the BSS (consisting of the AP and wireless 
stations) is part of a subnet, and that this subnet connects to other subnets via some 
router interface. Address 3 contains the MAC address of this router  interface.
Figure 7.13 ‚ô¶ The 802.11 frame
Frame
control
2
2
2
4
1
1
1
1
1
1
1
1
2
6
6
6
2
6
0-2312
4
Frame (numbers indicate Ô¨Åeld length in bytes):
Address
1
Duration
Payload
CRC
Protocol
version
To
AP
From
AP
More
frag
Power
mgt
More
data
Address
2
Address
3
Address
4
Seq
control
Type
Subtype
Retry
WEP
Rsvd
Frame control Ô¨Åeld expanded (numbers indicate Ô¨Åeld length in bits):

To gain further insight into the purpose of address 3, let‚Äôs walk through an inter-
networking example in the context of Figure 7.14. In this figure, there are two APs, 
each of which is responsible for a number of wireless stations. Each of the APs has a 
direct connection to a router, which in turn connects to the global Internet. We should 
keep in mind that an AP is a link-layer device, and thus neither ‚Äúspeaks‚Äù IP nor 
understands IP addresses. Consider now moving a datagram from the router interface 
R1 to the wireless Station H1. The router is not aware that there is an AP between it 
and H1; from the router‚Äôs perspective, H1 is just a host in one of the subnets to which 
it (the router) is connected.
‚Ä¢ The router, which knows the IP address of H1 (from the destination address of the 
datagram), uses ARP to determine the MAC address of H1, just as in an ordinary Eth-
ernet LAN. After obtaining H1‚Äôs MAC address, router interface R1 encapsulates the 
datagram within an Ethernet frame. The source address field of this frame contains 
R1‚Äôs MAC address, and the destination address field contains H1‚Äôs MAC address.
‚Ä¢ When the Ethernet frame arrives at the AP, the AP converts the 802.3 Ethernet 
frame to an 802.11 frame before transmitting the frame into the wireless chan-
nel. The AP fills in address 1 and address 2 with H1‚Äôs MAC address and its own 
MAC address, respectively, as described above. For address 3, the AP inserts the 
MAC address of R1. In this manner, H1 can determine (from address 3) the MAC 
address of the router interface that sent the datagram into the subnet.
Figure 7.14 ‚ô¶  The use of address fields in 802.11 frames: Sending frames 
between H1 and R1
Internet
Router
AP
H1
R1
BSS 1
BSS 2
AP

Now consider what happens when the wireless station H1 responds by moving a 
datagram from H1 to R1.
‚Ä¢ H1 creates an 802.11 frame, filling the fields for address 1 and address 2 with the 
AP‚Äôs MAC address and H1‚Äôs MAC address, respectively, as described above. For 
address 3, H1 inserts R1‚Äôs MAC address.
‚Ä¢ When the AP receives the 802.11 frame, it converts the frame to an Ethernet frame. 
The source address field for this frame is H1‚Äôs MAC address, and the destination 
address field is R1‚Äôs MAC address. Thus, address 3 allows the AP to determine 
the appropriate destination MAC address when constructing the Ethernet frame.
In summary, address 3 plays a crucial role for internetworking the BSS with a wired 
LAN.
Sequence Number, Duration, and Frame Control Fields
Recall that in 802.11, whenever a station correctly receives a frame from another sta-
tion, it sends back an acknowledgment. Because acknowledgments can get lost, the 
sending station may send multiple copies of a given frame. As we saw in our discus-
sion of the rdt2.1 protocol (Section 3.4.1), the use of sequence numbers allows the 
receiver to distinguish between a newly transmitted frame and the retransmission of 
a previous frame. The sequence number field in the 802.11 frame thus serves exactly 
the same purpose here at the link layer as it did in the transport layer in Chapter 3.
Recall that the 802.11 protocol allows a transmitting station to reserve the chan-
nel for a period of time that includes the time to transmit its data frame and the time 
to transmit an acknowledgment. This duration value is included in the frame‚Äôs dura-
tion field (both for data frames and for the RTS and CTS frames).
As shown in Figure 7.13, the frame control field includes many subfields. We‚Äôll 
say just a few words about some of the more important subfields; for a more complete 
discussion, you are encouraged to consult the 802.11 specification [Held 2001; Crow 
1997; IEEE 802.11 1999]. The type and subtype fields are used to distinguish the asso-
ciation, RTS, CTS, ACK, and data frames. The to and from fields are used to define 
the meanings of the different address fields. (These meanings change depending on 
whether ad hoc or infrastructure modes are used and, in the case of infrastructure 
mode, whether a wireless station or an AP is sending the frame.) Finally the WEP field 
indicates whether encryption is being used or not (WEP is discussed in Chapter 8).
7.3.4 Mobility in the Same IP Subnet
In order to increase the physical range of a wireless LAN, companies and universities 
will often deploy multiple BSSs within the same IP subnet. This naturally raises the issue 
of mobility among the BSSs‚Äîhow do wireless stations seamlessly move from one BSS 
to another while maintaining ongoing TCP sessions? As we‚Äôll see in this subsection,  
mobility can be handled in a relatively straightforward manner when the BSSs are part

of the subnet. When stations move between subnets, more sophisticated mobility man-
agement protocols will be needed, such as those we‚Äôll study in Sections 7.5 and 7.6.
Let‚Äôs now look at a specific example of mobility between BSSs in the same sub-
net. Figure 7.15 shows two interconnected BSSs with a host, H1, moving from BSS1 
to BSS2. Because in this example the interconnection device that connects the two 
BSSs is not a router, all of the stations in the two BSSs, including the APs, belong 
to the same IP subnet. Thus, when H1 moves from BSS1 to BSS2, it may keep its IP 
address and all of its ongoing TCP connections. If the interconnection device were a 
router, then H1 would have to obtain a new IP address in the subnet in which it was 
moving. This address change would disrupt (and eventually terminate) any on-going 
TCP connections at H1. In Section 7.6, we‚Äôll see how a network-layer mobility pro-
tocol, such as mobile IP, can be used to avoid this problem.
But what specifically happens when H1 moves from BSS1 to BSS2? As H1 wanders 
away from AP1, H1 detects a weakening signal from AP1 and starts to scan for a stronger 
signal. H1 receives beacon frames from AP2 (which in many corporate and university 
settings will have the same SSID as AP1). H1 then disassociates with AP1 and associates 
with AP2, while keeping its IP address and maintaining its ongoing TCP sessions.
This addresses the handover problem from the host and AP viewpoint. But what 
about the switch in Figure 7.15? How does it know that the host has moved from one 
AP to another? As you may recall from Chapter 6, switches are ‚Äúself-learning‚Äù and 
automatically build their forwarding tables. This self-learning feature nicely handles 
occasional moves (for example, when an employee gets transferred from one depart-
ment to another); however, switches were not designed to support highly mobile 
users who want to maintain TCP connections while moving between BSSs. To 
appreciate the problem here, recall that before the move, the switch has an entry in 
its forwarding table that pairs H1‚Äôs MAC address with the outgoing switch interface 
through which H1 can be reached. If H1 is initially in BSS1, then a datagram des-
tined to H1 will be directed to H1 via AP1. Once H1 associates with BSS2, however, 
its frames should be directed to AP2. One solution (a bit of a hack, really) is for AP2 
to send a broadcast Ethernet frame with H1‚Äôs source address to the switch just after 
Figure 7.15 ‚ô¶ Mobility in the same subnet
BSS 1
BSS 2
H1
Switch
AP 1
AP 2

the new association. When the switch receives the frame, it updates its forwarding 
table, allowing H1 to be reached via AP2. The 802.11f standards group is developing 
an inter-AP protocol to handle these and related issues.
Our discussion above has focused on mobility with the same LAN subnet. Recall 
that VLANs, which we studied in Section 6.4.4, can be used to connect together 
islands of LANs into a large virtual LAN that can span a large geographical region. 
Mobility among base stations within such a VLAN can be handled in exactly the 
same manner as above [Yu 2011].
LOCATION DISCOVERY: GPS AND WIFI POSITIONING
Many of the most useful and important smartphone apps today are location-based 
mobile apps, including Foursquare, Yelp, Uber, Pok√©mon Go, and Waze. These 
 software apps all make use of an API that allows them to extract their current geographi-
cal position directly from the smartphone. Have you ever wondered how your smart-
phone obtains its geographical position? Today, it is done by combining two systems, the 
Global Positioning System (GPS) and the WiFi Positioning System (WPS).
The GPS, with a constellation of 30+ satellites, broadcasts satellite location and 
timing information, which in turn is used by each GPS receiver to estimate its geoloca-
tion. The United States government created the system, maintains it, and makes it freely 
accessible to anyone with a GPS receiver. The satellites have very stable atomic clocks 
that are synchronized with one another and with ground clocks. The satellites also know 
their locations with great precision. Each GPS satellite continuously broadcasts a radio 
signal containing its current time and position. If a GPS receiver obtains this information 
from at least four satellites, it can solve triangulation equations to estimate its position.
GPS, however, cannot always provide accurate geolocations if it does not have 
line-of-sight with at least four GPS satellites or when there is interference from other 
high-frequency communication systems. This is particularly true in urban environments, 
where tall buildings frequently block GPS signals. This is where WiFi positioning 
 systems come to the rescue. WiFi positioning systems make use of databases of WiFi 
access points, which are independently maintained by various Internet companies, 
including Google, Apple, and Microsoft. Each database contains information about 
millions of WiFi access points, including each access point‚Äôs SSID and an estimate of 
its geographic location. To understand how a WiFi positioning system makes use of 
such a database, consider an Android smartphone along with the Google location ser-
vice. From each nearby access point, the smartphone receives and measures the signal  
strength of beacon signals (see Section 7.3.1), which contain the access point‚Äôs SSID. The  
smartphone can therefore continually send messages to the Google location service  
(in the cloud) that include the SSIDs of nearby access points and the corresponding 
signal strengths. It will also send its GPS position (obtained via the satellite broadcast 
CASE HISTORY

signals, as described above) when available. Using the signal-strength information, 
Google will estimate the distance between the smartphone and each of the WiFi 
access points. Leveraging these estimated distances, it can then solve triangulation 
equations to estimate the smartphone‚Äôs geolocation. Finally, this WiFi-based estimate is 
combined with the GPS satellite-based estimate to form an aggregate estimate, which 
is then sent back to the smartphone and used by the location-based mobile apps.
But you may still be wondering how Google (and Apple, Microsoft, and so on) 
obtain and maintain the database of access points, and in particular, the access 
point‚Äôs geographic location? Recall that for a given access point, every nearby 
Android smartphone will send to the Google location service the strength of the 
 signal received from the access point as well as the smartphone‚Äôs estimated location. 
Given that thousands of smartphones may be passing by the access point during any 
single day, Google‚Äôs location service will have lots of data at its disposition to use in 
estimating the access point‚Äôs position, again by solving triangulation equations. Thus, 
the access points help the smartphones determine their locations, and in turn the 
smartphones help the access points determine their locations!
7.3.5 Advanced Features in 802.11
We‚Äôll wrap up our coverage of 802.11 with a short discussion of two advanced capabili-
ties found in 802.11 networks. As we‚Äôll see, these capabilities are not completely speci-
fied in the 802.11 standard, but rather are made possible by mechanisms specified in 
the standard. This allows different vendors to implement these capabilities using their 
own (proprietary) approaches, presumably giving them an edge over the competition.
802.11 Rate Adaptation
We saw earlier in Figure 7.3 that different modulation techniques (with the different 
transmission rates that they provide) are appropriate for different SNR scenarios. 
Consider, for example, a mobile 802.11 user who is initially 20 meters away from 
the base station, with a high signal-to-noise ratio. Given the high SNR, the user can 
communicate with the base station using a physical-layer modulation technique that 
provides high transmission rates while maintaining a low BER. This is one happy 
user! Suppose now that the user becomes mobile, walking away from the base sta-
tion, with the SNR falling as the distance from the base station increases. In this case, 
if the modulation technique used in the 802.11 protocol operating between the base 
station and the user does not change, the BER will become unacceptably high as the 
SNR decreases, and eventually no transmitted frames will be received correctly.
For this reason, some 802.11 implementations have a rate adaptation capability 
that adaptively selects the underlying physical-layer modulation technique to use 
based on current or recent channel characteristics. If a node sends two frames in a 
row without receiving an acknowledgment (an implicit indication of bit errors on

the channel), the transmission rate falls back to the next lower rate. If 10 frames 
in a row are acknowledged, or if a timer that tracks the time since the last fallback 
expires, the transmission rate increases to the next higher rate. This rate adapta-
tion mechanism shares the same ‚Äúprobing‚Äù philosophy as TCP‚Äôs congestion-control 
mechanism‚Äîwhen conditions are good (reflected by ACK receipts), the transmis-
sion rate is increased until something ‚Äúbad‚Äù happens (the lack of ACK receipts); 
when something ‚Äúbad‚Äù happens, the transmission rate is reduced. 802.11 rate adapta-
tion and TCP congestion control are thus similar to the young child who is constantly 
pushing his/her parents for more and more (say candy for a young child, later curfew 
hours for the teenager) until the parents finally say ‚ÄúEnough!‚Äù and the child backs 
off (only to try again later after conditions have hopefully improved!). A number 
of other schemes have also been proposed to improve on this basic automatic rate-
adjustment scheme [Kamerman 1997; Holland 2001; Lacage 2004].
Power Management
Power is a precious resource in mobile devices, and thus the 802.11 standard pro-
vides power-management capabilities that allow 802.11 nodes to minimize the 
amount of time that their sense, transmit, and receive functions and other circuitry 
need to be ‚Äúon.‚Äù 802.11 power management operates as follows. A node is able to 
explicitly alternate between sleep and wake states (not unlike a sleepy student in a 
classroom!). A node indicates to the access point that it will be going to sleep by set-
ting the power-management bit in the header of an 802.11 frame to 1. A timer in the 
node is then set to wake up the node just before the AP is scheduled to send its bea-
con frame (recall that an AP typically sends a beacon frame every 100 msec). Since 
the AP knows from the set power-transmission bit that the node is going to sleep, it 
(the AP) knows that it should not send any frames to that node, and will buffer any 
frames destined for the sleeping host for later transmission.
A node will wake up just before the AP sends a beacon frame, and quickly enter 
the fully active state (unlike the sleepy student, this wakeup requires only 250 micro-
seconds [Kamerman 1997]!). The beacon frames sent out by the AP contain a list of 
nodes whose frames have been buffered at the AP. If there are no buffered frames 
for the node, it can go back to sleep. Otherwise, the node can explicitly request that 
the buffered frames be sent by sending a polling message to the AP. With an inter-
beacon time of 100 msec, a wakeup time of 250 microseconds, and a similarly small 
time to receive a beacon frame and check to ensure that there are no buffered frames, 
a node that has no frames to send or receive can be asleep 99% of the time, resulting 
in a significant energy savings.
7.3.6 Personal Area Networks: Bluetooth
Bluetooth networks seem to have quickly become part of everyday life. Perhaps 
you‚Äôve used a Bluetooth network as a ‚Äúcable replacement‚Äù technology to interconnect

your computer with a wireless keyboard, mouse, or other peripheral device. Or per-
haps you‚Äôve used a Bluetooth network to connect your wireless earbuds, speaker, 
watch, or health monitoring band to your smartphone or to connect your smartphone 
to a car‚Äôs audio system. In all of these cases, Bluetooth operates over short ranges (tens 
of meters or less), at low power, and at low cost. For this reason, Bluetooth networks 
are sometimes referred to as wireless personal area networks (WPANs) or piconets.
Although Bluetooth networks are small and relatively simple by design, they‚Äôre 
packed with many of the link-level networking techniques that we‚Äôve studied earlier 
including time division multiplexing (TDM) and frequency division (Section 6.3.1), 
randomized backoff (Section 6.3.2), polling (Section 6.3.3), error detection and cor-
rection (Section 6.2), reliable data transfer via ACKs and NAKS (Section 3.4.1). And 
that‚Äôs just considering Bluetooth‚Äôs link layer!
Bluetooth networks operate in the unlicensed 2.4 GHz Industrial, Scientific 
and Medical (ISM) radio band along with other home appliances such as micro-
waves, garage door openers, and cordless phones. As a result, Bluetooth networks 
are designed explicitly with noise and interference in mind. The Bluetooth wire-
less channel is operated in a TDM manner, with time slots of 625 microseconds. 
During each time slot, a sender transmits on one of 79 channels, with the channel 
(frequency) changing in a known but pseudo-random manner from slot to slot. This 
form of channel hopping, known as frequency-hopping spread spectrum (FHSS), 
is used so that interference from another device or appliance operating in the ISM 
band will only interfere with Bluetooth communications in at most a subset of the 
slots. Bluetooth data rates can reach up to 3 Mbps.
Bluetooth networks are ad hoc networks‚Äîno network infrastructure (e.g., an 
access point) is needed. Instead, Bluetooth devices must organize themselves into a 
piconet of up to eight active devices, as shown in Figure 7.16. One of these devices 
Figure 7.16 ‚ô¶ A Bluetooth piconet
Radius of
coverage
Master device
Client device
Parked device
Key:
M
M
C
C
C
C
P
P
P
P
P

is designated as the master, with the remaining devices acting as clients. The mas-
ter node truly rules the piconet‚Äîits clock determines time in the piconet (e.g., 
determines TDM slot boundaries), it determine the slot-to-slot frequency hopping 
sequence, it controls entry of client devices into the piconet, it controls the power 
(100 mW, 2.5mW, or 1 mW) at which client devices transmit; and uses polling to 
grant clients permission to transmit once admitted to the network. In addition to the 
active devices, there can also be up to 255 ‚Äúparked‚Äù devices in the piconet. These 
parked devices are often in some form of ‚Äúsleep mode‚Äù to conserve energy (as 
we saw with 802.11 power management) and will awaken periodically, according 
to the master‚Äôs schedule, to receive beacon messages from the master. A parked 
device cannot communicate until its status has been changed from parked to active 
by the master node.
Because Bluetooth ad hoc networks must be self-organizing, it‚Äôs worth looking 
into how they bootstrap their network structure. When a master node wants to form a 
Bluetooth network, it must first determine which other Bluetooth devices are within 
range; this is the neighbor discovery problem. The master does this by broadcasting 
a series of 32 inquiry messages, each on a different frequency channel, and repeats 
the transmission sequence for up to 128 times. A client device listens on its chosen 
frequency, hoping to hear one of the master‚Äôs inquiry messages on this frequency. 
When it hears an inquiry message, it backs off a random amount of time between 
0 and 0.3 seconds (to avoid collisions with other responding nodes, reminiscent of 
Ethernet‚Äôs binary backoff) and then responds to the master with a message contain-
ing its device ID.
Once the Bluetooth master has discovered all of the potential clients within 
range, it then invites those clients that it wishes to join the piconet. This second phase 
is known as Bluetooth paging, and is reminiscent of 802.11 clients associating with 
a base station. Through the paging process, the master will inform the client of the 
frequency-hopping pattern to be used, and the sender‚Äôs clock. The master begins 
the paging process by again sending 32 identical paging invitation messages, each 
now addressed to a specific client, but again using different frequencies, since that 
client has yet to learn the frequency-hopping pattern. Once the client replies with an 
ACK message to the paging invitation message, the master sends frequency-hopping 
information, clock synchronization information and an active member address to the 
client, and then finally polls the client, now using the frequency-hopping pattern, to 
ensure that the client is connected into the network.
In our discussion above, we have only touched on Bluetooth‚Äôs wireless net-
working. Higher level protocols provide for reliable data packet transfer, circuit-
like streaming of audio and video, changing transmission power levels, changing 
active/parked state (and other states), and more. More recent versions of Bluetooth 
have addressed low energy and security considerations. For more information about 
Bluetooth, the interested reader should consult [Bisdikian 2001, Colbach 2017, and 
Bluetooth 2020].

7.4 Cellular Networks: 4G and 5G
In the previous section, we examined how a host can access the Internet when within 
the vicinity of an 802.11 WiFi access point (AP). But as we‚Äôve seen, APs have small 
coverage areas, and a host certainly will not be able to associate with every AP it 
encounters. As a result, WiFi access is hardly ubiquitous for a user on the move.
By contrast, 4G cellular network access has rapidly become pervasive. A recent 
measurement study of more than one million US mobile cellular network subscribers 
found that they can find 4G signals more than 90% of the time, with download speeds 
of 20 Mbps and higher. Users of Korea‚Äôs three major cellular carriers are able to find 
a 4G signal between 95 and 99.5% of the time [Open Signal 2019]. As a result, it is 
now commonplace to stream HD videos or participate in videoconferences while on 
the move in a car, bus, or high-speed train. The ubiquity of 4G Internet access has 
also enabled myriad new IoT applications such as Internet-connected shared bike and 
scooter systems, and smartphone applications such as mobile payments (commonplace 
in China since 2018) and Internet-based messaging (WeChat, WhatsApp, and more).
The term cellular refers to the fact that the region covered by a cellular network 
is partitioned into a number of geographic coverage areas, known as cells. Each cell 
contains a base station that transmits signals to, and receives signals from, the mobile 
devices currently in its cell. The coverage area of a cell depends on many factors, includ-
ing the transmitting power of the base station, the transmitting power of the devices, 
obstructing buildings in the cell, and the height and type of the base station antennas.
In this section, we provide an overview of the current 4G and emerging 5G cellular  
networks. We‚Äôll consider the wireless first hop between the mobile device and the 
base station, as well as the cellular carrier‚Äôs all-IP core network that connects the  
wireless first hop into the carrier‚Äôs network, other carrier networks, and the larger 
Internet. Perhaps surprisingly (given the origins of mobile cellular networks in the 
telephony world, which had a very different network architecture from the Internet), 
we‚Äôll encounter many of the architectural principles in 4G networks that we encoun-
tered in our Internet-focused studies in Chapters 1‚Äì6, including protocol layering, an 
edge/core distinction, the interconnection of multiple provider networks to form a 
global ‚Äúnetwork of networks,‚Äù and the clear separation of data and control planes with 
logically centralized control. We‚Äôll now see these principles through the lens of mobile 
cellular networks (rather than through an Internet lens) and thus see these principles 
instantiated in different ways. And of course, with a carrier‚Äôs network having an all-
IP core, we‚Äôll also encounter many of the Internet protocols that we now know well. 
We‚Äôll cover additional 4G topics‚Äîmobility management in Section 7.6, and 4G secu-
rity in Section 8.8‚Äîlater, after developing the basic principles needed for these topics.
Our discussion here of 4G and 5G networks will be relatively brief. Mobile 
cellular networking is an area with great breadth and depth, with many universi-
ties offering several courses on the topic. Readers seeking a deeper understanding 
are encouraged to see [Goodman 1997; Kaaranen 2001; Lin 2001; Korhonen 2003;

Schiller 2003; Palat 2009; Scourias 2012; Turner 2012; Akyildiz 2010], as well as 
the particularly excellent and exhaustive books [Mouly 1992; Sauter 2014].
Just as Internet RFCs define Internet-standard architecture and protocols, 4G 
and 5G networks are also defined by standards documents known as Technical Spec-
ifications. These documents are freely available online at [3GPP 2020]. Just like 
RFCs, technical specifications can make for rather dense and detailed reading. But 
when you have a question, they are the definitive source for answers!
7.4.1 4G LTE Cellular Networks: Architecture and Elements
The 4G networks that are pervasive as of this writing in 2020 implement the 4G 
Long-Term Evolution standard, or more succinctly 4G LTE. In this section, we‚Äôll 
describe 4G LTE networks. Figure 7.17 shows the major elements of the 4G LTE 
network architecture. The network broadly divides into the radio network at the 
cellular network‚Äôs edge and the core network. All network elements communicate 
with each other using the IP protocol we studied in Chapter 4. As with earlier 2G 
and 3G networks, 4G LTE is full of rather obtuse acronyms and element names. 
We‚Äôll try to cut through that jumble by first focusing on element functions and 
how the various elements of a 4G LTE network interact with each other in both 
the data and the control planes:
‚Ä¢ Mobile Device. This is a smartphone, tablet, laptop, or IoT device that connects 
into a cellular carrier‚Äôs network. This is where applications such as web browsers, 
map apps, voice and videoconference apps, mobile payment apps, and so much 
more are run. The mobile device typically implements the full 5-layer Internet pro-
tocol stack, including the transport and application layers, as we saw with hosts at 
the Internet‚Äôs network edge. The mobile device is a network endpoint, with an IP 
address (obtained through NAT, as we‚Äôll see). The mobile device also has a glob-
ally unique 64-bit identifier called the International Mobile  Subscriber Iden-
tity (IMSI), which is stored on its SIM (Subscriber Identity Module) card. The 
IMSI identifies the subscriber in the worldwide cellular carrier network system, 
including the country and home cellular carrier network to which the subscriber 
belongs. In some ways, the IMSI is analogous to a MAC address. The SIM card 
also stores information about the services that the subscriber is able to access and 
encryption key information for that subscriber. In the official 4G LTE jargon, 
the mobile device is referred to as User Equipment (UE). However, in this text-
book, we‚Äôll use the more reader-friendly term ‚Äúmobile device‚Äù throughout. We 
also note here that a mobile device is not always mobile; for example, the device 
might be a fixed temperature sensor or a surveillance camera.
‚Ä¢ Base Station. The base station sits at the ‚Äúedge‚Äù of the carrier‚Äôs network and is 
responsible for managing the wireless radio resources and the mobile devices 
with its coverage area (shown as a hexagonal cell in Figure 7.17). As we‚Äôll see, a 
mobile device will interact with a base station to attach to the carrier‚Äôs network. 
The base station coordinates device authentication and allocation of resources

(channel access) in the radio access network. In this sense, cellular base station 
functions are comparable (but by no means identical) to those of APs in wireless 
LANs. But cellular base stations have several other important roles not found in 
wireless LANs. In particular, base stations create device-specific IP tunnels from 
the mobile device to gateways and interact among themselves to handle device 
mobility among cells. Nearby base stations also coordinate among themselves to 
manage the radio spectrum to minimize interference between cells. In the offi-
cial 4G LTE terminology, the base station is referred to as an ‚ÄúeNode-B,‚Äù which 
is rather opaque and non-descriptive. In this textbook, we will instead use the 
reader-friendlier term ‚Äúbase station‚Äù throughout.
 
 
As an aside, if you find LTE terminology a bit opaque, you aren‚Äôt alone! 
The etymology of ‚ÄúeNode-B‚Äù is rooted in earlier 3G terminology, where network 
function points were referred to as ‚Äúnodes,‚Äù with ‚ÄúB‚Äù harkening back to earlier 
‚ÄúBase Station (BS)‚Äù 1G terminology or ‚ÄúBase Transceiver Station (BTS)‚Äù in 2G 
terminology. 4G LTE is an ‚Äúe‚Äùvolution over 3G, and hence, an ‚Äúe‚Äù now precedes 
‚ÄúNode-B‚Äù in 4G LTE terminology. This name opaqueness shows no signs in 
stopping! In 5G systems, eNode-B functions are now referred to as ‚Äúng-eNB‚Äù; 
perhaps you can guess what that acronym stands for!
‚Ä¢ Home Subscriber Server (HSS). As shown in Figure 7.18, the HSS is a 
 control-plane element. The HSS is a database, storing information about the 
mobile devices for which the HSS‚Äôs network is their home network. It is used in 
conjunction with the MME (discussed below) for device authentication.
‚Ä¢ Serving Gateway (S-GW), Packet Data Network Gateway (P-GW), and 
other¬†network routers. As shown in Figure 7.18, the Serving Gateway and the 
Packet Data Network Gateway are two routers (often collocated in practice) that 
Mobility
Management
Entity (MME)
Serving Gateway
(S-GW)
Home
Subscriber
Service (HSS)  
PDN gateway
(P-GW)
all-IP Enhanced Packet Core (EPC)
radio access
network
to
Internet 
Mobile device  
Base
station
Figure 7.17 ‚ô¶ Elements of the 4G LTE architecture

lie on the data path between the mobile device and the Internet. The PDN 
Gateway also provides NAT IP addresses to mobile devices and performs NAT 
functions (see Section 4.3.4). The PDN Gateway is the last LTE element that a 
datagram originating at a mobile device encounters before entering the larger 
Internet. To the outside world, the P-GW looks like any other gateway router; 
the mobility of the mobile nodes within the cellular carrier‚Äôs LTE network is 
hidden from the outside world behind the P-GW. In addition to these gateway 
routers, a cellular carrier‚Äôs all-IP core will have additional routers whose role 
is similar to that of traditional IP routers‚Äîto forward IP datagrams among 
themselves along paths that will typically terminate at elements of the LTE 
core network.
‚Ä¢ Mobility Management Entity (MME). The MME is also a control-plane element, 
as shown in Figure 7.18. Along with the HSS, it plays an important role in authen-
ticating a device wanting to connect into its network. It also sets up the tunnels on 
the data path from/to the device and the PDN Internet gateway router, and maintains 
information about an active mobile device‚Äôs cell location within the carrier‚Äôs cel-
lular network. But, as shown in Figure 7.18, it is not in the forwarding path for the 
mobile device‚Äôs datagrams being sent to and from the Internet.
 Authentication. It is important for the network and the mobile device attaching 
to the network to mutually authenticate each other‚Äîfor the network to know 
that the attaching device is indeed the device associated with a given IMSI, and 
for the mobile device to know that the network to which it is attaching is also a 
legitimate cellular carrier network. We will cover authentication in Chapter 8 
and cover 4G authentication in Section 8.8. Here, we simply note that the MME 
plays a middleman role between the mobile and Home Subscriber Service 
(HSS) in the mobile‚Äôs home network. Specifically, after receiving an attach 
request from mobile device, the local MME contacts the HSS in the mobile‚Äôs 
home network. The mobile‚Äôs home HSS then returns enough encrypted infor-
mation to the local MME to prove to the mobile device that the home HSS 
is performing authentication through this MME, and for the mobile device to 
prove to the MME that it is indeed the mobile associated with that IMSI. When 
a mobile device is attached to its home network, the HSS to be contacted dur-
ing authentication is located within that same home network. However, when a 
mobile device is roaming on a visited network operated by a different cellular 
network carrier, the MME in that roaming network will need to contact the HSS 
in the mobile device‚Äôs home network.
 Path setup. As shown in the bottom half of Figure 7.18, the data path from the 
mobile device to the carrier‚Äôs gateway router consists of a wireless first hop 
between the mobile device and the base station, and concatenated IP tunnels 
between the base station and the Serving Gateway, and the Serving Gateway 
and the PDN Gateway. Tunnels are setup under the control of the MME and 
used for data forwarding (rather than direct forwarding among network routers) 
to facilitate device mobility‚Äîwhen a device moves, only the tunnel endpoint

Control 
plane
User data
plane
MME
HSS
Base station
Base station
P-GW
S-GW
S-GW
P-GW
Figure 7.18 ‚ô¶ LTE data-plane and control-plane elements
terminating at the base station needs to be changed, while other tunnel end-
points, and the Quality of Service associated with a tunnel, remain unchanged.
 Cell location tracking. As the device moves between cells, the base stations 
will update the MME on the device‚Äôs location. If the mobile device is in a 
sleep mode but nonetheless moving between cells, the base stations can no 
longer track the device‚Äôs location. In this case, it will be the responsibility of 
the MME to locate the device for wakeup, through a process known as paging.
Table 7.2 summarizes the key LTE architectural elements that we have dis-
cussed above and compares these functions with those we encountered in our study 
of WiFi wireless LANs (WLANs).
LTE Element
Description
Similar WLAN function(s)
Mobile device (UE: User equipment)
End user‚Äôs IP-capable wireless/mobile device  
(e.g., smartphone, tablet, laptop)
Host, end-system
Base Station (eNode-B)
Network side of wireless access link  
into LTE network
Access point (AP), although the LTE base station 
performs many functions not found in WLANs
The Mobility Management  
Entity (MME)
Coordinator for mobile device services: 
authentication, mobility management
Access point (AP), although the MME performs 
many functions not found in WLANs
Home Subscriber Server (HSS)
Located in a mobile device‚Äôs home network, 
providing authentication, access privileges in  
home and visited networks
No WLAN equivalent
Serving Gateway (S-GW),  
PDN-Gateway (P-GW)
Routers in a cellular carrier‚Äôs network, coordinating 
forwarding to outside of the carrier‚Äôs network
iBGP and eBGP routers in access ISP network
Radio Access Network
Wireless link between mobile device and a  
base station
802.11 wireless link between mobile and AP
Table 7.2 ‚ô¶ LTE Elements, and similar WLAN (WiFi) functions

THE ARCHITECTURAL EVOLUTION FROM 2G TO 3G TO 4G
In a relatively short span of 20 years, cellular carrier networks have undergone an 
astonishing transition from being almost exclusively circuit-switched telephone net-
works to being all-IP packet-switched data networks which include voice as just one 
of many applications. How did this transition happen from an architectural stand-
point? Was there a ‚Äúflag day,‚Äù when the previous telephony-oriented networks were 
turned ‚Äúoff‚Äù and the all-IP cellular network was turned ‚Äúon‚Äù? Or did elements in the 
previous telephony-oriented networks begin taking on dual circuit (legacy) and packet 
(new) functionality, as we saw with the IPv4-to-IPv6 transition in Section 4.3.5?
Figure 7.19 is taken from the earlier 7th edition of this textbook, which covered 
both 2G and 3G cellular networks. (We have retired this historical material, which is 
still available on this book‚Äôs website, in favor of a deeper coverage of 4G LTE in this 
8th edition). Although the 2G network is a circuit-switched mobile telephone network, 
a comparison of Figures 7.17 and 7.19 illustrates a similar conceptual structure, 
albeit for voice rather than for data services‚Äîa wireless edge controlled by a base 
station, a gateway from the carrier‚Äôs network to the outside world, and aggregation 
points between the base stations and the gateway.
CASE HISTORY
Base station 
controller
Base station 
controller
Mobile
Switching
Center
Gateway Mobile
Switching
Center
Base Station
System (BSS)
Base Station System (BSS)
Public Telephone
Network
G
Figure 7.19 ‚ô¶  Elements of the 2G cellular architecture, supporting circuit-switched 
voice service with the carrier‚Äôs core network

Gateway Mobile
Switching Center
G
Radio Network
Controller (RNC)
Gateway GPRS
Support Node
Serving GPRS
Support Node
G
Mobile Switching
Center
Public Telephone
Network
Public
Internet
Figure 7.20 ‚ô¶  3G system architecture: supporting separate circuit-switched  
voice service and packet-switched data service with the carrier‚Äôs 
core network
Figure 7.20 (also taken from the 7th edition of this textbook) shows the main archi-
tectural components of the 3G cellular architecture, which supports both circuit-switched 
voice service and packet-switched data services. Here, the transition from a voice-only 
network to a combined voice and data network is clear: the existing core 2G cellular 
voice network elements remained untouched. However, additional cellular data func-
tionality was added in parallel to, and functioned independently from, the existing core 
voice network at that time. As shown in Figure 7.20, the splitting point into these two 
separate core voice and data networks happened at the network edge, at the base  
station in the radio access network. The alternative‚Äîintegrating new data services 
directly into the core elements of the existing cellular voice network‚Äîwould have raised 
the same challenges encountered in integrating new (IPv6) and legacy (IPv4) technolo-
gies in the Internet. The carriers also wanted to leverage and exploit their considerable 
investment of existing infrastructure (and profitable services!) in their existing cellular voice 
network.

7.4.2 LTE Protocols Stacks
Since the 4G LTE architecture is an all-IP architecture, we‚Äôre already very famil-
iar¬† with the higher-layer protocols in the LTE protocol stack, in particular IP, 
TCP, UDP, and various application layer protocols, from our studies in Chapters 2  
through 5. Consequently, the new LTE protocols that we‚Äôll focus on here are pri-
marily at the link and physical layers, and in mobility management.
Figure 7.21 shows the user-plane protocol stacks at the LTE mobile node, the 
base station, and the serving gateway. We‚Äôll touch on several of LTE‚Äôs control-plane 
protocols later when we study LTE mobility management (Section 7.6) and security 
(Section 8.8). As we can see from Figure 7.21, most of the new and interesting user-
plane protocol activity is happening at the wireless radio link between the mobile 
device and the base station.
LTE divides the mobile device‚Äôs link layer into three sublayers:
‚Ä¢ Packet Data Convergence. This uppermost sublayer of the link layer sits just 
below IP. The Packet Data Convergence Protocol (PDCP) [3GPP PDCP 2019] 
performs IP header/compression in order to decrease the number of bits sent over 
the wireless link, and encryption/decryption of the IP datagram using keys that 
were established via signaling messages between the LTE mobile device and the 
Mobility Management Entity (MME) when the mobile device first attached to the 
network; we‚Äôll cover aspects of LTE security in Section 8.8.2.
‚Ä¢ Radio Link Control. The Radio Link Control (RLC) Protocol [3GPP RLCP 
2018]¬†performs two important functions: (i) fragmenting (on the sending side) 
and reassembly (on the receiving) of IP datagrams that are too large to fit into 
Base station
PDN Gateway 
(P-GW)
Serving Gateway 
(S-GW)
to/ from
Internet
IP
Packet Data
Convergence
Radio Link
Medium Access
Link
GTP-U
IP
Link
Physical
UDP
GTP-U
IP
Link
Physical
UDP
GTP-U
IP
Link
Physical
UDP
Application
Physical
Transport
IP
Packet Data
Convergence
Radio Link
Medium Access
Physical
Figure 7.21 ‚ô¶ LTE data-plane protocol stacks

the underlying link-layer frames, and (ii) link-layer reliable data transfer at the 
through the use of an ACK/NAK-based ARQ protocol. Recall the we‚Äôve studied 
the basic elements of ARQ protocols in Section 3.4.1.
‚Ä¢ Medium Access Control (MAC). The MAC layer performs transmission sched-
uling, that is, the requesting and use of the radio transmission slots described 
in Section 7.4.4. The MAC sublayer also performs additional error detection/ 
correction functions, including the use of redundant bit transmission as a forward 
error-correction technique. The amount of redundancy can be adapted to channel 
conditions.
Figure 7.21 also shows the use of tunnels in the user data path. As discussed 
above, these tunnels are established, under MME control, when the mobile device 
first attaches to the network. Each tunnel between two endpoints has a unique tunnel 
endpoint identifier (TEID). When the base station receives datagrams from the mobile 
device, it encapsulates them using the GPRS Tunneling Protocol [3GPP GTPv1-U  
2019], including the TEID, and sends them in UDP segments to the Serving Gateway 
at the other end of the tunnel. On the receiving side, the base station decapsulates tun-
neled UDP datagrams, extracts the encapsulated IP datagram destined for the mobile 
device, and forwards that IP datagram over the wireless hop to the mobile device.
7.4.3 LTE Radio Access Network
LTE uses a combination of frequency division multiplexing and time division multi-
plexing on the downstream channel, known as orthogonal frequency division multi-
plexing (OFDM) [Hwang 2009]. (The term ‚Äúorthogonal‚Äù comes from the fact the 
signals being sent on different frequency channels are created so that they interfere 
very little with each other, even when channel frequencies are tightly spaced). In 
LTE, each active mobile device is allocated one or more 0.5 ms time slots in one or 
more of the channel frequencies. Figure 7.22 shows an allocation of eight time slots 
over four frequencies. By being allocated increasingly more time slots (whether on 
the same frequency or on different frequencies), a mobile device is able to achieve 
increasingly higher transmission rates. Slot (re)allocation among mobile devices can 
be performed as often as once every millisecond. Different modulation schemes can 
also be used to change the transmission rate; see our earlier discussion of Figure 7.3 
and dynamic selection of modulation schemes in WiFi networks.
The particular allocation of time slots to mobile devices is not mandated by the 
LTE standard. Instead, the decision of which mobile devices will be allowed to trans-
mit in a given time slot on a given frequency is determined by the scheduling algo-
rithms provided by the LTE equipment vendor and/or the network operator. With 
opportunistic scheduling [Bender 2000; Kolding 2003; Kulkarni 2005], matching the 
physical-layer protocol to the channel conditions between the sender and receiver 
and choosing the receivers to which packets will be sent based on channel conditions 
allow the base station to make best use of the wireless medium. In addition, user

priorities and contracted levels of service (e.g., silver, gold, or platinum) can be used 
in scheduling downstream packet transmissions. In addition to the LTE capabilities 
described above, LTE-Advanced allows for downstream bandwidths of hundreds of 
Mbps by allocating aggregated channels to a mobile device [Akyildiz 2010].
7.4.4  Additional LTE Functions: Network Attachment and 
Power Management
Let‚Äôs conclude or study of 4G LTE here by considering two additional important 
LTE functions: (i) the process with which a mobile device first attaches to the net-
work and (ii) the techniques used by the mobile device, in conjunction with core 
network elements, to manage its power use.
Network Attachment
The process by which a mobile device attaches to the cellular carrier‚Äôs network 
divides broadly into three phases:
‚Ä¢ Attachment to a Base Station. This first phase of device attachment is similar in 
purpose to, but quite different in practice from, the 802.11 association protocol 
that we studied in Section 7.31. A mobile device wishing to attach to a cellular 
carrier network will begin a bootstrap process to learn about, and then associate 
with, a nearby base station. The mobile device initially searches all channels in all 
frequency bands for a primary synchronization signal that is periodically broadcast  
Figure 7.22 ‚ô¶  Twenty 0.5-ms slots organized into 10 ms frames at each 
frequency. An eight-slot allocation is shown shaded.
f1
f2
f3
f4
f5
f6
0
0.5
1.0
1.5
2.0
2.5
9.0
9.5
10.0

every 5 ms by a base station. Once this signal is found, the mobile device remains on 
this frequency and locates the secondary synchronization signal. With information 
found in this second signal, the device can locate (following several further steps) 
additional information such as channel bandwidth, channel configurations, and the 
cellular carrier information of that base station. Armed with this information, the 
mobile device can select a base station to associate with (preferentially attaching to 
its home network, if available) and establish a control-plane signaling connection 
across the wireless hop with that base station. This mobile-to-base-station channel 
will be used through the remainder of the network attachment process.
‚Ä¢ Mutual Authentication. In our earlier description of the Mobility Management 
Entity (MME) in Section 7.4.1, we noted that the base station contacts the local 
MME to perform mutual authentication‚Äîa process that we‚Äôll study in further 
detail in Section 8.8.2. This is the second phase of network attachment, allowing 
the network to know that the attaching device is indeed the device associated 
with a given IMSI, and the mobile device to know that the network to which it 
is attaching is also a legitimate cellular carrier network. Once this second phase 
of network attachment is complete, the MME and mobile device have mutually 
authenticated each other, and the MME also knows the identity of the base station 
to which the mobile is attached. Armed with this information, the MME is now 
ready to configure the Mobile-device-to-PDN-gateway data path.
‚Ä¢ Mobile-device-to-PDN-gateway Data Path Configuration. The MME contacts 
the PDN gateway (which also provides a NAT address for the mobile device), 
the Serving gateway, and the base station to establish the two tunnels shown in 
Figure 7.21. Once this phase is complete, the mobile device is able to send/receive 
IP datagrams via the base station through these tunnels to and from the Internet!
Power Management: Sleep Modes
Recall in our earlier discussion of advanced features in 802.11 (Section 7.3.5) and 
Bluetooth (Section 7.3.6) that a radio in a wireless device may enter a sleep state to 
save power when it is not transmitting or receiving in order to minimize the amount 
of time that the mobile device‚Äôs circuitry needs to be ‚Äúon‚Äù for sending/receiving data, 
and for channel sensing. In 4G LTE, a sleeping mobile device can be in one of two 
different sleep states. In the discontinuous reception state, which is typically entered 
after several hundred milliseconds of inactivity [Sauter 2014], the mobile device and 
the base station will schedule periodic times in advance (typically several hundred 
milliseconds apart) at which the mobile device will wake up and actively monitor the 
channel for downstream (base station to mobile device) transmissions; apart from 
these scheduled times, however, the mobile device‚Äôs radio will be sleeping.
If the discontinuous reception state might be considered a ‚Äúlight sleep,‚Äù the second 
sleep state‚Äîthe Idle state‚Äîwhich follows even longer periods of 5 to 10 seconds of 
inactivity, might be thought of as a ‚Äúdeep sleep.‚Äù While in this deep sleep, the mobile 
device‚Äôs radio wakes up and monitors the channel even less frequently. Indeed, this 
sleep is so deep that if the mobile device moves into a new cell in the carrier‚Äôs network

while sleeping, it need not inform the base station with which it was previous associ-
ated. Thus, when waking up periodically from this deep sleep, the mobile device will 
need to re-establish an association with a (potentially new) base station in order to 
check for paging messages broadcast by the MME to base stations nearby the base 
station with which the mobile was last associated. These control-plane paging mes-
sages, which are broadcast by these base stations to all mobile devices in their cells, 
indicate which mobile devices should fully wake up and re-establish a new data-plane 
connection to a base station (see Figure 7.18) in order to receive incoming packets.
7.4.5 The Global Cellular Network: A Network of Networks
Having now studied the 4G cellular network architecture, let‚Äôs take a step back at 
take a look at how the global cellular network‚Äîitself a ‚Äúnetwork of networks‚Äù like 
the Internet‚Äîis organized.
Figure 7.23 shows a user‚Äôs mobile smartphone connected via a 4G base station 
into its home network. The user‚Äôs home mobile network is operated by a cellular 
Home cellular
carrier network
Gateway
Home Subscriber
Server
Visited mobile
carrier network 
Gateway
Public Internet
and IPX
Figure 7.23 ‚ô¶  The global cellular data network: a network of networks.

carrier such as Verizon, AT&T, T-Mobile, or Sprint in the United States; Orange in 
France; or SK Telecom in Korea. The user‚Äôs home network, in turn, is connected 
to the networks of other cellular carriers and to the global Internet, though one or 
more gateway routers in the home network, as shown in Figure 7.23. The mobile 
networks themselves interconnect with each other either via the public Internet or via 
an Internet Protocol Packet eXchange (IPX) Network [GSMA 2018a]. An IPX is a 
managed network specifically for interconnecting cellular carriers, similar to Internet 
eXchange Points (see Figure 1.15) for peering among ISPs. From Figure 7.23, we 
can see that the global cellular network is indeed a ‚Äúnetwork of networks‚Äù‚Äîjust like 
the Internet (recall Figure 1.15 and Section 5.4). 4G networks can also peer with 3G 
cellular voice/data networks and earlier voice-only networks.
We‚Äôll return shortly to additional 4G LTE topics‚Äîmobility management in 
Section 7.6, and 4G security in Section 8.8.2‚Äîlater, after developing the basic 
principles needed for these topics. Let‚Äôs now take a quick look at the emerging 
5G networks.
7.4.6 5G Cellular Networks
The ultimate wide-area data service would be one with ubiquitous gigabit connec-
tion speeds, extremely low latency, and unrestricted limitations on the number of 
users and devices that could be supported in any region. Such a service would open 
the door to all kinds of new applications, including pervasive augmented reality and 
virtual reality, control of autonomous vehicles via wireless connections, control of 
robots in factories via wireless connections, and replacement of residential access 
technologies, such as DSL and cable, with fixed wireless Internet services (that is, 
residential wireless connections from base stations to modems in homes).
It is expected that 5G, for which progressively improved versions are likely to 
be rolled out in the 2020 decade, will make a big step towards achieving the goals 
of the ultimate wide-area data service. It is predicted that 5G will provide roughly a 
10x increase in peak bitrate, a 10x decrease in latency, and a 100x increase in traffic 
capacity over 4G [Qualcomm 2019].
Principally, 5G refers to ‚Äú5G NR (New Radio),‚Äù which is the standard adopted 
by 3GPP. Other 5G technologies besides NR do exist, however. For example, Veri-
zon‚Äôs proprietary 5G TF network operates on 28 and 39 GHz frequencies and is used 
only for fixed wireless Internet service, not in smartphones.
5G standards divide frequencies into two groups: FR1 (450 MHz‚Äì6 GHz) and 
FR2 (24 GHz‚Äì52 GHz). Most early deployments will be in the FR1 space, although 
there are early deployments as of 2020 in the FR2 space for fixed Internet residential 
access as mentioned just above. Importantly, the physical layer (that is, wireless) 
aspects of 5G are not backward-compatible with 4G mobile communications systems 
such as LTE: in particular, it can‚Äôt be delivered to existing smartphones by deploying 
base station upgrades or software updates. Therefore, in the transition to 5G, wireless 
carriers will need to make substantial investments in physical infrastructure.

FR2 frequencies are also known as millimeter wave frequencies. While 
millimeter wave frequencies allow for much faster data speeds, they come with  
two major drawbacks:
‚Ä¢ Millimeter wave frequencies have much shorter range from base station to receiv-
ers. This makes millimeter wave technology unsuitable in rural areas and requires 
denser deployments of base stations in urban areas.
‚Ä¢ Millimeter wave communication is highly susceptible to atmospheric interfer-
ence. Nearby foliage and rain can cause problems for outdoor use.
5G is not one cohesive standard, but instead consists of three co-existing standards 
[Dahlman 2018]:
‚Ä¢ eMBB (Enhanced Mobile Broadband). Initial deployments of 5G NR have 
focused on eMBB, which provides for increased bandwidth for higher down-
load and upload speeds, as well as a moderate reduction in latency when 
compared to 4G LTE. eMBB enables rich media applications, such as mobile 
augmented reality and virtual reality, as well as mobile 4K resolution and 360¬∞ 
video streaming.
‚Ä¢ URLLC (Ultra Reliable Low-Latency Communications). URLLC is targeted 
towards applications that are highly latency-sensitive, such as factory automation 
and autonomous driving. URLLC is targeting latencies of¬†1msec. As of this writ-
ing, technologies that enable URLLC are still being standardized.
‚Ä¢ mMTC (Massive Machine Type Communications). mMTC is a narrowband 
access type for sensing, metering, and monitoring applications. One priority for 
the design of 5G networks is to lower barriers for network connectivity for IoT 
devices. In addition to lowering latency, emerging technologies for 5G networks 
are focusing on reducing power requirements, making the use of IoT devices 
more pervasive than has been with 4G LTE.
5G and Millimeter Wave Frequencies
Many 5G innovations will be a direct result of working in the millimeter wave fre-
quencies in the 24 GHz‚Äì52 GHz band. For example, these frequencies offer the 
potential of achieving 100x increase in capacity over 4G. To get some insight into 
this, capacity can be defined as the product of three terms [Bj√∂rnson 2017]:
capacity =  cell density *  available spectrum *  spectral efficiency
where cell density is in units of cells/km2, available spectrum is in units of Hertz, 
and spectral efficiency is a measure of how efficiently each base station can com-
municate with users and is in units of bps/Hz/cell. By multiplying these units out, it 
is easy to see that capacity is in units of bps/km2. For each of these three terms, the 
values will be larger for 5G than for 4G:

‚Ä¢ Because millimeter frequencies have much shorter range than 4G LTE fre-
quencies, more base stations are required, which in turn increases the cell 
density.
‚Ä¢ Because 5G FR2 operates in a much larger frequency band (52 ‚àí 24 = 28 GHz) 
than 4G LTE (up to about 2 GHz), it has more available spectrum.
‚Ä¢ With regard to spectral efficiency, information theory says that if you want to 
double spectral efficiency, a 17-fold increase in power is needed [Bj√∂rnson 
2017]. Instead of increasing power, 5G uses MIMO-technology (the same tech-
nology we encountered in our study of 802.11 networks in Section 7.3), which 
uses multiple antennas at each base station. Rather than broadcasting signals in 
all directions, each MIMO antenna employs beam forming and directs the signal 
at the user. MIMO technology allows a base station to send to 10‚Äì20 users at the 
same time in the same frequency band.
By increasing all three terms in the capacity equation, 5G is expected to provide 
a 100x increase in capacity in urban areas. Similarly, owing to the much wider fre-
quency band, 5G is expected to provide peak download rates of 1 Gbps or higher.
Millimeter wave signals are, however, easily blocked by buildings and trees. 
Small cell stations are needed to fill in coverage gaps between base stations and 
users. In a highly populous region, the distance between two small cells could vary 
from 10 to 100 meters [Dahlman 2018].
5G Core Network
The 5G Core network is the data network that manages all of the 5G mobile voice, 
data and Internet connections. The 5G Core network is being redesigned to better 
integrate with the Internet and cloud-based services, and also includes distributed 
servers and caches across the network, thereby reducing latency. Network function 
virtualization (as discussed in Chapters 4 and 5), and network slicing for different 
applications and services, will be managed in the core.
The new 5G Core specification introduces major changes in the way mobile 
networks support a wide variety of services with varied performance. As in the case 
of the 4G core network (recall Figures 7.17 and 7.18), the 5G core relays data traffic 
from end devices, authenticates devices, and manages device mobility. The 5G core 
also contains all of the network elements that we encountered in Section 7.4.2‚Äîthe 
mobile devices, the cells, the base stations, and the Mobility Management Entity 
(now divided into two sub-elements, as discussed below), the HSS, and the Serving 
and PDN gateways.
Although the 4G and 5G core networks perform similar functions, there are some 
major differences in that the new 5G core architecture. The 5G Core is designed for 
complete control and user-plane separation (see Chapter 5). The 5G Core consists 
purely of virtualized software-based network functions. This new architecture will give

operators the flexibility to meet the diverse requirements of the different 5G applica-
tions. Some of the new 5G core network functions include [Rommer 2019]:
‚Ä¢ User-Plane Function (UPF). Control and user-plane separation (see Chapter 5) 
allows packet processing to be distributed and pushed to the network edge.
‚Ä¢ Access and Mobility Management Function (AMF). The 5G Core essentially 
decomposes the 4G Mobility Management Entity (MME) into two functional 
elements: AMF and SMF. The AMF receives all the connection and session 
information from end-user equipment but only handles connection and mobility 
management tasks.
‚Ä¢ Session Management Function (SMF). Session management is handled by the 
Session Management Function (SMF). The SMF is responsible for interacting 
with the decoupled data plane. The SMF also performs IP address management 
and plays the role of DHCP.
As of this writing (2020), 5G is in its early stages of deployment, and many 5G 
standards have yet to be finalized. Only time will tell whether 5G will become a per-
vasive broadband wireless service, whether it will successfully compete with WiFi 
for indoor wireless service, whether it will become a critical component of factory 
automation and the autonomous vehicle infrastructure, and whether it will take us a 
big step forward toward the ultimate wide-area wireless service.
7.5 Mobility Management: Principles
Having covered the wireless nature of the communication links in a wireless net-
work, it‚Äôs now time to turn our attention to the mobility that these wireless links ena-
ble. In the broadest sense, a mobile device is one that changes its point of attachment 
into the network over time. Because the term mobility has taken on many meanings 
in both the computer and telephony worlds, it will serve us well first to carefully 
consider forms of mobility.
7.5.1 Device Mobility: a Network-layer Perspective
From the network layer‚Äôs standpoint, a physically mobile device will present a very 
different set of challenges to the network layer, depending on how active the device is 
as it moves between points of attachment to the network. At the one end of the spec-
trum, scenario (a) in Figure 7.24 is the mobile user who himself/herself physically 
moves between networks, but powers down the mobile device when moving. For 
example, a student might disconnect from a wireless classroom network and power 
down his/her device, head to the dining commons and connect to the wireless access

network there while eating, and then disconnect and power down from the dining 
commons network, walk to the library, and connect to the library‚Äôs wireless network 
while studying. From a networking perspective, this device is not mobile‚Äîit attaches 
to an access network and remains in that access network while on. In this case, the 
device serially associates with, and later disassociates from, each wireless access 
network encountered. This case of device (non-)mobility can be completely handled 
using the networking mechanisms we‚Äôve already studied in Sections 7.3 and 7.4.
In scenario (b) in Figure 7.24, the device is physically mobile but remains 
attached to the same access network. This device is also not mobile from a network-
layer perspective. Additionally, if the device remains associated with the same 802.11 
AP or LTE base station, the device is not even mobile from a link-layer perspective.
From a network standpoint, our interest in device mobility really starts with case (c), 
where a device changes its access network (e.g., 802.11 WLAN or LTE cell) while 
continuing to send and receiving IP datagrams, and while maintaining higher-level 
(e.g., TCP) connections. Here, the network will need to provide handover‚Äîa trans-
fer of responsibility for forwarding datagrams to/from one AP or base station to the 
mobile device‚Äîas the device moves among WLANs or among LTE cells. We‚Äôll 
cover handover in detail in Section 7.6. If the handover occurs within access net-
works belonging to a single network provider, that provider can orchestrate handover 
on its own. When a mobile device roams between multiple provider networks, as in 
scenario (d), the providers must orchestrate handover together, which considerably 
complicates the handover process.
7.5.2 Home Networks and Roaming on Visited Networks
As we learned in our discussions of cellular 4G LTE networks in Section 7.4.1, every 
subscriber has a ‚Äúhome‚Äù with some cellular provider. We learned that the Home 
Subscriber Service (HSS) stores information about each of its subscribers, includ-
ing a globally unique device ID (embedded in a subscriber‚Äôs SIM card), information 
about services that the subscriber may access, cryptographic keys to be used for 
(a) Device mobility
 
between access
 
networks, but
 
powered down
 
while moving
 
between access
 
networks
(b) Device mobility
 
only within same
 
wireless access
 
network, in single
 
provider network
(c) Device mobility
 
among access
 
networks in single
 
provider network,
 
while maintaining
 
ongoing connections
(d) Device mobility
 
among multiple
 
provider networks,
 
while maintaining
 
ongoing connections
Figure 7.24 ‚ô¶  Various degrees of mobility, from a network-layer  
perspective

communication, and billing/charging information. When a device is connected to a 
cellular network, other than its home network, that device is said to be roaming on a 
visited network. When a mobile device attaches to, and roams on, a visited network, 
coordination will be required between the home network and the visited network.
The Internet does not have a similarly strong notion of a home network or a vis-
ited network. In practice, a student‚Äôs home network might be the network operated 
by his/her school; for mobile professionals, their home network might be their com-
pany network. The visited network might be the network of a school or a company 
they are visiting. But there is no notion of a home/visited network deeply embedded 
in the Internet‚Äôs architecture. The Mobile IP protocol [Perkins 1998, RFC 5944], 
which we will cover briefly in Section 7.6, was a proposal that strongly incorporated 
the notion of home/visited networks. But Mobile IP has seen limited deployment/use 
in practice. There are also activities underway that are built on top of the existing IP 
infrastructure to provide authenticated network access across visited IP networks. 
Eduroam [Eduroam 2020] is one such activity.
The notion of a mobile device having a home network provides two important 
advantages: the home network provides a single location where information about 
that device can be found, and (as we will see) it can serve as a coordination point for 
communication to/from a roaming mobile device.
To appreciate the potential value of the central point of information and coordi-
nation, consider the human analogy of a 20-something adult Bob moving out of the 
family home. Bob becomes mobile, living in a series of dormitories and apartments, 
and often changing addresses. If an old friend Alice wants to get in touch, how can 
Alice find the current address of Bob? One common way is to contact the family, 
since a mobile 20-something adult will often register his or her current address with 
the family (if for no other reason than so that the parents can send money to help pay 
the rent!). The family home becomes that unique location that others can go to as a 
first step in communicating with Bob. Additionally, later postal communication from 
Alice may be either indirect (e.g., with mail being sent first to Bob‚Äôs family home and 
then forwarded to Bob) or direct (e.g., with Alice using the address obtained from 
Bob‚Äôs parents to send mail directly to Bob).
7.5.3 Direct and Indirect Routing to/from a Mobile Device
Let us now consider the conundrum faced by the Internet-connected host (that we 
will refer to as a correspondent) in Figure 7.25 wishing to communicate with a 
mobile device that might be located within that mobile device‚Äôs cellular home net-
work, or might be roaming in a visited network. In our development below, we‚Äôll 
adopt a 4G/5G cellular network perspective, since these networks have such a long 
history of supporting device mobility. But as we‚Äôll see, the fundamental challenges 
and basic solution approaches for supporting device mobility are equally applicable 
in both cellular networks and in the Internet.
As shown in Figure 7.25, we‚Äôll assume that the mobile device has a globally 
unique identifier associated with it. In 4G, LTE cellular networks (see Section 7.4),

this would be the International Mobile Subscriber Identity (IMSI) and an associated 
phone number, stored on a mobile device‚Äôs SIM card. For mobile Internet users, this 
would be a permanent IP address in the IP address range of its home network, as in 
the case of the Mobile IP architecture.
What approaches might be used in a mobile network architecture that would 
allow a datagram sent by the correspondent to reach that mobile device? Three basic 
approaches can be identified and are discussed below. As we will see, the latter two 
of these are adopted in practice.
Leveraging the Existing IP Address Infrastructure
Perhaps the simplest approach to routing to a mobile device in a visited network is 
to simply use the existing IP addressing infrastructure‚Äîto add nothing new to the 
architecture. What could be easier!
Recall from our discussion of Figure 4.21 that an ISP uses BGP to advertise 
routes to destination networks by enumerating the CIDRized address ranges of reach-
able networks. A visited network could thus advertise to all other networks that a 
Home network
gateway
Visited Network
79.129/16
Home Network
128.119/16
Visited network
gateway
Mobility
manager
Home
Subscriber
Service
Correspondent
Public or private
Internet
Mobility
manager
NAT IP:
10.0.0.99
IMSI
78:4f:43:98:d9:27
Permanent IP:
128.119.40.186
IMSI
78:4f:43:98:d9:27
Figure 7.25 ‚ô¶ Elements of a mobile network architecture

particular mobile device is resident in its network simply by advertising a highly 
specific address‚Äîthe mobile device‚Äôs full 32-bit IP permanent address‚Äîessentially 
informing other networks that it has the path to be used to forward datagrams to that 
mobile device. These neighboring networks would then propagate this routing infor-
mation throughout the network as part of the normal BGP procedure of updating rout-
ing information and forwarding tables. Since datagrams will always be forwarded to 
the router advertising the most specific destination for that address (see Section 4.3), 
all datagrams addressed to that mobile device will be forwarded to the visited net-
work. If the mobile device leaves one visited network and joins another, the new vis-
ited network can advertise a new, highly specific route to the mobile device, and the 
old visited network can withdraw its routing information regarding the mobile device.¬†
This solves two problems at once, and does so without making changes to the 
network-layer infrastructure! Other networks know the location of the mobile device, 
and it is easy to route datagrams to the mobile device, since the forwarding tables 
will direct datagrams to the visited network. The killer drawback, however, is that 
of scalability‚Äînetwork routers would have to maintain forwarding table entries for 
potentially billions of mobile devices, and update a device‚Äôs entry each time it roams 
to a different network. Clearly, this approach would not work in practice. Some addi-
tional drawbacks are explored in the problems at the end of this chapter.
An alternative, more practical, approach (and one that has been adopted in prac-
tice) is to push mobility functionality from the network core to the network edge‚Äî
a recurring theme in our study of Internet architecture. A natural way to do this 
is via the mobile device‚Äôs home network. In much the same way that parents of 
the mobile 20-something adult track their child‚Äôs location, a mobility management 
entity (MME) in the mobile device‚Äôs home network could track the visited network 
in which the mobile device resides. This information might reside in a database, 
shown as the HSS database in Figure 7.25. A protocol operating between the visited 
network and the home network will be needed to update the network in which the 
mobile device resides. You might recall that we encountered the MME and HSS 
elements in our study of 4G LTE. We‚Äôll reuse their element names here, since they 
are so descriptive, and also because they are pervasively deployed in 4G networks.
Let‚Äôs next consider the visited network elements shown in Figure 7.25 in more 
detail. The mobile device will clearly need an IP address in the visited network. 
The possibilities here include using a permanent address associated with the mobile 
device‚Äôs home network, allocating a new address in the address range of the visited 
network, or providing an IP address via NAT (see Section 4.3.4). In the latter two 
cases, a mobile device has a transient identifier (a newly allocated IP address) in 
addition to its permanent identifiers stored in the HSS in its home network. These 
cases are analogous to a writer addressing a letter to the address of the house in which 
our mobile 20-something adult is currently living. In the case of a NAT address, 
datagrams destined to the mobile device would eventually reach the NAT gateway 
router in the visited network, which would then perform NAT address translation and 
forward the datagram to the mobile device.

We have now seen a number of elements of a solution to the correspondent‚Äôs 
dilemma in Figure 7.24: home and visited networks, the MME and HSS, and mobile 
device addressing. But how should datagrams be addressed and forwarded to the 
mobile device? Since only the HSS (and not network-wide routers) knows the loca-
tion of the mobile device, the correspondent cannot simply address a datagram to the 
mobile device‚Äôs permanent address and send it into the network. Something more 
must be done. Two approaches can be identified: indirect and direct routing.
Indirect Routing to a Mobile Device
Let‚Äôs again consider the correspondent that wants to send a datagram to a mobile 
device. In the indirect routing approach, the correspondent simply addresses the 
datagram to the mobile device‚Äôs permanent address and sends the datagram into 
the network, blissfully unaware of whether the mobile device is resident in its home 
network or in a visited network; mobility is thus completely transparent to the cor-
respondent. Such datagrams are first routed, as usual, to the mobile device‚Äôs home 
network. This is illustrated in step 1 in Figure 7.26.
Home
network
gateway
Visited Network
79.129/16
Home Network
128.119/16
Visited network
gateway
Mobility
manager
Home
Subscriber
Service
Correspondent
Mobility
manager
NAT IP:
10.0.0.99
IMSI
78:4f:43:98:d9:27
Permanent IP:
128.119.40.186
IMSI
78:4f:43:98:d9:27
3
4b
1
2
4a
Figure 7.26 ‚ô¶ Indirect routing to a mobile device

Let‚Äôs now turn our attention to the HSS, which is responsible for interacting with 
visited networks to track the mobile device‚Äôs location, and the home network‚Äôs gate-
way router. One job of this gateway router is to be on the lookout for an arriving data-
gram addressed to a device whose home is in that network, but that currently resides 
in a visited network. The home network gateway intercepts this datagram, consults 
with the HSS to determine the visited network where the mobile device is resident, 
and forwards the datagram toward the visited network gateway router‚Äîstep 2 in Fig-
ure 7.26. The visited network gateway router then forwards the datagram toward the 
mobile device‚Äîstep 3 in Figure 7.26. If NAT translation is used, as in Figure 7.26, 
the visited network gateway router performs NAT translation.
It is instructive to consider the rerouting at the home network in bit more detail. 
Clearly, the home network gateway will need to forward the arriving datagram to 
the gateway router in the visited network. On the other hand, it is desirable to leave 
the correspondent‚Äôs datagram intact, since the application receiving the datagram 
should be unaware that the datagram was forwarded via the home network. Both 
goals can be satisfied by having the home gateway encapsulate the correspondent‚Äôs 
original complete datagram within a new (larger) datagram. This larger datagram 
is then addressed and delivered to the visited network‚Äôs gateway router, which will 
decapsulate the datagram‚Äîthat is, remove the correspondent‚Äôs original datagram 
from within the larger encapsulating datagram‚Äîand forward (step 3 in Figure 7.26) 
the original datagram to the mobile device. The sharp reader will note that the encap-
sulation/decapsulation described here is precisely the notion of tunneling, discussed 
in Section 4.3 in the context of IPv6; indeed, we also discussed the use of tunneling 
in the context of Figure 7.18, when we introduced the 4G LTE data plane.
Finally, let‚Äôs consider how the mobile device sends datagrams to the corre-
spondent. In the context of Figure 7.26, the mobile device will clearly need to for-
ward the datagram through the visited gateway router, in order to perform NAT 
translation. But how then should the visited gateway router forward the datagram 
to the correspondent? As shown in Figure 7.26, there are two options here: (4a) the 
datagram could be tunneled back to the home gateway router, and sent to the corre-
spondent from there, or (4b) the datagram could be transmitted from the visited net-
work directly to the correspondent‚Äîan approach known as local breakout [GSMA 
2019a] in LTE. 
Let‚Äôs summarize our discussion of indirect routing by reviewing the new net-
work-layer functionality required to support mobility.
‚Ä¢ A mobile-device‚Äìto‚Äìvisited-network association protocol. The mobile device will 
need to associate with the visited network, and will similarly need to disassociate 
when leaving the visited network.
‚Ä¢ A visited-network‚Äìto‚Äìhome-network-HSS registration protocol. The visited net-
work will need to register the mobile device‚Äôs location with the HSS in the home 
network, and perhaps use information obtained from the HSS in performing 
device authentication.

‚Ä¢ A datagram tunneling protocol between in the home network gateway and the 
visited network gateway router. The sending side performs encapsulation and 
forwarding of the correspondent‚Äôs original datagram; on the receiving side, the 
gateway router performs decapsulation, NAT translation, and forwarding of the 
original datagram to the mobile device.
The previous discussion provides all the needed elements for a mobile device to 
maintain an ongoing connection with a correspondent as the device moves among 
networks. When a device roams from one visited network to another, the new visited 
network information needs to be updated in the home network HSS, and the home-
gateway-router-to-visited-gateway-router tunnel endpoint needs to be moved. But will 
the mobile device see an interrupted flow of datagrams as it moves between networks? 
As long as the time between the mobile device disconnection from one visited network 
and its attachment to the next visited network is small, few datagrams will be lost. 
Recall from Chapter 3 that end-to-end connections can experience datagram loss due 
to network congestion. Hence, occasional datagram loss within a connection when a 
device moves between networks is by no means a catastrophic problem. If loss-free 
communication is required, upper-layer mechanisms will recover from datagram loss, 
whether such loss results from network congestion or from device mobility.
Our discussion above has been purposefully somewhat generic. An indirect 
routing approach is used in the mobile IP standard [RFC 5944], as well as in 4G 
LTE networks [Sauter 2014]. Their details, in particular the tunneling procedures 
employed, differ just a bit from our generic discussion above.
Direct Routing to a Mobile Device
The indirect routing approach illustrated in Figure 7.26 suffers from an inefficiency 
known as the triangle routing problem‚Äîdatagrams addressed to the mobile device 
must be forwarded first to the home network and then to the visited network, even 
when a much more efficient route exists between the correspondent and the roaming 
mobile device. In the worst case, imagine a mobile user who is roaming on the same 
network that is the home network for an overseas colleague who our mobile user is 
visiting. The two are sitting side-by-side and exchanging data. Datagrams between 
the mobile user and his overseas colleague will be forwarded to the mobile user‚Äôs 
home network and then back again to the visited network!
Direct routing overcomes the inefficiency of triangle routing, but does so at the 
cost of additional complexity. In the direct routing approach, shown in Figure 7.27, 
the correspondent first discovers the visited network in which the mobile is resident. 
This is done by querying the HSS in the mobile device‚Äôs home network, assuming 
(as in the case of indirect routing) that the mobile device‚Äôs visited network is regis-
tered in the HSS. This is shown as steps 1 and 2 in Figure 7.27. The correspondent 
then tunnels datagrams from its network directly to the gateway router in the mobile 
device‚Äôs visited network.

While direct routing overcomes the triangle routing problem, it introduces two 
important additional challenges:
‚Ä¢ A mobile-user location protocol is needed for the correspondent to query the HSS 
to obtain the mobile device‚Äôs visited network (steps 1 and 2 in Figure 7.27). This 
is in addition to the protocol needed for the mobile device to register its location 
with its HSS.
‚Ä¢ When the mobile device moves from one visited network to another, how will the 
correspondent know to now forward datagrams to the new visited network? In the 
case of indirect routing, this problem was easily solved by updating the HSS in 
the home network, and changing the tunnel endpoint to terminate at the gateway 
router of the new visited network. However, with direct routing, this change in vis-
ited networks is not so easily handled, as the HSS is queried by the correspondent 
only at the beginning of the session. Thus, additional protocol mechanisms would 
be required to proactively update the correspondent each time the mobile device 
moves. Two problems at the end of this chapter explore solutions to this problem.
Home
network
gateway
Visited Network
79.129/16
Home Network
128.119/16
Visited network
gateway
Home
Subscriber
Service
Correspondent
Mobility
manager
NAT IP:
10.0.0.99
IMSI
78:4f:43:98:d9:27
Permanent IP:
128.119.40.186
IMSI
78:4f:43:98:d9:27
3
2
Mobility
manager
4
1
Figure 7.27 ‚ô¶ Direct routing to a mobile device

7.6 Mobility Management in Practice
In the previous section, we identified key fundamental challenges and potential solu-
tions in developing a network architecture to support device mobility: the notions of 
home and visited networks; the home network‚Äôs role as a central point of informa-
tion and control for mobile devices subscribed to that home network; control-plane 
functions needed by a home network‚Äôs mobility management entity to track a mobile 
device roaming among visited networks; and data-plane approaches of direct and 
indirect routing to enable a correspondent and a mobile device to exchange data-
grams. Let‚Äôs now look at how these principles are put into practice! In Section 7.2.1, 
we‚Äôll study mobility management in 4G/5G networks; in Section 7.2.1, we‚Äôll look at 
Mobile IP, which has been proposed for the Internet.
7.6.1 Mobility Management in 4G/5G Networks
Our earlier study of 4G and emerging 5G architectures in Section 7.4 acquainted 
us with all of the network elements that play a central role in 4G/5G mobility 
management. Let‚Äôs now illustrate how those elements interoperate with each other 
to provide mobility services in today‚Äôs 4G/5G networks [Sauter 2014; GSMA 
2019b], which have their roots in earlier 3G cellular voice and data networks 
[Sauter 2014], and even earlier 2G voice-only networks [Mouly 1992]. This will 
help us synthesize what we‚Äôve learned so far, allow us to introduce a few more 
advanced topics as well, and provide a lens into what might be in store for 5G 
mobility management.
Let‚Äôs consider a simple scenario in which a mobile user (e.g., a passenger in a 
car), with a smartphone attaches to a visited 4G/5G network, begins streaming a HD 
video from a remote server, and then moves from the cell coverage of one 4G/5G 
base station to another. The four major steps in this scenario are shown in Figure 7.28:
 1. Mobile device and base station association. The mobile device associates with 
a base station in the visited network.
 2. Control-plane configuration of network elements for the mobile device. The 
visited and home networks establish control-plane state indicating that the 
mobile device is resident in the visited network.
 3. Data-plane configuration of forwarding tunnels for the mobile device. The vis-
ited network and the home network establish tunnels through which the mobile 
device and streaming server can send/receive IP datagrams, using indirect rout-
ing through the home network‚Äôs Packet Data Network gateway (P-GW).
 4. Mobile device handover from one base station to another. The mobile device 
changes its point of attachment to the visited network, via handover from one 
base station to another.
Let‚Äôs now consider each of these four steps in more detail.

1. Base station association. Recall that in Section 7.4.2, we studied the procedures 
by which a mobile device associates with a base station. We learned that the mobile 
device listens on all frequencies for primary signals being transmitted by base sta-
tions in its area. The mobile device acquires progressively more information about 
these base stations, ultimately selecting the base station with which to associate, 
and bootstrapping a control- signaling channel with that base station. As part of this 
association, the mobile device provides the base station with its International Mobile 
Subscriber Identity (IMSI), which uniquely identifies the mobile device as well as its 
home network and other additional subscriber information.
2. Control-plane configuration of LTE network elements for the mobile device.
Once the mobile-device-to-base-station signaling channel has been established, the 
base station can contact the MME in the visited network. The MME will consult and 
configure a number of 4G/5G elements in both the home and visited networks to 
establish state on behalf of the mobile node:
‚Ä¢ The MME will use to the IMSI and other information provided by the mobile 
device to retrieve authentication, encryption, and available network service infor-
mation for that subscriber. That information might be in the MME‚Äôs local cache, 
retrieved from another MME that the mobile device had recently contacted, or 
retrieved from the HSS in the mobile device‚Äôs home network. The mutual authen-
tication process (which we will cover in more detail in Section 8.8) ensures that 
Visited Network
PDN gateway
(P-GW)
Streaming
server
Home Network
Internet
PDN gateway
(P-GW)
Serving
gateway (S-GW)
Base
station
HSS
MME
Base
station
2
3
1
4
Figure 7.28 ‚ô¶ An example 4G/5G mobility scenario

the visited network is sure about the identity of the mobile device and that the 
device can authenticate the network to which it is attaching.
‚Ä¢ The MME informs the HSS in the mobile device‚Äôs home network that the mobile 
device is now resident in the visited network, and the HSS updates its database.
‚Ä¢ The base station and the mobile device select parameters for the data-plane chan-
nel to be established between the mobile device and the base station (recall that a 
control plane signaling channel is already in operation).
3. Data-plane configuration of forwarding tunnels for the mobile device. 
The MME next configures the data plane for the mobile device, as shown in  
Figure 7.29. Two tunnels are established. One tunnel is between the base station  
and a Serving Gateway in the visited network. The second tunnel is between that 
Serving Gateway and the PDN Gateway router in the mobile device‚Äôs home network. 
4G LTE implements this form of symmetric indirect routing‚Äîall traffic to/from the 
mobile device will be tunneled through the device‚Äôs home network. 4G/5G tunnels 
use the GPRS Tunneling Protocol (GTP), specified in [3GPP GTPv1-U 2019]. The 
Tunnel Endpoint ID (TEID) in the GTP header indicates which tunnel a datagram 
belongs, allowing multiple flows to be multiplexed and de-multiplexed by GTP 
between tunnel endpoints.
It is instructive to compare the configuration of tunnels in Figure 7.29 (the case 
of mobile roaming in a visited network) with that of Figure 7.18 (the case of mobility 
Visited Network
PDN gateway
(P-GW)
Streaming
server
Home Network
Internet
PDN gateway
(P-GW)
Serving
gateway (S-GW)
Base
station
MME
Base
station
Figure 7.29 ‚ô¶  Tunneling in 4G/5G networks between the Serving  
Gateway in the visited  network and the PDN gateway  
in the home network

only within the mobile device‚Äôs home network). We see that in both cases, the Serv-
ing Gateway is co-resident in the same network as the mobile device, but PDN Gate-
way (which is always the PDN Gateway in the mobile device‚Äôs home network) may 
be in a different network than the mobile device. This is precisely indirect routing. 
An alternative to indirect routing, known as local breakout [GSMA 2019a] has been 
specified in which the Serving Gateway establishes a tunnel to the PDN Gateway in 
the local, visited network. In practice, however, local breakout is not widely used 
[Sauter 2014].
 Once the tunnels have been configured and activated, the mobile device 
can now forward packets to/from the Internet via the PDN gateway in its home 
network!
4. Handover management. A handover occurs when a mobile device changes 
its association from one base station to another. The handover process described 
below is the same, regardless of whether the mobile device is resident in its home 
network, or is roaming in a visited network.
As shown in Figure 7.30, datagrams to/from the device are initially (before 
handover) forwarded to the mobile through one base station (which we‚Äôll refer to as 
the source base station), and after handover are routed to the mobile device through 
another base station (which we‚Äôll refer to as the target base station). As we will see, 
a handover between base stations results not only in the mobile device transmitting/
receiving to/from a new base station but also in a change of the base-station side of 
the Serving-Gateway-to-base-station tunnel in Figure 7.29. In the simplest case of 
PDN gateway
(P-GW)
Serving
gateway (S-GW)
MME
Source base
station
Target base
station
5
5
7
3
1
2
4
6
Figure 7.30 ‚ô¶  Steps in handing over a mobile device from the source 
base station to the target base station

handover, when the two base stations are near each other and in the same network, 
all changes occurring as a result of handover are thus relatively local. In particular, 
the PDN gateway being used by the Serving Gateway remains blissfully unaware of 
device mobility. Of course, more complicated handoff scenarios will require the use 
of more complex mechanisms [Sauter 2014; GSMA 2019a].
There may be several reasons for handover to occur. For example, the signal 
between the current base station and the mobile may have deteriorated to such an 
extent that communication is severely impaired. Or a cell may have become over-
loaded, handling a large amount of traffic; handing over mobile devices to less 
congested nearby cells may alleviate this congestion. A mobile device periodically 
measures characteristics of a beacon signal from its current base station as well as 
signals from nearby base stations that it can ‚Äúhear.‚Äù These measurements are reported 
once or twice a second to the mobile device‚Äôs current (source) base station. Based on 
these measurements, the current loads of mobiles in nearby cells, and other factors, 
the source base station may choose to initiate a handover. The 4G/5G standards do 
not specify a specific algorithm to be used by a base station to determine whether or 
not to perform handover, or which target base station to choose; this is an active area 
of research [Zheng 2008; Alexandris 2016].
Figure 7.30 illustrates the steps involved when a source base station decides to 
hand over a mobile device to the target base station.
 1. The current (source) base station selects the target base station, and sends a 
Handover Request message to the target base station.
 2. The target base station checks whether it has the resources to support the mobile 
device and its quality of service requirements. If so, it pre-allocates channel 
resources (e.g., time slots) on its radio access network and other resources 
for that device. This pre-allocation of resources frees the mobile device from 
having to go through the time-consuming base-station association protocol 
discussed earlier, allowing handover to be executed as fast as possible. The 
target base station replies to the source base station with a Handover Request 
Acknowledge message, containing all the information at the target base station 
that the mobile device will need to associate with the new base station.
 3. The source base station receives the Handover Request Acknowledgement 
message and informs the mobile device of the target base station‚Äôs identity and 
channel access information. At this point, the mobile device can begin send-
ing/receiving datagrams to/from the new target base station. From the mobile 
device‚Äôs point of view, handover is now complete! However, there is still a bit 
of work to be done within the network.
 4. The source base station will also stop forwarding datagrams to the mobile 
device and instead forward any tunneled datagrams it receives to the target 
base station, which will later forward these datagrams to the mobile device.
 5. The target base station informs the MME that it (the target base station) will be 
the new base station servicing the mobile device. The MME, in turn, signals

to the Serving Gateway and the target base station to reconfigure the Serving-
Gateway-to-base-station tunnel to terminate at the target base station, rather 
than at the source base station.
 6. The target base station confirms back to the source base station that the tunnel 
has been reconfigured, allowing the source base station to release resources 
associated with that mobile device.
 7. At this point, the target base station can also begin delivering datagrams to the 
mobile device, including datagrams forwarded to the target base station by the 
source base station during handover, as well as datagrams newly arriving on 
the reconfigured tunnel from the Serving Gateway. It can also forward outgo-
ing datagrams received from the mobile device into the tunnel to the Serving 
Gateway.
The roaming configurations in today‚Äôs 4G LTE networks, such as that dis-
cussed above, will also be used in future emerging 5G networks [GSMA 2019c]. 
Recall, however, from our discussion in Section 7.4.6 that the 5G networks will be 
denser, with significantly smaller cell sizes. This will make handover an even more 
critically important network function. In addition, low handover latency will be 
critical for many real-time 5G applications. The migration of the cellular network 
control plane to the SDN framework that we studied earlier in Chapter 5 [GSMA 
2018b; Condoluci 2018] promises to enable implementations of a higher-capacity, 
lower-latency 5G cellular network control plane. The application of SDN in a 5G 
context is the subject of considerable research [Giust 2015; Ordonez-Lucena 2017; 
Nguyen 2016].
7.6.2 Mobile IP
Today‚Äôs Internet does not have any widely deployed infrastructure that provides the 
type of services for ‚Äúon the go‚Äù mobile users that we encountered for 4G/5G cellular 
networks. But this is certainly not due to the lack of technical solutions for providing 
such services in an Internet setting! Indeed, the Mobile IP architecture and protocols 
[RFC 5944] that we will briefly discuss below have been standardized by Internet 
RFCs for more than 20 years, and research has continued on new, more secure and 
more generalized mobility solutions [Venkataramani 2014].
Instead, it has perhaps been the lack of motivating business and use cases [Arkko 
2012] and the timely development and deployment of alternative mobility solutions 
in cellular networks that has blunted the deployment of Mobile IP. Recall that 20 
years ago, 2G cellular networks had already provided a solution for mobile voice 
services (the ‚Äúkiller app‚Äù for mobile users); additionally, next generation 3G net-
works supporting voice and data were on the horizon. Perhaps the dual technology 
solution‚Äîmobile services via cellular networks when we are truly mobile and ‚Äúon 
the go‚Äù (i.e., the rightmost side of the mobility spectrum in Figure 7.24) and Internet 
services via 802.11 networks or wireline networks when we are stationary or moving

locally (i.e., the leftmost side of the mobility spectrum in Figure 7.24)‚Äîthat we had 
20 years ago and still have today will persist into the future.
It will nonetheless be instructive to briefly overview the Mobile IP standard 
here, as it provides many of the same services as cellular networks and implements 
many of the same basic mobility principles. Earlier editions of this textbook have 
provided a more in-depth study of Mobile IP than we will provide here; the inter-
ested reader can find this retired material on this textbook‚Äôs website. The Internet 
architecture and protocols for supporting mobility, collectively known as Mobile IP, 
are defined primarily in RFC 5944 for IPv4. Mobile IP, like 4G/5G, is a complex 
standard, and would require an entire book to describe in detail; indeed one such 
book is [Perkins 1998b]. Our modest goal here is to provide an overview of the most 
important aspects of Mobile IP.
The overall architecture and elements of Mobile IP are strikingly similar to that 
of cellular provider networks. There is a strong notion of a home network, in which a 
mobile device has a permanent IP address, and visited networks (known as ‚Äúforeign‚Äù 
networks in Mobile IP), where the mobile device will be allocated a care-of-address. 
The home agent in Mobile IP has a similar function to the LTE HSS: it tracks the 
location of a mobile device by receiving updates from foreign agents in foreign net-
works visited by that mobile device, just as the HSS receives updates from Mobil-
ity Management Entities (MMEs) in visited networks in which a 4G mobile device 
resides. And both 4G/5G and Mobile IP use indirect routing to a mobile node, using 
tunnels to connect the gateway routers in the home and visited/foreign networks. 
Table 7.3 summarizes the elements of the Mobile IP architecture, along with a com-
parison with similar elements in 4G/5G networks
4G/5G element
Mobile IP element
Discussion
Home network
Home network
Visited network
Foreign network
IMSI identifier
Permanent IP address
Globally unique routable address information
Home Subscriber Service (HSS)
Home agent
Mobility Management Entity (MME)
Foreign agent
Data plane: indirect forwarding via the home 
network, with tunneling between the home 
and visited network, and tunneling within the 
network in which the mobile device resides
Data plane: indirect forwarding via 
the home network, with tunneling 
between the home and visited 
network
Base station (eNode-B)
Access Point (AP)
No specific AP technology is specified in Mobile IP
Radio Access Network
WLAN
No specific WLAN technology is specified in Mobile IP
Table 7.3 ‚ô¶ Commonalities between 4G/5G and Mobile IP architectures

The mobile IP standard consists of three main pieces:
‚Ä¢ Agent discovery. Mobile IP defines the protocols used by a foreign agent to adver-
tise its mobility services to a mobile device that wishes to attach to its network. 
Those services will include providing a care-of-address to the mobile device for 
use in the foreign network, registration of the mobile device with the home agent 
in the mobile device‚Äôs home network, and forwarding of datagrams to/from the 
mobile device, among other services.
‚Ä¢ Registration with the home agent. Mobile IP defines the protocols used by the 
mobile device and/or foreign agent to register and deregister a care-of-address 
with a mobile device‚Äôs home agent.
‚Ä¢ Indirect routing of datagrams. Mobile IP also defines the manner in which data-
grams are forwarded to mobile devices by a home agent, including rules for for-
warding datagrams and handling error conditions, and several forms of tunneling 
[RFC 2003, RFC 2004].
Again, our coverage here of Mobile IP has been intentionally brief. The inter-
ested reader should consult the references in this section, or more-detailed discus-
sions of Mobile IP in earlier editions of this textbook.
7.7 Wireless and Mobility: Impact on  
 Higher-Layer Protocols
In this chapter, we‚Äôve seen that wireless networks differ significantly from their 
wired counterparts at both the link layer (as a result of wireless channel charac-
teristics such as fading, multipath, and hidden terminals) and at the network layer  
(as a result of mobile users who change their points of attachment to the network). 
But are there important differences at the transport and application layers? It‚Äôs tempt-
ing to think that these differences will be minor, since the network layer provides the 
same best-effort delivery service model to upper layers in both wired and wireless 
networks. Similarly, if protocols such as TCP or UDP are used to provide transport-
layer services to applications in both wired and wireless networks, then the applica-
tion layer should remain unchanged as well. In one sense, our intuition is right‚ÄîTCP 
and UDP can (and do) operate in networks with wireless links. On the other hand, 
transport protocols in general, and TCP in particular, can sometimes have very dif-
ferent performance in wired and wireless networks, and it is here, in terms of perfor-
mance, that differences are manifested. Let‚Äôs see why.
Recall that TCP retransmits a segment that is either lost or corrupted on the path 
between sender and receiver. In the case of mobile users, loss can result from either 
network congestion (router buffer overflow) or from handover (e.g., from delays 
in rerouting segments to a mobile‚Äôs new point of attachment to the network). In all

cases, TCP‚Äôs receiver-to-sender ACK indicates only that a segment was not received 
intact; the sender is unaware of whether the segment was lost due to congestion, 
during handover, or due to detected bit errors. In all cases, the sender‚Äôs response is 
the same‚Äîto retransmit the segment. TCP‚Äôs congestion-control response is also the 
same in all cases‚ÄîTCP decreases its congestion window, as discussed in Section 3.7. 
By unconditionally decreasing its congestion window, TCP implicitly assumes that 
segment loss results from congestion rather than corruption or handover. We saw in 
Section 7.2 that bit errors are much more common in wireless networks than in wired 
networks. When such bit errors occur or when handover loss occurs, there‚Äôs really 
no reason for the TCP sender to decrease its congestion window (and thus decrease 
its sending rate). Indeed, it may well be the case that router buffers are empty and 
packets are flowing along the end-to-end path unimpeded by congestion.
Researchers realized in the early to mid 1990s that given high bit error rates on 
wireless links and the possibility of handover loss, TCP‚Äôs congestion-control response 
could be problematic in a wireless setting. Three broad classes of approaches are 
possible for dealing with this problem:
‚Ä¢ Local recovery. Local recovery protocols recover from bit errors when and where 
(e.g., at the wireless link) they occur, for example, the 802.11 ARQ protocol we 
studied in Section 7.3, or more sophisticated approaches that use both ARQ and 
FEC [Ayanoglu 1995] that we saw in use in 4G/5G networks in Section 7.4.2.
‚Ä¢ TCP sender awareness of wireless links. In the local recovery approaches, the 
TCP sender is blissfully unaware that its segments are traversing a wireless link. 
An alternative approach is for the TCP sender and receiver to be aware of the 
existence of a wireless link, to distinguish between congestive losses occurring 
in the wired network and corruption/loss occurring at the wireless link, and to 
invoke congestion control only in response to congestive wired-network losses. 
[Liu 2003] investigates techniques for distinguishing between losses on the wired 
and wireless segments of an end-to-end path. [Huang 2013] provides insights on 
developing transport protocol mechanisms and applications that are more LTE-
friendly.
‚Ä¢ Split-connection approaches. In a split-connection approach [Bakre 1995], the 
end-to-end connection between the mobile user and the other end point is broken 
into two transport-layer connections: one from the mobile host to the wireless 
access point, and one from the wireless access point to the other communication  
end point (which we‚Äôll assume here is a wired host). The end-to-end connection 
is¬†thus formed by the concatenation of a wireless part and a wired part. The trans-
port layer over the wireless segment can be a standard TCP connection [Bakre 
1995], or a specially tailored error recovery protocol on top of UDP. [Yavatkar 
1994] investigates the use of a transport-layer selective repeat protocol over the 
wireless connection. Measurements reported in [Wei 2006] indicate that split TCP 
connections have been widely used in cellular data networks, and that significant 
improvements can indeed be made through the use of split TCP connections.

Our treatment of TCP over wireless links has been necessarily brief here. 
 In-depth surveys of TCP challenges and solutions in wireless networks can be found 
in [Hanabali 2005; Leung 2006]. We encourage you to consult the references for 
details of this ongoing area of research.
Having considered transport-layer protocols, let us next consider the effect of 
wireless and mobility on application-layer protocols. Because of the shared nature of 
the wireless spectrum, applications that operate over wireless links, particularly over 
cellular wireless links, must treat bandwidth as a scarce commodity. For example, a 
Web server serving content to a Web browser executing on a 4G smartphone will 
likely not be able to provide the same image-rich content that it gives to a browser 
operating over a wired connection. Although wireless links do provide challenges at 
the application layer, the mobility they enable also makes possible a rich set of loca-
tion-aware and context-aware applications [Baldauf 2007]. More generally, wireless 
and mobile networks will continue to play a key role in realizing the ubiquitous com-
puting environments of the future [Weiser 1991]. It‚Äôs fair to say that we‚Äôve only seen 
the tip of the iceberg when it comes to the impact of wireless and mobile networks on 
networked applications and their protocols!
7.8 Summary
Wireless and mobile networks first revolutionized telephony and are now having 
an increasingly profound impact in the world of computer networks as well. With 
their anytime, anywhere, untethered access into the global network infrastructure, 
they are not only making network access more ubiquitous, they are also enabling an 
exciting new set of location-dependent services. Given the growing importance of 
wireless and mobile networks, this chapter has focused on the principles, common 
link technologies, and network architectures for supporting wireless and mobile 
communication.
We began this chapter with an introduction to wireless and mobile networks, 
drawing an important distinction between the challenges posed by the wireless nature 
of the communication links in such networks, and by the mobility that these wireless 
links enable. This allowed us to better isolate, identify, and master the key concepts in 
each area. We focused first on wireless communication, considering the characteristics 
of a wireless link in Section 7.2. In Sections 7.3 and 7.4, we examined the link-level 
aspects of the IEEE 802.11 (WiFi) wireless LAN standard, Bluetooth, and 4G/5G  
cellular neworks. We then turned our attention to the issue of mobility. In Section 7.5, 
we identified several forms of mobility, with points along this spectrum posing dif-
ferent challenges and admitting different solutions. We considered the problems of 
locating and routing to a mobile user, as well as approaches for handing over the 
mobile user who dynamically moves from one point of attachment to the network to 
another. We examined how these issues were addressed in 4G/5G networks and in the

Mobile IP standard. Finally, we considered the impact of wireless links and mobility 
on transport-layer protocols and networked applications in  Section 7.7.
Although we have devoted an entire chapter to the study of wireless and mobile 
networks, an entire book (or more) would be required to fully explore this exciting 
and rapidly expanding field. We encourage you to delve more deeply into this field 
by consulting the many references provided in this chapter.
Homework Problems and Questions
Chapter 7 Review Questions
SECTION 7.1
  R1. What does it mean for a wireless network to be operating in ‚Äúinfrastructure 
mode‚Äù? If the network is not in infrastructure mode, what mode of operation 
is it in, and what is the difference between that mode of operation and infra-
structure mode?
  R2. What are the four types of wireless networks identified in our taxonomy in 
Section 7.1? Which of these types of wireless networks have you used?
SECTION 7.2
  R3. What are the differences between the following types of wireless channel 
impairments: path loss, multipath propagation, interference from other sources?
  R4. As a mobile node gets farther and farther away from a base station, what are 
two actions that a base station could take to ensure that the loss probability of 
a transmitted frame does not increase?
SECTION 7.3
  R5. Describe the role of the beacon frames in 802.11.
  R6. True or false: Before an 802.11 station transmits a data frame, it must first 
send an RTS frame and receive a corresponding CTS frame.
  R7. Why are acknowledgments used in 802.11 but not in wired Ethernet?
  R8. True or false: Ethernet and 802.11 use the same frame structure.
  R9. Describe how the RTS threshold works.
 R10. Suppose the IEEE 802.11 RTS and CTS frames were as long as the standard 
DATA and ACK frames. Would there be any advantage to using the CTS and 
RTS frames? Why or why not?
 R11. Section 7.3.4 discusses 802.11 mobility, in which a wireless station moves 
from one BSS to another within the same subnet. When the APs are intercon-
nected with a switch, an AP may need to send a frame with a spoofed MAC 
address to get the switch to forward the frame properly. Why?

R12. What are the differences between a master device in a Bluetooth network and 
a base station in an 802.11 network?
 R13. What is the role of the base station in 4G/5G cellular architecture? With 
which other 4G/5G network elements (mobile device, MME, HSS, Serving 
Gateway Router, PDN Gateway Router) does it directly communicate with in 
the control plane? In the data plane?
 R14. What is an International Mobile Subscriber Identity (IMSI)?
 R15. What is the role of the Home Subscriber Service (HSS) in 4G/5G cellular 
architecture? With which other 4G/5G network elements (mobile device, 
base station, MME, Serving Gateway Router, PDN Gateway Router) does it 
directly communicate with in the control plane? In the data plane?
 R16. What is the role of the Mobility Management Entity (MME) in 4G/5G 
cellular architecture? With which other 4G/5G network elements (mobile 
device, base station, HSS, Serving Gateway Router, PDN Gateway 
Router) does it directly communicate with in the control plane? In the 
data plane?
 R17. Describe the purpose of two tunnels in the data plane of the 4G/5G cellular 
architecture. When a mobile device is attached to its own home network, at 
which 4G/5G network element (mobile device, base station, HSS, MME, 
Serving Gateway Router, PDN Gateway Router) does each end of each of the 
two tunnels terminate?
 R18. What are the three sublayers in the link layer in the LTE protocol stack? 
Briefly describe their functions.
 R19. Does the LTE wireless access network use FDMA, TDMA, or both? Explain 
your answer.
 R20. Describe the two possible sleep modes of a 4G/5G mobile device. In each of 
these sleep modes, will the mobile device remain associated with the same 
base station between the time it goes to sleep and the time it wakes up and 
first sends/receives a new datagram?
 R21. What is meant by a ‚Äúvisited network‚Äù and a ‚Äúhome network‚Äù in 4G/5G cel-
lular architecture?
 R22. List three important differences between 4G and 5G cellular networks.
SECTION 7.5
 R23. What does it mean that a mobile device is said to be ‚Äúroaming?‚Äù
 R24. What is meant by ‚Äúhand over‚Äù of a network device?
 R25. What is the difference between direct and indirect routing of datagrams to/
from a roaming mobile host?
 R26. What does ‚Äútriangle routing‚Äù mean?

SECTION 7.6
 R27. Describe the similarity and differences in tunnel configuration when a mobile 
device is resident in its home network, versus when it is roaming in a visited 
network.
R 28. When a mobile device is handed over from one base station to another in a 
4G/5G network, which network element makes the decision to initiate that 
handover? Which network element chooses the target base station to which 
the mobile device will be handed over?
R 29. Describe how and when the forwarding path of datagrams entering the visited net-
work and destined to the mobile device changes before, during, and after hand over.
R 30. Consider the following elements of the Mobile IP architecture: the home net-
work, foreign network permanent IP address, home agent, foreign agent, data 
plane forwarding, Access Point (AP), and WLANs at the network edge. What 
are the closest equivalent elements in the 4G/5G cellular network architecture?
SECTION 7.7
 R31. What are three approaches that can be used to avoid having a single wireless 
link degrade the performance of an end-to-end transport-layer TCP connection?
Problems
 P1. Consider the single-sender CDMA example in Figure 7.5. What would be the 
sender‚Äôs output (for the 2 data bits shown) if the sender‚Äôs CDMA code were 
(1, -1, 1, -1, 1, 1, 1, -1)?
 P2. Consider sender 2 in Figure 7.6. What is the sender‚Äôs output to the channel 
(before it is added to the signal from sender 1), Z2 i,m?
 P3. Suppose that the receiver in Figure 7.6 wanted to receive the data being sent 
by sender 2. Show (by calculation) that the receiver is indeed able to recover 
sender 2‚Äôs data from the aggregate channel signal by using sender 2‚Äôs code.
 P4. For the two-sender, two-receiver example, give an example of two CDMA 
codes containing 1 and 21 values that do not allow the two receivers to 
extract the original transmitted bits from the two CDMA senders.
 P5. Suppose there are two ISPs providing WiFi access in a particular caf√©, with 
each ISP operating its own AP and having its own IP address block.
a. Further suppose that by accident, each ISP has configured its AP to oper-
ate over channel 11. Will the 802.11 protocol completely break down in 
this situation? Discuss what happens when two stations, each associated 
with a different ISP, attempt to transmit at the same time.
b. Now suppose that one AP operates over channel 1 and the other over 
channel 11. How do your answers change?

P6. In step 4 of the CSMA/CA protocol, a station that successfully transmits a 
frame begins the CSMA/CA protocol for a second frame at step 2, rather than 
at step 1. What rationale might the designers of CSMA/CA have had in mind 
by having such a station not transmit the second frame immediately (if the 
channel is sensed idle)?
 P7. Suppose an 802.11b station is configured to always reserve the channel with 
the RTS/CTS sequence. Suppose this station suddenly wants to  transmit  
1,500 bytes of data, and all other stations are idle at this time. As a  function of 
SIFS and DIFS, and ignoring propagation delay and assuming no bit errors, cal-
culate the time required to transmit the frame and receive the acknowledgment.
 P8. Consider the scenario shown in Figure 7.31, in which there are four wireless 
nodes, A, B, C, and D. The radio coverage of the four nodes is shown via 
the shaded ovals; all nodes share the same frequency. When A transmits, it 
can only be heard/received by B; when B transmits, both A and C can hear/
receive from B; when C transmits, both B and D can hear/receive from C; 
when D transmits, only C can hear/receive from D.
 
 Suppose now that each node has an infinite supply of messages that it wants 
to send to each of the other nodes. If a message‚Äôs destination is not an imme-
diate neighbor, then the message must be relayed. For example, if A wants 
to send to D, a message from A must first be sent to B, which then sends 
the message to C, which then sends the message to D. Time is slotted, with 
a message transmission time taking exactly one time slot, e.g., as in slotted 
Aloha. During a slot, a node can do one of the following: (i) send a message, 
(ii) receive a message (if exactly one message is being sent to it), (iii) remain 
silent. As always, if a node hears two or more simultaneous transmissions, 
a collision occurs and none of the transmitted messages are received suc-
cessfully. You can assume here that there are no bit-level errors, and thus if 
exactly one message is sent, it will be received correctly by those within the 
transmission radius of the sender.
a. Suppose now that an omniscient controller (i.e., a controller that knows the 
state of every node in the network) can command each node to do whatever 
it (the omniscient controller) wishes, that is, to send a message, to receive a 
Figure 7.31 ‚ô¶ Scenario for problem P8
A
B
C
D

message, or to remain silent. Given this omniscient controller, what is the 
maximum rate at which a data message can be transferred from C to A, given 
that there are no other messages between any other source/destination pairs?
b. Suppose now that A sends messages to B, and D sends messages to C. 
What is the combined maximum rate at which data messages can flow 
from A to B and from D to C?
c. Suppose now that A sends messages to B, and C sends messages to D. 
What is the combined maximum rate at which data messages can flow 
from A to B and from C to D?
d. Suppose now that the wireless links are replaced by wired links. Repeat 
questions (a) through (c) again in this wired scenario.
e. Now suppose we are again in the wireless scenario, and that for every  
data message sent from source to destination, the destination will send an 
ACK message back to the source (e.g., as in TCP). Also suppose that each ACK 
message takes up one slot. Repeat questions (a)‚Äì(c) above for this scenario.
  P9. Describe the format of the Bluetooth frame. You will have to do some read-
ing outside of the text to find this information. Is there anything in the frame 
format that inherently limits the number of active nodes in an network to 
eight active nodes? Explain.
 P10. Consider the following idealized LTE scenario. The downstream channel  
(see Figure 7.22) is slotted in time, across F frequencies. There are four nodes, 
A, B, C, and D, reachable from the base station at rates of 10 Mbps, 5 Mbps, 
2.5 Mbps, and 1 Mbps, respectively, on the downstream channel. These rates 
assume that the base station utilizes all time slots available on all F frequen-
cies to send to just one station. The base station has an infinite amount of data 
to send to each of the nodes, and can send to any one of these four nodes using 
any of the F frequencies during any time slot in the  downstream sub-frame.
a. What is the maximum rate at which the base station can send to the nodes, 
assuming it can send to any node it chooses during each time slot? Is your 
solution fair? Explain and define what you mean by ‚Äúfair.‚Äù
b. If there is a fairness requirement that each node must receive an equal 
amount of data during each one second interval, what is the average 
transmission rate by the base station (to all nodes) during the downstream 
sub-frame? Explain how you arrived at your answer.
c. Suppose that the fairness criterion is that any node can receive at most 
twice as much data as any other node during the sub-frame. What is the 
average transmission rate by the base station (to all nodes) during the sub-
frame? Explain how you arrived at your answer.
 P11. In Section 7.5, one proposed solution that allowed mobile users to maintain 
their IP addresses as they moved among foreign networks was to have a foreign 
network advertise a highly specific route to the mobile user and use the existing

routing infrastructure to propagate this information throughout the network. We 
identified scalability as one concern. Suppose that when a mobile user moves 
from one network to another, the new foreign network advertises a specific route 
to the mobile user, and the old foreign network withdraws its route. Consider 
how routing information propagates in a distance-vector algorithm (particularly 
for the case of interdomain routing among networks that span the globe).
a. Will other routers be able to route datagrams immediately to the new for-
eign network as soon as the foreign network begins advertising its route?
b. Is it possible for different routers to believe that different foreign networks 
contain the mobile user?
c. Discuss the timescale over which other routers in the network will eventu-
ally learn the path to the mobile users.
 P12. In 4G/5G networks, what effect will handoff have on end-to-end delays of 
datagrams between the source and destination?
 P13. Consider a mobile device that powers on and attaches to an LTE visited 
network A, and assume that indirect routing to the mobile device from its 
home network H is being used. Subsequently, while roaming, the device 
moves out of range of visited network A and moves into range of an LTE 
visited network B. You will design a handover process from a base sta-
tion BS.A in visited network A to a base station BS.B in visited network B. 
Sketch the series of steps that would need to be taken, taking care to identify 
the network elements involved (and the networks to which they belong), to 
accomplish this handover. Assume that following handover, the tunnel from 
the home network to the visited network will terminate in visiting network B.
 P14. Consider again the scenario in Problem P13. But now assume that the tunnel 
from home network H to visited network A will continue to be used. That is, 
visited network A will serve as an anchor point following handover. (Aside: 
this is actually the process used for routing circuit-switched voice calls to 
a roaming mobile phone in 2G GSM networks.) In this case, additional 
tunnel(s) will need to be built to reach the mobile device in its resident visited 
network B. Once again, sketch the series of steps that would need to be taken, 
taking care to identify the network elements involved (and the networks to 
which they belong), to accomplish this handover.
 
What are one advantage and one disadvantage of this approach over the 
approach taken in your solution to Problem P13?
Wireshark Lab: WiFi
At the Web site for this textbook, www.pearsonhighered.com/cs-resources, also  
mirrored on the instructors‚Äô website, http://gaia.cs.umass.edu/kurose_ross, you‚Äôll 
find a Wireshark lab for this chapter that captures and studies the 802.11 frames 
exchanged between a wireless laptop and an access point.

Please describe a few of the most exciting projects you have worked on during your 
career. What were the biggest challenges?
In the mid-90s at USC and ISI, I had the great fortune to work with the likes of Steve 
Deering, Mark Handley, and Van Jacobson on the design of multicast routing protocols (in 
particular, PIM). I tried to carry many of the architectural design lessons from multicast 
into the design of ecological monitoring arrays, where for the first time I really began to 
take applications and multidisciplinary research seriously. The need for jointly innovating 
in the social and technological space is what interests me so much about my latest area of 
research, mobile health. The challenges in multicast routing, environmental sensing and 
AN INTERVIEW WITH‚Ä¶
Deborah Estrin
Courtesy of Deborah Estrin
Deborah Estrin is a Professor of Computer Science and Associate 
Dean for Impact at Cornell Tech in New York City and a Professor 
of Public Health at Weill Cornell Medical College. She received 
her Ph.D. (1985) in Computer Science from M.I.T. and her B.S. 
(1980) from UC Berkeley. Estrin‚Äôs early research focused on the 
design of network protocols, including multicast and inter-domain 
routing. In 2002 Estrin founded the NSF-funded Science and 
Technology Center at UCLA, Center for Embedded Networked 
Sensing (CENS http://cens.ucla.edu.). CENS launched new areas 
of multi-disciplinary computer systems research from sensor networks 
for environmental monitoring, to participatory sensing and mobile 
health. As described in her 2013 TEDMED talk, she explores how 
individuals can benefit from the pervasive data byproducts of digi-
tal and IoT interactions for health and life management. Professor 
Estrin is an elected member of the American Academy of Arts and 
Sciences (2007), the National Academy of Engineering (2009), 
and the National Academy of Medicine (2019). She is a Fellow of 
the IEEE, ACM, and AAAS. She was selected as the first ACM-W 
Athena Lecturer (2006), awarded the Anita Borg Institute‚Äôs Women 
of Vision Award for Innovation (2007), inducted into the WITI hall 
of fame (2008), received honorary doctorates from EPFL (2008) 
and¬†Uppsala University (2011), and was selected as a MacArthur 
Fellow (2018).

mobile health are as diverse as the problem domains, but what they have in common is the 
need to keep our eyes open to whether we have the problem definition right as we iterate 
between design and deployment, prototype and pilot. None of these are problems that could 
be solved solely analytically, or with simulation or even in constructed laboratory experi-
ments. They challenged our ability to retain clean architectures in the presence of messy 
problems and contexts, and they required extensive collaboration.
What changes and innovations do you see happening in wireless networks and mobility 
in the future?
In a prior edition of this interview I said that I have never put much faith into predicting the 
future, but I did go on to speculate that we might see the end of feature phones (i.e., those 
that are not programmable and are used only for voice and text messaging) as smart phones 
become more and more powerful and the primary point of Internet access for many‚Äîand 
now not so many years later that is clearly the case. I also predicted that we would see the 
continued proliferation of embedded SIMs by which all sorts of devices have the ability 
to communicate via the cellular network at low data rates. While that has occurred, we see 
many devices and ‚ÄúInternet of Things‚Äù that use embedded WiFi and other lower power, 
shorter range, forms of connectivity to local hubs. I did not anticipate at that time the emer-
gence of a large consumer wearables market or interactive voice agents like Siri and Alexa. 
By the time the next edition is published I expect broad proliferation of personal applica-
tions that leverage data from IoT and other digital traces.
Where do you see the future of networking and the Internet?
Again I think it‚Äôs useful to look both back and forward. Previously I commented that the 
efforts in named data and software-defined networking would emerge to create a more 
manageable, evolvable, and richer infrastructure and more generally represent moving the 
role of architecture higher up in the stack. In the beginnings of the Internet, architecture was 
layer 4 and below, with applications being more siloed/monolithic, sitting on top. Now data 
and analytics dominate transport. The adoption of SDN (which I was really happy to see 
introduced into the 7th edition of this book) has been well beyond what I ever anticipated. 
That said, new challenges have emerged from higher up in the stack. Machine Learning 
based systems and services favor scale, particularly when they rely on continuous consumer 
engagement (clicks) for financial viability. The resulting information ecosystem has become 
far more monolithic than in earlier decades. This is a challenge for networking, the Internet, 
and frankly our society.

What people inspired you professionally?
There are three people who come to mind. First, Dave Clark, the secret sauce and under- 
sung hero of the Internet community. I was lucky to be around in the early days to see him 
act as the ‚Äúorganizing principle‚Äù of the IAB and Internet governance; the priest of rough 
consensus and running code. Second, Scott Shenker, for his intellectual brilliance, integrity, 
and persistence. I strive for, but rarely attain, his clarity in defining problems and solutions. 
He is always the first person I e-mail for advice on matters large and small. Third, my sister 
Judy Estrin, who had the creativity and commitment to spend the first half of her career 
bringing ideas and concepts to market; and now has the courage to study, write, and advise 
on how to rebuild it to support a healthier democracy.
What are your recommendations for students who want careers in computer science  
and networking?
First, build a strong foundation in your academic work, balanced with any and every real- 
world work experience you can get. As you look for a working environment, seek opportu-
nities in problem areas you really care about and with smart teams that you can learn from 
and work with to build things that matter.



classes of Internet attacks, including malware attacks, denial of service, sniffing, 
source masquerading, and message modification and deletion. Although we have 
since learned a tremendous amount about computer networks, we still haven‚Äôt exam-
ined how to secure networks from those attacks. Equipped with our newly acquired 
expertise in computer networking and Internet protocols, we‚Äôll now study in-depth 
secure communication and, in particular, how computer networks can be defended 
from those nasty bad guys.
Let us introduce Alice and Bob, two people who want to communicate and wish 
to do so ‚Äúsecurely.‚Äù This being a networking text, we should remark that Alice and 
Bob could be two routers that want to exchange routing tables securely, a client and 
server that want to establish a secure transport connection, or two e-mail applications 
that want to exchange secure e-mail‚Äîall case studies that we will consider later in 
this chapter. Alice and Bob are well-known fixtures in the security community, per-
haps because their names are more fun than a generic entity named ‚ÄúA‚Äù that wants 
to communicate securely with a generic entity named ‚ÄúB.‚Äù Love affairs, wartime 
communication, and business transactions are the commonly cited human needs for 
secure communications; preferring the first to the latter two, we‚Äôre happy to use 
Alice and Bob as our sender and receiver, and imagine them in this first scenario.
We said that Alice and Bob want to communicate and wish to do so ‚Äúsecurely,‚Äù 
but what precisely does this mean? As we will see, security (like love) is a many-
splendored thing; that is, there are many facets to security. Certainly, Alice and 
Bob would like for the contents of their communication to remain secret from 
an eavesdropper. They probably would also like to make sure that when they are 
Security in 
Computer 
Networks
8
CHAPTER
607

communicating, they are indeed communicating with each other, and that if their 
communication is tampered with by an eavesdropper, that this tampering is detected. 
In the first part of this chapter, we‚Äôll cover the fundamental cryptography techniques 
that allow for encrypting communication, authenticating the party with whom one is 
communicating, and ensuring message integrity.
In the second part of this chapter, we‚Äôll examine how the fundamental 
 cryptography principles can be used to create secure networking protocols. Once 
again taking a top-down approach, we‚Äôll examine secure protocols in each of the 
(top four) layers, beginning with the application layer. We‚Äôll examine how to secure 
e-mail, how to secure a TCP connection, how to provide blanket security at the net-
work layer, and how to secure a wireless LAN. In the third part of this chapter we‚Äôll 
consider operational security, which is about protecting organizational networks 
from attacks. In particular, we‚Äôll take a careful look at how firewalls and intrusion 
detection systems can enhance the security of an organizational network.
8.1 What Is Network Security?
Let‚Äôs begin our study of network security by returning to our lovers, Alice and Bob, 
who want to communicate ‚Äúsecurely.‚Äù What precisely does this mean? Certainly, 
Alice wants only Bob to be able to understand a message that she has sent, even 
though they are communicating over an insecure medium where an intruder (Trudy, 
the intruder) may intercept whatever is transmitted from Alice to Bob. Bob also 
wants to be sure that the message he receives from Alice was indeed sent by Alice, 
and Alice wants to make sure that the person with whom she is communicating is 
indeed Bob. Alice and Bob also want to make sure that the contents of their messages 
have not been altered in transit. They also want to be assured that they can communi-
cate in the first place (i.e., that no one denies them access to the resources needed to 
communicate). Given these considerations, we can identify the following desirable 
properties of secure communication.
‚Ä¢ Confidentiality. Only the sender and intended receiver should be able to under-
stand the contents of the transmitted message. Because eavesdroppers may 
intercept the message, this necessarily requires that the message be somehow 
encrypted so that an intercepted message cannot be understood by an interceptor. 
This aspect of confidentiality is probably the most commonly perceived mean-
ing of the term secure communication. We‚Äôll study cryptographic techniques for 
encrypting and decrypting data in Section 8.2.
‚Ä¢ Message integrity. Alice and Bob want to ensure that the content of their 
 communication is not altered, either maliciously or by accident, in transit. Exten-
sions to the checksumming techniques that we encountered in reliable transport

and data link protocols can be used to provide such message integrity. We will 
study message integrity in Section 8.3.
‚Ä¢ End-point authentication. Both the sender and receiver should be able to confirm 
the identity of the other party involved in the communication‚Äîto confirm that the 
other party is indeed who or what they claim to be. Face-to-face human commu-
nication solves this problem easily by visual recognition. When communicating 
entities exchange messages over a medium where they cannot see the other party, 
authentication is not so simple. When a user wants to access an inbox, how does 
the mail server verify that the user is the person he or she claims to be? We study 
end-point authentication in Section 8.4.
‚Ä¢ Operational security. Almost all organizations (companies, universities, and so 
on) today have networks that are attached to the public Internet. These networks 
therefore can potentially be compromised. Attackers can attempt to deposit worms 
into the hosts in the network, obtain corporate secrets, map the internal network 
configurations, and launch DoS attacks. We‚Äôll see in Section 8.9 that operational 
devices such as firewalls and intrusion detection systems are used to counter 
attacks against an organization‚Äôs network. A firewall sits between the organiza-
tion‚Äôs network and the public network, controlling packet access to and from 
the network. An intrusion detection system performs ‚Äúdeep packet  inspection,‚Äù 
 alerting the network administrators about suspicious activity.
Having established what we mean by network security, let‚Äôs next consider 
exactly what information an intruder may have access to, and what actions can be 
taken by the intruder. Figure 8.1 illustrates the scenario. Alice, the sender, wants to 
send data to Bob, the receiver. In order to exchange data securely, while meeting 
the requirements of confidentiality, end-point authentication, and message integrity, 
Alice and Bob will exchange control messages and data messages (in much the same 
way that TCP senders and receivers exchange control segments and data  segments).  
Secure
sender
Alice
Trudy
Channel
Control, data messages
Secure
receiver
Bob
Data
Data
Figure 8.1 ‚ô¶ Sender, receiver, and intruder (Alice, Bob, and Trudy)

All or some of these messages will typically be encrypted. As discussed in Section 1.6, 
an intruder can potentially perform
‚Ä¢ eavesdropping‚Äîsniffing and recording control and data messages on the  channel.
‚Ä¢ modification, insertion, or deletion of messages or message content.
As we‚Äôll see, unless appropriate countermeasures are taken, these capabilities 
allow an intruder to mount a wide variety of security attacks: snooping on communi-
cation (possibly stealing passwords and data), impersonating another entity, hijack-
ing an ongoing session, denying service to legitimate network users by overloading 
system resources, and so on. A summary of reported attacks is maintained at the 
CERT Coordination Center [CERT 2020].
Having established that there are indeed real threats loose in the Internet, what 
are the Internet equivalents of Alice and Bob, our friends who need to communicate 
securely? Certainly, Bob and Alice might be human users at two end systems, for 
example, a real Alice and a real Bob who really do want to exchange secure e-mail. 
They might also be participants in an electronic commerce transaction. For example, 
a real Bob might want to transfer his credit card number securely to a Web server 
to purchase an item online. Similarly, a real Alice might want to interact with her 
bank online. The parties needing secure communication might themselves also be 
part of the network infrastructure. Recall that the domain name system (DNS, see 
Section 2.4) or routing daemons that exchange routing information (see Chapter 5) 
require secure communication between two parties. The same is true for network 
management applications, a topic we examined in Chapter 5). An intruder that could 
actively interfere with DNS lookups (as discussed in Section 2.4), routing computa-
tions  (Sections 5.3 and 5.4), or network management functions (Sections 5.5 and 5.7) 
could wreak havoc in the Internet.
Having now established the framework, a few of the most important definitions, 
and the need for network security, let us next delve into cryptography. While the use 
of cryptography in providing confidentiality is self-evident, we‚Äôll see shortly that it 
is also central to providing end-point authentication and message integrity‚Äîmaking 
cryptography a cornerstone of network security.
8.2 Principles of Cryptography
Although cryptography has a long history dating back at least as far as Julius  Caesar, 
modern cryptographic techniques, including many of those used in the Internet, are 
based on advances made in the past 30 years. Kahn‚Äôs book, The  Codebreakers [Kahn 
1967], and Singh‚Äôs book, The Code Book: The Science of Secrecy from Ancient 
Egypt to Quantum Cryptography [Singh 1999], provide a fascinating look at the

long history of cryptography. A complete discussion of cryptography itself requires 
a complete book [Bishop 2003; Kaufman 2002; Schneier 2015] and so we only touch 
on the essential aspects of cryptography, particularly as they are practiced on the 
Internet. We also note that while our focus in this section will be on the use of 
cryptography for confidentiality, we‚Äôll see shortly that cryptographic techniques are 
inextricably woven into authentication, message integrity, nonrepudiation, and more.
Cryptographic techniques allow a sender to disguise data so that an intruder can 
gain no information from the intercepted data. The receiver, of course, must be able 
to recover the original data from the disguised data. Figure 8.2 illustrates some of the 
important terminology.
Suppose now that Alice wants to send a message to Bob. Alice‚Äôs message in 
its original form (e.g., ‚ÄúBob, I love you. Alice‚Äù) is known as  plaintext, 
or cleartext. Alice encrypts her plaintext message using an encryption algorithm 
so that the encrypted message, known as ciphertext, looks unintelligible to any 
intruder. Interestingly, in many modern cryptographic systems, including those used 
in the Internet, the encryption technique itself is known‚Äîpublished, standardized, 
and available to everyone (e.g., [RFC 1321; RFC 3447; RFC 2420; NIST 2001]), 
even a potential intruder! Clearly, if everyone knows the method for encoding data, 
then there must be some secret information that prevents an intruder from decrypting 
the transmitted data. This is where keys come in.
In Figure 8.2, Alice provides a key, KA, a string of numbers or characters, as 
input to the encryption algorithm. The encryption algorithm takes the key and the 
plaintext message, m, as input and produces ciphertext as output. The notation 
KA(m) refers to the ciphertext form (encrypted using the key KA) of the plaintext 
message, m. The actual encryption algorithm that uses key KA will be evident from 
the context. Similarly, Bob will provide a key, KB, to the decryption algorithm 
Figure 8.2 ‚ô¶ Cryptographic components
Encryption
algorithm
Ciphertext
Channel
Trudy
Alice
Bob
Decryption
algorithm
Plaintext
Key:
Key
Plaintext
KA
KB

that takes the ciphertext and Bob‚Äôs key as input and produces the original plain-
text as output. That is, if Bob receives an encrypted message KA(m), he decrypts it 
by computing KB(KA(m)) = m. In symmetric key systems, Alice‚Äôs and Bob‚Äôs keys 
are identical and are secret. In public key systems, a pair of keys is used. One of 
the keys is known to both Bob and Alice (indeed, it is known to the whole world).  
The other key is known only by either Bob or Alice (but not both). In the following 
two subsections, we consider symmetric key and public key systems in more detail.
8.2.1 Symmetric Key Cryptography
All cryptographic algorithms involve substituting one thing for another, for exam-
ple, taking a piece of plaintext and then computing and substituting the appropriate 
ciphertext to create the encrypted message. Before studying a modern key-based 
cryptographic system, let us first get our feet wet by studying a very old, very simple 
symmetric key algorithm attributed to Julius Caesar, known as the Caesar cipher  
(a cipher is a method for encrypting data).
For English text, the Caesar cipher would work by taking each letter in the plain-
text message and substituting the letter that is k letters later (allowing wraparound; 
that is, having the letter z followed by the letter a) in the alphabet. For example, if 
k = 3, then the letter a in plaintext becomes d in ciphertext; b in plaintext becomes 
e in ciphertext, and so on. Here, the value of k serves as the key. As an example, the 
plaintext message ‚Äúbob, i love you. Alice‚Äù becomes ‚Äúere, l oryh 
brx. dolfh‚Äù in ciphertext. While the ciphertext does indeed look like gibberish, 
it wouldn‚Äôt take long to break the code if you knew that the Caesar cipher was being 
used, as there are only 25 possible key values.
An improvement on the Caesar cipher is the monoalphabetic cipher, which also 
substitutes one letter of the alphabet with another letter of the alphabet.  However, 
rather than substituting according to a regular pattern (e.g., substitution with an offset 
of k for all letters), any letter can be substituted for any other letter, as long as each 
letter has a unique substitute letter, and vice versa. The substitution rule in Figure 8.3 
shows one possible rule for encoding plaintext.
The plaintext message ‚Äúbob, i love you. Alice‚Äù becomes ‚Äúnkn, s 
gktc wky. Mgsbc.‚Äù Thus, as in the case of the Caesar cipher, this looks like 
gibberish. A monoalphabetic cipher would also appear to be better than the Caesar 
cipher in that there are 26! (on the order of 1026) possible pairings of letters rather 
than 25 possible pairings. A brute-force approach of trying all 1026 possible pairings 
Figure 8.3 ‚ô¶ A monoalphabetic cipher
Plaintext letter:
a b c d e f g h i j k l m n o p q r s t u v w x y z
Ciphertext letter:
m n b v c x z a s d f g h j k l p o i u y t r e w q

would require far too much work to be a feasible way of breaking the encryption 
algorithm and decoding the message. However, by statistical analysis of the plain-
text language, for example, knowing that the letters e and t are the most frequently 
occurring letters in typical English text (accounting for 13 percent and 9 percent of 
letter occurrences), and knowing that particular two-and three-letter occurrences of 
letters appear quite often together (for example, ‚Äúin,‚Äù ‚Äúit,‚Äù ‚Äúthe,‚Äù ‚Äúion,‚Äù ‚Äúing,‚Äù and so 
forth) make it relatively easy to break this code. If the intruder has some knowledge 
about the possible contents of the message, then it is even easier to break the code. 
For example, if Trudy the intruder is Bob‚Äôs wife and suspects Bob of having an 
affair with Alice, then she might suspect that the names ‚Äúbob‚Äù and ‚Äúalice‚Äù appear in 
the text. If Trudy knew for certain that those two names appeared in the ciphertext 
and had a copy of the example ciphertext message above, then she could immedi-
ately determine seven of the 26 letter pairings, requiring 109 fewer possibilities to 
be checked by a brute-force method. Indeed, if Trudy suspected Bob of having an 
affair, she might well expect to find some other choice words in the message as well.
When considering how easy it might be for Trudy to break Bob and Alice‚Äôs 
encryption scheme, one can distinguish three different scenarios, depending on what 
information the intruder has.
‚Ä¢ Ciphertext-only attack. In some cases, the intruder may have access only to the 
intercepted ciphertext, with no certain information about the contents of the plain-
text message. We have seen how statistical analysis can help in a ciphertext-only 
attack on an encryption scheme.
‚Ä¢ Known-plaintext attack. We saw above that if Trudy somehow knew for sure 
that ‚Äúbob‚Äù and ‚Äúalice‚Äù appeared in the ciphertext message, then she could have 
determined the (plaintext, ciphertext) pairings for the letters a, l, i, c, e, b, and o. 
Trudy might also have been fortunate enough to have recorded all of the cipher-
text transmissions and then found Bob‚Äôs own decrypted version of one of the 
transmissions scribbled on a piece of paper. When an intruder knows some of the 
(plaintext, ciphertext) pairings, we refer to this as a known-plaintext attack on 
the encryption scheme.
‚Ä¢ Chosen-plaintext attack. In a chosen-plaintext attack, the intruder is able to 
choose the plaintext message and obtain its corresponding ciphertext form. For 
the simple encryption algorithms we‚Äôve seen so far, if Trudy could get Alice to 
send the message, ‚ÄúThe quick brown fox jumps over the lazy 
dog,‚Äù she could completely break the encryption scheme. We‚Äôll see shortly that 
for more sophisticated encryption techniques, a chosen-plaintext attack does not 
necessarily mean that the encryption technique can be broken.
Five hundred years ago, techniques improving on monoalphabetic encryp-
tion, known as polyalphabetic encryption, were invented. The idea behind 
 polyalphabetic encryption is to use multiple monoalphabetic ciphers, with a specific

monoalphabetic cipher to encode a letter in a specific position in the plaintext mes-
sage. Thus, the same letter, appearing in different positions in the plaintext message, 
might be encoded differently. An example of a polyalphabetic encryption scheme is 
shown in Figure 8.4. It has two Caesar ciphers (with k = 5 and k = 19), shown as 
rows. We might choose to use these two Caesar ciphers, C1 and C2, in the repeating 
pattern C1, C2, C2, C1, C2. That is, the first letter of plaintext is to be encoded using 
C1, the second and third using C2, the fourth using C1, and the fifth using C2. The 
pattern then repeats, with the sixth letter being encoded using C1, the seventh with 
C2, and so on. The plaintext message ‚Äúbob, i love you.‚Äù is thus encrypted 
‚Äúghu, n etox dhz.‚Äù Note that the first b in the plaintext message is encrypted 
using C1, while the second b is encrypted using C2. In this example, the encryption 
and decryption ‚Äúkey‚Äù is the knowledge of the two Caesar keys (k = 5, k = 19) and 
the pattern C1, C2, C2, C1, C2.
Block Ciphers
Let us now move forward to modern times and examine how symmetric key encryp-
tion is done today. We focus on block ciphers, which are used in many secure Internet 
protocols, including PGP (for secure e-mail), TLS (for securing TCP connections), 
and IPsec (for securing the network-layer transport).
In a block cipher, the message to be encrypted is processed in blocks of k bits. 
For example, if k = 64, then the message is broken into 64-bit blocks, and each block 
is encrypted independently. To encode a block, the cipher uses a one-to-one map-
ping to map the k-bit block of cleartext to a k-bit block of ciphertext. Let‚Äôs look at an 
example. Suppose that k = 3, so that the block cipher maps 3-bit inputs  (cleartext) 
to 3-bit outputs (ciphertext). One possible mapping is given in Table 8.1. Notice that 
this is a one-to-one mapping; that is, there is a different output for each input. This 
block cipher breaks the message up into 3-bit blocks and encrypts each block accord-
ing to the above mapping. You should verify that the message 010110001111 gets 
encrypted into 101000111001.
Continuing with this 3-bit block example, note that the mapping in Table 8.1 
is just one mapping of many possible mappings. How many possible mappings are 
there? To answer this question, observe that a mapping is nothing more than a permu-
tation of all the possible inputs. There are 23 (= 8) possible inputs (listed under the 
Figure 8.4 ‚ô¶ A polyalphabetic cipher using two Caesar ciphers
Plaintext letter:
a b c d e f g h i j k l m n o p q r s t u v w x y z
C1(k = 5): 
C2(k = 19): 
f g h i j k l m n o p q r s t u v w x y z a b c d e
t u v w x y z a b c d e f g h i j k l m n o p q r s

input columns). These eight inputs can be permuted in 8! = 40,320 different ways. 
Since each of these permutations specifies a mapping, there are 40,320 possible map-
pings. We can view each of these mappings as a key‚Äîif Alice and Bob both know 
the mapping (the key), they can encrypt and decrypt the messages sent between them.
The brute-force attack for this cipher is to try to decrypt ciphtertext by using all 
mappings. With only 40,320 mappings (when k = 3), this can quickly be accom-
plished on a desktop PC. To thwart brute-force attacks, block ciphers typically use 
much larger blocks, consisting of k = 64 bits or even larger. Note that the number of 
possible mappings for a general k-block cipher is 2k!, which is astronomical for even 
moderate values of k (such as k = 64).
Although full-table block ciphers, as just described, with moderate values of k 
can produce robust symmetric key encryption schemes, they are unfortunately dif-
ficult to implement. For k = 64 and for a given mapping, Alice and Bob would need to 
maintain a table with 264 input values, which is an infeasible task. Moreover, if Alice 
and Bob were to change keys, they would have to each regenerate the table. Thus, 
a full-table block cipher, providing predetermined mappings between all inputs and 
outputs (as in the example above), is simply out of the question.
Instead, block ciphers typically use functions that simulate randomly permuted 
tables. An example (adapted from [Kaufman 2002]) of such a function for k = 64 bits  
is shown in Figure 8.5. The function first breaks a 64-bit block into 8 chunks, with 
each chunk consisting of 8 bits. Each 8-bit chunk is processed by an 8-bit to 8-bit  
table, which is of manageable size. For example, the first chunk is processed by the 
table denoted by T1. Next, the 8 output chunks are reassembled into a 64-bit block. 
The positions of the 64 bits in the block are then scrambled (permuted) to produce a 
64-bit output. This output is fed back to the 64-bit input, where another cycle begins. 
After n such cycles, the function provides a 64-bit block of ciphertext. The purpose 
of the rounds is to make each input bit affect most (if not all) of the final output bits. 
(If only one round were used, a given input bit would affect only 8 of the 64 output 
bits.) The key for this block cipher algorithm would be the eight permutation tables 
(assuming the scramble function is publicly known).
Today there are a number of popular block ciphers, including DES (standing 
for Data Encryption Standard), 3DES, and AES (standing for Advanced Encryption 
Table 8.1 ‚ô¶ A specific 3-bit block cipher
input
output
input
output
000
110
100
011
001
111
101
010
010
101
110
000
011
100
111
001

Standard). Each of these standards uses functions, rather than predetermined tables, 
along the lines of Figure 8.5 (albeit more complicated and specific to each cipher). 
Each of these algorithms also uses a string of bits for a key. For example, DES uses 
64-bit blocks with a 56-bit key. AES uses 128-bit blocks and can operate with keys 
that are 128, 192, and 256 bits long. An algorithm‚Äôs key determines the specific 
‚Äúmini-table‚Äù mappings and permutations within the algorithm‚Äôs internals. The brute-
force attack for each of these ciphers is to cycle through all the keys, applying the 
decryption algorithm with each key. Observe that with a key length of n, there are 2n 
possible keys. NIST [NIST 2001] estimates that a machine that could crack 56-bit 
DES in one second (that is, try all 256 keys in one second) would take approximately 149 
trillion years to crack a 128-bit AES key.
Cipher-Block Chaining
In computer networking applications, we typically need to encrypt long messages  
or long streams of data. If we apply a block cipher as described by simply chopping 
up the message into k-bit blocks and independently encrypting each block, a subtle 
but important problem occurs. To see this, observe that two or more of the cleartext 
blocks can be identical. For example, the cleartext in two or more blocks could be 
‚ÄúHTTP/1.1‚Äù. For these identical blocks, a block cipher would, of course, produce 
the same ciphertext. An attacker could potentially guess the cleartext when it sees 
identical ciphertext blocks and may even be able to decrypt the entire message by 
identifying identical ciphtertext blocks and using knowledge about the underlying 
protocol structure [Kaufman 2002].
Figure 8.5 ‚ô¶ An example of a block cipher
64-bit output
Loop
for n
rounds
8 bits
8 bits
T1
8 bits
8 bits
T2
8 bits
8 bits
T3
8 bits
64-bit input
8 bits
T4
8 bits
8 bits
T5
8 bits
8 bits
T6
8 bits
8 bits
T7
8 bits
8 bits
T8
64-bit scrambler

To address this problem, we can mix some randomness into the ciphertext so 
that identical plaintext blocks produce different ciphertext blocks. To explain this 
idea, let m(i) denote the ith plaintext block, c(i) denote the ith ciphertext block, and 
a ‚äï b denote the exclusive-or (XOR) of two bit strings, a and b. (Recall that the  
0 ‚äï 0 = 1 ‚äï 1 = 0 and 0 ‚äï 1 = 1 ‚äï 0 = 1, and the XOR of two bit strings is  
done on a bit-by-bit basis. So, for example, 10101010 ‚äï 11110000 = 01011010.) 
Also, denote the block-cipher encryption algorithm with key S as KS. The basic idea 
is as follows. The sender creates a random k-bit number r(i) for the ith block and 
calculates c(i) = KS(m(i) ‚äï r(i )). Note that a new k-bit random number is chosen for 
each block. The sender then sends c(1), r(1), c(2), r(2), c(3), r(3), and so on. Since the 
receiver receives c(i) and r(i), it can recover each block of the plaintext by computing 
m(i) = KS(c(i)) ‚äï r(i ). It is important to note that, although r(i) is sent in the clear 
and thus can be sniffed by Trudy, she cannot obtain the plaintext m(i), since she does 
not know the key KS. Also note that if two plaintext blocks m(i) and m(j) are the same, 
the corresponding ciphertext blocks c(i) and c(j) will be different (as long as the 
random numbers r(i) and r(j) are different, which occurs with very high probability).
As an example, consider the 3-bit block cipher in Table 8.1. Suppose the plain-
text is 010010010. If Alice encrypts this directly, without including the randomness, 
the resulting ciphertext becomes 101101101. If Trudy sniffs this ciphertext, because 
each of the three cipher blocks is the same, she can correctly surmise that each of the 
three plaintext blocks are the same. Now suppose instead Alice generates the ran-
dom blocks r(1) = 001, r(2) = 111, and r(3) = 100 and uses the above technique 
to generate the ciphertext c(1) = 100, c(2) = 010, and c(3) = 000. Note that the 
three ciphertext blocks are different even though the plaintext blocks are the same. 
Alice then sends c(1), r(1), c(2), and r(2). You should verify that Bob can obtain the 
original plaintext using the shared key KS.
The astute reader will note that introducing randomness solves one problem but 
creates another: namely, Alice must transmit twice as many bits as before. Indeed, 
for each cipher bit, she must now also send a random bit, doubling the required band-
width. In order to have our cake and eat it too, block ciphers typically use a technique 
called Cipher Block Chaining (CBC). The basic idea is to send only one random 
value along with the very first message, and then have the sender and receiver use 
the computed coded blocks in place of the subsequent random number. Specifically, 
CBC operates as follows:
 1. Before encrypting the message (or the stream of data), the sender generates a 
random k-bit string, called the Initialization Vector (IV). Denote this initial-
ization vector by c(0). The sender sends the IV to the receiver in cleartext.
 2. For the first block, the sender calculates m(1) ‚äï c(0), that is, calculates the exclu-
sive-or of the first block of cleartext with the IV. It then runs the result through 
the block-cipher algorithm to get the corresponding ciphertext block; that is, 
c(1) = KS(m(1) ‚äï c(0)). The sender sends the encrypted block c(1) to the receiver.
 3. For the ith block, the sender generates the ith ciphertext block from c(i) =  
KS(m(i) ‚äï c(i - 1)).

Let‚Äôs now examine some of the consequences of this approach. First, the receiver 
will still be able to recover the original message. Indeed, when the receiver receives 
c(i), it decrypts it with KS to obtain s(i) = m(i) ‚äï c(i - 1); since the receiver also 
knows c(i - 1), it then obtains the cleartext block from m(i) = s(i) ‚äï c(i - 1). 
Second, even if two cleartext blocks are identical, the corresponding ciphtertexts 
(almost always) will be different. Third, although the sender sends the IV in the 
clear, an intruder will still not be able to decrypt the ciphertext blocks, since the 
intruder does not know the secret key, S. Finally, the sender only sends one overhead 
block (the IV), thereby negligibly increasing the bandwidth usage for long messages  
(consisting of hundreds of blocks).
As an example, let‚Äôs now determine the ciphertext for the 3-bit block cipher in 
Table 8.1 with plaintext 010010010 and IV = c(0) = 001. The sender first uses the  
IV to calculate c(1) = KS(m(1) ‚äï c(0)) = 100. The sender then calculates c(2) = 
KS(m(2) ‚äï c(1)) = KS(010 ‚äï 100) = 000, and c(3) = KS(m(3) ‚äï c(2)) = KS(010 ‚äï  
000) = 101. The reader should verify that the receiver, knowing the IV and KS can 
recover the original plaintext.
CBC has an important consequence when designing secure network proto-
cols: we‚Äôll need to provide a mechanism within the protocol to distribute the IV 
from sender to receiver. We‚Äôll see how this is done for several protocols later in 
this chapter.
8.2.2 Public Key Encryption
For more than 2,000 years (since the time of the Caesar cipher and up to the 1970s), 
encrypted communication required that the two communicating parties share a com-
mon secret‚Äîthe symmetric key used for encryption and decryption. One difficulty 
with this approach is that the two parties must somehow agree on the shared key; but 
to do so in itself requires secure communication. Perhaps the parties could first meet 
and agree on the key in person (for example, two of Caesar‚Äôs centurions might meet 
at the Roman baths) and thereafter communicate with encryption. In a networked 
world, however, communicating parties may never meet and may never converse 
except over the network. 
Is it possible for two parties to communicate with encryption without having 
a shared secret key that is known in advance? In 1976, Diffie and Hellman [Diffie 
1976] demonstrated an algorithm (known now as Diffie-Hellman Key Exchange) 
to do just that‚Äîa radically different and marvelously elegant approach toward 
secure communication that has led to the development of today‚Äôs public key cryp-
tography systems. We‚Äôll see shortly that public key cryptography systems also 
have several wonderful properties that make them useful not only for encryption, 
but for authentication and digital signatures as well. Interestingly, it has come to light 
that ideas similar to those in [Diffie 1976] and [RSA 1978] had been independently 
developed in the early 1970s in a series of secret reports by researchers at the 
Communications-Electronics Security Group in the United  Kingdom [Ellis 1987].

As is often the case, great ideas can spring up independently in many places; for-
tunately, public key advances took place not only in private, but also in the public 
view, as well.
The use of public key cryptography is conceptually quite simple. Suppose Alice 
wants to communicate with Bob. As shown in Figure 8.6, rather than Bob and Alice 
sharing a single secret key (as in the case of symmetric key systems), Bob (the recipi-
ent of Alice‚Äôs messages) instead has two keys‚Äîa public key that is available to 
everyone in the world (including Trudy the intruder) and a private key that is known 
only to Bob. We will use the notation K+
B and K-
B to refer to Bob‚Äôs public and pri-
vate keys, respectively. In order to communicate with Bob, Alice first fetches Bob‚Äôs 
public key. Alice then encrypts her message, m, to Bob using Bob‚Äôs public key and 
a known (for example, standardized) encryption algorithm; that is, Alice computes 
K+
B(m). Bob receives Alice‚Äôs encrypted message and uses his private key and a known 
(for example, standardized) decryption algorithm to decrypt Alice‚Äôs encrypted mes-
sage. That is, Bob computes K-
B(K+
B(m)). We will see below that there are encryption/
decryption algorithms and techniques for choosing public and private keys such that 
K-
B(K+
B(m)) = m; that is, applying Bob‚Äôs public key, K+
B, to a message, m (to get 
K+
B(m)), and then applying Bob‚Äôs private key, K-
B, to the encrypted version of m (that 
is, computing K-
B(K+
B(m))) gives back m. This is a remarkable result! In this manner, 
Alice can use Bob‚Äôs publicly available key to send a secret message to Bob without 
either of them having to distribute any secret keys! We will see shortly that we can 
interchange the public key and private key encryption and get the same remarkable 
result‚Äì‚Äìthat is, K-
B (B +(m)) = K+
B (K-
B(m)) = m.
Although public-key cryptography is appealing, one concern immediately 
springs to mind. Since Bob‚Äôs encryption key is public, anyone can send an encrypted 
Figure 8.6 ‚ô¶ Public key cryptography
Encryption
algorithm
Ciphertext
Decryption
algorithm
Plaintext
message, m
Plaintext
message, m
Private decryption key
m = KB
‚Äì(KB
+(m))
KB
‚Äì
KB
+(m)
Public encryption key
KB
+

message to Bob, including Alice or someone pretending to be Alice. In the case of 
a single shared secret key, the fact that the sender knows the secret key implicitly 
identifies the sender to the receiver. In the case of public key cryptography, however, 
this is no longer the case since anyone can send an encrypted message to Bob using 
Bob‚Äôs publicly available key. A digital signature, a topic we will study in Section 8.3, 
is needed to bind a sender to a message.
RSA
While there may be many algorithms that address these concerns, the RSA  algorithm 
(named after its founders, Ron Rivest, Adi Shamir, and Leonard Adleman) has 
become almost synonymous with public key cryptography. Let‚Äôs first see how RSA 
works and then examine why it works.
RSA makes extensive use of arithmetic operations using modulo-n arithmetic. 
So let‚Äôs briefly review modular arithmetic. Recall that x mod n simply means the 
remainder of x when divided by n; so, for example, 19 mod 5 = 4. In modular arith-
metic, one performs the usual operations of addition, multiplication, and exponen-
tiation. However, the result of each operation is replaced by the integer remainder 
that is left when the result is divided by n. Adding and multiplying with modular 
arithmetic is facilitated with the following handy facts:
[(a mod n) + (b mod n)] mod n = (a + b) mod n
[(a mod n) - (b mod n)] mod n = (a - b) mod n
[(a mod n) # (b mod n)] mod n = (a # b) mod n
It follows from the third fact that (a mod n)d mod n = ad mod n, which is an identity 
that we will soon find very useful.
Now suppose that Alice wants to send to Bob an RSA-encrypted message, as 
shown in Figure 8.6. In our discussion of RSA, let‚Äôs always keep in mind that a mes-
sage is nothing but a bit pattern, and every bit pattern can be uniquely represented by 
an integer number (along with the length of the bit pattern). For example, suppose 
a message is the bit pattern 1001; this message can be represented by the decimal 
integer 9. Thus, when encrypting a message with RSA, it is equivalent to encrypting 
the unique integer number that represents the message.
There are two interrelated components of RSA:
‚Ä¢ The choice of the public key and the private key
‚Ä¢ The encryption and decryption algorithm
To generate the public and private RSA keys, Bob performs the following steps:
 1. Choose two large prime numbers, p and q. How large should p and q be? The 
larger the values, the more difficult it is to break RSA, but the longer it takes

to perform the encoding and decoding. RSA Laboratories recommends that 
the product of p and q be on the order of 1,024 bits. For a discussion of how to 
find large prime numbers, see [Caldwell 2020].
 2. Compute n = pq and z = (p - 1)(q - 1).
 3. Choose a number, e, less than n, that has no common factors (other than 1) 
with z. (In this case, e and z are said to be relatively prime.) The letter e is used 
since this value will be used in encryption.
 4. Find a number, d, such that ed - 1 is exactly divisible (that is, with no  remainder) 
by z. The letter d is used because this value will be used in decryption. Put another 
way, given e, we choose d such that
ed mod z = 1
 5. The public key that Bob makes available to the world, K+
B, is the pair of numbers 
(n, e); his private key, K-
B, is the pair of numbers (n, d).
The encryption by Alice and the decryption by Bob are done as follows:
‚Ä¢ Suppose Alice wants to send Bob a bit pattern represented by the integer  number 
m (with m 6 n). To encode, Alice performs the exponentiation me, and then 
computes the integer remainder when me is divided by n. In other words, the 
encrypted value, c, of Alice‚Äôs plaintext message, m, is
c = me mod n
 
The bit pattern corresponding to this ciphertext c is sent to Bob.
‚Ä¢ To decrypt the received ciphertext message, c, Bob computes
m = cd mod n
which requires the use of his private key (n, d).
As a simple example of RSA, suppose Bob chooses p = 5 and q = 7.  (Admittedly, 
these values are far too small to be secure.) Then n = 35 and z = 24. Bob chooses  
e = 5, since 5 and 24 have no common factors. Finally, Bob chooses d = 29, since 
5 # 29 - 1 (that is, ed - 1) is exactly divisible by 24. Bob makes the two values, n = 35 
and e = 5, public and keeps the value d = 29 secret. Observing these two public 
values, suppose Alice now wants to send the letters l, o, v, and e to Bob. Interpreting 
each letter as a number between 1 and 26 (with a being 1, and z being 26), Alice and 
Bob perform the encryption and decryption shown in Tables 8.2 and 8.3, respectively. 
Note that in this example, we consider each of the four letters as a distinct message. 
A more realistic example would be to convert the four letters into their 8-bit ASCII 
representations and then encrypt the integer corresponding to the resulting 32-bit bit 
pattern. (Such a realistic example generates numbers that are much too long to print 
in a textbook!)

Given that the ‚Äútoy‚Äù example in Tables 8.2 and 8.3 has already produced some 
extremely large numbers, and given that we saw earlier that p and q should each be 
several hundred bits long, several practical issues regarding RSA come to mind. 
How does one choose large prime numbers? How does one then choose e and d? 
How does one perform exponentiation with large numbers? A discussion of these 
important issues is beyond the scope of this book; see [Kaufman 2002] and the refer-
ences therein for details.
Session Keys
We note here that the exponentiation required by RSA is a rather time-consuming 
process. As a result, RSA is often used in practice in combination with symmet-
ric key cryptography. For example, if Alice wants to send Bob a large amount of 
encrypted data, she could do the following. First Alice chooses a key that will 
be used to encode the data itself; this key is referred to as a session key, and is 
denoted by KS. Alice must inform Bob of the session key, since this is the shared 
 symmetric key they will use with a symmetric key cipher (e.g., with DES or AES). 
Alice encrypts the session key using Bob‚Äôs public key, that is, computes c = (KS)e 
mod n. Bob receives the RSA-encrypted session key, c, and decrypts it to obtain 
Table 8.2 ‚ô¶ Alice‚Äôs RSA encryption, e = 5, n = 35
Plaintext Letter
m: numeric representation
me
Ciphertext c = me mod n
l
12
248832
17
o
15
759375
15
v
22
5153632
22
e
5
3125
10
Table 8.3 ‚ô¶ Bob‚Äôs RSA decryption, d = 29, n = 35
Ciphertext c
cd
m = cd mod n
Plaintext Letter
17
4819685721067509150915091411825223071697
12
l
15
127834039403948858939111232757568359375
15
o
22
851643319086537701956194499721106030592
22
v
10
1000000000000000000000000000000
5
e

the session key, KS. Bob now knows the session key that Alice will use for her 
encrypted data transfer.
Why Does RSA Work?
RSA encryption/decryption appears rather magical. Why should it be that by apply-
ing the encryption algorithm and then the decryption algorithm, one recovers the 
original message? In order to understand why RSA works, again denote n = pq, 
where p and q are the large prime numbers used in the RSA algorithm.
Recall that, under RSA encryption, a message (uniquely represented by an  integer), 
m, is exponentiated to the power e using modulo-n arithmetic, that is,
c = me mod n
Decryption is performed by raising this value to the power d, again using modulo-n 
arithmetic. The result of an encryption step followed by a decryption step is thus  
(me mod n)d mod n. Let‚Äôs now see what we can say about this quantity. As mentioned 
earlier, one important property of modulo arithmetic is (a mod n)d mod n = ad mod n 
for any values a, n, and d. Thus, using a = me in this property, we have
(me mod n)d mod n = med mod n
It therefore remains to show that med mod n = m. Although we‚Äôre trying to 
remove some of the magic about why RSA works, to establish this, we‚Äôll need to use a 
rather magical result from number theory here. Specifically, we‚Äôll need the result that 
says if p and q are prime, n = pq, and z = (p - 1)(q - 1), then xy mod n is the same as 
x(y mod z) mod n [Kaufman 2002]. Applying this result with x = m and y = ed we have
med mod n = m(ed mod z) mod n
But remember that we have chosen e and d such that ed mod z = 1. This gives us
med mod n = m1 mod n = m
which is exactly the result we are looking for! By first exponentiating to the power of 
e (that is, encrypting) and then exponentiating to the power of d (that is,  decrypting), 
we obtain the original value, m. Even more wonderful is the fact that if we first 
exponentiate to the power of d and then exponentiate to the power of e‚Äîthat is, we 
reverse the order of encryption and decryption, performing the decryption operation 
first and then applying the encryption operation‚Äîwe also obtain the original value, 
m. This wonderful result follows immediately from the modular arithmetic:
(md mod n)e mod n = mde mod n = med mod n = (me mod n)d mod n

The security of RSA relies on the fact that there are no known algorithms for 
quickly factoring a number, in this case the public value n, into the primes p and q. If 
one knew p and q, then given the public value e, one could easily compute the secret 
key, d. On the other hand, it is not known whether or not there exist fast algorithms for 
factoring a number, and in this sense, the security of RSA is not guaranteed. With recent 
advances in quantum computing, and published fast factoring algorithms for quantum 
computers, there are concerns that RSA may not be secure forever [MIT TR 2019]. But 
the practical realization of these algorithms still appears to be far in the future.
Another popular public-key encryption algorithm is the Diffie-Hellman algo-
rithm, which we will briefly explore in the homework problems. Diffie-Hellman 
is not as versatile as RSA in that it cannot be used to encrypt messages of arbitrary 
length; it can be used, however, to establish a symmetric session key, which is in turn 
used to encrypt messages.
8.3 Message Integrity and Digital Signatures
In the previous section, we saw how encryption can be used to provide confidenti-
ality to two communicating entities. In this section, we turn to the equally impor-
tant cryptography topic of providing message integrity (also known as message 
 authentication). Along with message integrity, we will discuss two related topics in 
this section: digital signatures and end-point authentication.
We define the message integrity problem using, once again, Alice and Bob. 
Suppose Bob receives a message (which may be encrypted or may be in plaintext) 
and he believes this message was sent by Alice. To authenticate this message, Bob 
needs to verify:
 1. The message indeed originated from Alice.
 2. The message was not tampered with on its way to Bob.
We‚Äôll see in Sections 8.4 through 8.7 that this problem of message integrity is a criti-
cal concern in just about all secure networking protocols.
As a specific example, consider a computer network using a link-state routing 
algorithm (such as OSPF) for determining routes between each pair of routers in the 
network (see Chapter 5). In a link-state algorithm, each router needs to broadcast a 
link-state message to all other routers in the network. A router‚Äôs link-state message 
includes a list of its directly connected neighbors and the direct costs to these neigh-
bors. Once a router receives link-state messages from all of the other routers, it can 
create a complete map of the network, run its least-cost routing algorithm, and con-
figure its forwarding table. One relatively easy attack on the routing algorithm is for 
Trudy to distribute bogus link-state messages with incorrect link-state information. 
Thus, the need for message integrity‚Äîwhen router B receives a link-state message 
from router A, router B should verify that router A actually created the message and, 
further, that no one tampered with the message in transit.

In this section, we describe a popular message integrity technique that is used 
by many secure networking protocols. But before doing so, we need to cover another 
important topic in cryptography‚Äîcryptographic hash functions.
8.3.1 Cryptographic Hash Functions
As shown in Figure 8.7, a hash function takes an input, m, and computes a fixed-size 
string H(m) known as a hash. The Internet checksum (Chapter 3) and CRCs (Chapter 6) 
meet this definition. A cryptographic hash function is required to have the follow-
ing additional property:
‚Ä¢ It is computationally infeasible to find any two different messages x and y such 
that H(x) = H(y).
Informally, this property means that it is computationally infeasible for an 
intruder to substitute one message for another message that is protected by the hash 
function. That is, if (m, H(m)) are the message and the hash of the message created 
by the sender, then an intruder cannot forge the contents of another message, y, that 
has the same hash value as the original message.
Let‚Äôs convince ourselves that a simple checksum, such as the Internet checksum, 
would make a poor cryptographic hash function. Rather than performing 1s comple-
ment arithmetic (as in the Internet checksum), let us compute a checksum by treating 
each character as a byte and adding the bytes together using 4-byte chunks at a time. 
Suppose Bob owes Alice $100.99 and sends an IOU to Alice consisting of the text 
string ‚ÄúIOU100.99BOB.‚Äù The ASCII representation (in hexadecimal notation) for 
these letters is 49,4F,55,31,30,30,2E,39,39,42,4F,42.
Figure 8.8 (top) shows that the 4-byte checksum for this message is B2 
C1 D2 AC. A slightly different message (and a much more costly one for Bob) 
Figure 8.7 ‚ô¶ Hash functions
Many-to-one
hash function
Long message: m
Dear Alice:
This is a VERY long letter
since there is so much to
say.....
..........
..........
Bob
Fixed-length
hash: H(m)
Opgmdvboijrtnsd
gghPPdogm;lcvkb

is shown in the bottom half of Figure 8.8. The messages ‚ÄúIOU100.99BOB‚Äù and 
‚ÄúIOU900.19BOB‚Äù have the same checksum. Thus, this simple checksum algorithm 
violates the requirement above. Given the original data, it is simple to find another 
set of data with the same checksum. Clearly, for security purposes, we are going to 
need a more powerful hash function than a checksum.
The MD5 hash algorithm of Ron Rivest [RFC 1321] is in wide use today. 
It computes a 128-bit hash in a four-step process consisting of a padding step 
(adding a one followed by enough zeros so that the length of the message satisfies  
certain conditions), an append step (appending a 64-bit representation of the mes-
sage length before padding), an initialization of an accumulator, and a final loop-
ing step in which the message‚Äôs 16-word blocks are processed (mangled) in four 
rounds. For a description of MD5 (including a C source code implementation) see 
[RFC 1321].
The second major hash algorithm in use today is the Secure Hash Algorithm 
(SHA-1) [FIPS 1995]. This algorithm is based on principles similar to those used 
in the design of MD4 [RFC 1320], the predecessor to MD5. SHA-1, a US federal 
standard, is required for use whenever a cryptographic hash algorithm is needed for 
federal applications. It produces a 160-bit message digest. The longer output length 
makes SHA-1 more secure.
8.3.2 Message Authentication Code
Let‚Äôs now return to the problem of message integrity. Now that we understand hash 
functions, let‚Äôs take a first stab at how we might perform message integrity:
Figure 8.8 ‚ô¶  Initial message and fraudulent message have the same 
 checksum!
Message
I O U 1
0 0 . 9
9 B O B
ASCII
Representation
49
4F
55
31
30
30
2E
39
39
42
4F
42
B2
C1
D2
AC
Checksum
Message
I O U 9
0 0 . 1
9 B O B
ASCII
Representation
49
4F
55
39
30
30
2E
31
39
42
4F
42
B2
C1
D2
AC
Checksum

1. Alice creates message m and calculates the hash H(m) (for example, with SHA-1).
 2. Alice then appends H(m) to the message m, creating an extended message  
(m, H(m)), and sends the extended message to Bob.
 3. Bob receives an extended message (m, h) and calculates H(m). If H(m) = h, 
Bob concludes that everything is fine.
This approach is obviously flawed. Trudy can create a bogus message m¬¥ in which 
she says she is Alice, calculate H(m¬¥), and send Bob (m¬¥, H(m¬¥)). When Bob receives 
the message, everything checks out in step 3, so Bob doesn‚Äôt suspect any funny 
 business.
To perform message integrity, in addition to using cryptographic hash functions, 
Alice and Bob will need a shared secret s. This shared secret, which is nothing more 
than a string of bits, is called the authentication key. Using this shared secret, mes-
sage integrity can be performed as follows:
 1. Alice creates message m, concatenates s with m to create m + s, and calculates 
the hash H(m + s) (for example, with SHA-1). H(m + s) is called the message 
authentication code (MAC).
 2. Alice then appends the MAC to the message m, creating an extended message 
(m, H(m + s)), and sends the extended message to Bob.
 3. Bob receives an extended message (m, h) and knowing s, calculates the MAC 
H(m + s). If H(m + s) = h, Bob concludes that everything is fine.
A summary of the procedure is shown in Figure 8.9. Readers should note that the 
MAC here (standing for ‚Äúmessage authentication code‚Äù) is not the same MAC used 
in link-layer protocols (standing for ‚Äúmedium access control‚Äù)!
One nice feature of a MAC is that it does not require an encryption algorithm. 
Indeed, in many applications, including the link-state routing algorithm described 
earlier, communicating entities are only concerned with message integrity and are 
Figure 8.9 ‚ô¶ Message authentication code (MAC)
H(.)
H(.)
m
m
m
m
s
s
s
+
Internet
Compare
Key:
= Message
= Shared secret
H(m+s)
H(m+s)

not concerned with message confidentiality. Using a MAC, the entities can authen-
ticate the messages they send to each other without having to integrate complex 
encryption algorithms into the integrity process.
As you might expect, a number of different standards for MACs have been pro-
posed over the years. The most popular standard today is HMAC, which can be used 
either with MD5 or SHA-1. HMAC actually runs data and the authentication key 
through the hash function twice [Kaufman 2002; RFC 2104].
There still remains an important issue. How do we distribute the shared authen-
tication key to the communicating entities? For example, in the link-state routing 
algorithm, we would somehow need to distribute the secret authentication key to 
each of the routers in the autonomous system. (Note that the routers can all use the 
same authentication key.) A network administrator could actually accomplish this by 
physically visiting each of the routers. Or, if the network administrator is a lazy guy, 
and if each router has its own public key, the network administrator could distribute 
the authentication key to any one of the routers by encrypting it with the router‚Äôs 
public key and then sending the encrypted key over the network to the router.
8.3.3 Digital Signatures
Think of the number of the times you‚Äôve signed your name to a piece of paper 
 during the last week. You sign checks, credit card receipts, legal documents, and 
letters. Your signature attests to the fact that you (as opposed to someone else) have 
acknowledged and/or agreed with the document‚Äôs contents. In a digital world, one 
often wants to indicate the owner or creator of a document, or to signify one‚Äôs agree-
ment with a document‚Äôs content. A digital signature is a cryptographic technique 
for achieving these goals in a digital world.
Just as with handwritten signatures, digital signing should be done in a way that 
is verifiable and nonforgeable. That is, it must be possible to prove that a document 
signed by an individual was indeed signed by that individual (the signature must be 
verifiable) and that only that individual could have signed the document (the signa-
ture cannot be forged).
Let‚Äôs now consider how we might design a digital signature scheme. Observe that 
when Bob signs a message, Bob must put something on the message that is unique to 
him. Bob could consider attaching a MAC for the signature, where the MAC is created 
by appending his key (unique to him) to the message, and then taking the hash. But for 
Alice to verify the signature, she must also have a copy of the key, in which case the 
key would not be unique to Bob. Thus, MACs are not going to get the job done here.
Recall that with public-key cryptography, Bob has both a public and private key, 
with both of these keys being unique to Bob. Thus, public-key cryptography is an 
excellent candidate for providing digital signatures. Let us now examine how it is done.
Suppose that Bob wants to digitally sign a document, m. We can think of 
the document as a file or a message that Bob is going to sign and send. As shown 
in Figure 8.10, to sign this document, Bob simply uses his private key, K-
B, to compute

K-
B(m). At first, it might seem odd that Bob is using his private key (which, as we saw 
in Section 8.2, was used to decrypt a message that had been encrypted with his public 
key) to sign a document. But recall that encryption and decryption are nothing more 
than mathematical operations (exponentiation to the power of e or d in RSA; see Sec-
tion 8.2) and recall that Bob‚Äôs goal is not to scramble or obscure the contents of the 
document, but rather to sign the document in a manner that is verifiable and nonforge-
able. Bob‚Äôs digital signature of the document is K-
B(m).
Does the digital signature K-
B(m) meet our requirements of being verifiable and 
nonforgeable? Suppose Alice has m and K-
B(m). She wants to prove in court (being 
litigious) that Bob had indeed signed the document and was the only person who 
could have possibly signed the document. Alice takes Bob‚Äôs public key, K+
B, and 
applies it to the digital signature, K-
B(m), associated with the document, m. That is, 
she computes K+
B(K-
B(m)), and voil√†, with a dramatic flurry, she produces m, which 
exactly matches the original document! Alice then argues that only Bob could have 
signed the document, for the following reasons:
‚Ä¢ Whoever signed the message must have used the private key, K-
B, in computing 
the signature K-
B(m), such that K+
B(K-
B(m)) = m.
‚Ä¢ The only person who could have known the private key, K-
B, is Bob. Recall from 
our discussion of RSA in Section 8.2 that knowing the public key, K+
B, is of no 
help in learning the private key, K-
B. Therefore, the only person who could know 
K-
B is the person who generated the pair of keys, (K+
B, K-
B), in the first place, Bob. 
(Note that this assumes, though, that Bob has not given K-
B to anyone, nor has 
anyone stolen K-
B from Bob.)
It is also important to note that if the original document, m, is ever modified to 
some alternate form, m¬¥, the signature that Bob created for m will not be valid for m¬¥, 
Figure 8.10 ‚ô¶ Creating a digital signature for a document
Encryption
algorithm
Message: m
Bob‚Äôs private
key, KB
‚Äì
Dear Alice:
Sorry I have been unable
to write for so long. Since
we.....
..........
..........
Bob
Signed message:
KB
‚Äì (m)
fadfg54986fgnzmcnv
T98734ngldskg02j
ser09tugkjdÔ¨Çg
..........

since K+
B(K-
B(m)) does not equal m¬¥. Thus, we see that digital signatures also provide 
message integrity, allowing the receiver to verify that the message was unaltered as 
well as the source of the message.
One concern with signing data by encryption is that encryption and decryption 
are computationally expensive. Given the overheads of encryption and decryption, 
signing data via complete encryption/decryption can be overkill. A more efficient 
approach is to introduce hash functions into the digital signature. Recall from 
 Section 8.3.2 that a hash algorithm takes a message, m, of arbitrary length and com-
putes a fixed-length ‚Äúfingerprint‚Äù of the message, denoted by H(m). Using a hash 
function, Bob signs the hash of a message rather than the message itself, that is, 
Bob calculates K-
B(H(m)). Since H(m) is generally much smaller than the original 
message m, the computational effort required to create the digital signature is sub-
stantially reduced.
In the context of Bob sending a message to Alice, Figure 8.11 provides a sum-
mary of the operational procedure of creating a digital signature. Bob puts his origi-
nal long message through a hash function. He then digitally signs the resulting hash 
with his private key. The original message (in cleartext) along with the digitally 
signed message digest (henceforth referred to as the digital signature) is then sent  
to Alice. Figure 8.12 provides a summary of the operational procedure of the signature. 
Alice applies the sender‚Äôs public key to the message to obtain a hash result. Alice also 
Figure 8.11 ‚ô¶ Sending a digitally signed message
Bob‚Äôs private
key, KB
‚Äì
Many-to-one
hash function
Long message
Dear Alice:
This is a VERY long letter
since there is so much to
say.....
..........
..........
Bob
Fixed-length
hash
Opgmdvboijrtnsd
gghPPdogm;lcvkb
Signed
hash
Package to send
to Alice
Fgkopdgoo69cmxw
54psdterma[asofmz
Encryption
algorithm

applies the hash function to the cleartext message to obtain a second hash result. If the 
two hashes match, then Alice can be sure about the integrity and author of the message.
Before moving on, let‚Äôs briefly compare digital signatures with MACs, since 
they have parallels, but also have important subtle differences. Both digital signa-
tures and MACs start with a message (or a document). To create a MAC out of the 
message, we append an authentication key to the message, and then take the hash of 
the result. Note that neither public key nor symmetric key encryption is involved in 
creating the MAC. To create a digital signature, we first take the hash of the message 
and then encrypt the message with our private key (using public key cryptography). 
Thus, a digital signature is a ‚Äúheavier‚Äù technique, since it requires an underlying 
Public Key Infrastructure (PKI) with certification authorities as described below. 
We‚Äôll see in Section 8.4 that PGP‚Äîa popular secure e-mail system‚Äîuses digital 
signatures for message integrity. We‚Äôve seen already that OSPF uses MACs for mes-
sage integrity. We‚Äôll see in Sections 8.5 and 8.6 that MACs are also used for popular 
transport-layer and network-layer security protocols.
Bob‚Äôs public
key, KB
+
Long message
Dear Alice:
This is a VERY long letter
since there is so much to
say.....
..........
..........
Bob
Fixed-length
hash
Opgmdvboijrtnsd
gghPPdogm;lcvkb
Signed
hash
Fgkopdgoo69cmxw
54psdterma[asofmz
Many-to-one
hash function
Compare
Fixed-length
hash
Opgmdvboijrtnsd
gghPPdogm;lcvkb
Encryption
algorithm
Figure 8.12 ‚ô¶ Verifying a signed message

Public Key Certification
An important application of digital signatures is public key certification, that is, 
certifying that a public key belongs to a specific entity. Public key certification is 
used in many popular secure networking protocols, including IPsec and TLS.
To gain insight into this problem, let‚Äôs consider an Internet-commerce version of 
the classic ‚Äúpizza prank.‚Äù Alice is in the pizza delivery business and accepts orders 
over the Internet. Bob, a pizza lover, sends Alice a plaintext message that includes 
his home address and the type of pizza he wants. In this message, Bob also includes 
a digital signature (that is, a signed hash of the original plaintext message) to prove to 
Alice that he is the true source of the message. To verify the signature, Alice obtains 
Bob‚Äôs public key (perhaps from a public key server or from the e-mail message) 
and checks the digital signature. In this manner she makes sure that Bob, rather than 
some adolescent prankster, placed the order.
This all sounds fine until clever Trudy comes along. As shown in Figure 8.13, 
Trudy is indulging in a prank. She sends a message to Alice in which she says she is 
Bob, gives Bob‚Äôs home address, and orders a pizza. In this message she also includes 
her (Trudy‚Äôs) public key, although Alice naturally assumes it is Bob‚Äôs public key. 
Trudy also attaches a digital signature, which was created with her own (Trudy‚Äôs) 
private key. After receiving the message, Alice applies Trudy‚Äôs public key (thinking 
that it is Bob‚Äôs) to the digital signature and concludes that the plaintext message was 
indeed created by Bob. Bob will be very surprised when the delivery person brings a 
pizza with pepperoni and anchovies to his home!
We see from this example that for public key cryptography to be useful, you 
need to be able to verify that you have the actual public key of the entity (person, 
router, browser, and so on) with whom you want to communicate. For example, 
when Alice wants to communicate with Bob using public key cryptography, she 
needs to verify that the public key that is supposed to be Bob‚Äôs is indeed Bob‚Äôs.
Binding a public key to a particular entity is typically done by a Certification 
Authority (CA), whose job is to validate identities and issue certificates. A CA has 
the following roles:
 1. A CA verifies that an entity (a person, a router, and so on) is who it says it is. 
There are no mandated procedures for how certification is done. When dealing 
with a CA, one must trust the CA to have performed a suitably rigorous identity 
verification. For example, if Trudy were able to walk into the Fly-by-Night CA 
and simply announce ‚ÄúI am Alice‚Äù and receive certificates associated with the 
identity of Alice, then one shouldn‚Äôt put much faith in public keys certified by 
the Fly-by-Night CA. On the other hand, one might (or might not!) be more 
willing to trust a CA that is part of a federal or state program. You can trust the 
identity associated with a public key only to the extent to which you can trust a 
CA and its identity verification techniques. What a tangled web of trust we spin!
 2. Once the CA verifies the identity of the entity, the CA creates a certificate 
that binds the public key of the entity to the identity. The certificate contains

Figure 8.13 ‚ô¶ Trudy masquerades as Bob using public key cryptography
Trudy‚Äôs private
key, KT
‚Äì
Trudy‚Äôs public
key, KT
+
Signed (using
Trudy's private key)
message digest
Fgkopdgoo69cmxw
54psdterma[asofmz
Message
Alice,
Deliver a pizza to me.
                               Bob
Many-to-one
hash function
Alice uses Trudy‚Äôs
public key, thinking
it is Bob‚Äôs, and
concludes the
message is from Bob
PIZZA
Encryption
algorithm
the public key and globally unique identifying information about the owner of 
the public key (for example, a human name or an IP address). The certificate is 
digitally signed by the CA. These steps are shown in Figure 8.14.
Let us now see how certificates can be used to combat pizza-ordering prank-
sters, like Trudy, and other undesirables. When Bob places his order he also sends his 
CA-signed certificate. Alice uses the CA‚Äôs public key to check the validity of Bob‚Äôs 
certificate and extract Bob‚Äôs public key.
Both the International Telecommunication Union (ITU) and the IETF have 
developed standards for CAs. ITU X.509 [ITU 2005a] specifies an authentication 
service as well as a specific syntax for certificates. [RFC 1422] describes CA-
based key management for use with secure Internet e-mail. It is compatible with 
X.509 but goes beyond X.509 by establishing procedures and conventions for a 
key management architecture. Table 8.4 describes some of the important fields in 
a certificate.

Table 8.4 ‚ô¶ Selected fields in an X.509 and RFC 1422 public key
Field Name
Description
Version
Version number of X.509 specification
Serial number
CA-issued unique identifier for a certificate
Signature
Specifies the algorithm used by CA to sign this certificate
Issuer name
Identity of CA issuing this certificate, in distinguished name (DN) [RFC 4514] format
Validity period
Start and end of period of validity for certificate
Subject name
Identity of entity whose public key is associated with this certificate, in DN format
Subject public key
The subject‚Äôs public key as well indication of the public key algorithm (and algorithm 
parameters) to be used with this key
8.4 End-Point Authentication
End-point authentication is the process of one entity proving its identity to another 
entity over a computer network, for example, a user proving its identity to an e-mail 
server. As humans, we authenticate each other in many ways: We recognize each 
 other‚Äôs faces when we meet, we recognize each other‚Äôs voices on the telephone, we are 
authenticated by the customs official who checks us against the picture on our passport.
Figure 8.14 ‚ô¶ Bob has his public key certified by the CA
Bob‚Äôs CA-signed
certiÔ¨Åcate containing
his public key, KB
+
CertiÔ¨Åcation
Authority (CA)
(KB
+, B)
CA‚Äôs private
key, KCA
‚Äì
Encryption
algorithm

In this section, we consider how one party can authenticate another party when 
the two are communicating over a network. We focus here on authenticating a ‚Äúlive‚Äù 
party, at the point in time when communication is actually occurring. A concrete 
example is a user authenticating him or herself to an e-mail server. This is a subtly 
different problem from proving that a message received at some point in the past did 
indeed come from that claimed sender, as studied in Section 8.3.
When performing authentication over the network, the communicating parties 
cannot rely on biometric information, such as a visual appearance or a voiceprint. 
Indeed, we will see in our later case studies that it is often network elements such as 
routers and client/server processes that must authenticate each other. Here, authen-
tication must be done solely on the basis of messages and data exchanged as part of 
an authentication protocol. Typically, an authentication protocol would run before 
the two communicating parties run some other protocol (for example, a reliable data 
transfer protocol, a routing information exchange protocol, or an e-mail protocol). 
The authentication protocol first establishes the identities of the parties to each  other‚Äôs 
satisfaction; only after authentication do the parties get down to the work at hand.
As in the case of our development of a reliable data transfer (rdt) protocol in 
Chapter 3, we will find it instructive here to develop various versions of an authentica-
tion protocol, which we will call ap (authentication protocol), and poke holes in each 
version as we proceed. (If you enjoy this stepwise evolution of a design, you might also 
enjoy [Bryant 1988], which recounts a fictitious narrative between designers of an open- 
network authentication system, and their discovery of the many subtle issues involved.)
Let‚Äôs assume that Alice needs to authenticate herself to Bob.
Perhaps the simplest authentication protocol we can imagine is one where 
Alice simply sends a message to Bob saying she is Alice. This protocol is shown in 
Figure 8.15. The flaw here is obvious‚Äîthere is no way for Bob actually to know that 
the person sending the message ‚ÄúI am Alice‚Äù is indeed Alice. For example, Trudy 
(the intruder) could just as well send such a message.
Figure 8.15 ‚ô¶ Protocol ap1.0 and a failure scenario
Alice
I am Alice
Bob
Trudy
Trudy
Alice
I am Alice
Bob

Authentication Protocol ap2.0
If Alice has a well-known network address (e.g., an IP address) from which she 
always communicates, Bob could attempt to authenticate Alice by verifying that 
the source address on the IP datagram carrying the authentication message matches 
Alice‚Äôs well-known address. In this case, Alice would be authenticated. This might 
stop a very network-naive intruder from impersonating Alice, but it wouldn‚Äôt stop 
the determined student studying this book, or many others!
From our study of the network and data link layers, we know that it is not that 
hard (for example, if one had access to the operating system code and could build 
one‚Äôs own operating system kernel, as is the case with Linux and several other 
freely available operating systems) to create an IP datagram, put whatever IP source 
address we want (for example, Alice‚Äôs well-known IP address) into the IP datagram, 
and send the datagram over the link-layer protocol to the first-hop router. From then 
on, the incorrectly source-addressed datagram would be dutifully forwarded to Bob. 
This approach, shown in Figure 8.16, is a form of IP spoofing. IP spoofing can be 
avoided if Trudy‚Äôs first-hop router is configured to forward only datagrams con-
taining Trudy‚Äôs IP source address [RFC 2827]. However, this capability is not uni-
versally deployed or enforced. Bob would thus be foolish to assume that Trudy‚Äôs 
network manager (who might be Trudy herself) had configured Trudy‚Äôs first-hop 
router to forward only appropriately addressed datagrams.
Authentication Protocol ap3.0
One classic approach to authentication is to use a secret password. The password is 
a shared secret between the authenticator and the person being authenticated. Gmail, 
Facebook, telnet, FTP, and many other services use password authentication. In pro-
tocol ap3.0, Alice thus sends her secret password to Bob, as shown in Figure 8.17.
Figure 8.16 ‚ô¶ Protocol ap2.0 and a failure scenario
Alice
I am Alice
Alice‚Äôs IP addr.
Bob
Trudy
Alice
I am Alice
Alice‚Äôs IP addr.
Bob
Trudy

Since passwords are so widely used, we might suspect that protocol ap3.0 is 
fairly secure. If so, we‚Äôd be wrong! The security flaw here is clear. If Trudy eaves-
drops on Alice‚Äôs communication, then she can learn Alice‚Äôs password. Lest you think 
this is unlikely, consider the fact that when you Telnet to another machine and log  
in, the login password is sent unencrypted to the Telnet server. Someone connected 
to the Telnet client or server‚Äôs LAN can possibly sniff (read and store) all packets 
transmitted on the LAN and thus steal the login password. In fact, this is a well-
known approach for stealing passwords (see, for example, [Jimenez 1997]). Such a 
threat is obviously very real, so ap3.0 clearly won‚Äôt do.
Authentication Protocol ap3.1
Our next idea for fixing ap3.0 is naturally to encrypt the password. By encrypting 
the password, we can prevent Trudy from learning Alice‚Äôs password. If we assume 
that Alice and Bob share a symmetric secret key, KA-B, then Alice can encrypt the 
password and send her identification message, ‚ÄúI am Alice,‚Äù and her encrypted 
password to Bob. Bob then decrypts the password and, assuming the password is cor-
rect, authenticates Alice. Bob feels comfortable in authenticating Alice since Alice 
not only knows the password, but also knows the shared secret key value needed to 
encrypt the password. Let‚Äôs call this protocol ap3.1.
While it is true that ap3.1 prevents Trudy from learning Alice‚Äôs password, the 
use of cryptography here does not solve the authentication problem. Bob is subject 
Figure 8.17 ‚ô¶ Protocol ap3.0 and a failure scenario
Alice
I am Alice,
password
OK
Bob
Trudy
Alice
I am Alice,
password
OK
Bob
Trudy
Tape recorder
Key:

to a playback attack: Trudy need only eavesdrop on Alice‚Äôs communication, record 
the encrypted version of the password, and play back the encrypted version of the 
password to Bob to pretend that she is Alice. The use of an encrypted password in 
ap3.1 doesn‚Äôt make the situation manifestly different from that of protocol ap3.0 in 
Figure 8.17.
Authentication Protocol ap4.0
The failure scenario in Figure 8.17 resulted from the fact that Bob could not distin-
guish between the original authentication of Alice and the later playback of Alice‚Äôs 
original authentication. That is, Bob could not tell if Alice was live (that is, was 
currently really on the other end of the connection) or whether the messages he was 
receiving were a recorded playback of a previous authentication of Alice. The very 
(very) observant reader will recall that the three-way TCP handshake protocol needed 
to address the same problem‚Äîthe server side of a TCP connection did not want to 
accept a connection if the received SYN segment was an old copy (retransmission) 
of a SYN segment from an earlier connection. How did the TCP server side solve 
the problem of determining whether the client was really live? It chose an initial 
sequence number that had not been used in a very long time, sent that number to the 
client, and then waited for the client to respond with an ACK segment containing that 
number. We can adopt the same idea here for authentication purposes.
A nonce is a number that a protocol will use only once in a lifetime. That is, 
once a protocol uses a nonce, it will never use that number again. Our ap4.0 protocol 
uses a nonce as follows:
 1. Alice sends the message ‚ÄúI am Alice‚Äù to Bob.
 2. Bob chooses a nonce, R, and sends it to Alice.
 3. Alice encrypts the nonce using Alice and Bob‚Äôs symmetric secret key, KA-B, and 
sends the encrypted nonce, KA-B (R), back to Bob. As in protocol ap3.1, it is the 
fact that Alice knows KA-B and uses it to encrypt a value that lets Bob know that the 
message he receives was generated by Alice. The nonce is used to ensure that 
Alice is live.
 4. Bob decrypts the received message. If the decrypted nonce equals the nonce he 
sent Alice, then Alice is authenticated.
Protocol ap4.0 is illustrated in Figure 8.18. By using the once-in-a-lifetime 
value, R, and then checking the returned value, KA-B (R), Bob can be sure that Alice 
is both who she says she is (since she knows the secret key value needed to encrypt 
R) and live (since she has encrypted the nonce, R, that Bob just created).
The use of a nonce and symmetric key cryptography forms the basis of ap4.0. A 
natural question is whether we can use a nonce and public key cryptography (rather 
than symmetric key cryptography) to solve the authentication problem. This issue is 
explored in the problems at the end of the chapter.

8.5 Securing E-Mail
In previous sections, we examined fundamental issues in network security, including 
symmetric key and public key cryptography, end-point authentication, key distribu-
tion, message integrity, and digital signatures. We are now going to examine how 
these tools are being used to provide security in the Internet.
Interestingly, it is possible to provide security services in any of the top four 
layers of the Internet protocol stack. When security is provided for a specific applica-
tion-layer protocol, the application using the protocol will enjoy one or more security 
services, such as confidentiality, authentication, or integrity. When security is pro-
vided for a transport-layer protocol, all applications that use that protocol enjoy the 
security services of the transport protocol. When security is provided at the network 
layer on a host-to-host basis, all transport-layer segments (and hence all application-
layer data) enjoy the security services of the network layer. When security is pro-
vided on a link basis, then the data in all frames traveling over the link receive the 
security services of the link.
In Sections 8.5 through 8.8, we examine how security tools are being used in 
the application, transport, network, and link layers. Being consistent with the general 
structure of this book, we begin at the top of the protocol stack and discuss security at 
the application layer. Our approach is to use a specific application, e-mail, as a case 
study for application-layer security. We then move down the protocol stack. We‚Äôll 
examine the TLS protocol (which provides security at the transport layer), IPsec 
(which provides security at the network layer), and the security of the IEEE 802.11 
wireless LAN protocol.
You might be wondering why security functionality is being provided at more 
than one layer in the Internet. Wouldn‚Äôt it suffice simply to provide the security 
functionality at the network layer and be done with it? There are two answers to this 
question. First, although security at the network layer can offer ‚Äúblanket coverage‚Äù 
by encrypting all the data in the datagrams (that is, all the transport-layer segments) 
Figure 8.18 ‚ô¶ Protocol ap4.0 and a failure scenario
Alice
R
KA‚ÄìB(R)
I am Alice
Bob

and by authenticating all the source IP addresses, it can‚Äôt provide user-level secu-
rity. For example, a commerce site cannot rely on IP-layer security to authenticate 
a customer who is purchasing goods at the commerce site. Thus, there is a need 
for security functionality at higher layers as well as blanket coverage at lower lay-
ers. Second, it is generally easier to deploy new Internet services, including security 
services, at the higher layers of the protocol stack. While waiting for security to be 
broadly deployed at the network layer, which is probably still many years in the 
future, many application developers ‚Äújust do it‚Äù and introduce security functional-
ity into their favorite applications. A classic example is Pretty Good Privacy (PGP), 
which provides secure e-mail (discussed later in this section). Requiring only client 
and server application code, PGP was one of the first security technologies to be 
broadly used in the Internet.
8.5.1 Secure E-Mail
We now use the cryptographic principles of Sections 8.2 through 8.3 to create a 
secure e-mail system. We create this high-level design in an incremental manner, 
at each step introducing new security services. When designing a secure e-mail sys-
tem, let us keep in mind the racy example introduced in Section 8.1‚Äîthe love affair 
between Alice and Bob. Imagine that Alice wants to send an e-mail message to Bob, 
and Trudy wants to intrude.
Before plowing ahead and designing a secure e-mail system for Alice and Bob, 
we should consider which security features would be most desirable for them. First 
and foremost is confidentiality. As discussed in Section 8.1, neither Alice nor Bob 
wants Trudy to read Alice‚Äôs e-mail message. The second feature that Alice and Bob 
would most likely want to see in the secure e-mail system is sender authentication. 
In particular, when Bob receives the message ‚ÄúI don‚Äôt love you anymore. 
I never want to see you again. Formerly yours, Alice,‚Äù 
he would naturally want to be sure that the message came from Alice and not from 
Trudy. Another feature that the two lovers would appreciate is message integrity, 
that is, assurance that the message Alice sends is not modified while en route to 
Bob. Finally, the e-mail system should provide receiver authentication; that is, Alice 
wants to make sure that she is indeed sending the letter to Bob and not to someone 
else (for example, Trudy) who is impersonating Bob.
So let‚Äôs begin by addressing the foremost concern, confidentiality. The most 
straightforward way to provide confidentiality is for Alice to encrypt the message 
with symmetric key technology (such as DES or AES) and for Bob to decrypt the 
message on receipt. As discussed in Section 8.2, if the symmetric key is long enough, 
and if only Alice and Bob have the key, then it is extremely difficult for anyone else 
(including Trudy) to read the message. Although this approach is straightforward, it 
has the fundamental difficulty that we discussed in Section 8.2‚Äîdistributing a sym-
metric key so that only Alice and Bob have copies of it. So we naturally consider an 
alternative approach‚Äîpublic key cryptography (using, for example, RSA). In the

public key approach, Bob makes his public key publicly available (e.g., in a public 
key server or on his personal Web page), Alice encrypts her message with Bob‚Äôs 
public key, and she sends the encrypted message to Bob‚Äôs e-mail address. When Bob 
receives the message, he simply decrypts it with his private key. Assuming that Alice 
knows for sure that the public key is Bob‚Äôs public key, this approach is an excellent 
means to provide the desired confidentiality. One problem, however, is that public 
key encryption is relatively inefficient, particularly for long messages.
To overcome the efficiency problem, let‚Äôs make use of a session key (discussed 
in Section 8.2.2). In particular, Alice (1) selects a random symmetric session key, KS, 
(2) encrypts her message, m, with the symmetric key, (3) encrypts the symmetric 
key with Bob‚Äôs public key, KB +, (4) concatenates the encrypted message and the 
encrypted symmetric key to form a ‚Äúpackage,‚Äù and (5) sends the package to Bob‚Äôs 
e-mail address. The steps are illustrated in Figure 8.19. (In this and the subsequent 
figures, the circled ‚Äú+‚Äù represents concatenation and the circled ‚Äú-‚Äù represents 
deconcatenation.) When Bob receives the package, he (1) uses his private key, K-
B, 
to obtain the symmetric key, KS, and (2) uses the symmetric key KS to decrypt the mes-
sage m.
Having designed a secure e-mail system that provides confidentiality, let‚Äôs now 
design another system that provides both sender authentication and message integ-
rity. We‚Äôll suppose, for the moment, that Alice and Bob are no longer concerned with 
confidentiality (they want to share their feelings with everyone!), and are concerned 
only about sender authentication and message integrity. To accomplish this task, we 
use digital signatures and message digests, as described in Section 8.3. Specifically, 
Alice (1) applies a hash function, H (e.g., MD5), to her message, m, to obtain a 
message digest, (2) signs the result of the hash function with her private key, K-
A, to 
create a digital signature, (3) concatenates the original (unencrypted) message with 
the signature to create a package, and (4) sends the package to Bob‚Äôs e-mail address. 
When Bob receives the package, he (1) applies Alice‚Äôs public key, K+
A, to the signed 
Figure 8.19 ‚ô¶  Alice used a symmetric session key, KS, to send a secret 
e-mail to Bob
KS(.)
KS(.)
KS(m)
KS(m)
KS
KS
KB
+(.)
KB
+(KS)
KB
+(KS)
m
m
+
‚Äì
Internet
KB
‚Äì(.)
Alice sends e-mail message m
Bob receives e-mail message m

message digest and (2) compares the result of this operation with his own hash, H, 
of the message. The steps are illustrated in Figure 8.20. As discussed in Section 8.3, 
if the two results are the same, Bob can be pretty confident that the message came 
from Alice and is unaltered.
Now let‚Äôs consider designing an e-mail system that provides confidentiality, 
sender authentication, and message integrity. This can be done by combining the 
procedures in Figures 8.19 and 8.20. Alice first creates a preliminary package, 
exactly as in Figure 8.20, that consists of her original message along with a digitally 
signed hash of the message. She then treats this preliminary package as a message in 
itself and sends this new message through the sender steps in Figure 8.19, creating a 
new package that is sent to Bob. The steps applied by Alice are shown in Figure 8.21. 
When Bob receives the package, he first applies his side of Figure 8.19 and then his 
Figure 8.20 ‚ô¶  Using hash functions and digital signatures to provide 
 sender authentication and message integrity
H(.)
KA
‚Äì(.)
KA
+(.)
KA
‚Äì(H(m))
KA
‚Äì(H(m))
m
m
m
+
‚Äì
Internet
Alice sends e-mail message m
Bob receives e-mail message m
H(.)
Compare
Figure 8.21 ‚ô¶  Alice uses symmetric key cyptography, public key 
 cryptography, a hash function, and a digital signature to 
 provide secrecy, sender authentication, and message integrity
H(.)
KA
‚Äì(.)
KS(.)
KS
KA
‚Äì(H(m))
m
m
+
+
to Internet
KB
+(.)

side of Figure 8.20. It should be clear that this design achieves the goal of provid-
ing confidentiality, sender authentication, and message integrity. Note that, in this 
scheme, Alice uses public key cryptography twice: once with her own private key 
and once with Bob‚Äôs public key. Similarly, Bob also uses public key cryptography 
twice‚Äîonce with his private key and once with Alice‚Äôs public key.
The secure e-mail design outlined in Figure 8.21 probably provides satisfactory  
security for most e-mail users for most occasions. However, there is still one important 
issue that remains to be addressed. The design in Figure 8.21 requires Alice to obtain 
Bob‚Äôs public key, and requires Bob to obtain Alice‚Äôs public key. The distribution  
of these public keys is a nontrivial problem. For example, Trudy might masquerade 
as Bob and give Alice her own public key while saying that it is Bob‚Äôs public key, 
enabling her to receive the message meant for Bob. As we learned in Section 8.3, a 
popular approach for securely distributing public keys is to certify the public keys 
using a CA.
8.5.2 PGP
Written by Phil Zimmermann in 1991, Pretty Good Privacy (PGP) is a nice exam-
ple of an e-mail encryption scheme [PGP 2020]. The PGP design is, in essence, the 
same as the design shown in Figure 8.21. Depending on the version, the PGP soft-
ware uses MD5 or SHA for calculating the message digest; CAST, triple-DES, or 
IDEA for symmetric key encryption; and RSA for the public key encryption.
When PGP is installed, the software creates a public key pair for the user. The 
public key can be posted on the user‚Äôs Web site or placed in a public key server. The 
private key is protected by the use of a password. The password has to be entered 
every time the user accesses the private key. PGP gives the user the option of dig-
itally signing the message, encrypting the message, or both digitally signing and 
encrypting. Figure 8.22 shows a PGP signed message. This message appears after  
the MIME header. The encoded data in the message is K-
A (H(m)), that is, the digi-
tally signed message digest. As we discussed above, in order for Bob to verify the 
integrity of the message, he needs to have access to Alice‚Äôs public key.
Figure 8.22 ‚ô¶ A PGP signed message
-----BEGIN PGP SIGNED MESSAGE-----
Hash:  SHA1
Bob:
Can I see you tonight?
Passionately yours, Alice
-----BEGIN PGP SIGNATURE-----
Version: PGP for Personal Privacy 5.0
Charset:
noconv
yhHJRHhGJGhgg/12EpJ+lo8gE4vB3mqJhFEvZP9t6n7G6m5Gw2
-----END PGP SIGNATURE-----

Figure 8.23 shows a secret PGP message. This message also appears after the 
MIME header. Of course, the plaintext message is not included within the secret e-mail 
message. When a sender (such as Alice) wants both confidentiality and integrity, PGP 
contains a message like that of Figure 8.23 within the message of Figure 8.22.
PGP also provides a mechanism for public key certification, but the mechanism 
is quite different from the more conventional CA. PGP public keys are certified by 
a web of trust. Alice herself can certify any key/username pair when she believes 
the pair really belong together. In addition, PGP permits Alice to say that she trusts 
another user to vouch for the authenticity of more keys. Some PGP users sign each 
other‚Äôs keys by holding key-signing parties. Users physically gather, exchange 
 public keys, and certify each other‚Äôs keys by signing them with their private keys.
8.6 Securing TCP Connections: TLS
In the previous section, we saw how cryptographic techniques can provide confiden-
tiality, data integrity, and end-point authentication to a specific application, namely, 
e-mail. In this section, we‚Äôll drop down a layer in the protocol stack and examine how 
cryptography can enhance TCP with security services, including confidentiality, data 
integrity, and end-point authentication. This enhanced version of TCP is commonly 
known as Transport Layer Security (TLS), which has been standardized by the 
IETF [RFC 4346]. An earlier and similar version of this protocol is SSL version 3.
The SSL protocol was originally designed by Netscape, but the basic ideas 
behind securing TCP had predated Netscape‚Äôs work (for example, see Woo [Woo 
1994]). Since its inception, SSL and its successor TLS have enjoyed broad deploy-
ment. TLS is supported by all popular Web browsers and Web servers, and it is used 
by Gmail and essentially all Internet commerce sites (including Amazon, eBay, and 
TaoBao). Hundreds of billions of dollars are spent over TLS every year. In fact, if you 
have ever purchased anything over the Internet with your credit card, the communica-
tion between your browser and the server for this purchase almost certainly went over 
TLS. (You can identify that TLS is being used by your browser when the URL begins 
with https: rather than http.)
Figure 8.23 ‚ô¶ A secret PGP message
-----BEGIN PGP MESSAGE-----
Version: PGP for Personal Privacy 5.0
u2R4d+/jKmn8Bc5+hgDsqAewsDfrGdszX68liKm5F6Gc4sDfcXyt
RfdS10juHgbcfDssWe7/K=lKhnMikLo0+1/BvcX4t==Ujk9PbcD4
Thdf2awQfgHbnmKlok8iy6gThlp
-----END PGP MESSAGE

To understand the need for TLS, let‚Äôs walk through a typical Internet commerce 
scenario. Bob is surfing the Web and arrives at the Alice Incorporated site, which is 
selling perfume. The Alice Incorporated site displays a form in which Bob is sup-
posed to enter the type of perfume and quantity desired, his address, and his pay-
ment card number. Bob enters this information, clicks on Submit, and expects to 
receive (via ordinary postal mail) the purchased perfumes; he also expects to receive 
a charge for his order in his next payment card statement. This all sounds good, but 
if no security measures are taken, Bob could be in for a few surprises.
‚Ä¢ If no confidentiality (encryption) is used, an intruder could intercept Bob‚Äôs order 
and obtain his payment card information. The intruder could then make purchases 
at Bob‚Äôs expense.
‚Ä¢ If no data integrity is used, an intruder could modify Bob‚Äôs order, having him 
purchase ten times more bottles of perfume than desired.
‚Ä¢ Finally, if no server authentication is used, a server could display Alice Incor-
porated‚Äôs famous logo when in actuality the site maintained by Trudy, who is 
masquerading as Alice Incorporated. After receiving Bob‚Äôs order, Trudy could 
take Bob‚Äôs money and run. Or Trudy could carry out an identity theft by collect-
ing Bob‚Äôs name, address, and credit card number.
TLS addresses these issues by enhancing TCP with confidentiality, data integrity, 
server authentication, and client authentication.
TLS is often used to provide security to transactions that take place over HTTP. 
However, because TLS secures TCP, it can be employed by any application that 
runs over TCP. TLS provides a simple Application Programmer Interface (API) with 
sockets, which is similar and analogous to TCP‚Äôs API. When an application wants to 
employ TLS, the application includes SSL classes/libraries. As shown in Figure 8.24, 
although TLS technically resides in the application layer, from the developer‚Äôs 
 perspective it is a transport protocol that provides TCP‚Äôs services enhanced with 
security services.
Figure 8.24 ‚ô¶  Although TLS technically resides in the application layer, 
from the developer‚Äôs perspective it is a transport-layer 
 protocol
TCP
TLS sublayer
IP
Application
Application
layer
TCP enhanced with TLS
TLS socket
TCP socket
TCP
IP
Application
TCP API
TCP socket

8.6.1 The Big Picture
We begin by describing a simplified version of TLS, one that will allow us to get a 
big-picture understanding of the why and how of TLS. We will refer to this simpli-
fied version of TLS as ‚Äúalmost-TLS.‚Äù After describing almost-TLS, in the next sub-
section we‚Äôll then describe the real TLS, filling in the details. Almost-TLS (and TLS) 
has three phases: handshake, key derivation, and data transfer. We now describe 
these three phases for a communication session between a client (Bob) and a server 
(Alice), with Alice having a private/public key pair and a certificate that binds her 
identity to her public key.
Handshake
During the handshake phase, Bob needs to (a) establish a TCP connection with Alice, 
(b) verify that Alice is really Alice, and (c) send Alice a master secret key, which 
will be used by both Alice and Bob to generate all the symmetric keys they need for 
the TLS session. These three steps are shown in Figure 8.25. Note that once the TCP 
connection is established, Bob sends Alice a hello message. Alice then responds with 
her certificate, which contains her public key. As discussed in Section 8.3, because 
the certificate has been certified by a CA, Bob knows for sure that the public key in 
the certificate belongs to Alice. Bob then generates a Master Secret (MS) (which will 
only be used for this TLS session), encrypts the MS with Alice‚Äôs public key to create 
Figure 8.25 ‚ô¶  The almost-TLS handshake, beginning with a TCP 
 connection
TCP SYN
TCP/SYNACK
Decrypts EMS with
KA
‚Äì to get MS
EMS = KA
+(MS)
TCP ACK
TLS hello
certiÔ¨Åcate
(b)
(a)
(c) 
Create Master
Secret (MS)

the Encrypted Master Secret (EMS), and sends the EMS to Alice. Alice decrypts the 
EMS with her private key to get the MS. After this phase, both Bob and Alice (and 
no one else) know the master secret for this TLS session.
Key Derivation
In principle, the MS, now shared by Bob and Alice, could be used as the symmetric 
session key for all subsequent encryption and data integrity checking. It is, however, 
generally considered safer for Alice and Bob to each use different cryptographic 
keys, and also to use different keys for encryption and integrity checking. Thus, both 
Alice and Bob use the MS to generate four keys:
‚Ä¢ EB = session encryption key for data sent from Bob to Alice
‚Ä¢ MB = session HMAC key for data sent from Bob to Alice, where HMAC [RFC 
2104] is a standardized hashed  message authentication code (MAC) that we 
encountered in section 8.3.2
‚Ä¢ EA = session encryption key for data sent from Alice to Bob
‚Ä¢ MA = session HMAC key for data sent from Alice to Bob
Alice and Bob each generate the four keys from the MS. This could be done by sim-
ply slicing the MS into four keys. (But in reality TLS it is a little more complicated, 
as we‚Äôll see.) At the end of the key derivation phase, both Alice and Bob have all four 
keys. The two encryption keys will be used to encrypt data; the two HMAC keys will 
be used to verify the integrity of the data.
Data Transfer
Now that Alice and Bob share the same four session keys (EB, MB, EA, and MA), 
they can start to send secured data to each other over the TCP connection. Since TCP 
is a byte-stream protocol, a natural approach would be for TLS to encrypt application 
data on the fly and then pass the encrypted data on the fly to TCP. But if we were to 
do this, where would we put the HMAC for the integrity check? We certainly do not 
want to wait until the end of the TCP session to verify the integrity of all of Bob‚Äôs 
data that was sent over the entire session! To address this issue, TLS breaks the data 
stream into records, appends an HMAC to each record for integrity checking, and 
then encrypts the record+HMAC. To create the HMAC, Bob inputs the record data 
along with the key MB into a hash function, as discussed in Section 8.3. To encrypt 
the package record+HMAC, Bob uses his session encryption key EB. This encrypted 
package is then passed to TCP for transport over the Internet.
Although this approach goes a long way, it still isn‚Äôt bullet-proof when it comes 
to providing data integrity for the entire message stream. In particular, suppose 
Trudy is a woman-in-the-middle and has the ability to insert, delete, and replace 
segments in the stream of TCP segments sent between Alice and Bob. Trudy, for

example, could capture two segments sent by Bob, reverse the order of the seg-
ments, adjust the TCP sequence numbers (which are not encrypted), and then send 
the two reverse-ordered segments to Alice. Assuming that each TCP segment 
encapsulates exactly one record, let‚Äôs now take a look at how Alice would process 
these segments.
 1. TCP running in Alice would think everything is fine and pass the two records 
to the TLS sublayer.
 2. TLS in Alice would decrypt the two records.
 3. TLS in Alice would use the HMAC in each record to verify the data integrity 
of the two records.
 4. TLS would then pass the decrypted byte streams of the two records to the 
application layer; but the complete byte stream received by Alice would not be 
in the correct order due to reversal of the records!
You are encouraged to walk through similar scenarios for when Trudy removes seg-
ments or when Trudy replays segments.
The solution to this problem, as you probably guessed, is to use sequence num-
bers. TLS does this as follows. Bob maintains a sequence number counter, which 
begins at zero and is incremented for each TLS record he sends. Bob doesn‚Äôt actually 
include a sequence number in the record itself, but when he calculates the HMAC, 
he includes the sequence number in the HMAC calculation. Thus, the HMAC is now 
a hash of the data plus the HMAC key MB plus the current sequence number. Alice 
tracks Bob‚Äôs sequence numbers, allowing her to verify the data integrity of a record 
by including the appropriate sequence number in the HMAC calculation. This use 
of TLS sequence numbers prevents Trudy from carrying out a woman-in-the-middle 
attack, such as reordering or replaying segments. (Why?)
TLS Record
The TLS record (as well as the almost-TLS record) is shown in Figure 8.26. The 
record consists of a type field, version field, length field, data field, and HMAC field. 
Note that the first three fields are not encrypted. The type field indicates whether the 
record is a handshake message or a message that contains application data. It is also 
used to close the TLS connection, as discussed below. TLS at the receiving end uses 
the length field to extract the TLS records out of the incoming TCP byte stream. The 
version field is self-explanatory.
Figure 8.26 ‚ô¶ Record format for TLS
Version
Length
Type
Data
HMAC
Encrypted with EB

8.6.2 A More Complete Picture
The previous subsection covered the almost-TLS protocol; it served to give us a 
basic understanding of the why and how of TLS. Now that we have a basic under-
standing, we can dig a little deeper and examine the essentials of the actual TLS pro-
tocol. In parallel to reading this description of the TLS protocol, you are encouraged 
to complete the Wireshark TLS lab, available at the textbook‚Äôs Web site.
TLS Handshake
SSL does not mandate that Alice and Bob use a specific symmetric key algorithm or 
a specific public-key algorithm. Instead, TLS allows Alice and Bob to agree on the 
cryptographic algorithms at the beginning of the TLS session, during the handshake 
phase. Additionally, during the handshake phase, Alice and Bob send nonces to each 
other, which are used in the creation of the session keys (EB, MB, EA, and MA). The 
steps of the real TLS handshake are as follows:
 1. The client sends a list of cryptographic algorithms it supports, along with a 
 client nonce.
 2. From the list, the server chooses a symmetric algorithm (for example, AES) 
and a public key algorithm (for example, RSA with a specific key length), and 
HMAC algorithm (MD5 or SHA-1) along with the HMAC keys. It sends back 
to the client its choices, as well as a certificate and a server nonce.
 3. The client verifies the certificate, extracts the server‚Äôs public key, generates a 
Pre-Master Secret (PMS), encrypts the PMS with the server‚Äôs public key, and 
sends the encrypted PMS to the server.
 4. Using the same key derivation function (as specified by the TLS standard), 
the client and server independently compute the Master Secret (MS) from the 
PMS and nonces. The MS is then sliced up to generate the two encryption and 
two HMAC keys. Furthermore, when the chosen symmetric cipher employs 
CBC (such as 3DES or AES), then two Initialization Vectors (IVs)‚Äîone for 
each side of the connection‚Äîare also obtained from the MS. Henceforth, all 
 messages sent between client and server are encrypted and authenticated (with 
the HMAC).
 5. The client sends the HMAC of all the handshake messages.
 6. The server sends the HMAC of all the handshake messages.
The last two steps protect the handshake from tampering. To see this, observe 
that in step 1, the client typically offers a list of algorithms‚Äîsome strong, some 
weak. This list of algorithms is sent in cleartext, since the encryption algorithms and 
keys have not yet been agreed upon. Trudy, as a woman-in-the-middle, could delete 
the stronger algorithms from the list, forcing the client to select a weak algorithm. To

prevent such a tampering attack, in step 5, the client sends the HMAC of the concat-
enation of all the handshake messages it sent and received. The server can compare 
this HMAC with the HMAC of the handshake messages it received and sent. If there 
is an inconsistency, the server can terminate the connection. Similarly, the server 
sends the HMAC of the handshake messages it has seen, allowing the client to check 
for inconsistencies.
You may be wondering why there are nonces in steps 1 and 2. Don‚Äôt sequence 
numbers suffice for preventing the segment replay attack? The answer is yes, but 
they don‚Äôt alone prevent the ‚Äúconnection replay attack.‚Äù Consider the following 
connection replay attack. Suppose Trudy sniffs all messages between Alice and 
Bob. The next day, Trudy masquerades as Bob and sends to Alice exactly the same 
sequence of messages that Bob sent to Alice on the previous day. If Alice doesn‚Äôt 
use nonces, she will respond with exactly the same sequence of messages she sent 
the previous day. Alice will not suspect any funny business, as each message she 
receives will pass the integrity check. If Alice is an e-commerce server, she will 
think that Bob is placing a second order (for exactly the same thing). On the other 
hand, by including a nonce in the protocol, Alice will send different nonces for each 
TCP session, causing the encryption keys to be different on the two days. Therefore, 
when Alice receives played-back TLS records from Trudy, the records will fail the 
integrity checks, and the bogus e-commerce transaction will not succeed. In sum-
mary, in TLS, nonces are used to defend against the ‚Äúconnection replay attack‚Äù and 
sequence numbers are used to defend against replaying individual packets during an 
ongoing session.
Connection Closure
At some point, either Bob or Alice will want to end the TLS session. One approach 
would be to let Bob end the TLS session by simply terminating the underlying TCP 
connection‚Äîthat is, by having Bob send a TCP FIN segment to Alice. But such a 
naive design sets the stage for the truncation attack whereby Trudy once again gets 
in the middle of an ongoing TLS session and ends the session early with a TCP 
FIN. If Trudy were to do this, Alice would think she received all of Bob‚Äôs data 
when  actuality she only received a portion of it. The solution to this problem is to 
indicate in the type field whether the record serves to terminate the TLS session. 
(Although the TLS type is sent in the clear, it is authenticated at the receiver using 
the record‚Äôs HMAC.) By including such a field, if Alice were to receive a TCP FIN 
before  receiving a closure TLS record, she would know that something funny was 
going on.
This completes our introduction to TLS. We‚Äôve seen that it uses many of the 
cryptography principles discussed in Sections 8.2 and 8.3. Readers who want to 
explore TLS on yet a deeper level can read Rescorla‚Äôs highly readable book on SSL/
TLS [Rescorla 2001].

8.7 Network-Layer Security: IPsec and Virtual 
Private Networks
The IP security protocol, more commonly known as IPsec, provides security at the 
network layer. IPsec secures IP datagrams between any two network-layer entities, 
including hosts and routers. As we will soon describe, many institutions (corpora-
tions, government branches, non-profit organizations, and so on) use IPsec to create 
virtual private networks (VPNs) that run over the public Internet.
Before getting into the specifics of IPsec, let‚Äôs step back and consider what 
it means to provide confidentiality at the network layer. With network-layer con-
fidentiality between a pair of network entities (for example, between two routers, 
between two hosts, or between a router and a host), the sending entity encrypts the 
payloads of all the datagrams it sends to the receiving entity. The encrypted payload 
could be a TCP segment, a UDP segment, an ICMP message, and so on. If such 
a network-layer service were in place, all data sent from one entity to the other‚Äî
including e-mail, Web pages, TCP handshake messages, and management mes-
sages (such as ICMP and SNMP)‚Äîwould be hidden from any third party that might 
be sniffing the network. For this reason, network-layer security is said to provide  
‚Äúblanket coverage.‚Äù
In addition to confidentiality, a network-layer security protocol could potentially 
provide other security services. For example, it could provide source authentication, 
so that the receiving entity can verify the source of the secured datagram. A network-
layer security protocol could provide data integrity, so that the receiving entity can 
check for any tampering of the datagram that may have occurred while the datagram 
was in transit. A network-layer security service could also provide replay-attack pre-
vention, meaning that Bob could detect any duplicate datagrams that an attacker 
might insert. We will soon see that IPsec indeed provides mechanisms for all these 
security services, that is, for confidentiality, source authentication, data  integrity, and 
replay-attack prevention.
8.7.1 IPsec and Virtual Private Networks (VPNs)
An institution that extends over multiple geographical regions often desires its own 
IP network, so that its hosts and servers can send data to each other in a secure and 
confidential manner. To achieve this goal, the institution could actually deploy a 
stand-alone physical network‚Äîincluding routers, links, and a DNS  infrastructure‚Äî
that is completely separate from the public Internet. Such a disjoint network, dedi-
cated to a particular institution, is called a private network. Not surprisingly, a 
private network can be very costly, as the institution needs to purchase, install, and 
maintain its own physical network infrastructure.
Instead of deploying and maintaining a private network, many institutions 
today create VPNs over the existing public Internet. With a VPN, the institution‚Äôs 
inter-office traffic is sent over the public Internet rather than over a physically

independent network. But to provide confidentiality, the inter-office traffic is 
encrypted before it enters the public Internet. A simple example of a VPN is shown 
in Figure¬†8.27. Here the institution consists of a headquarters, a branch office, and 
traveling salespersons that typically access the Internet from their hotel rooms. 
(There is only one salesperson shown in the figure.) In this VPN, whenever two 
hosts within headquarters send IP datagrams to each other or whenever two hosts 
within the branch office want to communicate, they use good-old vanilla IPv4 (that 
is, without IPsec services). However, when two of the institution‚Äôs hosts commu-
nicate over a path that traverses the public Internet, the traffic is encrypted before 
it enters the Internet.
To get a feel for how a VPN works, let‚Äôs walk through a simple example in the 
context of Figure 8.27. When a host in headquarters sends an IP datagram to a sales-
person in a hotel, the gateway router in headquarters converts the vanilla IPv4 data-
gram into an IPsec datagram and then forwards this IPsec datagram into the Internet. 
This IPsec datagram actually has a traditional IPv4 header, so that the routers in the 
public Internet process the datagram as if it were an ordinary IPv4 datagram‚Äîto 
them, the datagram is a perfectly ordinary datagram. But, as shown Figure 8.27, 
the payload of the IPsec datagram includes an IPsec header, which is used for IPsec 
processing; furthermore, the payload of the IPsec datagram is encrypted. When the 
Figure 8.27 ‚ô¶ Virtual private network (VPN)
IP
header
IPsec
header
Secure
payload
IP
header
IPsec
header
Secure
payload
IP
header
IPsec
header
Secure
payload
IP
header
Payload
IP
header
Payload
Laptop w/IPsec
Router
w/IPv4 and
IPsec
Router
w/IPv4 and
IPsec
Branch OfÔ¨Åce
Headquarters
Salesperson
in Hotel
Public
Internet

IPsec datagram arrives at the salesperson‚Äôs laptop, the OS in the laptop decrypts the 
payload (and provides other security services, such as verifying data integrity) and 
passes the unencrypted payload to the upper-layer protocol (for example, to TCP  
or UDP).
We have just given a high-level overview of how an institution can employ 
IPsec to create a VPN. To see the forest through the trees, we have brushed aside 
many important details. Let‚Äôs now take a closer look.
8.7.2 The AH and ESP Protocols
IPsec is a rather complex animal‚Äîit is defined in more than a dozen RFCs. Two 
important RFCs are RFC 4301, which describes the overall IP security architecture, 
and RFC 6071, which provides an overview of the IPsec protocol suite. Our goal in 
this textbook, as usual, is not simply to re-hash the dry and arcane RFCs, but instead 
take a more operational and pedagogic approach to describing the protocols.
In the IPsec protocol suite, there are two principal protocols: the Authentication 
Header (AH) protocol and the Encapsulation Security Payload (ESP) protocol. 
When a source IPsec entity (typically a host or a router) sends secure datagrams to a 
destination entity (also a host or a router), it does so with either the AH protocol or 
the ESP protocol. The AH protocol provides source authentication and data integrity 
but does not provide confidentiality. The ESP protocol provides source authentica-
tion, data integrity, and confidentiality. Because confidentiality is often critical for 
VPNs and other IPsec applications, the ESP protocol is much more widely used than 
the AH protocol. In order to de-mystify IPsec and avoid much of its complication, we 
will henceforth focus exclusively on the ESP protocol. Readers wanting to learn also 
about the AH protocol are encouraged to explore the RFCs and other online resources.
8.7.3 Security Associations
IPsec datagrams are sent between pairs of network entities, such as between two hosts, 
between two routers, or between a host and router. Before sending IPsec datagrams 
from source entity to destination entity, the source and destination entities create a 
network-layer logical connection. This logical connection is called a security asso-
ciation (SA). An SA is a simplex logical connection; that is, it is unidirectional from 
source to destination. If both entities want to send secure datagrams to each other, then 
two SAs (that is, two logical connections) need to be established, one in each direction.
For example, consider once again the institutional VPN in Figure 8.27. This 
institution consists of a headquarters office, a branch office and, say, n traveling 
salespersons. For the sake of example, let‚Äôs suppose that there is bi-directional IPsec 
traffic between headquarters and the branch office and bi-directional IPsec traffic 
between headquarters and the salespersons. In this VPN, how many SAs are there? 
To answer this question, note that there are two SAs between the headquarters gate-
way router and the branch-office gateway router (one in each direction); for each

salesperson‚Äôs laptop, there are two SAs between the headquarters gateway router 
and the laptop (again, one in each direction). So, in total, there are (2 + 2n) SAs. Keep 
in mind, however, that not all traffic sent into the Internet by the gateway routers or 
by the laptops will be IPsec secured. For example, a host in headquarters may want 
to access a Web server (such as Amazon or Google) in the public Internet. Thus, 
the gateway router (and the laptops) will emit into the Internet both vanilla IPv4 
 datagrams and secured IPsec datagrams.
Let‚Äôs now take a look ‚Äúinside‚Äù an SA. To make the discussion tangible and 
 concrete, let‚Äôs do this in the context of an SA from router R1 to router R2 in 
 Figure¬†8.28. (You can think of Router R1 as the headquarters gateway router and 
Router R2 as the branch office gateway router from Figure 8.27.) Router R1 will 
maintain state information about this SA, which will include:
‚Ä¢ A 32-bit identifier for the SA, called the Security Parameter Index (SPI)
‚Ä¢ The origin interface of the SA (in this case 200.168.1.100) and the destination 
interface of the SA (in this case 193.68.2.23)
‚Ä¢ The type of encryption to be used (for example, 3DES with CBC)
‚Ä¢ The encryption key
‚Ä¢ The type of integrity check (for example, HMAC with MD5)
‚Ä¢ The authentication key
Whenever router R1 needs to construct an IPsec datagram for forwarding over 
this SA, it accesses this state information to determine how it should authenticate 
and encrypt the datagram. Similarly, router R2 will maintain the same state informa-
tion for this SA and will use this information to authenticate and decrypt any IPsec 
datagram that arrives from the SA.
An IPsec entity (router or host) often maintains state information for many SAs. 
For example, in the VPN example in Figure 8.27 with n salespersons, the headquar-
ters gateway router maintains state information for (2 + 2n) SAs. An IPsec entity 
stores the state information for all of its SAs in its Security Association Database 
(SAD), which is a data structure in the entity‚Äôs OS kernel.
Figure 8.28 ‚ô¶ Security association (SA) from R1 to R2
Internet
SA
R1
172.16.1/24
Headquarters
Branch OfÔ¨Åce
200.168.1.100
193.68.2.23
172.16.2/24
R2

8.7.4 The IPsec Datagram
Having now described SAs, we can now describe the actual IPsec datagram. IPsec 
has two different packet forms, one for the so-called tunnel mode and the other for 
the so-called transport mode. The tunnel mode, being more appropriate for VPNs, 
is more widely deployed than the transport mode. In order to further de-mystify 
IPsec and avoid much of its complication, we henceforth focus exclusively on the 
tunnel mode. Once you have a solid grip on the tunnel mode, you should be able to 
easily learn about the transport mode on your own.
The packet format of the IPsec datagram is shown in Figure 8.29. You might 
think that packet formats are boring and insipid, but we will soon see that the IPsec 
datagram actually looks and tastes like a popular Tex-Mex delicacy! Let‚Äôs examine 
the IPsec fields in the context of Figure 8.28. Suppose router R1 receives an ordinary 
IPv4 datagram from host 172.16.1.17 (in the headquarters network) which is destined 
to host 172.16.2.48 (in the branch-office network). Router R1 uses the  following 
recipe to convert this ‚Äúoriginal IPv4 datagram‚Äù into an IPsec datagram:
‚Ä¢ Appends to the back of the original IPv4 datagram (which includes the original 
header fields!) an ‚ÄúESP trailer‚Äù field
‚Ä¢ Encrypts the result using the algorithm and key specified by the SA
‚Ä¢ Appends to the front of this encrypted quantity a field called ‚ÄúESP header‚Äù; the 
resulting package is called the ‚Äúenchilada‚Äù
‚Ä¢ Creates an authentication MAC over the whole enchilada using the algorithm and 
key specified in the SA
‚Ä¢ Appends the MAC to the back of the enchilada forming the payload
‚Ä¢ Finally, creates a brand new IP header with all the classic IPv4 header fields 
(together normally 20 bytes long), which it appends before the payload
Note that the resulting IPsec datagram is a bona fide IPv4 datagram, with the 
traditional IPv4 header fields followed by a payload. But in this case, the payload 
Figure 8.29 ‚ô¶ IPsec datagram format
New IP
header
ESP
header
ESP
trailer
ESP
MAC
Original
IP header
Original IP
datagram payload
Encrypted
‚ÄúEnchilada‚Äù authenticated
Pad
length
Padding
Next
header
SPI
Seq #

contains an ESP header, the original IP datagram, an ESP trailer, and an ESP authen-
tication field (with the original datagram and ESP trailer encrypted). The original IP 
datagram has 172.16.1.17 for the source IP address and 172.16.2.48 for the destina-
tion IP address. Because the IPsec datagram includes the original IP datagram, these 
addresses are included (and encrypted) as part of the payload of the IPsec packet. But 
what about the source and destination IP addresses that are in the new IP header, that 
is, in the left-most header of the IPsec datagram? As you might expect, they are set 
to the source and destination router interfaces at the two ends of the tunnels, namely, 
200.168.1.100 and 193.68.2.23. Also, the protocol number in this new IPv4 header 
field is not set to that of TCP, UDP, or SMTP, but instead to 50, designating that this 
is an IPsec datagram using the ESP protocol.
After R1 sends the IPsec datagram into the public Internet, it will pass through 
many routers before reaching R2. Each of these routers will process the datagram as if it 
were an ordinary datagram‚Äîthey are completely oblivious to the fact that the datagram 
is carrying IPsec-encrypted data. For these public Internet routers, because the destina-
tion IP address in the outer header is R2, the ultimate destination of the datagram is R2.
Having walked through an example of how an IPsec datagram is constructed, 
let‚Äôs now take a closer look at the ingredients in the enchilada. We see in Figure 8.29 
that the ESP trailer consists of three fields: padding; pad length; and next header. 
Recall that block ciphers require the message to be encrypted to be an integer mul-
tiple of the block length. Padding (consisting of meaningless bytes) is used so that 
when added to the original datagram (along with the pad length and next header 
fields), the resulting ‚Äúmessage‚Äù is an integer number of blocks. The pad-length field 
indicates to the receiving entity how much padding was inserted (and thus needs to 
be removed). The next header identifies the type (e.g., UDP) of data contained in the 
payload-data field. The payload data (typically the original IP datagram) and the ESP 
trailer are concatenated and then encrypted.
Appended to the front of this encrypted unit is the ESP header, which is sent in 
the clear and consists of two fields: the SPI and the sequence number field. The SPI 
indicates to the receiving entity the SA to which the datagram belongs; the receiving 
entity can then index its SAD with the SPI to determine the appropriate authentica-
tion/decryption algorithms and keys. The sequence number field is used to defend 
against replay attacks.
The sending entity also appends an authentication MAC. As stated earlier, the 
sending entity calculates a MAC over the whole enchilada (consisting of the ESP 
header, the original IP datagram, and the ESP trailer‚Äîwith the datagram and trailer 
being encrypted). Recall that to calculate a MAC, the sender appends a secret MAC 
key to the enchilada and then calculates a fixed-length hash of the result.
When R2 receives the IPsec datagram, R2 observes that the destination IP 
address of the datagram is R2 itself. R2 therefore processes the datagram. Because 
the protocol field (in the left-most IP header) is 50, R2 sees that it should apply 
IPsec ESP processing to the datagram. First, peering into the enchilada, R2 uses the 
SPI to determine to which SA the datagram belongs. Second, it calculates the MAC 
of the enchilada and verifies that the MAC is consistent with the value in the ESP

MAC field. If it is, it knows that the enchilada comes from R1 and has not been tam-
pered with. Third, it checks the sequence-number field to verify that the datagram is 
fresh (and not a replayed datagram). Fourth, it decrypts the encrypted unit using the 
decryption algorithm and key associated with the SA. Fifth, it removes padding and 
extracts the original, vanilla IP datagram. And finally, sixth, it forwards the original 
datagram into the branch office network toward its ultimate destination. Whew, what 
a complicated recipe, huh? Well no one ever said that preparing and unraveling an 
enchilada was easy!
There is actually another important subtlety that needs to be addressed. It centers 
on the following question: When R1 receives an (unsecured) datagram from a host 
in the headquarters network, and that datagram is destined to some destination IP 
address outside of headquarters, how does R1 know whether it should be converted to 
an IPsec datagram? And if it is to be processed by IPsec, how does R1 know which SA 
(of many SAs in its SAD) should be used to construct the IPsec datagram? The prob-
lem is solved as follows. Along with a SAD, the IPsec entity also maintains another 
data structure called the Security Policy Database (SPD). The SPD indicates what 
types of datagrams (as a function of source IP address, destination IP address, and 
protocol type) are to be IPsec processed; and for those that are to be IPsec processed, 
which SA should be used. In a sense, the information in a SPD indicates ‚Äúwhat‚Äù to 
do with an arriving datagram; the information in the SAD indicates ‚Äúhow‚Äù to do it.
Summary of IPsec Services
So what services does IPsec provide, exactly? Let us examine these services from 
the perspective of an attacker, say Trudy, who is a woman-in-the-middle, sitting 
somewhere on the path between R1 and R2 in Figure 8.28. Assume throughout this 
 discussion that Trudy does not know the authentication and encryption keys used 
by the SA. What can and cannot Trudy do? First, Trudy cannot see the original 
datagram. If fact, not only is the data in the original datagram hidden from Trudy, 
but so is the protocol number, the source IP address, and the destination IP address. 
For datagrams sent over the SA, Trudy only knows that the datagram originated 
from 200.168.1.100 and is destined to 193.68.2.23. She does not know if it is carry-
ing TCP, UDP, or ICMP data; she does not know if it is carrying HTTP, SMTP, or 
some other type of application data. This confidentiality thus goes a lot farther than 
SSL. Second, suppose Trudy tries to tamper with a datagram in the SA by flipping 
some of its bits. When this tampered datagram arrives at R2, it will fail the integ-
rity check (using the MAC), thwarting Trudy‚Äôs vicious attempts once again. Third, 
suppose Trudy tries to masquerade as R1, creating a IPsec datagram with source 
200.168.1.100 and destination 193.68.2.23. Trudy‚Äôs attack will be futile, as this 
datagram will again fail the integrity check at R2. Finally, because IPsec includes 
sequence numbers, Trudy will not be able create a successful replay attack. In sum-
mary, as claimed at the beginning of this section, IPsec provides‚Äîbetween any pair 
of devices that process packets through the network layer‚Äîconfidentiality, source 
authentication, data integrity, and replay-attack prevention.

8.7.5 IKE: Key Management in IPsec
When a VPN has a small number of end points (for example, just two routers as 
in Figure 8.28), the network administrator can manually enter the SA information 
(encryption/authentication algorithms and keys, and the SPIs) into the SADs of the 
endpoints. Such ‚Äúmanual keying‚Äù is clearly impractical for a large VPN, which 
may consist of hundreds or even thousands of IPsec routers and hosts. Large, geo-
graphically distributed deployments require an automated mechanism for creating 
the SAs. IPsec does this with the Internet Key Exchange (IKE) protocol, specified 
in RFC 5996.
IKE has some similarities with the handshake in SSL (see Section 8.6). Each 
IPsec entity has a certificate, which includes the entity‚Äôs public key. As with SSL, 
the IKE protocol has the two entities exchange certificates, negotiate authentication 
and encryption algorithms, and securely exchange key material for creating session 
keys in the IPsec SAs. Unlike SSL, IKE employs two phases to carry out these tasks.
Let‚Äôs investigate these two phases in the context of two routers, R1 and R2,  
in Figure 8.28. The first phase consists of two exchanges of message pairs between 
R1 and R2:
‚Ä¢ During the first exchange of messages, the two sides use Diffie-Hellman (see 
Homework Problems) to create a bi-directional IKE SA between the routers. To 
keep us all confused, this bi-directional IKE SA is entirely different from the 
IPsec SAs discussed in Sections 8.6.3 and 8.6.4. The IKE SA provides an authen-
ticated and encrypted channel between the two routers. During this first message-
pair exchange, keys are established for encryption and authentication for the IKE 
SA. Also established is a master secret that will be used to compute IPSec SA 
keys later in phase 2. Observe that during this first step, RSA public and private 
keys are not used. In particular, neither R1 nor R2 reveals its identity by signing 
a message with its private key.
‚Ä¢ During the second exchange of messages, both sides reveal their identity to each 
other by signing their messages. However, the identities are not revealed to a pas-
sive sniffer, since the messages are sent over the secured IKE SA channel. Also 
during this phase, the two sides negotiate the IPsec encryption and authentication 
algorithms to be employed by the IPsec SAs.
In phase 2 of IKE, the two sides create an SA in each direction. At the end of 
phase 2, the encryption and authentication session keys are established on both sides 
for the two SAs. The two sides can then use the SAs to send secured datagrams, as 
described in Sections 8.7.3 and 8.7.4. The primary motivation for having two phases 
in IKE is computational cost‚Äîsince the second phase doesn‚Äôt involve any public-
key cryptography, IKE can generate a large number of SAs between the two IPsec 
entities with relatively little computational cost.

8.8 Securing Wireless LANs and 4G/5G  
Cellular Networks
Security is a particularly important concern in wireless networks, where the attacker 
can sniff frames by simply positioning a receiving device anywhere within the trans-
mission range of the sender. This is true in both 802.11 wireless LANs, as well as in 
4G/5G cellular networks. In both settings, we‚Äôll see extensive use of the fundamental 
security techniques that we studied earlier in this chapter, including the use of nonces 
for authentication, cryptographic hashing for message integrity, derivation of shared 
symmetric keys for encrypting user-session data, and the extensive use of the AES 
encryption standard. We will also see, as is also the case in wired Internet settings, 
that wireless security protocols have undergone constant evolution, as researchers 
and hackers discover weaknesses and flaws in existing security protocols.
In this section, we present a brief introduction to wireless security in both 
802.11(WiFi) and 4G/5G settings. For a more in-depth treatment, see the highly read-
able 802.11 security books [Edney 2003; Wright 2015], the excellent coverage of 
3G/4G/5G security in [Sauter 2014], and recent surveys [Zou 2016; Kohlios 2018].
8.8.1  Authentication and Key Agreement in 802.11 
 Wireless LANs
Let‚Äôs start our discussion of 802.11 security by identifying two (of many [Zou 2016]) 
critical security concerns that we‚Äôll want an 802.11 network to handle:
‚Ä¢ Mutual authentication. Before a mobile device is allowed to fully attach to an 
access point and send datagrams to remote hosts, the network will typically want 
to first authenticate the device‚Äîto verify the identity of the mobile device attach-
ing to the network, and to check that device‚Äôs access privileges. Similarly, the 
mobile device will want to authenticate the network to which it is attaching‚Äîto 
make sure that the network it is joining is truly the network to which it wants to 
attach. This two-way authentication is known as mutual authentication.
‚Ä¢ Encryption. Since 802.11 frames will be exchanged over a wireless channel that 
can be sniffed and manipulated by potential ne‚Äôer do-wells, it will be important to 
encrypt link-level frames carrying user-level data exchanged between the mobile 
device and the access point (AP). Symmetric key encryption is used in practice, 
since encryption and decryption must be performed at high speeds. The mobile 
device and AP will need to derive the symmetric encryption and decryption keys 
to be used.
Figure 8.30 illustrates the scenario of a mobile device wishing to attach to 
an¬†802.11 network. We see the two usual network components that we encountered

in our earlier study of 802.11 networks in Section 7.3‚Äîthe mobile device and the 
AP. We also see a new architectural component, the authentication server (AS) that 
will be responsible for authenticating the mobile device. The authentication server 
might be co-located in the AP, but more typically and as shown in Figure 8.30, it is 
implemented as a separate server that provides authentication services. For authen-
tication, the AP serves as a pass-through device, relaying authentication and key 
derivation messages between the mobile device and the authentication server. Such 
an authentication server would typically provide authentication services for all APs 
within its network.
We can identify four distinct phases to the process of mutual authentication and 
encryption-key derivation and use in Figure 8.30:
 1. Discovery. In the discovery phase, the AP advertises its presence and the forms 
of authentication and encryption that can be provided to the mobile device. The 
mobile device then requests the specific forms of authentication and encryption 
that it desires. Although the device and AP are already exchanging messages, 
the device has not yet been authenticated nor does it have an encryption key 
for frame transmission over the wireless link, and so several more steps will be 
required before the device can communicate securely through the AP.
 2. Mutual authentication and shared symmetric key derivation. This is the most 
critical step in ‚Äúsecuring‚Äù the 802.11 channel. As we will see, this step is 
Mobile device
AP:
access point
Wired
network
AS:
authentication
server
1
Discovery of
security capabilities
4
Encrypted communication between
mobile device and a remote host via AP 
Shared symmetric session key
distribution
2
3
Mutual authentication and shared symmetric key derivation 
Figure 8.30 ‚ô¶  Mutual authentication and encryption-key derivation 
in WPA

greatly facilitated by assuming (which is true in practice in both 802.11 and 
4G/5G networks) that the authentication server and the mobile device already 
have a shared common secret before starting mutual authentication. In this 
step, the device and the authentication server will use this shared secret along 
with nonces (to prevent relay attacks) and cryptographic hashing (to ensure 
message integrity) in authenticating each other. They will also derive the 
shared session key to be used by the mobile device and the AP to encrypt 
frames transmitted over the 802.11 wireless link.
 3. Shared symmetric session key distribution. Since the symmetric encryption key 
is derived at the mobile device and the authentication server, a protocol will be 
needed for the authentication server to inform the AP of the shared symmetric 
session key. While this is rather straightforward, it still is a necessary step.
 4. Encrypted communication between mobile device and a remote host via the 
AP. This communication happens as we saw earlier in Section 7.3.2, with the 
link-layer frames sent between the mobile device and the AP being encrypted 
using the shared session key created and distributed by Steps 2 and 3. AES 
symmetric key cryptography, which we covered earlier in Section 8.2.1, is 
typically used in practice for encrypting/decrypting 802.11 frame data.
Mutual Authentication and Shared Symmetric Session Key Derivation
The topics of mutual authentication and shared symmetric session key derivation are the 
central components of 802.11 security. Since it is here that security flaws in various ear-
lier versions of 802.11 security have been discovered, let‚Äôs tackle these  challenges first.
The issue of 802.11security has attracted considerable attention in both technical 
circles and in the media. While there has been considerable discussion, there has been 
little debate‚Äîthere is universal agreement that the original 802.11security specifica-
tion known collectively as Wired Equivalent Privacy (WEP) contained a number 
of serious security flaws [Fluhrer 2001; Stubblefield 2002]. Once these flaws were 
discovered, public domain software was soon available exploiting these holes, mak-
ing users of WEP-secured 802.11 WLANs as open to security attacks as users who 
used no security features at all. Readers interested in learning about WEP can consult 
the references, as well as earlier editions of this textbook, which covered WEP. As 
always, retired material from this book is available on the Companion Website.
WiFi Protected Access (WPA1) was developed in 2003 by the WiFi Alli-
ance [WiFi 2020] to overcome WEP‚Äôs security flaws. The initial version of WPA1 
improved on WEP by introducing message integrity checks, and avoiding attacks 
that allowed a user to infer encryption keys after observing the stream of encrypted 
messages for a period of time. WPA1 soon gave way to WPA2, which mandated the 
use of AES symmetric key encryption.
At the heart of WPA is a four-way handshake protocol that performs both 
mutual authentication and shared symmetric session-key derivation. The handshake 
protocol is shown in Figure 8.31 in simplified form. Note that both the mobile device 
(M) and the authentication server (AS) begin knowing a shared secret key KAS-M

(e.g., a password). One of their tasks will be to derive a shared symmetric session-
key, KM-AP, which will be used to encrypt/decrypt frames that are later transmitted 
between the mobile device (M) and the AP.
Mutual authentication and shared symmetric session-key derivation are 
 accomplished in the first two steps, a and b, of the four-way handshake shown in 
Figure 8.31. Steps c and d are used to derive a second key used for group communi-
cation; see [Kohlios 2018; Zou 2016] for details.
 a. In this first step, the authentication server (AS) generates a nonce, NonceAS, 
and sends it to the mobile device. Recall from Section 8.4 that nonces are  
used to avoid playback attacks and prove the ‚Äúliveness‚Äù of the other side  
being authenticated.
 b. The mobile device, M, receives the nonce, NonceAS, from the AS and gener-
ates its own nonce, NonceM. The mobile device then generates the symmetric 
shared session key, KM-AP, using NonceAS, NonceM, the initial shared secret 
key KAS-M, its MAC address, and the MAC address of the AS. It then sends 
its nonce, NonceM, and an HMAC-signed (see Figure 8.9) value that encodes 
NonceAS and the original shared secret.¬†
 
 The AS receives this message from M. By looking at the HMAC-signed ver-
sion of the nonce it had just recently sent, NonceAS, the authentication server 
knows the mobile device is live; because the mobile device was able to encrypt 
using the shared secret key, KAS-M, the AS also knows that the mobile device 
M:
Mobile device
AS:
authentication
server
a
b
NonceAS
Derive session key KM-AP
using KAS-M, NonceAS,
NonceM
Derive session key KM-AP
using KAS-M, NonceAS,
NonceM
KAS-M
KAS-M
NonceM, HMAC(f(KAS-M,NonceAS))
c
d
c, d: used for group key derivation
Figure 8.31 ‚ô¶ The WPA2 four-way handshake

is indeed who it claims to be (i.e., a device that knows the shared initial 
secret). The AS has thus authenticated the mobile device! The AS can also 
now perform the exact same computation as the mobile device to derive the 
shared symmetric session-key, KM-AP, using the NonceM it received, NonceAS, 
the initial shared secret key KAS-M, its MAC address and the MAC address of 
the mobile device. At this point both the mobile device and the authentication 
server have computed the same shared symmetric key, KM-AP, which will be 
used to encrypt/decrypt frames transmitted between the mobile device and the 
AP. The AS informs the AP of this key value in Step 3 in Figure 8.30.
WPA3 was released in June 2018 as an update to WPA2. The update addresses an 
attack on the four-way handshake protocol that could induce the reuse of previously 
used nonces [Vanhoef 2017] but still permits the use of the four-way handshake as a 
legacy protocol and includes longer key lengths, among other changes [WiFi 2019].
802.11 Security Messaging Protocols
Figure 8.32 shows the protocols used to implement the 802.11 security framework 
discussed above. The Extensible Authentication Protocol (EAP) [RFC 3748] defines 
the end-to-end message formats used in a simple request/response mode of interaction 
between the mobile device and authentication server, and are certified under WPA2. 
As shown in Figure 8.32, EAP messages are encapsulated using EAPoL (EAP over 
LAN) and sent over the 802.11 wireless link. These EAP messages are then decap-
sulated at the access point, and then re-encapsulated using the RADIUS protocol for 
M:
Mobile device
AP:
access point
Wired
network
AS:
authentication
server
EAP TLS
EAP
EAP over LAN (EAPoL)
RADIUS
IEEE 802.11
UDP/IP
Figure 8.32 ‚ô¶  EAP is an end-to-end protocol. EAP messages are  
encapsulated using EAPoL over the wireless link  
between the mobile device and the access point,  
and using RADIUS over UDP/IP between the access  
point and the authentication server

transmission over UDP/IP to the authentication server. While the RADIUS server 
and protocol [RFC 2865] are not required, they are de facto standard components. 
The recently standardized DIAMETER protocol [RFC 3588] is projected to eventu-
ally replace RADIUS in the future.
8.8.2  Authentication and Key Agreement in  
4G/5G Cellular Networks
In this section, we describe mutual authentication and key-generation mechanisms 
in 4G/5G networks. Many of the approaches we‚Äôll encounter here parallel those 
that we just studied in 802.11 networks, with the notable exception that in 4G/5G 
 networks, mobile devices may be attached to their home network (i.e., the cellular 
carrier  network to which they are subscribed), or may be roaming on a visited net-
work. In this latter case, the visited and home networks will need to interact when 
authenticating a mobile device and generating encryption keys. Before continuing, 
you may want to re-familiarize yourself with 4G/5G network architecture by re-
reading Sections 7.4 and 7.7.1.
The goals of mutual authentication and key generation are the same in the 4G/5G 
setting as in the 802.11 setting. In order to encrypt the contents of frames being 
transmitted over the wireless channel, the mobile device and base station will need 
to derive a shared symmetric encryption key. In addition, the network to which the 
mobile device is attaching will need to authenticate the device‚Äôs identity and check 
its access privileges. Similarly, the mobile device will also want to authenticate the 
network to which it is attaching. While the network‚Äôs need to authenticate a mobile 
device may be obvious, the need for authentication in the reverse direction may not 
be so clear. However, there are documented cases of ne‚Äôer-do-wells operating rogue 
cellular base stations that entice unsuspecting mobile devices to attach to the rogue 
network, exposing a device to a number of attacks [Li 2017]. So, as in the case of 
802.11 WLANs, a mobile device should exercise abundant caution when attaching 
to a cellular network!
Figure 8.33 illustrates the scenario of mobile device attaching to a 4G cellu-
lar network. At the top of Figure 8.33, we see many of the 4G components that we 
encountered earlier in Section 7.4‚Äîthe mobile device (M), the base station (BS), the 
mobility management entity (MME) in the network to which the mobile device wants 
to attach, and the home subscriber service (HSS) in the mobile device‚Äôs home net-
work. A comparison of Figures 8.30 and 8.33 shows the similarities and differences 
between the 802.11 and 4G security settings. We again see a mobile device and a 
base station; the user session-key derived during network attachment, KBS-M, will be 
used to encrypt/decrypt frames transmitted over their wireless link. The 4G MME and 
HSS together will play a role similar to that of the authentication server in the 802.11 
setting. Note that the HSS and the mobile device also share a common secret, KHSS-M, 
known to both entities before authentication begins. This key is stored in the mobile 
device‚Äôs SIM card, and in the HSS database in the mobile device‚Äôs home network.

The 4G Authentication and Key Agreement (AKA) protocol consists of the fol-
lowing steps:
 a. Authentication request to HSS. When the mobile device first requests, via a 
base station, to attach to the network, it sends an attach message containing its 
international mobile subscriber identity (IMSI) that is relayed to the Mobility 
Management Entity (MME). The MME will then send the IMSI and informa-
tion about the visited network (shown as ‚ÄúVN info‚Äù in Figure 8.33) to the 
Home Subscriber Service (HSS) in the device‚Äôs home network. In Section 7.4, 
we described how the MME is able to communicate with the HSS through the 
all-IP global network of interconnected cellular networks.
 b. Authentication response from HSS. The HSS performs cryptographic 
 operations using the shared-in-advance secret key, KHSS-M, to derive an 
 authentication token, auth_token, and an expected authentication response 
token, xresHSS. auth_token contains information encrypted by the HSS  
using KHSS-M that will allow the mobile device to know that whoever 
computed auth_token knows the secret key. For example, suppose the 
HSS  computes KHSS-M(IMSI), that is, encrypts the device‚Äôs IMSI using KHSS-M 
and sends that value as auth_token. When the mobile device receives that 
encrypted value and uses its secret key to decrypt this value, that is, to  compute 
Mobility Management
Entity (MME) 
Base Station
(BS) 
HSS
Home Subscriber
Service (HSS)
Mobile Device
(M)   
Visited Network 
 Home Network
KHSS-M
attach
attach
AUTH_REQ (IMSI, VN info)
auth token
auth token
resM
resM
d
OK, keys
OK
key derivation
KHSS-M
KBS-M
c
e
d
db
a
AUTH_RESP (auth token,xresHSS,keys)
Figure 8.33 ‚ô¶  Mutual authentication and key agreement in a 4G LTE 
cellular network

KHSS-M(KHSS-M(IMSI)) 5 IMSI, it knows that the HSS that generated auth_
token knows its secret key. The mobile device can thus authenticate the HSS.
 
 The expected authentication response token, xresHSS, contains a value that the 
mobile device will need to be able to compute (using KHSS-M) and return to the 
MME to prove that it (the mobile device) knows the secret key, thus authenti-
cating the mobile device to the MME.
 
 Note that the MME only plays a middleman role here, receiving the authenti-
cation response message, keeping xresHSS for later use, extracting the authen-
tication token and forwarding it to the mobile device. In particular it need not 
know, and will not learn, the secret key, KHSS-M.
 c. Authentication response from mobile device. The mobile device receives 
auth_token and computes KHSS-M(KHSS-M(IMSI)) 5 IMSI, thus authenticating 
the HSS. The mobile device then computes a value resM‚Äîusing its secret key 
to make the exact same cryptographic calculation that the HSS had made to 
compute xresHSS‚Äîand sends this value to the MME.
 d. Mobile device authentication. The MMS compares the mobile-computed value 
of resM with the HSS-computed value of xresHSS. If they match, the mobile 
device is authenticated, since the mobile has proven to the MME that it and the 
HSS both know the common secret key. The MMS informs the base station 
and mobile device that mutual authentication is complete, and sends the base 
station keys that will be used in step e.
 e. Data plane and control plane key derivation. The mobile device and the base 
station will each determine the keys used for encrypting/decrypting their frame 
transmissions over the wireless channel. Separate keys will be derived for data 
plane and control plane frame transmissions. The AES encryption algorithm 
that we saw in use in 802.11 networks is also used in 4G/5G networks.
Our discussion above has focused on authentication and key agreement in 4G 
networks. Although much of the 4G security is being carried forward into 5G, there 
are some important changes:
‚Ä¢ First, note that in our discussion above that it is the MME in the visited network 
that makes the authentication decision. A significant change underway in 5G 
network security is to allow authentication services to be provided by the home 
network, with the visited network playing an even smaller middleman role. While 
the visited network may still reject an authentication from a mobile device, it is up 
to the home network to accept the authentication request in this new 5G scenario.
‚Ä¢ 5G networks will support the Authentication and Key Agreement (AKA) proto-
col described above, as well as two new additional protocols for authentication 
and key agreement. One of these, known as AKA¬ø, is closely related to the 4G 
AKA protocol. It also uses the shared-in-advance secret key, KHSS-M. However, 
since it uses the EAP protocol that we encountered earlier in Figure 8.33 in the 
context of 802.11 authentication, 5G AKA¬ø has different message flows than that

of 4G AKA. The second new 5G protocol is meant for an IoT environment, and 
does not require a shared-in-advance secret key.
‚Ä¢ An additional change in 5G is to use public key cryptography techniques to 
encrypt a device‚Äôs permanent identity (i.e., its IMSI) so that it is never transmit-
ted in cleartext.
In this section, we have only briefly overviewed mutual authentication and key 
agreement in 4G /5G networks. As we have seen, they make extensive use of the 
security techniques that we studied earlier in this chapter. More details on 4G/5G 
security can be found in [3GPP SAE 2019; Cable Labs 2019; Cichonski 2017].
8.9 Operational Security: Firewalls and Intrusion 
Detection Systems
We‚Äôve seen throughout this chapter that the Internet is not a very safe place‚Äîbad 
guys are out there, wreaking all sorts of havoc. Given the hostile nature of the Inter-
net, let‚Äôs now consider an organization‚Äôs network and the network administrator who 
administers it. From a network administrator‚Äôs point of view, the world divides quite 
neatly into two camps‚Äîthe good guys (who belong to the organization‚Äôs network, 
and who should be able to access resources inside the organization‚Äôs network in a 
relatively unconstrained manner) and the bad guys (everyone else, whose access to 
network resources must be carefully scrutinized). In many organizations, ranging 
from medieval castles to modern corporate office buildings, there is a single point of 
entry/exit where both good guys and bad guys entering and leaving the organization 
are security-checked. In a castle, this was done at a gate at one end of the drawbridge; 
in a corporate building, this is done at the security desk. In a computer network, 
when traffic entering/leaving a network is security-checked, logged, dropped, or for-
warded, it is done by operational devices known as firewalls, intrusion detection 
systems (IDSs), and intrusion prevention systems (IPSs).
8.9.1 Firewalls
A firewall is a combination of hardware and software that isolates an organization‚Äôs 
internal network from the Internet at large, allowing some packets to pass and block-
ing others. A firewall allows a network administrator to control access between the 
outside world and resources within the administered network by managing the traffic 
flow to and from these resources. A firewall has three goals:
‚Ä¢ All traffic from outside to inside, and vice versa, passes through the firewall. 
Figure 8.34 shows a firewall, sitting squarely at the boundary between the admin-
istered network and the rest of the Internet. While large organizations may use

multiple levels of firewalls or distributed firewalls [Skoudis 2006], locating a 
firewall at a single access point to the network, as shown in Figure 8.34, makes it 
easier to manage and enforce a security-access policy.
‚Ä¢ Only authorized traffic, as defined by the local security policy, will be allowed 
to pass. With all traffic entering and leaving the institutional network passing 
through the firewall, the firewall can restrict access to authorized traffic.
‚Ä¢ The firewall itself is immune to penetration. The firewall itself is a device con-
nected to the network. If not designed or installed properly, it can be compro-
mised, in which case it provides only a false sense of security (which is worse 
than no firewall at all!).
Cisco and Check Point are two of the leading firewall vendors today. You can also easily 
create a firewall (packet filter) from a Linux box using iptables (public-domain software 
that is normally shipped with Linux). Furthermore, as discussed in Chapters 4 and 5, fire-
walls are now frequently implemented in routers and controlled remotely using SDNs.
Firewalls can be classified in three categories: traditional packet filters, state-
ful filters, and application gateways. We‚Äôll cover each of these in turn in the fol-
lowing subsections.
Figure 8.34 ‚ô¶  Firewall placement between the administered network and 
the outside world
Administered
network
Firewall
Public
Internet

Traditional Packet Filters
As shown in Figure 8.34, an organization typically has a gateway router connecting 
its internal network to its ISP (and hence to the larger public Internet). All traffic leav-
ing and entering the internal network passes through this router, and it is at this router 
where packet filtering occurs. A packet filter examines each datagram in isolation, 
determining whether the datagram should be allowed to pass or should be dropped 
based on administrator-specific rules. Filtering decisions are typically based on:
‚Ä¢ IP source or destination address
‚Ä¢ Protocol type in IP datagram field: TCP, UDP, ICMP, OSPF, and so on
‚Ä¢ TCP or UDP source and destination port
‚Ä¢ TCP flag bits: SYN, ACK, and so on
‚Ä¢ ICMP message type
‚Ä¢ Different rules for datagrams leaving and entering the network
‚Ä¢ Different rules for the different router interfaces
A network administrator configures the firewall based on the policy of the organ-
ization. The policy may take user productivity and bandwidth usage into account as 
well as the security concerns of an organization. Table 8.5 lists a number of possible 
polices an organization may have, and how they would be addressed with a packet 
filter. For example, if the organization doesn‚Äôt want any incoming TCP connections 
except those for its public Web server, it can block all incoming TCP SYN segments 
except TCP SYN segments with destination port 80 and the destination IP address 
corresponding to the Web server. If the organization doesn‚Äôt want its users to monop-
olize access bandwidth with Internet radio applications, it can block all not-critical 
Table 8.5 ‚ô¶  Policies and corresponding filtering rules for an organization‚Äôs 
network 130.207/16 with Web server at 130.207.244.203
Policy
Firewall Setting
No outside Web access.
Drop all outgoing packets to any IP address, port 80.
No incoming TCP connections, except those  
for organization‚Äôs public Web server only.
Drop all incoming TCP SYN packets to any IP except 
130.207.244.203, port 80.
Prevent Web-radios from eating up the  
available bandwidth.
Drop all incoming UDP packets‚Äîexcept DNS packets.
Prevent your network from being used for a  
smurf DoS attack.
Drop all ICMP ping packets going to a ‚Äúbroadcast‚Äù 
address (eg 130.207.255.255).
Prevent your network from being tracerouted.
Drop all outgoing ICMP TTL expired traffic.

UDP traffic (since Internet radio is often sent over UDP). If the organization doesn‚Äôt 
want its internal network to be mapped (tracerouted) by an outsider, it can block all 
ICMP TTL expired messages leaving the organization‚Äôs network.
A filtering policy can be based on a combination of addresses and port numbers. 
For example, a filtering router could forward all Telnet datagrams (those with a port 
number of 23) except those going to and coming from a list of specific IP addresses. 
This policy permits Telnet connections to and from hosts on the allowed list. Unfor-
tunately, basing the policy on external addresses provides no protection against data-
grams that have had their source addresses spoofed.
Filtering can also be based on whether or not the TCP ACK bit is set. This trick 
is quite useful if an organization wants to let its internal clients connect to external 
servers but wants to prevent external clients from connecting to internal servers. 
Recall from Section 3.5 that the first segment in every TCP connection has the ACK 
bit set to 0, whereas all the other segments in the connection have the ACK bit set to 1.  
Thus, if an organization wants to prevent external clients from initiating connections 
to internal servers, it simply filters all incoming segments with the ACK bit set to 0. 
This policy kills all TCP connections originating from the outside, but permits con-
nections originating internally.
Firewall rules are implemented in routers with access control lists, with each 
router interface having its own list. An example of an access control list for an organ-
ization 222.22/16 is shown in Table 8.6. This access control list is for an interface 
that connects the router to the organization‚Äôs external ISPs. Rules are applied to each 
datagram that passes through the interface from top to bottom. The first two rules 
together allow internal users to surf the Web: The first rule allows any TCP packet 
with destination port 80 to leave the organization‚Äôs network; the second rule allows 
any TCP packet with source port 80 and the ACK bit set to enter the organization‚Äôs 
network. Note that if an external source attempts to establish a TCP connection with 
Table 8.6 ‚ô¶ An access control list for a router interface
action
source address
dest address
protocol
source port
dest port
flag bit
allow
222.22/16
outside of 
222.22/16
TCP
> 1023
80
any
allow
outside of 
222.22/16
222.22/16
TCP
80
> 1023
ACK
allow
222.22/16
outside of 
222.22/16
UDP
> 1023
53
‚Äî
allow
outside of 
222.22/16
222.22/16
UDP
53
> 1023
‚Äî
deny
all
all
all
all
all
all

an internal host, the connection will be blocked, even if the source or destination 
port is 80. The second two rules together allow DNS packets to enter and leave the 
organization‚Äôs network. In summary, this rather restrictive access control list blocks 
all traffic except Web traffic initiated from within the organization and DNS traffic. 
[CERT Filtering 2012] provides a list of recommended port/protocol packet filterings 
to avoid a number of well-known security holes in existing network applications.
Readers with sharp memories may recall we encountered access control lists 
similar to Table 8.6 when we studied generalized forwarding in Section 4.4.3 of 
Chapter 4. Indeed, we provided an example there of how generalized forwarding 
rules can be used to build a packet-filtering firewall.
Stateful Packet Filters
In a traditional packet filter, filtering decisions are made on each packet in isola-
tion. Stateful filters actually track TCP connections, and use this knowledge to make 
 filtering decisions.
To understand stateful filters, let‚Äôs reexamine the access control list in 
Table 8.6. Although rather restrictive, the access control list in Table 8.6 neverthe-
less allows any packet arriving from the outside with ACK = 1 and source port 80 
to get through the filter. Such packets could be used by attackers in attempts to 
crash internal systems with malformed packets, carry out denial-of-service attacks, 
or map the internal network. The naive solution is to block TCP ACK packets as 
well, but such an approach would prevent the organization‚Äôs internal users from 
surfing the Web.
Stateful filters solve this problem by tracking all ongoing TCP connections in 
a connection table. This is possible because the firewall can observe the beginning 
of a new connection by observing a three-way handshake (SYN, SYNACK, and 
ACK); and it can observe the end of a connection when it sees a FIN packet for 
the connection. The firewall can also (conservatively) assume that the connection 
is over when it hasn‚Äôt seen any activity over the connection for, say, 60 seconds. 
An example connection table for a firewall is shown in Table 8.7. This connec-
tion table indicates that there are currently three ongoing TCP connections, all of 
which have been initiated from within the organization. Additionally, the stateful 
filter includes a new column, ‚Äúcheck connection,‚Äù in its access control list, as 
Table 8.7 ‚ô¶ Connection table for stateful filter
source address
dest address
source port
dest port
222.22.1.7
37.96.87.123
12699
80
222.22.93.2
199.1.205.23
37654
80
222.22.65.143
203.77.240.43
48712
80

shown in Table 8.8. Note that Table 8.8 is identical to the access control list in 
Table 8.6, except now it indicates that the connection should be checked for two 
of the rules.
Let‚Äôs walk through some examples to see how the connection table and the 
extended access control list work hand-in-hand. Suppose an attacker attempts 
to send a malformed packet into the organization‚Äôs network by sending a data-
gram with TCP source port 80 and with the ACK flag set. Further suppose that 
this packet has source port number 12543 and source IP address 150.23.23.155. 
When this packet reaches the firewall, the firewall checks the access control list in 
Table 8.7, which indicates that the connection table must also be checked before 
permitting this packet to enter the organization‚Äôs network. The firewall duly 
checks the connection table, sees that this packet is not part of an ongoing TCP 
connection, and rejects the packet. As a second example, suppose that an internal 
user wants to surf an external Web site. Because this user first sends a TCP SYN 
segment, the user‚Äôs TCP connection gets recorded in the connection table. When 
the Web server sends back packets (with the ACK bit necessarily set), the fire-
wall checks the table and sees that a corresponding connection is in progress. The 
firewall will thus let these packets pass, thereby not interfering with the internal 
user‚Äôs Web surfing activity.
Application Gateway
In the examples above, we have seen that packet-level filtering allows an organiza-
tion to perform coarse-grain filtering on the basis of the contents of IP and TCP/UDP 
headers, including IP addresses, port numbers, and acknowledgment bits. But what if 
an organization wants to provide a Telnet service to a restricted set of internal users 
Table 8.8 ‚ô¶ Access control list for stateful filter
action
source  
address
dest  
address
protocol
source port
dest port
flag bit
check 
conxion
allow
222.22/16
outside of 
222.22/16
TCP
> 1023
80
any
allow
outside of 
222.22/16
222.22/16
TCP
80
> 1023
ACK
X
allow
222.22/16
outside of 
222.22/16
UDP
> 1023
53
‚Äî
allow
outside of 
222.22/16
222.22/16
UDP
53
> 1023
‚Äî
X
deny
all
all
all
all
all
all

(as opposed to IP addresses)? And what if the organization wants such privileged 
users to authenticate themselves first before being allowed to create Telnet sessions 
to the outside world? Such tasks are beyond the capabilities of traditional and stateful 
filters. Indeed, information about the identity of the internal users is application-layer 
data and is not included in the IP/TCP/UDP headers.
To have finer-level security, firewalls must combine packet filters with appli-
cation gateways. Application gateways look beyond the IP/TCP/UDP headers and 
make policy decisions based on application data. An application gateway is an 
application-specific server through which all application data (inbound and out-
bound) must pass. Multiple application gateways can run on the same host, but each 
gateway is a separate server with its own processes.
To get some insight into application gateways, let‚Äôs design a firewall that allows 
only a restricted set of internal users to Telnet outside and prevents all external clients 
from Telneting inside. Such a policy can be accomplished by implementing a com-
bination of a packet filter (in a router) and a Telnet application gateway, as shown in 
Figure 8.35. The router‚Äôs filter is configured to block all Telnet connections except 
those that originate from the IP address of the application gateway. Such a filter 
configuration forces all outbound Telnet connections to pass through the application 
gateway. Consider now an internal user who wants to Telnet to the outside world. 
The user must first set up a Telnet session with the application gateway. An applica-
tion running in the gateway, which listens for incoming Telnet sessions, prompts the 
Figure 8.35 ‚ô¶ Firewall consisting of an application gateway and a filter
Application
gateway
Host-to-gateway
Telnet session
Gateway-to-remote
host Telnet session
Router 
and Ô¨Ålter

user for a user ID and password. When the user supplies this information, the appli-
cation gateway checks to see if the user has permission to Telnet to the outside world. 
If not, the Telnet connection from the internal user to the gateway is terminated by 
the gateway. If the user has permission, then the gateway (1) prompts the user for the 
host name of the external host to which the user wants to connect, (2) sets up a Telnet 
session between the gateway and the external host, and (3) relays to the external host 
all data arriving from the user, and relays to the user all data arriving from the exter-
nal host. Thus, the Telnet application gateway not only performs user authorization 
but also acts as a Telnet server and a Telnet client, relaying information between the 
user and the remote Telnet server. Note that the filter will permit step 2 because the 
gateway initiates the Telnet connection to the outside world.
ANONYMITY AND PRIVACY
Suppose you want to visit a controversial Web site (for example, a political activist 
site) and you (1) don‚Äôt want to reveal your IP address to the Web site, (2) don‚Äôt want 
your local ISP (which may be your home or office ISP) to know that you are visiting 
the site, and (3) don‚Äôt want your local ISP to see the data you are exchanging with 
the site. If you use the traditional approach of connecting directly to the Web site 
without any encryption, you fail on all three counts. Even if you use SSL, you fail 
on the first two counts: Your source IP address is presented to the Web site in every 
datagram you send; and the destination address of every packet you send can easily 
be sniffed by your local ISP.
To obtain privacy and anonymity, you can instead use a combination of a trusted 
proxy server and SSL, as shown in Figure 8.36. With this approach, you first make 
an SSL connection to the trusted proxy. You then send, into this SSL connection, 
CASE HISTORY
Figure 8.36 ‚ô¶ Providing anonymity and privacy with a proxy
Alice
Anonymizing
Proxy
SSL
Cleartext

an HTTP request for a page at the desired site. When the proxy receives the SSL-
encrypted HTTP request, it decrypts the request and forwards the cleartext HTTP 
request to the Web site. The Web site then responds to the proxy, which in turn for-
wards the response to you over SSL. Because the Web site only sees the IP address 
of the proxy, and not of your client‚Äôs address, you are indeed obtaining anony-
mous access to the Web site. And because all traffic between you and the proxy is 
encrypted, your local ISP cannot invade your privacy by logging the site you visited 
or recording the data you are exchanging. Many companies today (such as proxify 
.com) make available such proxy services.
Of course, in this solution, your proxy knows everything: It knows your IP address 
and the IP address of the site you‚Äôre surfing; and it can see all the traffic in  cleartext 
exchanged between you and the Web site. Such a solution, therefore, is only as 
good as the trustworthiness of the proxy. A more robust approach, taken by the 
TOR anonymizing and privacy service, is to route your traffic through a series of 
non- colluding proxy servers [TOR 2020]. In particular, TOR allows independent 
 individuals to contribute proxies to its proxy pool. When a user connects to a server 
using TOR, TOR randomly chooses (from its proxy pool) a chain of three proxies and 
routes all traffic between client and server over the chain. In this manner, assuming 
the proxies do not collude, no one knows that communication took place between 
your IP address and the target Web site. Furthermore, although cleartext is sent 
between the last proxy and the server, the last proxy doesn‚Äôt know what IP address  
is sending and receiving the cleartext.
Internal networks often have multiple application gateways, for example, gate-
ways for Telnet, HTTP, FTP, and e-mail. In fact, an organization‚Äôs mail server  
(see Section 2.3) and Web cache are application gateways.
Application gateways do not come without their disadvantages. First, a different 
application gateway is needed for each application. Second, there is a performance 
penalty to be paid, since all data will be relayed via the gateway. This becomes a 
concern particularly when multiple users or applications are using the same gateway 
machine. Finally, the client software must know how to contact the gateway when 
the user makes a request, and must know how to tell the application gateway what 
external server to connect to.
8.9.2 Intrusion Detection Systems
We‚Äôve just seen that a packet filter (traditional and stateful) inspects IP, TCP, UDP, 
and ICMP header fields when deciding which packets to let pass through the firewall. 
However, to detect many attack types, we need to perform deep packet inspection, 
that is, look beyond the header fields and into the actual application data that the 
packets carry. As we saw in Section 8.9.1, application gateways often do deep packet 
inspection. But an application gateway only does this for a specific application.

Clearly, there is a niche for yet another device‚Äîa device that not only examines 
the headers of all packets passing through it (like a packet filter), but also  performs 
deep packet inspection (unlike a packet filter). When such a device observes a 
 suspicious packet, or a suspicious series of packets, it could prevent those packets 
from entering the organizational network. Or, because the activity is only deemed 
as suspicious, the device could let the packets pass, but send alerts to a network 
administrator, who can then take a closer look at the traffic and take appropriate 
actions. A device that generates alerts when it observes potentially malicious traffic 
is called an intrusion detection system (IDS). A device that filters out suspicious 
traffic is called an intrusion prevention system (IPS). In this section we study 
both  systems‚ÄîIDS and IPS‚Äîtogether, since the most interesting technical aspect 
of these systems is how they detect suspicious traffic (and not whether they send 
alerts or drop packets). We will henceforth collectively refer to IDS systems and 
IPS systems as IDS systems.
An IDS can be used to detect a wide range of attacks, including network map-
ping (emanating, for example, from nmap), port scans, TCP stack scans, DoS band-
width-flooding attacks, worms and viruses, OS vulnerability attacks, and application 
vulnerability attacks. (See Section 1.6 for a survey of network attacks.) Today, 
thousands of organizations employ IDS systems. Many of these deployed systems 
are proprietary, marketed by Cisco, Check Point, and other security equipment ven-
dors. But many of the deployed IDS systems are public-domain systems, such as the 
immensely popular Snort IDS system (which we‚Äôll discuss shortly).
An organization may deploy one or more IDS sensors in its organizational net-
work. Figure 8.37 shows an organization that has three IDS sensors. When multi-
ple sensors are deployed, they typically work in concert, sending information about 
suspicious traffic activity to a central IDS processor, which collects and integrates 
the information and sends alarms to network administrators when deemed appropri-
ate. In Figure 8.37, the organization has partitioned its network into two regions: a 
high-security region, protected by a packet filter and an application gateway and 
monitored by IDS sensors; and a lower-security region‚Äîreferred to as the demilita-
rized zone (DMZ)‚Äîwhich is protected only by the packet filter, but also monitored 
by IDS sensors. Note that the DMZ includes the organization‚Äôs servers that need to 
communicate with the outside world, such as its public Web server and its authorita-
tive DNS server.
You may be wondering at this stage, why multiple IDS sensors? Why not just 
place one IDS sensor just behind the packet filter (or even integrated with the packet 
filter) in Figure 8.37? We will soon see that an IDS not only needs to do deep packet 
inspection, but must also compare each passing packet with tens of thousands of 
‚Äúsignatures‚Äù; this can be a significant amount of processing, particularly if the organ-
ization receives gigabits/sec of traffic from the Internet. By placing the IDS sensors 
further downstream, each sensor sees only a fraction of the organization‚Äôs traffic, 
and can more easily keep up. Nevertheless, high-performance IDS and IPS systems 
are available today, and many organizations can actually get by with just one sensor 
located near its access router.

IDS systems are broadly classified as either signature-based systems or  anomaly- 
based systems. A signature-based IDS maintains an extensive database of attack 
signatures. Each signature is a set of rules pertaining to an intrusion activity. A 
signature may simply be a list of characteristics about a single packet (e.g., source 
and destination port numbers, protocol type, and a specific string of bits in the 
packet payload), or may relate to a series of packets. The signatures are normally 
created by skilled network security engineers who research known attacks. An 
organization‚Äôs network administrator can customize the signatures or add its own 
to the database.
Operationally, a signature-based IDS sniffs every packet passing by it, com-
paring each sniffed packet with the signatures in its database. If a packet (or 
series of packets) matches a signature in the database, the IDS generates an 
alert. The alert could be sent to the network administrator in an e-mail message, 
could be sent to the network management system, or could simply be logged for 
future inspection.
Figure 8.37 ‚ô¶  An organization deploying a filter, an application gateway, 
and IDS sensors
Internet
Web
server
FTP
server
DNS
server
Internal
network
Application
gateway
Demilitarized zone
Filter
Key:
= IDS sensors

Signature-based IDS systems, although widely deployed, have a number of limi-
tations. Most importantly, they require previous knowledge of the attack to generate 
an accurate signature. In other words, a signature-based IDS is completely blind to 
new attacks that have yet to be recorded. Another disadvantage is that even if a sig-
nature is matched, it may not be the result of an attack, so that a false alarm is gener-
ated. Finally, because every packet must be compared with an extensive collection 
of signatures, the IDS can become overwhelmed with processing and actually fail to 
detect many malicious packets.
An anomaly-based IDS creates a traffic profile as it observes traffic in normal 
operation. It then looks for packet streams that are statistically unusual, for exam-
ple, an inordinate percentage of ICMP packets or a sudden exponential growth in 
port scans and ping sweeps. The great thing about anomaly-based IDS systems is 
that they don‚Äôt rely on previous knowledge about existing attacks‚Äîthat is, they can 
potentially detect new, undocumented attacks. On the other hand, it is an extremely 
challenging problem to distinguish between normal traffic and statistically unusual 
traffic. To date, most IDS deployments are primarily signature-based, although some 
include some anomaly-based features.
Snort
Snort is a public-domain, open source IDS with hundreds of thousands of existing 
deployments [Snort 2012; Koziol 2003]. It can run on Linux, UNIX, and Windows 
platforms. It uses the generic sniffing interface libpcap, which is also used by Wire-
shark and many other packet sniffers. It can easily handle 100 Mbps of traffic; for 
installations with gibabit/sec traffic rates, multiple Snort sensors may be needed.
To gain some insight into Snort, let‚Äôs take a look at an example of a Snort 
signature:
alert icmp $EXTERNAL_NET any -> $HOME_NET any 
(msg:‚ÄùICMP PING NMAP‚Äù; dsize: 0; itype: 8;)
This signature is matched by any ICMP packet that enters the organization‚Äôs network 
($HOME_NET) from the outside ($EXTERNAL_NET), is of type 8 (ICMP ping), and 
has an empty payload (dsize = 0). Since nmap (see Section 1.6) generates ping pack-
ets with these specific characteristics, this signature is designed to detect nmap ping 
sweeps. When a packet matches this signature, Snort generates an alert that includes 
the message ‚ÄúICMP PING NMAP‚Äù.
Perhaps what is most impressive about Snort is the vast community of users and 
security experts that maintain its signature database. Typically within a few hours 
of a new attack, the Snort community writes and releases an attack signature, which 
is then downloaded by the hundreds of thousands of Snort deployments distributed 
around the world. Moreover, using the Snort signature syntax, network administra-
tors can tailor the signatures to their own organization‚Äôs needs by either modifying 
existing signatures or creating entirely new ones.

8.10 Summary
In this chapter, we‚Äôve examined the various mechanisms that our secret lovers, Bob 
and Alice, can use to communicate securely. We‚Äôve seen that Bob and Alice are 
interested in confidentiality (so they alone are able to understand the contents of a 
transmitted message), end-point authentication (so they are sure that they are talking 
with each other), and message integrity (so they are sure that their messages are not 
altered in transit). Of course, the need for secure communication is not confined to 
secret lovers. Indeed, we saw in Sections 8.5 through 8.8 that security can be used in 
various layers in a network architecture to protect against bad guys who have a large 
arsenal of possible attacks at hand.
The first part of this chapter presented various principles underlying secure 
communication. In Section 8.2, we covered cryptographic techniques for encrypting 
and decrypting data, including symmetric key cryptography and public key cryp-
tography. DES and RSA were examined as specific case studies of these two major 
classes of cryptographic techniques in use in today‚Äôs networks.
In Section 8.3, we examined two approaches for providing message integ-
rity: message authentication codes (MACs) and digital signatures. The two 
approaches have a number of parallels. Both use cryptographic hash functions 
and both techniques enable us to verify the source of the message as well as the 
integrity of the message itself. One important difference is that MACs do not 
rely on encryption whereas digital signatures require a public key infrastruc-
ture. Both techniques are extensively used in practice, as we saw in Sections 8.5 
through 8.8. Furthermore, digital signatures are used to create digital certificates, 
which are important for verifying the validity of public keys. In Section 8.4, we 
 examined endpoint authentication and introduced nonces to defend against the 
replay attack.
In Sections 8.5 through 8.8 we examined several security networking protocols 
that enjoy extensive use in practice. We saw that symmetric key cryptography is at 
the core of PGP, SSL, IPsec, and wireless security. We saw that public key cryptog-
raphy is crucial for both PGP and TLS. We saw that PGP uses digital signatures for 
message integrity, whereas TLS and IPsec use MACs. We also explored security in 
wireless networks, including WiFi networks and 4G/5G cellular networks. Having 
now an understanding of the basic principles of cryptography, and having studied 
how these principles are actually used, you are now in position to design your own 
secure network protocols!
Armed with the techniques covered in Sections 8.2 through 8.8, Bob and 
Alice can communicate securely. But confidentiality is only a small part of the 
network security picture. As we learned in Section 8.9, increasingly, the focus in 
network security has been on securing the network infrastructure against a poten-
tial onslaught by the bad guys. In the latter part of this chapter, we thus covered 
firewalls and IDS systems which inspect packets entering and leaving an organiza-
tion‚Äôs network.

Homework Problems and Questions
Chapter 8 Review Problems
SECTION 8.1
  R1. What are the differences between message confidentiality and message integ-
rity? Can you have confidentiality without integrity? Can you have integrity 
without confidentiality? Justify your answer.
  R2. Internet entities (routers, switches, DNS servers, Web servers, user end 
systems, and so on) often need to communicate securely. Give three  
specific example pairs of Internet entities that may want secure  
communication.
SECTION 8.2
  R3. From a service perspective, what is an important difference between a 
symmetric-key system and a public-key system?
  R4. Suppose that an intruder has an encrypted message as well as the decrypted 
version of that message. Can the intruder mount a ciphertext-only attack, a 
known-plaintext attack, or a chosen-plaintext attack?
  R5. Consider an 8-bit block cipher. How many possible input blocks does  
this cipher have? How many possible mappings are there? If we view  
each mapping as a key, then how many possible keys does this  
cipher have?
  R6. Suppose N people want to communicate with each of N - 1 other peo-
ple using symmetric key encryption. All communication between any two 
people, i and j, is visible to all other people in this group of N, and no other 
person in this group should be able to decode their communication. How 
many keys are required in the system as a whole? Now suppose that public 
key encryption is used. How many keys are required in this case?
  R7. Suppose n = 10,000, a = 10,023, and b = 10,004. Use an identity of modu-
lar arithmetic to calculate in your head (a # b) mod n.
  R8. Suppose you want to encrypt the message 10101111 by encrypting the 
decimal number that corresponds to the message. What is the decimal 
number?
SECTIONS 8.3‚Äì8.4 
  R9. In what way does a hash provide a better message integrity check than a 
checksum (such as the Internet checksum)?
 R10. Can you ‚Äúdecrypt‚Äù a hash of a message to get the original message? Explain 
your answer.

sends (m, H(m) + s), where H(m) + s is the concatenation of H(m) and s. Is 
this variation flawed? Why or why not?
 R12. What does it mean for a signed document to be verifiable and nonforgeable?
 R13. In what way does the public-key encrypted message hash provide a better 
digital signature than the public-key encrypted message?
 R14. Suppose certifier.com creates a certificate for foo.com. Typically, the 
entire certificate would be encrypted with certifier.com‚Äôs public key.  
True or false?
 R15. Suppose Alice has a message that she is ready to send to anyone who asks. 
Thousands of people want to obtain Alice‚Äôs message, but each wants to be 
sure of the integrity of the message. In this context, do you think a MAC-
based or a digital-signature-based integrity scheme is more suitable? Why?
 R16. What is the purpose of a nonce in an end-point authentication protocol?
 R17. What does it mean to say that a nonce is a once-in-a-lifetime value? In whose 
lifetime?
 R18. Is the message integrity scheme based on HMAC susceptible to playback 
attacks? If so, how can a nonce be incorporated into the scheme to remove 
this susceptibility?
SECTIONS 8.5‚Äì8.8
 R19. Suppose that Bob receives a PGP message from Alice. How does Bob know 
for sure that Alice created the message (rather than, say, Trudy)? Does PGP 
use a MAC for message integrity?
 R20. In the TLS record, there is a field for TLS sequence numbers. True or false?
 R21. What is the purpose of the random nonces in the TLS handshake?
 R22. Suppose an TLS session employs a block cipher with CBC. True or false: 
The server sends to the client the IV in the clear.
 R23. Suppose Bob initiates a TCP connection to Trudy who is pretending to be 
Alice. During the handshake, Trudy sends Bob Alice‚Äôs certificate. In what 
step of the TLS handshake algorithm will Bob discover that he is not com-
municating with Alice?
 R24. Consider sending a stream of packets from Host A to Host B using IPsec. 
Typically, a new SA will be established for each packet sent in the stream. 
True or false?
 R25. Suppose that TCP is being run over IPsec between headquarters and the 
branch office in Figure 8.28. If TCP retransmits the same packet, then the 
two corresponding packets sent by R1 packets will have the same sequence 
number in the ESP header. True or false?
HOMEWORK PROBLEMS AND QUESTIONS     681

R26. An IKE SA and an IPsec SA are the same thing. True or false?
 R27. Consider WEP for 802.11. Suppose that the data is 10101100 and the key-
stream is 1111000. What is the resulting ciphertext?
SECTION 8.9
 R28. Stateful packet filters maintain two data structures. Name them and briefly 
describe what they do.
 R29. Consider a traditional (stateless) packet filter. This packet filter may filter 
packets based on TCP flag bits as well as other header fields. True or false?
 R30. In a traditional packet filter, each interface can have its own access control 
list. True or false?
 R31. Why must an application gateway work in conjunction with a router filter to 
be effective?
 R32. Signature-based IDSs and IPSs inspect into the payloads of TCP and UDP 
segments. True or false?
Problems
 P1. Using the monoalphabetic cipher in Figure 8.3, encode the message ‚ÄúThis is 
an easy problem.‚Äù Decode the message ‚Äúrmij‚Äôu uamu xyj.‚Äù
 P2. Show that Trudy‚Äôs known-plaintext attack, in which she knows the (cipher-
text, plaintext) translation pairs for seven letters, reduces the number of 
possible substitutions to be checked in the example in Section 8.2.1 by 
approximately 109.
 P3. Consider the polyalphabetic system shown in Figure 8.4. Will a chosen-
plaintext attack that is able to get the plaintext encoding of the message ‚ÄúThe 
quick brown fox jumps over the lazy dog.‚Äù be sufficient to decode all mes-
sages? Why or why not?
 P4. Consider the block cipher in Figure 8.5. Suppose that each block cipher 
Ti simply reverses the order of the eight input bits (so that, for example, 
11110000 becomes 00001111). Further suppose that the 64-bit scrambler 
does not modify any bits (so that the output value of the mth bit is equal to 
the input value of the mth bit). (a) With n = 3 and the original 64-bit input 
equal to 10100000 repeated eight times, what is the value of the output?  
(b) Repeat part (a) but now change the last bit of the original 64-bit input 
from a 0 to a 1. (c) Repeat parts (a) and (b) but now suppose that the 64-bit 
scrambler inverses the order of the 64 bits.
 P5. Consider the block cipher in Figure 8.5. For a given ‚Äúkey‚Äù Alice and Bob would 
need to keep eight tables, each 8 bits by 8 bits. For Alice (or Bob) to store all 
eight tables, how many bits of storage are necessary? How does this number 
compare with the number of bits required for a full-table 64-bit block cipher?

100100100. (a) Initially assume that CBC is not used. What is the resulting 
ciphertext? (b) Suppose Trudy sniffs the ciphertext. Assuming she knows that 
a 3-bit block cipher without CBC is being employed (but doesn‚Äôt know the 
specific cipher), what can she surmise? (c) Now suppose that CBC is used 
with IV = 111. What is the resulting ciphertext?
 P7. (a) Using RSA, choose p = 3 and q = 11, and encode the word ‚Äúdog‚Äù by 
encrypting each letter separately. Apply the decryption algorithm to the 
encrypted version to recover the original plaintext message. (b) Repeat part 
(a) but now encrypt ‚Äúdog‚Äù as one message m.
 P8. Consider RSA with p = 5 and q = 11.
a. What are n and z?
b. Let e be 3. Why is this an acceptable choice for e?
c. Find d such that de = 1 (mod z) and d 6 160.
d. Encrypt the message m = 8 using the key (n, e). Let c denote the corresponding 
ciphertext. Show all work. Hint: To simplify the calculations, use the fact:
 
[(a mod n) # (b mod n)] mod n = (a # b) mod n
 P9. In this problem, we explore the Diffie-Hellman (DH) public-key encryption 
algorithm, which allows two entities to agree on a shared key. The DH algo-
rithm makes use of a large prime number p and another large number g less 
than p. Both p and g are made public (so that an attacker would know them). 
In DH, Alice and Bob each independently choose secret keys, SA and SB, 
respectively. Alice then computes her public key, TA, by raising g to SA and 
then taking mod p. Bob similarly computes his own public key TB by raising 
g to SB and then taking mod p. Alice and Bob then exchange their public keys 
over the Internet. Alice then calculates the shared secret key S by raising TB 
to SA and then taking mod p. Similarly, Bob calculates the shared key S‚Ä≤ by 
raising TA to SB and then taking mod p.
a. Prove that, in general, Alice and Bob obtain the same symmetric key, that 
is, prove S = S‚Ä≤.
b. With p = 11 and g = 2, suppose Alice and Bob choose private keys SA = 5 
and SB = 12, respectively. Calculate Alice‚Äôs and Bob‚Äôs public keys, TA 
and TB. Show all work.
c. Following up on part (b), now calculate S as the shared symmetric key. 
Show all work.
d. Provide a timing diagram that shows how Diffie-Hellman can be attacked 
by a man-in-the-middle. The timing diagram should have three vertical 
lines, one for Alice, one for Bob, and one for the attacker Trudy.
PROBLEMS     683

P10. Suppose Alice wants to communicate with Bob using symmetric key cryp-
tography using a session key KS. In Section 8.2, we learned how public-key 
cryptography can be used to distribute the session key from Alice to Bob.  
In this problem, we explore how the session key can be distributed‚Äîwithout 
public key cryptography‚Äîusing a key distribution center (KDC). The KDC 
is a server that shares a unique secret symmetric key with each registered 
user. For Alice and Bob, denote these keys by KA@KDC and KB@KDC. Design a 
scheme that uses the KDC to distribute KS to Alice and Bob. Your scheme 
should use three messages to distribute the session key: a message from Alice 
to the KDC; a message from the KDC to Alice; and finally a message from 
Alice to Bob. The first message is KA@KDC (A, B). Using the notation, KA@KDC, 
KB@KDC, S, A, and B answer the following questions.
a. What is the second message?
b. What is the third message?
 P11. Compute a third message, different from the two messages in Figure 8.8, that 
has the same checksum as the messages in Figure 8.8.
 P12. Suppose Alice and Bob share two secret keys: an authentication key S1 and a 
symmetric encryption key S2. Augment Figure 8.9 so that both integrity and 
confidentiality are provided.
 P13. In the BitTorrent P2P file distribution protocol (see Chapter 2), the seed 
breaks the file into blocks, and the peers redistribute the blocks to each other. 
Without any protection, an attacker can easily wreak havoc in a torrent by 
masquerading as a benevolent peer and sending bogus blocks to a small 
subset of peers in the torrent. These unsuspecting peers then redistribute the 
bogus blocks to other peers, which in turn redistribute the bogus blocks to 
even more peers. Thus, it is critical for BitTorrent to have a mechanism that 
allows a peer to verify the integrity of a block, so that it doesn‚Äôt redistrib-
ute bogus blocks. Assume that when a peer joins a torrent, it initially gets a 
.torrent file from a fully trusted source. Describe a simple scheme that 
allows peers to verify the integrity of blocks.
 P14. The OSPF routing protocol uses a MAC rather than digital signatures to provide 
message integrity. Why do you think a MAC was chosen over digital signatures?
 P15. Consider our authentication protocol in Figure 8.18 in which Alice authen-
ticates herself to Bob, which we saw works well (i.e., we found no flaws in 
it). Now suppose that while Alice is authenticating herself to Bob, Bob must 
authenticate himself to Alice. Give a scenario by which Trudy, pretending to 
be Alice, can now authenticate herself to Bob as Alice. (Hint: Consider that 
the sequence of operations of the protocol, one with Trudy initiating and one 
with Bob initiating, can be arbitrarily interleaved. Pay particular attention to 
the fact that both Bob and Alice will use a nonce, and that if care is not taken, 
the same nonce can be used maliciously.)

P16. A natural question is whether we can use a nonce and public key cryptography to 
solve the end-point authentication problem in Section 8.4. Consider the following 
natural protocol: (1) Alice sends the message ‚ÄúI am Alice‚Äù to Bob. (2) Bob 
chooses a nonce, R, and sends it to Alice. (3) Alice uses her private key to encrypt 
the nonce and sends the resulting value to Bob. (4) Bob applies Alice‚Äôs public key 
to the received message. Thus, Bob computes R and authenticates Alice.
a. Diagram this protocol, using the notation for public and private keys 
employed in the textbook.
b. Suppose that certificates are not used. Describe how Trudy can become  
a ‚Äúwoman-in-the-middle‚Äù by intercepting Alice‚Äôs messages and then 
 pretending to be Alice to Bob.
 P17. Figure 8.21 shows the operations that Alice must perform with PGP to pro-
vide confidentiality, authentication, and integrity. Diagram the corresponding 
operations that Bob must perform on the package received from Alice.
 P18. Suppose Alice wants to send an e-mail to Bob. Bob has a public-private key  
pair (K  B
+, K  B
-), and Alice has Bob‚Äôs certificate. But Alice does not have a public, 
private key pair. Alice and Bob (and the entire world) share the same hash 
function H(#).
a. In this situation, is it possible to design a scheme so that Bob can verify 
that Alice created the message? If so, show how with a block diagram for 
Alice and Bob.
b. Is it possible to design a scheme that provides confidentiality for sending 
the message from Alice to Bob? If so, show how with a block diagram for 
Alice and Bob.
 P19. Consider the Wireshark output below for a portion of an SSL session.
a. Is Wireshark packet 112 sent by the client or server?
b. What is the server‚Äôs IP address and port number?
c. Assuming no loss and no retransmissions, what will be the sequence num-
ber of the next TCP segment sent by the client?
d. How many SSL records does Wireshark packet 112 contain?
e. Does packet 112 contain a Master Secret or an Encrypted Master Secret or 
neither?
f. Assuming that the handshake type field is 1 byte and each length field is 
3 bytes, what are the values of the first and last bytes of the Master Secret 
(or Encrypted Master Secret)?
g. The client encrypted handshake message takes into account how many 
SSL records?
h. The server encrypted handshake message takes into account how many 
SSL records?

P20. In Section 8.6.1, it is shown that without sequence numbers, Trudy (a woman-
in-the middle) can wreak havoc in a TLS session by interchanging TCP 
segments. Can Trudy do something similar by deleting a TCP segment? What 
does she need to do to succeed at the deletion attack? What effect will it have?
(Wireshark screenshot reprinted by permission of the Wireshark Foundation.)
 P21. Suppose Alice and Bob are communicating over a TLS session. Suppose an 
attacker, who does not have any of the shared keys, inserts a bogus TCP segment 
into a packet stream with correct TCP checksum and sequence numbers (and 
correct IP addresses and port numbers). Will TLS at the receiving side accept the 
bogus packet and pass the payload to the receiving application? Why or why not?
 P22. The following true/false questions pertain to Figure 8.28.
a. When a host in 172.16.1/24 sends a datagram to an Amazon.com server, 
the router R1 will encrypt the datagram using IPsec.

b. When a host in 172.16.1/24 sends a datagram to a host in 172.16.2/24, the 
router R1 will change the source and destination address of the IP datagram.
c. Suppose a host in 172.16.1/24 initiates a TCP connection to a Web server 
in 172.16.2/24. As part of this connection, all datagrams sent by R1 will 
have protocol number 50 in the left-most IPv4 header field.
d. Consider sending a TCP segment from a host in 172.16.1/24 to a host in 
172.16.2/24. Suppose the acknowledgment for this segment gets lost, so 
that TCP resends the segment. Because IPsec uses sequence numbers, R1 
will not resend the TCP segment.
 P23. Consider the example in Figure 8.28. Suppose Trudy is a woman-in-the-
middle, who can insert datagrams into the stream of datagrams going from 
R1 and R2. As part of a replay attack, Trudy sends a duplicate copy of one 
of the datagrams sent from R1 to R2. Will R2 decrypt the duplicate datagram 
and forward it into the branch-office network? If not, describe in detail how 
R2 detects the duplicate datagram.
 P24. Provide a filter table and a connection table for a stateful firewall that is as 
restrictive as possible but accomplishes the following:
a. Allows all internal users to establish Telnet sessions with external hosts.
b. Allows external users to surf the company Web site at 222.22.0.12.
c. But otherwise blocks all inbound and outbound traffic.
 
 The internal network is 222.22/16. In your solution, suppose that the connec-
tion table is currently caching three connections, all from inside to outside. 
You‚Äôll need to invent appropriate IP addresses and port numbers.
 P25. Suppose Alice wants to visit the Web site activist.com using a TOR-like 
 service. This service uses two non-colluding proxy servers, Proxy1 and 
Proxy2. Alice first obtains the certificates (each containing a public key)  
for Proxy1 and Proxy2 from some central server. Denote K1
+( ), K2
+( ), K1
-( ), 
and K2
-( ) for the encryption/decryption with public and private RSA keys.
a. Using a timing diagram, provide a protocol (as simple as possible) that 
enables Alice to establish a shared session key S1 with Proxy1. Denote 
S1(m) for encryption/decryption of data m with the shared key S1.
b. Using a timing diagram, provide a protocol (as simple as possible) that 
allows Alice to establish a shared session key S2 with Proxy2 without 
revealing her IP address to Proxy2.
c. Assume now that shared keys S1 and S2 are now established. Using a 
timing diagram, provide a protocol (as simple as possible and not using 
public-key cryptography) that allows Alice to request an html page from 
activist.com without revealing her IP address to Proxy2 and without 
revealing to Proxy1 which site she is visiting. Your diagram should end 
with an HTTP request arriving at activist.com.

Wireshark Lab: SSL
In this lab (available from the book Web site), we investigate the Secure Sockets 
Layer (SSL) protocol. Recall from Section 8.6 that SSL is used for securing a TCP 
connection, and that it is extensively used in practice for secure Internet transactions. 
In this lab, we will focus on the SSL records sent over the TCP connection. We will 
attempt to delineate and classify each of the records, with a goal of understanding the 
why and how for each record. We investigate the various SSL record types as well 
as the fields in the SSL messages. We do so by analyzing a trace of the SSL records 
sent between your host and an e-commerce server.
IPsec Lab
In this lab (available from the book Web site), we will explore how to create IPsec 
SAs between linux boxes. You can do the first part of the lab with two ordinary linux 
boxes, each with one Ethernet adapter. But for the second part of the lab, you will 
need four linux boxes, two of which having two Ethernet adapters. In the second half 
of the lab, you will create IPsec SAs using the ESP protocol in the tunnel mode. You 
will do this by first manually creating the SAs, and then by having IKE create the SAs.

What led you to specialize in the networking security area?
This is going to sound odd, but the answer is simple: It was fun. My background was in 
 systems programming and systems administration, which leads fairly naturally to security. 
And I‚Äôve always been interested in communications, ranging back to part-time systems 
 programming jobs when I was in college.
My work on security continues to be motivated by two things‚Äîa desire to keep com-
puters useful, which means that their function can‚Äôt be corrupted by attackers, and a desire 
to protect privacy.
What was your vision for Usenet at the time that you were developing it? And now?
We originally viewed it as a way to talk about computer science and computer program-
ming around the country, with a lot of local use for administrative matters, for-sale ads, and 
so on. In fact, my original prediction was one to two messages per day, from 50 to 100 sites 
at the most‚Äîever. However, the real growth was in people-related topics, including‚Äîbut not  
limited to‚Äîhuman interactions with computers. My favorite newsgroups, over the years, 
have been things like rec.woodworking, as well as sci.crypt.
To some extent, netnews has been displaced by the Web. Were I to start designing it 
today, it would look very different. But it still excels as a way to reach a very broad audi-
ence that is interested in the topic, without having to rely on particular Web sites.
Has anyone inspired you professionally? In what ways?
Professor Fred Brooks‚Äîthe founder and original chair of the computer science department 
at the University of North Carolina at Chapel Hill, the manager of the team that developed 
the IBM S/360 and OS/360, and the author of The Mythical Man-Month‚Äîwas a tremendous 
AN INTERVIEW WITH‚Ä¶
Steven M. Bellovin
Courtesy of Steven Bellovin
Steven M. Bellovin joined the faculty at Columbia University after 
many years at the Network Services Research Lab at AT&T Labs 
Research in Florham Park, New Jersey. His focus is on networks, 
security, and why the two are incompatible. In 1995, he was 
awarded the Usenix Lifetime Achievement Award for his work in the 
creation of Usenet, the first newsgroup exchange network that linked 
two or more computers and allowed users to share information and 
join in discussions. Steve is also an elected member of the National 
Academy of Engineering. He received his BA from Columbia 
University and his PhD from the University of North Carolina at 
Chapel Hill.




Network security: private communication in a public world, 2002, 713 pages, Charlie
Kaufman, Radia Perlman, Mike Speciner, 0130460192, 9780130460196, Prentice Hall
PTR,
2002
DOWNLOAD 
http://bit.ly/1SDYqyS
http://goo.gl/RTekk
http://www.powells.com/s?kw=Network+security%3A+private+communication+in+a+public+world
The classic guide to network security--now fully updated!"Bob and Alice are back!"Widely regarded
as the most comprehensive yet comprehensible guide to network security, the first edition of
Network Security received critical acclaim for its lucid and witty explanations of the inner workings
of network security protocols. In the second edition, this most distinguished of author teams draws
on hard-won experience to explain the latest developments in this field that has become so critical
to our global network-dependent society."Network Security, Second Edition" brings together clear,
insightful, and clever explanations of every key facet of information security, from the basics to
advanced cryptography and authentication, secure Web and email services, and emerging security
standards. Coverage includes: All-new discussions of the Advanced Encryption Standard (AES),
IPsec, SSL, and Web securityCryptography: In-depth, exceptionally clear introductions to secret
and public keys, hashes, message digests, and other crucial conceptsAuthentication: Proving
identity across networks, common attacks against authentication systems, authenticating people,
and avoiding the pitfalls of authentication handshakesCore Internet security standards: Kerberos
4/5, IPsec, SSL, PKIX, and X.509Email security: Key elements of a secure email system-plus
detailed coverage of PEM, S/MIME, and PGPWeb security: Security issues associated with URLs,
HTTP, HTML, and cookiesSecurity implementations in diverse platforms, including Windows,
NetWare, and Lotus NotesThe authors go far beyond documenting standards and technology: They
contrast competing schemes, explain strengths and weaknesses, and identify the crucial errors
most likely to compromise secure systems. Network Security will appeal to a wide range of
professionals, from those who design or evaluate security systems to system administrators and
programmers who want a better understanding of this important field. It can also be used as a
textbook at the graduate or advanced undergraduate level.Prentice Hall Series in Computer
Networking
and
Distributed
Systems
DOWNLOAD 
http://tiny.cc/KDK26k
http://www.filestube.to/s2/Network-security-private-communication-in-a-public-world
http://bit.ly/1jsf8Tx
Crypt & N/W Security , Kahate, Sep 7, 2008, Computer networks, 792 pages. Security being one of
the main concerns of any organization, this title clearly explains the concepts behind Cryptography
and the principles employed behind Network Security.
VPNs a beginner's guide, John Mairs, 2002, Computers, 584 pages. .
Network Security and Ethical Hacking , Rajat Khare, Nov 1, 2006, Computers, 344 pages.
Information security is a highly integral aspect of our daily life, both real-world and virtual.
Information security is essentially a process for securing the information of.
Human nature the shooting script : screenplay and interview, Charlie Kaufman, May 9, 2002,
Performing Arts, 122 pages. In the Newmarket Shooting Script Series format the complete script of
the new film by the Academy Award-nominated screenwriter of Being John Malkovich. Directed by
the award.
AAA and Network Security for Mobile Access Radius, Diameter, EAP, PKI and IP Mobility, Madjid
Nakhjiri, Mahsa Nakhjiri, Nov 1, 2005, Technology & Engineering, 318 pages. AAA (Authentication,

Authorization, Accounting) describes a framework for intelligently controlling access to network
resources, enforcing policies, and providing the.
Security Protocols: 8th International Workshops Cambridge, UK, Volume 8 8th International
Workshops Cambridge, UK, April 3-5, 2000 Revised Papers, Bruce Christianson, Sep 12, 2001,
Business & Economics, 255 pages. The Cambridge International Workshop on Security Protocols
has now run for eight years. Each year we set a theme, focusing upon a speci?c aspect of security
protocols, and.
Networks And Information Security , V.S.Bagad, Jan 1, 2009, , 292 pages. .
Network security data and voice communications, Fred Simonds, 1996, Computers, 395 pages. .
Computer Network Security , Joseph Migga Kizza, Apr 7, 2005, Business & Economics, 534 pages.
Here is a broad-ranging, comprehensive survey of computer network security concepts, methods,
and practices. Coverage details network security tools, policies, and.
Designing network security , Merike Kaeo, 1999, Computers, 426 pages. Corporate network
security issues still very much fill the media today. "Designing Network Security" offers a practical
approach to the implementation of secure network design.
Network and Internet security , Vijay Ahuja, 1996, Computers, 324 pages. This book identifies
and addresses the security issues for client/server networks. The rapid growth in networking and
in particular, the Internet, have increased the risks to.
Eternal Sunshine of the Spotless Mind , Charlie Kaufman, Rob Feld, 2004, Eternal sunshine of the
spotless mind. (Motion picture), 166 pages. In the new Charlie Kaufman film, Joel (Jim Carrey) is
stunned to discover that his girlfriend Clementine (Kate Winslet) has had her memories of their
tumultuous relationship.
Being John Malkovich , Charlie Kaufman, Oct 20, 2000, Performing Arts, 118 pages. Cast size:
large..
Network Security Current Status and Future Directions, Dimitrios N. Serpanos, Feb 9, 2007,
Computers, 608 pages. This book covers a wide range of topics dealing with network security. It
is focused on the current status of security protocols, architectures, implementations and policies.
Authentication systems for secure networks , Rolf Oppliger, 1996, Computers, 186 pages. This
unique book explores the use of cryptographic techniques in authentication and key distribution
systems. Systems such as Kerberos, NetSP, SPX, TESS, and SESAME are.
Computer Securuty ( 2Nd Ed.) , Dieter Gollman, Jul 1, 2009, , 376 pages. This is a brand new
edition of the best-selling computer security book. Written for self-study and course use, this book
will suit a variety of introductory and more advanced.

In the Spirit of 1992 Access to Western European Libraries and Literature, Mary M. Huston,
Maureen Pastine, 1992, Language Arts & Disciplines, 129 pages. This exciting volume is required
reading for every librarian who needs to understand the rapid changes in Western European
information services resulting from the new EuropeanNever Good Enough Health Care Workers and
the False Promise of Job Training, Ariel Ducey, 2009, Business & Economics, 300 pages. A
thoughtful and provocative critique of job training in the health care sector download Network
security:
private
communication
in
a
public
world
2002
http://www.filestube.to/s2/Network-security-private-communication-in-a-public-world

Medicine Moves to the Mall , David Charles Sloane, Beverlie Conant Sloane, 2003, Architecture, 198
pages. The shopping mall seems an unlikely place to go for health care services. Yet, the mall has
become home to such services as well as a model for redesigning other health careInterstate
Conflict in Latin America , Gregory F. Treverton, 1984, Latin America, 22 pages Selected Topics in
Field Quantization , Wolfgang Pauli, 2000, Science, 188 pages. Comprehensive, clearly presented
work considers such subjects as quantization of the electron-positron field, response to an external
field, quantization of free field, quantum A rich selection from the best of Nichols' work up to and
including his award-winning Privates on Parade A Day in the Death of Joe Egg is based on the
author's
experience
of.
http://www.alibris.co.uk/booksearch?browse=0&keyword=Network+security%3A+private+communication

Retail Franchising , Sidhpuria, 2009, Franchises (Retail trade), 180 pagesHealth care politics
ideological and interest group barriers to reform, Robert R. Alford, 1977, Medical, 294 pages Hard
Aground , James W. Hall, Jan 1, 1994, Fiction, 462 pages. While trying to solve the murder of his
older brother, Miami playboy Hap Tyler discovers the secret that may have led to his brother's
death:
a
450-year-old
sunken
Spanish

Threading the Concept Powerful Learning for the Music Classroom, Debra Gordon Hedden, 2010,
Music, 179 pages. "Published in partnership with MENC, the National Association for Music
Education."Across Peaks & Passes in Garhwal Himalaya , Harish Kapadia, 1999, Garhwal (India :
Region), 237 pages Stello A Session with Doctor Noir, Alfred de Vigny, Jan 1, 1963 In the wake of
the Toronto Blessing and other revival movements, Christians need this book more than ever.
Edwards,
the
central
figure
in
New
England's
first
Great
Awakening.
http://www.filestube.to/s2/Network-security-private-communication-in-a-public-world

Asian Company Handbook , , 1990, Business enterprises, . "Hong Kong, Malaysia, Republic of
Korea, Singapore, Taiwan and Thailand."--CoverThe Encyclopedia of Nutrition and Good Health ,
Robert A. Ronzio, 2003, Medical, 737 pages. Concise and extensively cross-referenced, this volume
is the perfect one-stop guide to the often-confusing mass of nutritional information available today.
More than 2,500 download Network security: private communication in a public world Charlie
Kaufman,
Radia
Perlman,
Mike
Speciner
http://comovywe.files.wordpress.com/2014/07/a-winter-affair-1999-sylvie-oday-memorial-anthology-of-ro

What is Marriage For? , E. J. Graff, 2004, Family & Relationships, 303 pages. In the wake of the
Massachusetts Supreme Judicial Court–≤–Ç‚Ñ¢s historic Goodridge decision, a reissue of the bible of
the same-sex marriage movement Will same-sex couples destroyThe Bomb , Theodore Taylor,
2007, Juvenile Fiction, 200 pages. It is 1946, a year after the bombings of Hiroshima and
Nagasaki, and World War II is over. But the U.S. government has decided that further tests of
atomic
bombs
must
be
http://comovywe.files.wordpress.com/2014/07/you-dont-have-problems-youre-just-bored-simple-solutions

Hearts on Fire Praying with Jesuits, Michael Harter, 2005, Religion, 194 pages. A collection of
prayers for all occasions gives voice to Ignatian spirituality, which finds God present in all things.
OriginalThe American Record Guide, Volume 32 , Peter Hugh Reed, 1965, Music download Network
security: private communication in a public world Charlie Kaufman, Radia Perlman, Mike Speciner
713
pages

The One True God A Biblical Study of the Doctrine of God, Paul David Washer, 2009, Bible, 192
pages. This book is essentially bound as a Wire-O bound journal, with a hardback cover wrapping
around the entire book, even the spine. This keeps the book in good shape for a longFrom Willow
Creek to Sacred Heart Rekindling My Love for Catholicism, Chris Haw, Oct 8, 2012, Religion, 256
pages. The bestselling coauthor of "Jesus for President" chronicles his fascinating spiritual journey
to
evangelical
Christianity
and
then
his
return
to
Catholicism.
A
respectful
and
download Network security: private communication in a public world

Quarterly Bulletin of Transport Statistics, Volume 10, Issue 2 , Japan. Un–ö—ïyush–ï–å, 1959Future
practice
alternatives
in
medicine
,
David
B.
Nash,
1993,
Medical,
406
pages

Acts of Contrition , John Cooney, Jan 1, 1994, Fiction, 416 pages. A suspenseful tale of passion
and intrigue centers on plots and counterplots set in the heart of the VaticanPrecalculus, the
MyMathLab Edition , Michael Sullivan, Mar 15, 2007, Mathematics, 744 pages. Did you decide NOT
to purchase the accompanying textbook at the beginning of the semester? Are you now wishing
that
you
had
a
textbook?
We
have
designed
a
product
just
for
your
http://wp.me/2aewD

The Eye of the I , David R. Hawkins, Jul 1, 2001, Body, Mind & Spirit, 360 pages. "This is the
second volume of a trilogy which began with "Power vs. force" and will be completed by the
publications of the third volume simply entitled "I"--P. [4] of coverEnglish bread and yeast cookery
, Elizabeth David, 1980, Cooking, 592 pages. This definitive work on bread making by one of the
world's most respected cookery experts has been completely adapted for the American kitchen and
contains recipes ranging from Acrobat 6 and PDF Solutions , Taz Tally, Mar 19, 2004, Computers,
411 pages. "If Taz writes a book, I can tell you this--I WANT IT!" Scott Kelby, President, National
Association of Photoshop Professionals (NAPP) With Acrobat 6, Adobe has delivered a Once
inseparable, the Michelet sisters were driven apart by war and by the men they married: a German
aristocrat who loved his country and a British diplomat working alongside. This study examines the
effects of a positive behavior support (PBS) program administered to participants (N = 64) in a
large inner-city public high school who demonstrate. No singer has been more mythologized and
more misunderstood than jazz legend Billie Holiday. This biography separates fact from fiction to
reveal
Lady
Day
in
all
stages
of
her.
http://www.jstor.org/stable/21126832125469
download Network security: private communication in a public world
created: 25th April 2012